{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import db.Db as db\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date Location  MinTemp  MaxTemp  Rainfall Evaporation Sunshine  \\\n0  2008-12-01   Albury     13.4     22.9       0.6          NA       NA   \n1  2008-12-02   Albury      7.4     25.1       0.0          NA       NA   \n2  2008-12-03   Albury     12.9     25.7       0.0          NA       NA   \n3  2008-12-04   Albury      9.2     28.0       0.0          NA       NA   \n4  2008-12-05   Albury     17.5     32.3       1.0          NA       NA   \n\n  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n0           W             44          W  ...          71           22   \n1         WNW             44        NNW  ...          44           25   \n2         WSW             46          W  ...          38           30   \n3          NE             24         SE  ...          45           16   \n4           W             41        ENE  ...          82           33   \n\n   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am Temp3pm RainToday  \\\n0       1007.7       1007.1         8        NA     16.9    21.8        No   \n1       1010.6       1007.8        NA        NA     17.2    24.3        No   \n2       1007.6       1008.7        NA         2     21.0    23.2        No   \n3       1017.6       1012.8        NA        NA     18.1    26.5        No   \n4       1010.8       1006.0         7         8     17.8    29.7        No   \n\n   RainTomorrow  \n0            No  \n1            No  \n2            No  \n3            No  \n4            No  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Location</th>\n      <th>MinTemp</th>\n      <th>MaxTemp</th>\n      <th>Rainfall</th>\n      <th>Evaporation</th>\n      <th>Sunshine</th>\n      <th>WindGustDir</th>\n      <th>WindGustSpeed</th>\n      <th>WindDir9am</th>\n      <th>...</th>\n      <th>Humidity9am</th>\n      <th>Humidity3pm</th>\n      <th>Pressure9am</th>\n      <th>Pressure3pm</th>\n      <th>Cloud9am</th>\n      <th>Cloud3pm</th>\n      <th>Temp9am</th>\n      <th>Temp3pm</th>\n      <th>RainToday</th>\n      <th>RainTomorrow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-12-01</td>\n      <td>Albury</td>\n      <td>13.4</td>\n      <td>22.9</td>\n      <td>0.6</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>W</td>\n      <td>44</td>\n      <td>W</td>\n      <td>...</td>\n      <td>71</td>\n      <td>22</td>\n      <td>1007.7</td>\n      <td>1007.1</td>\n      <td>8</td>\n      <td>NA</td>\n      <td>16.9</td>\n      <td>21.8</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-12-02</td>\n      <td>Albury</td>\n      <td>7.4</td>\n      <td>25.1</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>WNW</td>\n      <td>44</td>\n      <td>NNW</td>\n      <td>...</td>\n      <td>44</td>\n      <td>25</td>\n      <td>1010.6</td>\n      <td>1007.8</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>17.2</td>\n      <td>24.3</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-12-03</td>\n      <td>Albury</td>\n      <td>12.9</td>\n      <td>25.7</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>WSW</td>\n      <td>46</td>\n      <td>W</td>\n      <td>...</td>\n      <td>38</td>\n      <td>30</td>\n      <td>1007.6</td>\n      <td>1008.7</td>\n      <td>NA</td>\n      <td>2</td>\n      <td>21.0</td>\n      <td>23.2</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-12-04</td>\n      <td>Albury</td>\n      <td>9.2</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NE</td>\n      <td>24</td>\n      <td>SE</td>\n      <td>...</td>\n      <td>45</td>\n      <td>16</td>\n      <td>1017.6</td>\n      <td>1012.8</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>18.1</td>\n      <td>26.5</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-12-05</td>\n      <td>Albury</td>\n      <td>17.5</td>\n      <td>32.3</td>\n      <td>1.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>W</td>\n      <td>41</td>\n      <td>ENE</td>\n      <td>...</td>\n      <td>82</td>\n      <td>33</td>\n      <td>1010.8</td>\n      <td>1006.0</td>\n      <td>7</td>\n      <td>8</td>\n      <td>17.8</td>\n      <td>29.7</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 23 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, column_name = db.getWeather()\n",
    "data = pd.DataFrame(data=result, columns=column_name)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119590 entries, 0 to 119589\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           119590 non-null  object \n",
      " 1   Location       119590 non-null  object \n",
      " 2   MinTemp        119590 non-null  float64\n",
      " 3   MaxTemp        119590 non-null  float64\n",
      " 4   Rainfall       119590 non-null  float64\n",
      " 5   Evaporation    119590 non-null  object \n",
      " 6   Sunshine       119590 non-null  object \n",
      " 7   WindGustDir    119590 non-null  object \n",
      " 8   WindGustSpeed  119590 non-null  int64  \n",
      " 9   WindDir9am     119590 non-null  object \n",
      " 10  WindDir3pm     119590 non-null  object \n",
      " 11  WindSpeed9am   119590 non-null  int64  \n",
      " 12  WindSpeed3pm   119590 non-null  int64  \n",
      " 13  Humidity9am    119590 non-null  int64  \n",
      " 14  Humidity3pm    119590 non-null  int64  \n",
      " 15  Pressure9am    119590 non-null  float64\n",
      " 16  Pressure3pm    119590 non-null  float64\n",
      " 17  Cloud9am       119590 non-null  object \n",
      " 18  Cloud3pm       119590 non-null  object \n",
      " 19  Temp9am        119590 non-null  float64\n",
      " 20  Temp3pm        119590 non-null  float64\n",
      " 21  RainToday      119590 non-null  object \n",
      " 22  RainTomorrow   119590 non-null  object \n",
      "dtypes: float64(7), int64(5), object(11)\n",
      "memory usage: 21.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='RainTomorrow', ylabel='count'>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#first of all let us evaluate the target and find out if our data is imbalanced or not\n",
    "cols = [\"#C2C4E2\", \"#EED4E5\"]\n",
    "sns.countplot(x=data[\"RainTomorrow\"], palette=cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x1800 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation amongst numeric attributes\n",
    "corrmat = data.corr()\n",
    "cmap = sns.diverging_palette(260, -10, s=50, l=75, n=6, as_cmap=True)\n",
    "plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(corrmat, cmap=cmap, annot=True, square=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "10    119590\nName: Date, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsing datetime\n",
    "#exploring the length of date objects\n",
    "lengths = data[\"Date\"].str.len()\n",
    "lengths.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "        Date Location  MinTemp  MaxTemp  Rainfall Evaporation Sunshine  \\\n0 2008-12-01   Albury     13.4     22.9       0.6          NA       NA   \n1 2008-12-02   Albury      7.4     25.1       0.0          NA       NA   \n2 2008-12-03   Albury     12.9     25.7       0.0          NA       NA   \n3 2008-12-04   Albury      9.2     28.0       0.0          NA       NA   \n4 2008-12-05   Albury     17.5     32.3       1.0          NA       NA   \n\n  WindGustDir  WindGustSpeed WindDir9am  ... Temp3pm  RainToday  RainTomorrow  \\\n0           W             44          W  ...    21.8         No            No   \n1         WNW             44        NNW  ...    24.3         No            No   \n2         WSW             46          W  ...    23.2         No            No   \n3          NE             24         SE  ...    26.5         No            No   \n4           W             41        ENE  ...    29.7         No            No   \n\n   year  month     month_sin  month_cos day   day_sin   day_cos  \n0  2008     12 -2.449294e-16        1.0   1  0.201299  0.979530  \n1  2008     12 -2.449294e-16        1.0   2  0.394356  0.918958  \n2  2008     12 -2.449294e-16        1.0   3  0.571268  0.820763  \n3  2008     12 -2.449294e-16        1.0   4  0.724793  0.688967  \n4  2008     12 -2.449294e-16        1.0   5  0.848644  0.528964  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Location</th>\n      <th>MinTemp</th>\n      <th>MaxTemp</th>\n      <th>Rainfall</th>\n      <th>Evaporation</th>\n      <th>Sunshine</th>\n      <th>WindGustDir</th>\n      <th>WindGustSpeed</th>\n      <th>WindDir9am</th>\n      <th>...</th>\n      <th>Temp3pm</th>\n      <th>RainToday</th>\n      <th>RainTomorrow</th>\n      <th>year</th>\n      <th>month</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>day</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-12-01</td>\n      <td>Albury</td>\n      <td>13.4</td>\n      <td>22.9</td>\n      <td>0.6</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>W</td>\n      <td>44</td>\n      <td>W</td>\n      <td>...</td>\n      <td>21.8</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>-2.449294e-16</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.201299</td>\n      <td>0.979530</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-12-02</td>\n      <td>Albury</td>\n      <td>7.4</td>\n      <td>25.1</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>WNW</td>\n      <td>44</td>\n      <td>NNW</td>\n      <td>...</td>\n      <td>24.3</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>-2.449294e-16</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0.394356</td>\n      <td>0.918958</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-12-03</td>\n      <td>Albury</td>\n      <td>12.9</td>\n      <td>25.7</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>WSW</td>\n      <td>46</td>\n      <td>W</td>\n      <td>...</td>\n      <td>23.2</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>-2.449294e-16</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.571268</td>\n      <td>0.820763</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-12-04</td>\n      <td>Albury</td>\n      <td>9.2</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NE</td>\n      <td>24</td>\n      <td>SE</td>\n      <td>...</td>\n      <td>26.5</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>-2.449294e-16</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0.724793</td>\n      <td>0.688967</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-12-05</td>\n      <td>Albury</td>\n      <td>17.5</td>\n      <td>32.3</td>\n      <td>1.0</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>W</td>\n      <td>41</td>\n      <td>ENE</td>\n      <td>...</td>\n      <td>29.7</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>-2.449294e-16</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>0.848644</td>\n      <td>0.528964</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 30 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There don't seem to be any error in dates so parsing values into datetime\n",
    "data['Date'] = pd.to_datetime(data[\"Date\"])\n",
    "#Creating a collumn of year\n",
    "data['year'] = data.Date.dt.year\n",
    "\n",
    "# function to encode datetime into cyclic parameters.\n",
    "#As I am planning to use this data in a neural network I prefer the months and days in a cyclic continuous feature.\n",
    "\n",
    "def encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "data['month'] = data.Date.dt.month\n",
    "data = encode(data, 'month', 12)\n",
    "\n",
    "data['day'] = data.Date.dt.day\n",
    "data = encode(data, 'day', 31)\n",
    "\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Days In Year')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# roughly a year's span section\n",
    "section = data[:360]\n",
    "tm = section[\"day\"].plot(color=\"#C2C4E2\")\n",
    "tm.set_title(\"Distribution Of Days Over Year\")\n",
    "tm.set_ylabel(\"Days In month\")\n",
    "tm.set_xlabel(\"Days In Year\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Sine Encoded Months')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cyclic_month = sns.scatterplot(x=\"month_sin\", y=\"month_cos\", data=data, color=\"#C2C4E2\")\n",
    "cyclic_month.set_title(\"Cyclic Encoding of Month\")\n",
    "cyclic_month.set_ylabel(\"Cosine Encoded Months\")\n",
    "cyclic_month.set_xlabel(\"Sine Encoded Months\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Sine Encoded Day')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cyclic_day = sns.scatterplot(x='day_sin', y='day_cos', data=data, color=\"#C2C4E2\")\n",
    "cyclic_day.set_title(\"Cyclic Encoding of Day\")\n",
    "cyclic_day.set_ylabel(\"Cosine Encoded Day\")\n",
    "cyclic_day.set_xlabel(\"Sine Encoded Day\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Location', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Cloud9am', 'Cloud3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (data.dtypes == \"object\")\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 0\n",
      "Evaporation 0\n",
      "Sunshine 0\n",
      "WindGustDir 0\n",
      "WindDir9am 0\n",
      "WindDir3pm 0\n",
      "Cloud9am 0\n",
      "Cloud3pm 0\n",
      "RainToday 0\n",
      "RainTomorrow 0\n"
     ]
    }
   ],
   "source": [
    "# Missing values in categorical variables\n",
    "\n",
    "for i in object_cols:\n",
    "    print(i, data[i].isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Filling missing values with mode of the column in value\n",
    "\n",
    "for i in object_cols:\n",
    "    data[i].fillna(data[i].mode()[0], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric variables:\n",
      "['MinTemp', 'MaxTemp', 'Rainfall', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\n"
     ]
    }
   ],
   "source": [
    "# Get list of numeric variables\n",
    "t = (data.dtypes == \"float64\")\n",
    "num_cols = list(t[t].index)\n",
    "\n",
    "print(\"Numeric variables:\")\n",
    "print(num_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp 0\n",
      "MaxTemp 0\n",
      "Rainfall 0\n",
      "Pressure9am 0\n",
      "Pressure3pm 0\n",
      "Temp9am 0\n",
      "Temp3pm 0\n",
      "month_sin 0\n",
      "month_cos 0\n",
      "day_sin 0\n",
      "day_cos 0\n"
     ]
    }
   ],
   "source": [
    "# Missing values in numeric variables\n",
    "\n",
    "for i in num_cols:\n",
    "    print(i, data[i].isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119590 entries, 0 to 119589\n",
      "Data columns (total 30 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   Date           119590 non-null  datetime64[ns]\n",
      " 1   Location       119590 non-null  object        \n",
      " 2   MinTemp        119590 non-null  float64       \n",
      " 3   MaxTemp        119590 non-null  float64       \n",
      " 4   Rainfall       119590 non-null  float64       \n",
      " 5   Evaporation    119590 non-null  object        \n",
      " 6   Sunshine       119590 non-null  object        \n",
      " 7   WindGustDir    119590 non-null  object        \n",
      " 8   WindGustSpeed  119590 non-null  int64         \n",
      " 9   WindDir9am     119590 non-null  object        \n",
      " 10  WindDir3pm     119590 non-null  object        \n",
      " 11  WindSpeed9am   119590 non-null  int64         \n",
      " 12  WindSpeed3pm   119590 non-null  int64         \n",
      " 13  Humidity9am    119590 non-null  int64         \n",
      " 14  Humidity3pm    119590 non-null  int64         \n",
      " 15  Pressure9am    119590 non-null  float64       \n",
      " 16  Pressure3pm    119590 non-null  float64       \n",
      " 17  Cloud9am       119590 non-null  object        \n",
      " 18  Cloud3pm       119590 non-null  object        \n",
      " 19  Temp9am        119590 non-null  float64       \n",
      " 20  Temp3pm        119590 non-null  float64       \n",
      " 21  RainToday      119590 non-null  object        \n",
      " 22  RainTomorrow   119590 non-null  object        \n",
      " 23  year           119590 non-null  int64         \n",
      " 24  month          119590 non-null  int64         \n",
      " 25  month_sin      119590 non-null  float64       \n",
      " 26  month_cos      119590 non-null  float64       \n",
      " 27  day            119590 non-null  int64         \n",
      " 28  day_sin        119590 non-null  float64       \n",
      " 29  day_cos        119590 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int64(8), object(10)\n",
      "memory usage: 27.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values with median of the column in value\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i].fillna(data[i].median(), inplace=True)\n",
    "\n",
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119590 entries, 0 to 119589\n",
      "Data columns (total 30 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   Date           119590 non-null  datetime64[ns]\n",
      " 1   Location       119590 non-null  int32         \n",
      " 2   MinTemp        119590 non-null  float64       \n",
      " 3   MaxTemp        119590 non-null  float64       \n",
      " 4   Rainfall       119590 non-null  float64       \n",
      " 5   Evaporation    119590 non-null  int32         \n",
      " 6   Sunshine       119590 non-null  int32         \n",
      " 7   WindGustDir    119590 non-null  int32         \n",
      " 8   WindGustSpeed  119590 non-null  int64         \n",
      " 9   WindDir9am     119590 non-null  int32         \n",
      " 10  WindDir3pm     119590 non-null  int32         \n",
      " 11  WindSpeed9am   119590 non-null  int64         \n",
      " 12  WindSpeed3pm   119590 non-null  int64         \n",
      " 13  Humidity9am    119590 non-null  int64         \n",
      " 14  Humidity3pm    119590 non-null  int64         \n",
      " 15  Pressure9am    119590 non-null  float64       \n",
      " 16  Pressure3pm    119590 non-null  float64       \n",
      " 17  Cloud9am       119590 non-null  int32         \n",
      " 18  Cloud3pm       119590 non-null  int32         \n",
      " 19  Temp9am        119590 non-null  float64       \n",
      " 20  Temp3pm        119590 non-null  float64       \n",
      " 21  RainToday      119590 non-null  int32         \n",
      " 22  RainTomorrow   119590 non-null  int32         \n",
      " 23  year           119590 non-null  int64         \n",
      " 24  month          119590 non-null  int64         \n",
      " 25  month_sin      119590 non-null  float64       \n",
      " 26  month_cos      119590 non-null  float64       \n",
      " 27  day            119590 non-null  int64         \n",
      " 28  day_sin        119590 non-null  float64       \n",
      " 29  day_cos        119590 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int32(10), int64(8)\n",
      "memory usage: 22.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for i in object_cols:\n",
    "    data[i] = label_encoder.fit_transform(data[i])\n",
    "\n",
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                  count          mean       std       min       25%       50%  \\\nLocation       119590.0  6.084085e-17  1.000004 -1.650786 -0.873365 -0.018202   \nMinTemp        119590.0 -4.563064e-17  1.000004 -3.233061 -0.736009 -0.045001   \nMaxTemp        119590.0  4.030707e-16  1.000004 -2.989452 -0.770904 -0.083870   \nRainfall       119590.0  4.753192e-17  1.000004 -0.274179 -0.274179 -0.274179   \nEvaporation    119590.0 -1.007677e-16  1.000004 -1.977341 -0.624635  0.459160   \nSunshine       119590.0 -1.635098e-16  1.000004 -2.064135 -1.046486  0.569780   \nWindGustDir    119590.0 -1.102740e-16  1.000004 -1.685186 -1.087103  0.109064   \nWindGustSpeed  119590.0 -1.278609e-16  1.000004 -2.530298 -0.675656 -0.082171   \nWindDir9am     119590.0 -9.886639e-17  1.000004 -1.597101 -0.979023  0.051106   \nWindDir3pm     119590.0 -7.795234e-17  1.000004 -1.717770 -0.705782  0.103809   \nWindSpeed9am   119590.0  5.893958e-17  1.000004 -1.632937 -0.837149 -0.155044   \nWindSpeed3pm   119590.0 -1.330894e-17  1.000004 -2.199606 -0.701099 -0.009480   \nHumidity9am    119590.0  2.813889e-16  1.000004 -3.569925 -0.642462  0.037127   \nHumidity3pm    119590.0  1.045702e-16  1.000004 -2.458287 -0.718459  0.006469   \nPressure9am    119590.0 -1.505051e-14  1.000004 -5.234333 -0.655873 -0.007845   \nPressure3pm    119590.0  1.006536e-14  1.000004 -5.435375 -0.677120 -0.007545   \nCloud9am       119590.0 -1.121753e-16  1.000004 -1.797057 -0.947684  0.184813   \nCloud3pm       119590.0  1.045702e-16  1.000004 -1.886136 -1.009928  0.158348   \nTemp9am        119590.0 -4.487013e-16  1.000004 -3.145385 -0.742058 -0.059824   \nTemp3pm        119590.0 -3.498349e-16  1.000004 -2.967852 -0.757915 -0.084690   \nRainToday      119590.0 -8.650809e-17  1.000004 -0.532718 -0.532718 -0.532718   \nyear           119590.0 -2.084940e-14  1.000004 -2.274744 -0.691781  0.099701   \nmonth_sin      119590.0 -1.241771e-17  1.000004 -1.434665 -0.725855 -0.017045   \nmonth_cos      119590.0  1.616085e-17  1.000004 -1.390232 -1.201147  0.021117   \nday_sin        119590.0 -4.901729e-17  1.000004 -1.402132 -1.018026 -0.001695   \nday_cos        119590.0  3.636192e-17  1.000004 -1.392944 -1.056010 -0.045528   \n\n                    75%        max  \nLocation       0.836961   1.692124  \nMinTemp        0.724530   3.378629  \nMaxTemp        0.717670   3.523060  \nRainfall      -0.203300  43.151278  \nEvaporation    0.866602   0.866602  \nSunshine       0.829181   0.829181  \nWindGustDir    0.906509   1.504593  \nWindGustSpeed  0.585500   7.039651  \nWindDir9am     0.875209   1.699313  \nWindDir3pm     0.913399   1.520591  \nWindSpeed9am   0.640744   8.257575  \nWindSpeed3pm   0.566869   7.828868  \nHumidity9am    0.716717   1.657687  \nHumidity3pm    0.683069   2.374569  \nPressure9am    0.668359   3.288647  \nPressure3pm    0.676276   3.468545  \nCloud9am       1.034186   1.034186  \nCloud3pm       1.034556   1.034556  \nTemp9am        0.715443   3.568424  \nTemp3pm        0.690983   3.618051  \nRainToday     -0.532718   1.877167  \nyear           0.891183   1.682665  \nmonth_sin      0.691765   1.400575  \nmonth_cos      0.726791   1.432466  \nday_sin        1.014636   1.398742  \nday_cos        1.009915   1.453764  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Location</th>\n      <td>119590.0</td>\n      <td>6.084085e-17</td>\n      <td>1.000004</td>\n      <td>-1.650786</td>\n      <td>-0.873365</td>\n      <td>-0.018202</td>\n      <td>0.836961</td>\n      <td>1.692124</td>\n    </tr>\n    <tr>\n      <th>MinTemp</th>\n      <td>119590.0</td>\n      <td>-4.563064e-17</td>\n      <td>1.000004</td>\n      <td>-3.233061</td>\n      <td>-0.736009</td>\n      <td>-0.045001</td>\n      <td>0.724530</td>\n      <td>3.378629</td>\n    </tr>\n    <tr>\n      <th>MaxTemp</th>\n      <td>119590.0</td>\n      <td>4.030707e-16</td>\n      <td>1.000004</td>\n      <td>-2.989452</td>\n      <td>-0.770904</td>\n      <td>-0.083870</td>\n      <td>0.717670</td>\n      <td>3.523060</td>\n    </tr>\n    <tr>\n      <th>Rainfall</th>\n      <td>119590.0</td>\n      <td>4.753192e-17</td>\n      <td>1.000004</td>\n      <td>-0.274179</td>\n      <td>-0.274179</td>\n      <td>-0.274179</td>\n      <td>-0.203300</td>\n      <td>43.151278</td>\n    </tr>\n    <tr>\n      <th>Evaporation</th>\n      <td>119590.0</td>\n      <td>-1.007677e-16</td>\n      <td>1.000004</td>\n      <td>-1.977341</td>\n      <td>-0.624635</td>\n      <td>0.459160</td>\n      <td>0.866602</td>\n      <td>0.866602</td>\n    </tr>\n    <tr>\n      <th>Sunshine</th>\n      <td>119590.0</td>\n      <td>-1.635098e-16</td>\n      <td>1.000004</td>\n      <td>-2.064135</td>\n      <td>-1.046486</td>\n      <td>0.569780</td>\n      <td>0.829181</td>\n      <td>0.829181</td>\n    </tr>\n    <tr>\n      <th>WindGustDir</th>\n      <td>119590.0</td>\n      <td>-1.102740e-16</td>\n      <td>1.000004</td>\n      <td>-1.685186</td>\n      <td>-1.087103</td>\n      <td>0.109064</td>\n      <td>0.906509</td>\n      <td>1.504593</td>\n    </tr>\n    <tr>\n      <th>WindGustSpeed</th>\n      <td>119590.0</td>\n      <td>-1.278609e-16</td>\n      <td>1.000004</td>\n      <td>-2.530298</td>\n      <td>-0.675656</td>\n      <td>-0.082171</td>\n      <td>0.585500</td>\n      <td>7.039651</td>\n    </tr>\n    <tr>\n      <th>WindDir9am</th>\n      <td>119590.0</td>\n      <td>-9.886639e-17</td>\n      <td>1.000004</td>\n      <td>-1.597101</td>\n      <td>-0.979023</td>\n      <td>0.051106</td>\n      <td>0.875209</td>\n      <td>1.699313</td>\n    </tr>\n    <tr>\n      <th>WindDir3pm</th>\n      <td>119590.0</td>\n      <td>-7.795234e-17</td>\n      <td>1.000004</td>\n      <td>-1.717770</td>\n      <td>-0.705782</td>\n      <td>0.103809</td>\n      <td>0.913399</td>\n      <td>1.520591</td>\n    </tr>\n    <tr>\n      <th>WindSpeed9am</th>\n      <td>119590.0</td>\n      <td>5.893958e-17</td>\n      <td>1.000004</td>\n      <td>-1.632937</td>\n      <td>-0.837149</td>\n      <td>-0.155044</td>\n      <td>0.640744</td>\n      <td>8.257575</td>\n    </tr>\n    <tr>\n      <th>WindSpeed3pm</th>\n      <td>119590.0</td>\n      <td>-1.330894e-17</td>\n      <td>1.000004</td>\n      <td>-2.199606</td>\n      <td>-0.701099</td>\n      <td>-0.009480</td>\n      <td>0.566869</td>\n      <td>7.828868</td>\n    </tr>\n    <tr>\n      <th>Humidity9am</th>\n      <td>119590.0</td>\n      <td>2.813889e-16</td>\n      <td>1.000004</td>\n      <td>-3.569925</td>\n      <td>-0.642462</td>\n      <td>0.037127</td>\n      <td>0.716717</td>\n      <td>1.657687</td>\n    </tr>\n    <tr>\n      <th>Humidity3pm</th>\n      <td>119590.0</td>\n      <td>1.045702e-16</td>\n      <td>1.000004</td>\n      <td>-2.458287</td>\n      <td>-0.718459</td>\n      <td>0.006469</td>\n      <td>0.683069</td>\n      <td>2.374569</td>\n    </tr>\n    <tr>\n      <th>Pressure9am</th>\n      <td>119590.0</td>\n      <td>-1.505051e-14</td>\n      <td>1.000004</td>\n      <td>-5.234333</td>\n      <td>-0.655873</td>\n      <td>-0.007845</td>\n      <td>0.668359</td>\n      <td>3.288647</td>\n    </tr>\n    <tr>\n      <th>Pressure3pm</th>\n      <td>119590.0</td>\n      <td>1.006536e-14</td>\n      <td>1.000004</td>\n      <td>-5.435375</td>\n      <td>-0.677120</td>\n      <td>-0.007545</td>\n      <td>0.676276</td>\n      <td>3.468545</td>\n    </tr>\n    <tr>\n      <th>Cloud9am</th>\n      <td>119590.0</td>\n      <td>-1.121753e-16</td>\n      <td>1.000004</td>\n      <td>-1.797057</td>\n      <td>-0.947684</td>\n      <td>0.184813</td>\n      <td>1.034186</td>\n      <td>1.034186</td>\n    </tr>\n    <tr>\n      <th>Cloud3pm</th>\n      <td>119590.0</td>\n      <td>1.045702e-16</td>\n      <td>1.000004</td>\n      <td>-1.886136</td>\n      <td>-1.009928</td>\n      <td>0.158348</td>\n      <td>1.034556</td>\n      <td>1.034556</td>\n    </tr>\n    <tr>\n      <th>Temp9am</th>\n      <td>119590.0</td>\n      <td>-4.487013e-16</td>\n      <td>1.000004</td>\n      <td>-3.145385</td>\n      <td>-0.742058</td>\n      <td>-0.059824</td>\n      <td>0.715443</td>\n      <td>3.568424</td>\n    </tr>\n    <tr>\n      <th>Temp3pm</th>\n      <td>119590.0</td>\n      <td>-3.498349e-16</td>\n      <td>1.000004</td>\n      <td>-2.967852</td>\n      <td>-0.757915</td>\n      <td>-0.084690</td>\n      <td>0.690983</td>\n      <td>3.618051</td>\n    </tr>\n    <tr>\n      <th>RainToday</th>\n      <td>119590.0</td>\n      <td>-8.650809e-17</td>\n      <td>1.000004</td>\n      <td>-0.532718</td>\n      <td>-0.532718</td>\n      <td>-0.532718</td>\n      <td>-0.532718</td>\n      <td>1.877167</td>\n    </tr>\n    <tr>\n      <th>year</th>\n      <td>119590.0</td>\n      <td>-2.084940e-14</td>\n      <td>1.000004</td>\n      <td>-2.274744</td>\n      <td>-0.691781</td>\n      <td>0.099701</td>\n      <td>0.891183</td>\n      <td>1.682665</td>\n    </tr>\n    <tr>\n      <th>month_sin</th>\n      <td>119590.0</td>\n      <td>-1.241771e-17</td>\n      <td>1.000004</td>\n      <td>-1.434665</td>\n      <td>-0.725855</td>\n      <td>-0.017045</td>\n      <td>0.691765</td>\n      <td>1.400575</td>\n    </tr>\n    <tr>\n      <th>month_cos</th>\n      <td>119590.0</td>\n      <td>1.616085e-17</td>\n      <td>1.000004</td>\n      <td>-1.390232</td>\n      <td>-1.201147</td>\n      <td>0.021117</td>\n      <td>0.726791</td>\n      <td>1.432466</td>\n    </tr>\n    <tr>\n      <th>day_sin</th>\n      <td>119590.0</td>\n      <td>-4.901729e-17</td>\n      <td>1.000004</td>\n      <td>-1.402132</td>\n      <td>-1.018026</td>\n      <td>-0.001695</td>\n      <td>1.014636</td>\n      <td>1.398742</td>\n    </tr>\n    <tr>\n      <th>day_cos</th>\n      <td>119590.0</td>\n      <td>3.636192e-17</td>\n      <td>1.000004</td>\n      <td>-1.392944</td>\n      <td>-1.056010</td>\n      <td>-0.045528</td>\n      <td>1.009915</td>\n      <td>1.453764</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing attributes of scale data\n",
    "\n",
    "features = data.drop(['RainTomorrow', 'Date', 'day', 'month'], axis=1)  # dropping target and extra columns\n",
    "\n",
    "target = data['RainTomorrow']\n",
    "\n",
    "#Set up a standard scaler for the features\n",
    "col_names = list(features.columns)\n",
    "s_scaler = preprocessing.StandardScaler()\n",
    "features = s_scaler.fit_transform(features)\n",
    "features = pd.DataFrame(features, columns=col_names)\n",
    "\n",
    "features.describe().T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2000x1000 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Detecting outliers\n",
    "#looking at the scaled features\n",
    "colours = [\"#D0DBEE\", \"#C2C4E2\", \"#EED4E5\", \"#D1E6DC\", \"#BDE2E2\"]\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxenplot(data=features, palette=colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(107630, 27)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full data for\n",
    "features[\"RainTomorrow\"] = target\n",
    "\n",
    "#Dropping with outlier\n",
    "\n",
    "features = features[(features[\"MinTemp\"] < 2.3) & (features[\"MinTemp\"] > -2.3)]\n",
    "features = features[(features[\"MaxTemp\"] < 2.3) & (features[\"MaxTemp\"] > -2)]\n",
    "features = features[(features[\"Rainfall\"] < 4.5)]\n",
    "features = features[(features[\"Evaporation\"] < 2.8)]\n",
    "features = features[(features[\"Sunshine\"] < 2.1)]\n",
    "features = features[(features[\"WindGustSpeed\"] < 4) & (features[\"WindGustSpeed\"] > -4)]\n",
    "features = features[(features[\"WindSpeed9am\"] < 4)]\n",
    "features = features[(features[\"WindSpeed3pm\"] < 2.5)]\n",
    "features = features[(features[\"Humidity9am\"] > -3)]\n",
    "features = features[(features[\"Humidity3pm\"] > -2.2)]\n",
    "features = features[(features[\"Pressure9am\"] < 2) & (features[\"Pressure9am\"] > -2.7)]\n",
    "features = features[(features[\"Pressure3pm\"] < 2) & (features[\"Pressure3pm\"] > -2.7)]\n",
    "features = features[(features[\"Cloud9am\"] < 1.8)]\n",
    "features = features[(features[\"Cloud3pm\"] < 2)]\n",
    "features = features[(features[\"Temp9am\"] < 2.3) & (features[\"Temp9am\"] > -2)]\n",
    "features = features[(features[\"Temp3pm\"] < 2.3) & (features[\"Temp3pm\"] > -2)]\n",
    "\n",
    "features.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2000x1000 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#looking at the scaled features without outliers\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxenplot(data=features, palette=colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "107630"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_connection = db.getConnectDataFrame()\n",
    "features.to_sql(con=database_connection, name='weather_processed', if_exists='replace', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id         labels      corr\n",
      "0    1   RainTomorrow  1.000000\n",
      "1    2    Humidity3pm  0.453435\n",
      "2    3      RainToday  0.294478\n",
      "3    4    Humidity9am  0.269206\n",
      "4    5       Rainfall  0.264366\n",
      "5    6  WindGustSpeed  0.206806\n",
      "6    7       Cloud3pm  0.164293\n",
      "7    8       Cloud9am  0.151110\n",
      "8    9        MinTemp  0.086450\n",
      "9   10   WindSpeed9am  0.063437\n",
      "10  11   WindSpeed3pm  0.056912\n",
      "11  12    WindGustDir  0.042792\n",
      "12  13     WindDir9am  0.025351\n",
      "13  14     WindDir3pm  0.016407\n",
      "14  15       Location  0.006181\n",
      "15  16        day_cos -0.000899\n",
      "16  17        day_sin -0.001142\n",
      "17  18           year -0.010569\n",
      "18  19      month_sin -0.023054\n",
      "19  20        Temp9am -0.025784\n",
      "20  21       Sunshine -0.026128\n",
      "21  22    Evaporation -0.042372\n",
      "22  23      month_cos -0.053897\n",
      "23  24        MaxTemp -0.165297\n",
      "24  25        Temp3pm -0.200424\n",
      "25  26    Pressure3pm -0.207190\n",
      "26  27    Pressure9am -0.224642\n"
     ]
    },
    {
     "data": {
      "text/plain": "27"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_data = features.corr()[\"RainTomorrow\"].sort_values(ascending=False)\n",
    "df_corr = pd.DataFrame({\n",
    "    'id': [*range(1, corr_data.index.size + 1)],\n",
    "    'labels': corr_data.index,\n",
    "    'corr': corr_data.values})\n",
    "df_corr.set_index('id')\n",
    "print(df_corr)\n",
    "df_corr.to_sql(con=database_connection, name='correlation', if_exists='replace', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(107630, 26)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = features.drop([\"RainTomorrow\"], axis=1)\n",
    "y = features[\"RainTomorrow\"]\n",
    "\n",
    "# Splitting test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net architecture :\n",
      " -> Linear(26 , 32) -> relu -> Linear(32 , 32) -> relu -> Linear(32 , 1) -> sigmoid\n",
      "Epochs  1 / 10\n",
      "At: 1 [==========>] Loss 0.2635067946317505  - accuracy: 0.5\n",
      "At: 2 [==========>] Loss 0.30029749681474494  - accuracy: 0.4375\n",
      "At: 3 [==========>] Loss 0.25351143933437215  - accuracy: 0.5\n",
      "At: 4 [==========>] Loss 0.2780670203753416  - accuracy: 0.53125\n",
      "At: 5 [==========>] Loss 0.3039272415868881  - accuracy: 0.5\n",
      "At: 6 [==========>] Loss 0.24474146776804317  - accuracy: 0.5625\n",
      "At: 7 [==========>] Loss 0.2884381895213308  - accuracy: 0.4375\n",
      "At: 8 [==========>] Loss 0.24615019474122474  - accuracy: 0.53125\n",
      "At: 9 [==========>] Loss 0.27080965634886334  - accuracy: 0.5\n",
      "At: 10 [==========>] Loss 0.26045724198737374  - accuracy: 0.46875\n",
      "At: 11 [==========>] Loss 0.19884643494798712  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.26238274091738134  - accuracy: 0.5\n",
      "At: 13 [==========>] Loss 0.20838940872384742  - accuracy: 0.65625\n",
      "At: 14 [==========>] Loss 0.23728257109448747  - accuracy: 0.53125\n",
      "At: 15 [==========>] Loss 0.2002284580414066  - accuracy: 0.71875\n",
      "At: 16 [==========>] Loss 0.24937160079555395  - accuracy: 0.5625\n",
      "At: 17 [==========>] Loss 0.2210574695768841  - accuracy: 0.6875\n",
      "At: 18 [==========>] Loss 0.22044768127964048  - accuracy: 0.65625\n",
      "At: 19 [==========>] Loss 0.2041953887721472  - accuracy: 0.71875\n",
      "At: 20 [==========>] Loss 0.1898988782417215  - accuracy: 0.75\n",
      "At: 21 [==========>] Loss 0.22135274425067034  - accuracy: 0.6875\n",
      "At: 22 [==========>] Loss 0.22884304370376757  - accuracy: 0.59375\n",
      "At: 23 [==========>] Loss 0.12234411763761804  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.27675541868761483  - accuracy: 0.59375\n",
      "At: 25 [==========>] Loss 0.22529765570695592  - accuracy: 0.65625\n",
      "At: 26 [==========>] Loss 0.2268230083503427  - accuracy: 0.59375\n",
      "At: 27 [==========>] Loss 0.20226048585746265  - accuracy: 0.6875\n",
      "At: 28 [==========>] Loss 0.2108510503865012  - accuracy: 0.71875\n",
      "At: 29 [==========>] Loss 0.19583413048884202  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.2501968568809304  - accuracy: 0.65625\n",
      "At: 31 [==========>] Loss 0.19293664835893776  - accuracy: 0.75\n",
      "At: 32 [==========>] Loss 0.2651968519936217  - accuracy: 0.59375\n",
      "At: 33 [==========>] Loss 0.14284824181312214  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.20490453057158653  - accuracy: 0.65625\n",
      "At: 35 [==========>] Loss 0.2070759887102382  - accuracy: 0.65625\n",
      "At: 36 [==========>] Loss 0.1912718596818227  - accuracy: 0.71875\n",
      "At: 37 [==========>] Loss 0.19800217437884415  - accuracy: 0.65625\n",
      "At: 38 [==========>] Loss 0.20337789817927998  - accuracy: 0.625\n",
      "At: 39 [==========>] Loss 0.19544292517769088  - accuracy: 0.6875\n",
      "At: 40 [==========>] Loss 0.19124690760787666  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.09966010563551217  - accuracy: 0.96875\n",
      "At: 42 [==========>] Loss 0.17494073798288262  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.21033851864019965  - accuracy: 0.71875\n",
      "At: 44 [==========>] Loss 0.17361909464485667  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.12912364187693362  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.21297076259857536  - accuracy: 0.65625\n",
      "At: 47 [==========>] Loss 0.2017705454098342  - accuracy: 0.625\n",
      "At: 48 [==========>] Loss 0.15241463745544853  - accuracy: 0.84375\n",
      "At: 49 [==========>] Loss 0.11718247344671437  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.15250455543971167  - accuracy: 0.75\n",
      "At: 51 [==========>] Loss 0.1428738830530808  - accuracy: 0.8125\n",
      "At: 52 [==========>] Loss 0.2114996076404668  - accuracy: 0.71875\n",
      "At: 53 [==========>] Loss 0.15891970765760144  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.203201748808462  - accuracy: 0.75\n",
      "At: 55 [==========>] Loss 0.17828357084560564  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.17604572258974727  - accuracy: 0.71875\n",
      "At: 57 [==========>] Loss 0.15145747163521928  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.2023836441608411  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.19262740229463882  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.19988161324703851  - accuracy: 0.71875\n",
      "At: 61 [==========>] Loss 0.20754984131571516  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.19481055068727293  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.1909530223457138  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.19083223428962875  - accuracy: 0.6875\n",
      "At: 65 [==========>] Loss 0.21061961979944066  - accuracy: 0.6875\n",
      "At: 66 [==========>] Loss 0.1776706318627444  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.22332726923924684  - accuracy: 0.625\n",
      "At: 68 [==========>] Loss 0.12374608853012675  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.1605751093459013  - accuracy: 0.875\n",
      "At: 70 [==========>] Loss 0.17403152078301284  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.13465764494088675  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.15787547494316273  - accuracy: 0.78125\n",
      "At: 73 [==========>] Loss 0.16844483118853742  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.18985141140087686  - accuracy: 0.6875\n",
      "At: 75 [==========>] Loss 0.20295001480733058  - accuracy: 0.78125\n",
      "At: 76 [==========>] Loss 0.2172974036411291  - accuracy: 0.71875\n",
      "At: 77 [==========>] Loss 0.1707291910123099  - accuracy: 0.78125\n",
      "At: 78 [==========>] Loss 0.1258266618727794  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.19766261692571213  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.20937884463346007  - accuracy: 0.75\n",
      "At: 81 [==========>] Loss 0.15550250501079704  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.19068304284274296  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.13324388346961877  - accuracy: 0.84375\n",
      "At: 84 [==========>] Loss 0.1974757946417391  - accuracy: 0.78125\n",
      "At: 85 [==========>] Loss 0.17380566132990546  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.18041391912092675  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.1632308526228333  - accuracy: 0.84375\n",
      "At: 88 [==========>] Loss 0.2786823812975097  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.18098554199992162  - accuracy: 0.71875\n",
      "At: 90 [==========>] Loss 0.18689546488692993  - accuracy: 0.78125\n",
      "At: 91 [==========>] Loss 0.14301873251630814  - accuracy: 0.8125\n",
      "At: 92 [==========>] Loss 0.11015388911985483  - accuracy: 0.96875\n",
      "At: 93 [==========>] Loss 0.17043432859368937  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.18006891111975176  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.12961308383804931  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.13526601969901186  - accuracy: 0.875\n",
      "At: 97 [==========>] Loss 0.11556010268181842  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.2102490374554119  - accuracy: 0.6875\n",
      "At: 99 [==========>] Loss 0.12847574503582332  - accuracy: 0.875\n",
      "At: 100 [==========>] Loss 0.14818218483903456  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.13076580810654986  - accuracy: 0.90625\n",
      "At: 102 [==========>] Loss 0.23289181663349937  - accuracy: 0.6875\n",
      "At: 103 [==========>] Loss 0.14767069693542084  - accuracy: 0.78125\n",
      "At: 104 [==========>] Loss 0.1280533240920183  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.14387654878595552  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.1584627357674842  - accuracy: 0.78125\n",
      "At: 107 [==========>] Loss 0.14640281079836492  - accuracy: 0.78125\n",
      "At: 108 [==========>] Loss 0.1891653358372525  - accuracy: 0.75\n",
      "At: 109 [==========>] Loss 0.11152232228535883  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.20783000731215145  - accuracy: 0.71875\n",
      "At: 111 [==========>] Loss 0.09825314020992147  - accuracy: 0.9375\n",
      "At: 112 [==========>] Loss 0.16237934137254426  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.1538741611579913  - accuracy: 0.84375\n",
      "At: 114 [==========>] Loss 0.16964279959762146  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.1541944178354197  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.20420935003785257  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.14932669655154235  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.25546966025173967  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.1395970530169376  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.14046916591419162  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.16165651973165585  - accuracy: 0.78125\n",
      "At: 122 [==========>] Loss 0.17778711598913632  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.1850254414427858  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.19385981460025886  - accuracy: 0.75\n",
      "At: 125 [==========>] Loss 0.15494408694076706  - accuracy: 0.75\n",
      "At: 126 [==========>] Loss 0.21391929558634667  - accuracy: 0.75\n",
      "At: 127 [==========>] Loss 0.14624773082889275  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.2228598239139336  - accuracy: 0.625\n",
      "At: 129 [==========>] Loss 0.11539970069495604  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.21334480051034288  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.1398682254936  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.19870260193718872  - accuracy: 0.75\n",
      "At: 133 [==========>] Loss 0.1635959518510114  - accuracy: 0.78125\n",
      "At: 134 [==========>] Loss 0.1856213838293262  - accuracy: 0.75\n",
      "At: 135 [==========>] Loss 0.16862553095228627  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.1784676230612046  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.08750232656463369  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.14787480361986688  - accuracy: 0.8125\n",
      "At: 139 [==========>] Loss 0.11868923473652268  - accuracy: 0.90625\n",
      "At: 140 [==========>] Loss 0.1679183437156318  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.24429668090899487  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.1616528215762437  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.16644960862410296  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.11568389763401304  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.1432331190392241  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.13602540461198415  - accuracy: 0.78125\n",
      "At: 147 [==========>] Loss 0.16595785487362227  - accuracy: 0.8125\n",
      "At: 148 [==========>] Loss 0.13420016013723401  - accuracy: 0.8125\n",
      "At: 149 [==========>] Loss 0.16567709117694754  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.11405540955162469  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.14205741846637238  - accuracy: 0.875\n",
      "At: 152 [==========>] Loss 0.15332355931070382  - accuracy: 0.8125\n",
      "At: 153 [==========>] Loss 0.1190591601383916  - accuracy: 0.8125\n",
      "At: 154 [==========>] Loss 0.14393451388748357  - accuracy: 0.8125\n",
      "At: 155 [==========>] Loss 0.1936582765186109  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.13675273812568792  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.20678020554396465  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.1785549903022275  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.1338766759451533  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.14129545933048432  - accuracy: 0.8125\n",
      "At: 161 [==========>] Loss 0.11746888286723474  - accuracy: 0.8125\n",
      "At: 162 [==========>] Loss 0.1703178665845738  - accuracy: 0.6875\n",
      "At: 163 [==========>] Loss 0.13280424882791128  - accuracy: 0.8125\n",
      "At: 164 [==========>] Loss 0.1635841756488921  - accuracy: 0.8125\n",
      "At: 165 [==========>] Loss 0.17308478763964258  - accuracy: 0.78125\n",
      "At: 166 [==========>] Loss 0.1700660544781176  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.0826352660583479  - accuracy: 0.9375\n",
      "At: 168 [==========>] Loss 0.13468632096165506  - accuracy: 0.8125\n",
      "At: 169 [==========>] Loss 0.12850607023619562  - accuracy: 0.875\n",
      "At: 170 [==========>] Loss 0.14017913777254998  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.1911104650620066  - accuracy: 0.75\n",
      "At: 172 [==========>] Loss 0.1343520373783024  - accuracy: 0.8125\n",
      "At: 173 [==========>] Loss 0.24632250345325116  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.12495066870289434  - accuracy: 0.78125\n",
      "At: 175 [==========>] Loss 0.11948037707756995  - accuracy: 0.84375\n",
      "At: 176 [==========>] Loss 0.18171166884269896  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.09937133087729252  - accuracy: 0.96875\n",
      "At: 178 [==========>] Loss 0.16271433274738517  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.14521327139354545  - accuracy: 0.84375\n",
      "At: 180 [==========>] Loss 0.12751414855131818  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.08446400130653632  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.15254421654995426  - accuracy: 0.6875\n",
      "At: 183 [==========>] Loss 0.15271827524781076  - accuracy: 0.71875\n",
      "At: 184 [==========>] Loss 0.1251582814696746  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.10925689140013303  - accuracy: 0.90625\n",
      "At: 186 [==========>] Loss 0.15394516246505222  - accuracy: 0.8125\n",
      "At: 187 [==========>] Loss 0.14859785913953194  - accuracy: 0.84375\n",
      "At: 188 [==========>] Loss 0.11881368569370049  - accuracy: 0.84375\n",
      "At: 189 [==========>] Loss 0.16546772764400158  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.10136676035548567  - accuracy: 0.90625\n",
      "At: 191 [==========>] Loss 0.27890842122976844  - accuracy: 0.625\n",
      "At: 192 [==========>] Loss 0.1306526121667214  - accuracy: 0.875\n",
      "At: 193 [==========>] Loss 0.1700028806143516  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.15197633098049423  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.13201952505736392  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.13638793330746857  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.13467219076867581  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.10524946387629988  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.09370686124897779  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.16283159936214742  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.12902488563416958  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.1156381509947042  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.1156137647198694  - accuracy: 0.90625\n",
      "At: 204 [==========>] Loss 0.14099806783929636  - accuracy: 0.78125\n",
      "At: 205 [==========>] Loss 0.10672192256094491  - accuracy: 0.875\n",
      "At: 206 [==========>] Loss 0.09602978955670072  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.13644541474925545  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.185877466668254  - accuracy: 0.78125\n",
      "At: 209 [==========>] Loss 0.1801831288573667  - accuracy: 0.78125\n",
      "At: 210 [==========>] Loss 0.0984812906619846  - accuracy: 0.90625\n",
      "At: 211 [==========>] Loss 0.1444693969015685  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.1689259879644145  - accuracy: 0.78125\n",
      "At: 213 [==========>] Loss 0.18031464263959782  - accuracy: 0.71875\n",
      "At: 214 [==========>] Loss 0.1896006911905635  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.10976786197780769  - accuracy: 0.8125\n",
      "At: 216 [==========>] Loss 0.1535316559461873  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.18191848959994167  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.1330192443550574  - accuracy: 0.84375\n",
      "At: 219 [==========>] Loss 0.1665857646575563  - accuracy: 0.78125\n",
      "At: 220 [==========>] Loss 0.14778474823301063  - accuracy: 0.8125\n",
      "At: 221 [==========>] Loss 0.12329504652228468  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.125991647113662  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.2366265298422432  - accuracy: 0.625\n",
      "At: 224 [==========>] Loss 0.15931170732511984  - accuracy: 0.8125\n",
      "At: 225 [==========>] Loss 0.13828272211224005  - accuracy: 0.78125\n",
      "At: 226 [==========>] Loss 0.1526885490431655  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.12814099777336446  - accuracy: 0.875\n",
      "At: 228 [==========>] Loss 0.13872217786935104  - accuracy: 0.78125\n",
      "At: 229 [==========>] Loss 0.17192136056595392  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.14959927864561262  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.18615932319876216  - accuracy: 0.75\n",
      "At: 232 [==========>] Loss 0.1951167759760988  - accuracy: 0.75\n",
      "At: 233 [==========>] Loss 0.17084905299569308  - accuracy: 0.78125\n",
      "At: 234 [==========>] Loss 0.1427998340720388  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.19497328767118258  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.18796513180607163  - accuracy: 0.78125\n",
      "At: 237 [==========>] Loss 0.10243071688509704  - accuracy: 0.90625\n",
      "At: 238 [==========>] Loss 0.12986625098784443  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.11251679662321343  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.15950629317634957  - accuracy: 0.71875\n",
      "At: 241 [==========>] Loss 0.15142555174037053  - accuracy: 0.78125\n",
      "At: 242 [==========>] Loss 0.14978494429641445  - accuracy: 0.78125\n",
      "At: 243 [==========>] Loss 0.13329443021436696  - accuracy: 0.8125\n",
      "At: 244 [==========>] Loss 0.1620139524840753  - accuracy: 0.71875\n",
      "At: 245 [==========>] Loss 0.09913119564475319  - accuracy: 0.9375\n",
      "At: 246 [==========>] Loss 0.13219150043103728  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.11343828610469203  - accuracy: 0.8125\n",
      "At: 248 [==========>] Loss 0.0929188417012455  - accuracy: 0.90625\n",
      "At: 249 [==========>] Loss 0.11277475184950739  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.18164026583724002  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.15011333271294638  - accuracy: 0.78125\n",
      "At: 252 [==========>] Loss 0.0888360112216835  - accuracy: 0.9375\n",
      "At: 253 [==========>] Loss 0.1607029467687244  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.10669269940829859  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.1334390420576628  - accuracy: 0.84375\n",
      "At: 256 [==========>] Loss 0.1816839773363444  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.09399305527077653  - accuracy: 0.875\n",
      "At: 258 [==========>] Loss 0.16297717879830992  - accuracy: 0.8125\n",
      "At: 259 [==========>] Loss 0.13992920843751194  - accuracy: 0.78125\n",
      "At: 260 [==========>] Loss 0.1184444166922888  - accuracy: 0.875\n",
      "At: 261 [==========>] Loss 0.08596490392912369  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.12859422026036316  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.09276210336650788  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.09329695577174085  - accuracy: 0.90625\n",
      "At: 265 [==========>] Loss 0.155802729392666  - accuracy: 0.78125\n",
      "At: 266 [==========>] Loss 0.1956881070765274  - accuracy: 0.71875\n",
      "At: 267 [==========>] Loss 0.11455893520744118  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.1953392822335962  - accuracy: 0.65625\n",
      "At: 269 [==========>] Loss 0.11608664311444022  - accuracy: 0.90625\n",
      "At: 270 [==========>] Loss 0.2369302284499798  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.12525376267159272  - accuracy: 0.78125\n",
      "At: 272 [==========>] Loss 0.085482831565906  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.116228507241037  - accuracy: 0.84375\n",
      "At: 274 [==========>] Loss 0.16290850901648002  - accuracy: 0.75\n",
      "At: 275 [==========>] Loss 0.09181238751481226  - accuracy: 0.96875\n",
      "At: 276 [==========>] Loss 0.1832314764978445  - accuracy: 0.6875\n",
      "At: 277 [==========>] Loss 0.13821240866412304  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.11113481933272348  - accuracy: 0.875\n",
      "At: 279 [==========>] Loss 0.129490417030088  - accuracy: 0.84375\n",
      "At: 280 [==========>] Loss 0.15534204928030423  - accuracy: 0.84375\n",
      "At: 281 [==========>] Loss 0.13812033718679245  - accuracy: 0.78125\n",
      "At: 282 [==========>] Loss 0.1967872944188192  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.14108562671764086  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.10846498235233011  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.16054147150132578  - accuracy: 0.75\n",
      "At: 286 [==========>] Loss 0.07641709384545131  - accuracy: 0.84375\n",
      "At: 287 [==========>] Loss 0.1270170539392541  - accuracy: 0.875\n",
      "At: 288 [==========>] Loss 0.11258153966884915  - accuracy: 0.90625\n",
      "At: 289 [==========>] Loss 0.11367310368737871  - accuracy: 0.8125\n",
      "At: 290 [==========>] Loss 0.11580577353079086  - accuracy: 0.84375\n",
      "At: 291 [==========>] Loss 0.13246700577330092  - accuracy: 0.75\n",
      "At: 292 [==========>] Loss 0.15011693233703954  - accuracy: 0.78125\n",
      "At: 293 [==========>] Loss 0.16312083424605675  - accuracy: 0.78125\n",
      "At: 294 [==========>] Loss 0.15356477959799575  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.11899176776688906  - accuracy: 0.84375\n",
      "At: 296 [==========>] Loss 0.08979124606245337  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.125483889604672  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.13452060564898355  - accuracy: 0.875\n",
      "At: 299 [==========>] Loss 0.14591033627353056  - accuracy: 0.78125\n",
      "At: 300 [==========>] Loss 0.14229977733818097  - accuracy: 0.8125\n",
      "At: 301 [==========>] Loss 0.12857182908729084  - accuracy: 0.84375\n",
      "At: 302 [==========>] Loss 0.11678690666513052  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.10779530355092146  - accuracy: 0.90625\n",
      "At: 304 [==========>] Loss 0.15535064306474622  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.17908390304215607  - accuracy: 0.75\n",
      "At: 306 [==========>] Loss 0.13613294814398377  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.203874380028168  - accuracy: 0.6875\n",
      "At: 308 [==========>] Loss 0.12165338401087193  - accuracy: 0.875\n",
      "At: 309 [==========>] Loss 0.13888775853453023  - accuracy: 0.84375\n",
      "At: 310 [==========>] Loss 0.1726470438084381  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.07766322210744148  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.12611317210701742  - accuracy: 0.875\n",
      "At: 313 [==========>] Loss 0.09953924918052931  - accuracy: 0.875\n",
      "At: 314 [==========>] Loss 0.205255174904189  - accuracy: 0.6875\n",
      "At: 315 [==========>] Loss 0.1257146464469477  - accuracy: 0.875\n",
      "At: 316 [==========>] Loss 0.18223253248704185  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.23795441728029829  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.14823428839203495  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.11675338305643548  - accuracy: 0.84375\n",
      "At: 320 [==========>] Loss 0.1499652531928033  - accuracy: 0.78125\n",
      "At: 321 [==========>] Loss 0.1613141088613259  - accuracy: 0.75\n",
      "At: 322 [==========>] Loss 0.09647205484165276  - accuracy: 0.9375\n",
      "At: 323 [==========>] Loss 0.10040817703208459  - accuracy: 0.9375\n",
      "At: 324 [==========>] Loss 0.1435362770795935  - accuracy: 0.75\n",
      "At: 325 [==========>] Loss 0.09852567225963009  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.16970445470021314  - accuracy: 0.8125\n",
      "At: 327 [==========>] Loss 0.11591495681139076  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.10994488355744346  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.1531675586708947  - accuracy: 0.8125\n",
      "At: 330 [==========>] Loss 0.16707981544103462  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.16863911194174608  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.1798593433534447  - accuracy: 0.65625\n",
      "At: 333 [==========>] Loss 0.15330335404019654  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.09764218679085916  - accuracy: 0.84375\n",
      "At: 335 [==========>] Loss 0.12695149810660428  - accuracy: 0.84375\n",
      "At: 336 [==========>] Loss 0.13887005607470246  - accuracy: 0.78125\n",
      "At: 337 [==========>] Loss 0.16596279929908872  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.1176953898256512  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.13571972587025302  - accuracy: 0.84375\n",
      "At: 340 [==========>] Loss 0.12905967275871597  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.1484675251795584  - accuracy: 0.8125\n",
      "At: 342 [==========>] Loss 0.12342210293474662  - accuracy: 0.84375\n",
      "At: 343 [==========>] Loss 0.22057858409840644  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.16191534291717213  - accuracy: 0.78125\n",
      "At: 345 [==========>] Loss 0.15491400595489568  - accuracy: 0.78125\n",
      "At: 346 [==========>] Loss 0.14260869567627726  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.07339223980856666  - accuracy: 0.90625\n",
      "At: 348 [==========>] Loss 0.11929950424935282  - accuracy: 0.84375\n",
      "At: 349 [==========>] Loss 0.12210589896585877  - accuracy: 0.84375\n",
      "At: 350 [==========>] Loss 0.1480973565104735  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.19117467885205197  - accuracy: 0.6875\n",
      "At: 352 [==========>] Loss 0.12195313171670608  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.14427250740163644  - accuracy: 0.78125\n",
      "At: 354 [==========>] Loss 0.15660703394392186  - accuracy: 0.8125\n",
      "At: 355 [==========>] Loss 0.09708366780615127  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.16108850994078933  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.13129362254954088  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.14357773926376396  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.11128217608050367  - accuracy: 0.84375\n",
      "At: 360 [==========>] Loss 0.13883041716125882  - accuracy: 0.8125\n",
      "At: 361 [==========>] Loss 0.09350643432894899  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.10411709816987257  - accuracy: 0.84375\n",
      "At: 363 [==========>] Loss 0.09885670910709546  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.17723678653381064  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.13857118871288462  - accuracy: 0.75\n",
      "At: 366 [==========>] Loss 0.17735778847200265  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.14036514444990472  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.18254631859890996  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.154282085566054  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.169106623990244  - accuracy: 0.84375\n",
      "At: 371 [==========>] Loss 0.08658113616205172  - accuracy: 0.9375\n",
      "At: 372 [==========>] Loss 0.09260302207073637  - accuracy: 0.90625\n",
      "At: 373 [==========>] Loss 0.20192853060030486  - accuracy: 0.71875\n",
      "At: 374 [==========>] Loss 0.07244344296175786  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1072801334993001  - accuracy: 0.84375\n",
      "At: 376 [==========>] Loss 0.11433791497074208  - accuracy: 0.84375\n",
      "At: 377 [==========>] Loss 0.20039257974340027  - accuracy: 0.78125\n",
      "At: 378 [==========>] Loss 0.1786831027916006  - accuracy: 0.71875\n",
      "At: 379 [==========>] Loss 0.11508173087371655  - accuracy: 0.875\n",
      "At: 380 [==========>] Loss 0.15367336852419128  - accuracy: 0.78125\n",
      "At: 381 [==========>] Loss 0.1559377056368792  - accuracy: 0.8125\n",
      "At: 382 [==========>] Loss 0.10618660785715503  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.18256440801963536  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.1622511386545509  - accuracy: 0.84375\n",
      "At: 385 [==========>] Loss 0.1543139052344802  - accuracy: 0.78125\n",
      "At: 386 [==========>] Loss 0.19840683069589427  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.08687958916400282  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.20435250497636623  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.13561955103341344  - accuracy: 0.78125\n",
      "At: 390 [==========>] Loss 0.11640627517343191  - accuracy: 0.875\n",
      "At: 391 [==========>] Loss 0.09168078791438466  - accuracy: 0.9375\n",
      "At: 392 [==========>] Loss 0.10502417530956763  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.23282474691466631  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.10069060541983274  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.1576858339435128  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.16847731580833264  - accuracy: 0.8125\n",
      "At: 397 [==========>] Loss 0.12718276112285504  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.18593139343388282  - accuracy: 0.75\n",
      "At: 399 [==========>] Loss 0.17719991118815176  - accuracy: 0.71875\n",
      "At: 400 [==========>] Loss 0.15227996559790247  - accuracy: 0.84375\n",
      "At: 401 [==========>] Loss 0.1316351339541615  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.09975570118068003  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.08646391665734368  - accuracy: 0.875\n",
      "At: 404 [==========>] Loss 0.13365723417186332  - accuracy: 0.78125\n",
      "At: 405 [==========>] Loss 0.17335447584354058  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.11901062709210787  - accuracy: 0.84375\n",
      "At: 407 [==========>] Loss 0.16265962677820822  - accuracy: 0.78125\n",
      "At: 408 [==========>] Loss 0.15390289033005905  - accuracy: 0.78125\n",
      "At: 409 [==========>] Loss 0.16427700145235755  - accuracy: 0.78125\n",
      "At: 410 [==========>] Loss 0.10499774276313045  - accuracy: 0.84375\n",
      "At: 411 [==========>] Loss 0.10818937585165048  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.1362826494018064  - accuracy: 0.84375\n",
      "At: 413 [==========>] Loss 0.107250983681108  - accuracy: 0.90625\n",
      "At: 414 [==========>] Loss 0.12868445064517922  - accuracy: 0.78125\n",
      "At: 415 [==========>] Loss 0.12175644316778816  - accuracy: 0.875\n",
      "At: 416 [==========>] Loss 0.16645360349157975  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.12430539691865422  - accuracy: 0.84375\n",
      "At: 418 [==========>] Loss 0.1305767328234832  - accuracy: 0.8125\n",
      "At: 419 [==========>] Loss 0.08741898099011862  - accuracy: 0.9375\n",
      "At: 420 [==========>] Loss 0.13755594573205063  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.1161081856912517  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.10794460299162754  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.13160414065014137  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.16874418915358946  - accuracy: 0.78125\n",
      "At: 425 [==========>] Loss 0.18289392985062938  - accuracy: 0.6875\n",
      "At: 426 [==========>] Loss 0.15383269939794253  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.1753900295236893  - accuracy: 0.78125\n",
      "At: 428 [==========>] Loss 0.24835903671028964  - accuracy: 0.625\n",
      "At: 429 [==========>] Loss 0.17893261410248  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.12091388468976891  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.1266976153755594  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.13001186330164125  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.0967326218341026  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.11836542952601371  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.1374161044627328  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.13294272554171271  - accuracy: 0.875\n",
      "At: 437 [==========>] Loss 0.11132565700348651  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.11140593932368476  - accuracy: 0.90625\n",
      "At: 439 [==========>] Loss 0.10094921609365085  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.088472483053421  - accuracy: 0.9375\n",
      "At: 441 [==========>] Loss 0.14780129029152378  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.14454697156821167  - accuracy: 0.8125\n",
      "At: 443 [==========>] Loss 0.13515513815411112  - accuracy: 0.78125\n",
      "At: 444 [==========>] Loss 0.14125084068396712  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.13068530585460836  - accuracy: 0.75\n",
      "At: 446 [==========>] Loss 0.18111281170810375  - accuracy: 0.75\n",
      "At: 447 [==========>] Loss 0.1424777259732607  - accuracy: 0.84375\n",
      "At: 448 [==========>] Loss 0.14473214242861426  - accuracy: 0.78125\n",
      "At: 449 [==========>] Loss 0.09754399101778871  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.10742922673054318  - accuracy: 0.875\n",
      "At: 451 [==========>] Loss 0.09510853184900234  - accuracy: 0.84375\n",
      "At: 452 [==========>] Loss 0.14963212938632578  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.1293134928978646  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.20738038114378826  - accuracy: 0.75\n",
      "At: 455 [==========>] Loss 0.19748905816522383  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.15026806291152478  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.127557748217552  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.07567638896855419  - accuracy: 0.96875\n",
      "At: 459 [==========>] Loss 0.201907024454055  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.14625018923082445  - accuracy: 0.75\n",
      "At: 461 [==========>] Loss 0.1896235592057774  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.17154264376565537  - accuracy: 0.8125\n",
      "At: 463 [==========>] Loss 0.13389151844686853  - accuracy: 0.75\n",
      "At: 464 [==========>] Loss 0.1746284426981471  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.1948929923773831  - accuracy: 0.65625\n",
      "At: 466 [==========>] Loss 0.11607403604225328  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.15809726816594571  - accuracy: 0.8125\n",
      "At: 468 [==========>] Loss 0.08119630955422294  - accuracy: 0.9375\n",
      "At: 469 [==========>] Loss 0.13831640956658442  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.10599133680893504  - accuracy: 0.90625\n",
      "At: 471 [==========>] Loss 0.17573553995561525  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.13208986612676799  - accuracy: 0.84375\n",
      "At: 473 [==========>] Loss 0.15558405997293823  - accuracy: 0.75\n",
      "At: 474 [==========>] Loss 0.15119861201329865  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.11126120497974692  - accuracy: 0.875\n",
      "At: 476 [==========>] Loss 0.1543555150179453  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.1222615847307558  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.11852014763102604  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.1660271311181211  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.15474168206847425  - accuracy: 0.78125\n",
      "At: 481 [==========>] Loss 0.13150540356737014  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.08130358612719264  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.11616401696905282  - accuracy: 0.90625\n",
      "At: 484 [==========>] Loss 0.08673474501049726  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.13008415313963137  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.17326322660619836  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.15468322250737226  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.119110052896926  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.12387456099829108  - accuracy: 0.8125\n",
      "At: 490 [==========>] Loss 0.15028107094468857  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.16015551794729183  - accuracy: 0.78125\n",
      "At: 492 [==========>] Loss 0.17247801053462325  - accuracy: 0.75\n",
      "At: 493 [==========>] Loss 0.11189579540614392  - accuracy: 0.875\n",
      "At: 494 [==========>] Loss 0.14665848313749869  - accuracy: 0.84375\n",
      "At: 495 [==========>] Loss 0.11366295239761365  - accuracy: 0.875\n",
      "At: 496 [==========>] Loss 0.16712397430092324  - accuracy: 0.75\n",
      "At: 497 [==========>] Loss 0.1402904645732478  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.08129014542489141  - accuracy: 0.875\n",
      "At: 499 [==========>] Loss 0.1470450487068821  - accuracy: 0.75\n",
      "At: 500 [==========>] Loss 0.12937548495578516  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.16034516263005066  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.11844236455274221  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.09746591460010412  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.09555531368563575  - accuracy: 0.9375\n",
      "At: 505 [==========>] Loss 0.1671540883228188  - accuracy: 0.8125\n",
      "At: 506 [==========>] Loss 0.24340224601907381  - accuracy: 0.625\n",
      "At: 507 [==========>] Loss 0.11561534270677923  - accuracy: 0.78125\n",
      "At: 508 [==========>] Loss 0.11007416156172228  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.1399490063082775  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.16588572536137797  - accuracy: 0.65625\n",
      "At: 511 [==========>] Loss 0.10312913920001536  - accuracy: 0.875\n",
      "At: 512 [==========>] Loss 0.19932433095371305  - accuracy: 0.71875\n",
      "At: 513 [==========>] Loss 0.17147669880467709  - accuracy: 0.75\n",
      "At: 514 [==========>] Loss 0.15596722013466618  - accuracy: 0.8125\n",
      "At: 515 [==========>] Loss 0.1061142013250286  - accuracy: 0.875\n",
      "At: 516 [==========>] Loss 0.18324493965759714  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.14199051258093703  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.14578806986930326  - accuracy: 0.78125\n",
      "At: 519 [==========>] Loss 0.12477590777596734  - accuracy: 0.875\n",
      "At: 520 [==========>] Loss 0.11960943598116905  - accuracy: 0.84375\n",
      "At: 521 [==========>] Loss 0.12071685541410515  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.16569358402700918  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.13647237442800964  - accuracy: 0.8125\n",
      "At: 524 [==========>] Loss 0.09439016347857354  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.15390575469590478  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.17284985814437387  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.2187136444086796  - accuracy: 0.6875\n",
      "At: 528 [==========>] Loss 0.17417901903614424  - accuracy: 0.75\n",
      "At: 529 [==========>] Loss 0.10546103178585672  - accuracy: 0.90625\n",
      "At: 530 [==========>] Loss 0.1637416092231973  - accuracy: 0.78125\n",
      "At: 531 [==========>] Loss 0.1450211427080455  - accuracy: 0.84375\n",
      "At: 532 [==========>] Loss 0.13432940777549746  - accuracy: 0.8125\n",
      "At: 533 [==========>] Loss 0.08155214049614333  - accuracy: 0.90625\n",
      "At: 534 [==========>] Loss 0.14857254125594893  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.13456324957019236  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.159597740316737  - accuracy: 0.8125\n",
      "At: 537 [==========>] Loss 0.09793322392774403  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.086809731177307  - accuracy: 0.875\n",
      "At: 539 [==========>] Loss 0.08709633797016822  - accuracy: 0.90625\n",
      "At: 540 [==========>] Loss 0.23710894443972647  - accuracy: 0.6875\n",
      "At: 541 [==========>] Loss 0.15197459270071711  - accuracy: 0.84375\n",
      "At: 542 [==========>] Loss 0.14495955862869972  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.11779040211207524  - accuracy: 0.8125\n",
      "At: 544 [==========>] Loss 0.1749660301082362  - accuracy: 0.71875\n",
      "At: 545 [==========>] Loss 0.07064289270443767  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.16355694562191683  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.09711154541261441  - accuracy: 0.90625\n",
      "At: 548 [==========>] Loss 0.11942819480814124  - accuracy: 0.84375\n",
      "At: 549 [==========>] Loss 0.10541060895961346  - accuracy: 0.90625\n",
      "At: 550 [==========>] Loss 0.07470910839315889  - accuracy: 0.90625\n",
      "At: 551 [==========>] Loss 0.10810579978758944  - accuracy: 0.84375\n",
      "At: 552 [==========>] Loss 0.1281674170360624  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.10805742225588011  - accuracy: 0.875\n",
      "At: 554 [==========>] Loss 0.07540465110977093  - accuracy: 0.90625\n",
      "At: 555 [==========>] Loss 0.11228483806757915  - accuracy: 0.875\n",
      "At: 556 [==========>] Loss 0.13944623967521308  - accuracy: 0.84375\n",
      "At: 557 [==========>] Loss 0.1071145723886669  - accuracy: 0.875\n",
      "At: 558 [==========>] Loss 0.14733213314849025  - accuracy: 0.8125\n",
      "At: 559 [==========>] Loss 0.18474932567398916  - accuracy: 0.75\n",
      "At: 560 [==========>] Loss 0.11876445849122083  - accuracy: 0.8125\n",
      "At: 561 [==========>] Loss 0.14523965620217158  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.07208530224120391  - accuracy: 0.9375\n",
      "At: 563 [==========>] Loss 0.11198789536414239  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.15424551005904302  - accuracy: 0.8125\n",
      "At: 565 [==========>] Loss 0.10162517366327314  - accuracy: 0.875\n",
      "At: 566 [==========>] Loss 0.14366659321386516  - accuracy: 0.8125\n",
      "At: 567 [==========>] Loss 0.16328607800195838  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.2159014584577838  - accuracy: 0.625\n",
      "At: 569 [==========>] Loss 0.15710404914877216  - accuracy: 0.8125\n",
      "At: 570 [==========>] Loss 0.10078964212278205  - accuracy: 0.84375\n",
      "At: 571 [==========>] Loss 0.121614679933108  - accuracy: 0.84375\n",
      "At: 572 [==========>] Loss 0.09845707626170011  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.10377759732313993  - accuracy: 0.84375\n",
      "At: 574 [==========>] Loss 0.1513580619532396  - accuracy: 0.8125\n",
      "At: 575 [==========>] Loss 0.12679646010429965  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.10571164244002913  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.15494500157558477  - accuracy: 0.8125\n",
      "At: 578 [==========>] Loss 0.16229447667478203  - accuracy: 0.78125\n",
      "At: 579 [==========>] Loss 0.09183171199229631  - accuracy: 0.90625\n",
      "At: 580 [==========>] Loss 0.11003910679494308  - accuracy: 0.90625\n",
      "At: 581 [==========>] Loss 0.16810160120558076  - accuracy: 0.84375\n",
      "At: 582 [==========>] Loss 0.13024075992707743  - accuracy: 0.84375\n",
      "At: 583 [==========>] Loss 0.17372344614238863  - accuracy: 0.71875\n",
      "At: 584 [==========>] Loss 0.07907179594368938  - accuracy: 0.96875\n",
      "At: 585 [==========>] Loss 0.13953390298980506  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.08600738809842495  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.12168272579818726  - accuracy: 0.875\n",
      "At: 588 [==========>] Loss 0.14687696555682994  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.12480461526385808  - accuracy: 0.84375\n",
      "At: 590 [==========>] Loss 0.07889341504467591  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.14267107997973763  - accuracy: 0.8125\n",
      "At: 592 [==========>] Loss 0.08974515953224557  - accuracy: 0.9375\n",
      "At: 593 [==========>] Loss 0.15709863752629383  - accuracy: 0.84375\n",
      "At: 594 [==========>] Loss 0.14084287577462892  - accuracy: 0.84375\n",
      "At: 595 [==========>] Loss 0.15292794270254595  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.13892140474901743  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.1920015616516557  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.14284616718646131  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.1299606289151179  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.10264753692847814  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.11634432007941875  - accuracy: 0.8125\n",
      "At: 602 [==========>] Loss 0.12176940054296567  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.12365851864155371  - accuracy: 0.8125\n",
      "At: 604 [==========>] Loss 0.17209068158356633  - accuracy: 0.78125\n",
      "At: 605 [==========>] Loss 0.08704290586057234  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.12839701765787642  - accuracy: 0.84375\n",
      "At: 607 [==========>] Loss 0.14742513287678755  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.11327821959020495  - accuracy: 0.875\n",
      "At: 609 [==========>] Loss 0.113742224152227  - accuracy: 0.78125\n",
      "At: 610 [==========>] Loss 0.11752431160562044  - accuracy: 0.84375\n",
      "At: 611 [==========>] Loss 0.12005018765831102  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.10717422839938427  - accuracy: 0.875\n",
      "At: 613 [==========>] Loss 0.16430698867403326  - accuracy: 0.75\n",
      "At: 614 [==========>] Loss 0.14051836361321468  - accuracy: 0.75\n",
      "At: 615 [==========>] Loss 0.16564118123405508  - accuracy: 0.78125\n",
      "At: 616 [==========>] Loss 0.1800563463968594  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.13774014411307992  - accuracy: 0.78125\n",
      "At: 618 [==========>] Loss 0.15390924203738807  - accuracy: 0.84375\n",
      "At: 619 [==========>] Loss 0.1493057512577879  - accuracy: 0.78125\n",
      "At: 620 [==========>] Loss 0.15554044154573057  - accuracy: 0.8125\n",
      "At: 621 [==========>] Loss 0.056444395447128856  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.15696877321253025  - accuracy: 0.8125\n",
      "At: 623 [==========>] Loss 0.140741190012863  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.08318315574320781  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.14174219297774904  - accuracy: 0.8125\n",
      "At: 626 [==========>] Loss 0.12210257320443006  - accuracy: 0.78125\n",
      "At: 627 [==========>] Loss 0.14197240705482772  - accuracy: 0.78125\n",
      "At: 628 [==========>] Loss 0.11964821371582031  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.18140675696360392  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.22406388489642792  - accuracy: 0.65625\n",
      "At: 631 [==========>] Loss 0.17457893971918456  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.15361610457505176  - accuracy: 0.75\n",
      "At: 633 [==========>] Loss 0.17547282269968384  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.10541185604706887  - accuracy: 0.84375\n",
      "At: 635 [==========>] Loss 0.11821152495031216  - accuracy: 0.875\n",
      "At: 636 [==========>] Loss 0.13752450579843575  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.11656761052183699  - accuracy: 0.875\n",
      "At: 638 [==========>] Loss 0.14845249501778912  - accuracy: 0.8125\n",
      "At: 639 [==========>] Loss 0.12365024117447981  - accuracy: 0.84375\n",
      "At: 640 [==========>] Loss 0.18793533273580615  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.1020930496654974  - accuracy: 0.8125\n",
      "At: 642 [==========>] Loss 0.17238831879020372  - accuracy: 0.78125\n",
      "At: 643 [==========>] Loss 0.1216340315876939  - accuracy: 0.78125\n",
      "At: 644 [==========>] Loss 0.059182337262911375  - accuracy: 0.96875\n",
      "At: 645 [==========>] Loss 0.13959629816783375  - accuracy: 0.78125\n",
      "At: 646 [==========>] Loss 0.12555872638865623  - accuracy: 0.8125\n",
      "At: 647 [==========>] Loss 0.16871649848205156  - accuracy: 0.8125\n",
      "At: 648 [==========>] Loss 0.15832821528545685  - accuracy: 0.75\n",
      "At: 649 [==========>] Loss 0.17762508092703735  - accuracy: 0.6875\n",
      "At: 650 [==========>] Loss 0.09081300491970504  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.1707323779357123  - accuracy: 0.6875\n",
      "At: 652 [==========>] Loss 0.08466166952506825  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.1309981274613527  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.09579135362326269  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.14503931322358918  - accuracy: 0.75\n",
      "At: 656 [==========>] Loss 0.13051619738024306  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.1424690494935548  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.13604722375961445  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.1404769323146685  - accuracy: 0.78125\n",
      "At: 660 [==========>] Loss 0.1057873636569518  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.14540356798979076  - accuracy: 0.78125\n",
      "At: 662 [==========>] Loss 0.10133988150462075  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.09375005328102505  - accuracy: 0.90625\n",
      "At: 664 [==========>] Loss 0.1247291576910741  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.17062520171858822  - accuracy: 0.84375\n",
      "At: 666 [==========>] Loss 0.18652669201858596  - accuracy: 0.75\n",
      "At: 667 [==========>] Loss 0.13736753080962108  - accuracy: 0.8125\n",
      "At: 668 [==========>] Loss 0.13446897569096003  - accuracy: 0.78125\n",
      "At: 669 [==========>] Loss 0.1466672153453388  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.22372772573506788  - accuracy: 0.65625\n",
      "At: 671 [==========>] Loss 0.09229150126419138  - accuracy: 0.90625\n",
      "At: 672 [==========>] Loss 0.12244515163158608  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.06662999855513785  - accuracy: 0.96875\n",
      "At: 674 [==========>] Loss 0.13841343224067743  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.10964134418557404  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.15491807676760266  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.12782288804514558  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.09719605783981994  - accuracy: 0.875\n",
      "At: 679 [==========>] Loss 0.10905072005875532  - accuracy: 0.90625\n",
      "At: 680 [==========>] Loss 0.10488033727527675  - accuracy: 0.8125\n",
      "At: 681 [==========>] Loss 0.1444531316535162  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.11781626958994476  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.125757915493863  - accuracy: 0.8125\n",
      "At: 684 [==========>] Loss 0.10429478436015988  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.13917522218283304  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.11336890486841117  - accuracy: 0.90625\n",
      "At: 687 [==========>] Loss 0.08272916969159112  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.08430415061837405  - accuracy: 0.875\n",
      "At: 689 [==========>] Loss 0.14435706479573346  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.1112112958961992  - accuracy: 0.90625\n",
      "At: 691 [==========>] Loss 0.09974601196037697  - accuracy: 0.875\n",
      "At: 692 [==========>] Loss 0.121605808496946  - accuracy: 0.78125\n",
      "At: 693 [==========>] Loss 0.16268811696378943  - accuracy: 0.75\n",
      "At: 694 [==========>] Loss 0.18089873985221921  - accuracy: 0.6875\n",
      "At: 695 [==========>] Loss 0.14738791503191628  - accuracy: 0.84375\n",
      "At: 696 [==========>] Loss 0.1196287882256323  - accuracy: 0.84375\n",
      "At: 697 [==========>] Loss 0.18101150665683857  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.10346575767444549  - accuracy: 0.8125\n",
      "At: 699 [==========>] Loss 0.10474334964599158  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.062149474238828814  - accuracy: 0.90625\n",
      "At: 701 [==========>] Loss 0.12413413296072157  - accuracy: 0.84375\n",
      "At: 702 [==========>] Loss 0.09576782643172133  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.17555826314879244  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.15178922247246607  - accuracy: 0.8125\n",
      "At: 705 [==========>] Loss 0.20619507390633646  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.14623027196852637  - accuracy: 0.84375\n",
      "At: 707 [==========>] Loss 0.11096710249882112  - accuracy: 0.8125\n",
      "At: 708 [==========>] Loss 0.11238532268664396  - accuracy: 0.84375\n",
      "At: 709 [==========>] Loss 0.22854459671377847  - accuracy: 0.71875\n",
      "At: 710 [==========>] Loss 0.12625628932216182  - accuracy: 0.84375\n",
      "At: 711 [==========>] Loss 0.20224286793836785  - accuracy: 0.75\n",
      "At: 712 [==========>] Loss 0.16185956176458746  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.19686155114109033  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.22411625956633502  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.09569062167074746  - accuracy: 0.90625\n",
      "At: 716 [==========>] Loss 0.11717931628401615  - accuracy: 0.875\n",
      "At: 717 [==========>] Loss 0.058905513328962225  - accuracy: 0.96875\n",
      "At: 718 [==========>] Loss 0.17542713862401405  - accuracy: 0.78125\n",
      "At: 719 [==========>] Loss 0.10286855957404877  - accuracy: 0.875\n",
      "At: 720 [==========>] Loss 0.1136938536580519  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.10357327957160355  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.1784489412241741  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.14246731711497407  - accuracy: 0.78125\n",
      "At: 724 [==========>] Loss 0.08490927573435461  - accuracy: 0.9375\n",
      "At: 725 [==========>] Loss 0.12967315909431126  - accuracy: 0.8125\n",
      "At: 726 [==========>] Loss 0.16914807425007855  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.1399199354979943  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.14476583175062613  - accuracy: 0.71875\n",
      "At: 729 [==========>] Loss 0.16911319670242986  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.1623470540566827  - accuracy: 0.78125\n",
      "At: 731 [==========>] Loss 0.15962105439495144  - accuracy: 0.78125\n",
      "At: 732 [==========>] Loss 0.15612799794632815  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.11527470512717741  - accuracy: 0.84375\n",
      "At: 734 [==========>] Loss 0.13606251499918876  - accuracy: 0.78125\n",
      "At: 735 [==========>] Loss 0.10383026745862255  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.08008646782195895  - accuracy: 0.9375\n",
      "At: 737 [==========>] Loss 0.1389815432210289  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.12617769016626018  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.08204213923130294  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.13645293672285574  - accuracy: 0.78125\n",
      "At: 741 [==========>] Loss 0.10974054411176311  - accuracy: 0.875\n",
      "At: 742 [==========>] Loss 0.13973732221397656  - accuracy: 0.8125\n",
      "At: 743 [==========>] Loss 0.15208805444759127  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.15378623300588734  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.19118508771694645  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.10991735194878835  - accuracy: 0.84375\n",
      "At: 747 [==========>] Loss 0.13470571874987963  - accuracy: 0.8125\n",
      "At: 748 [==========>] Loss 0.15489688655132322  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.1330809460069024  - accuracy: 0.8125\n",
      "At: 750 [==========>] Loss 0.10009593047662005  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.145194230756697  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.0873155979774682  - accuracy: 0.875\n",
      "At: 753 [==========>] Loss 0.17756037498820637  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.14229115961965447  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.095388688314516  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.20620221808182926  - accuracy: 0.65625\n",
      "At: 757 [==========>] Loss 0.08316786539435084  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.1422523840826465  - accuracy: 0.75\n",
      "At: 759 [==========>] Loss 0.07254945479619836  - accuracy: 0.9375\n",
      "At: 760 [==========>] Loss 0.1700305273151032  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.1280289945745946  - accuracy: 0.75\n",
      "At: 762 [==========>] Loss 0.12539585415509752  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.15761356207336275  - accuracy: 0.78125\n",
      "At: 764 [==========>] Loss 0.12402021702199481  - accuracy: 0.78125\n",
      "At: 765 [==========>] Loss 0.13653569286365627  - accuracy: 0.84375\n",
      "At: 766 [==========>] Loss 0.13771029105951468  - accuracy: 0.875\n",
      "At: 767 [==========>] Loss 0.1239850786987817  - accuracy: 0.8125\n",
      "At: 768 [==========>] Loss 0.16232804420712357  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.13379521519216636  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.13210360241510008  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.1948706717759787  - accuracy: 0.75\n",
      "At: 772 [==========>] Loss 0.12399017139362722  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.08169582565813768  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.1267828784766635  - accuracy: 0.8125\n",
      "At: 775 [==========>] Loss 0.17389263916333514  - accuracy: 0.6875\n",
      "At: 776 [==========>] Loss 0.17720324091783002  - accuracy: 0.6875\n",
      "At: 777 [==========>] Loss 0.07687637777888551  - accuracy: 0.9375\n",
      "At: 778 [==========>] Loss 0.14945592980809813  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.11857892265862985  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.07980742750217537  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.15762782302040457  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.151175080939358  - accuracy: 0.8125\n",
      "At: 783 [==========>] Loss 0.14878097083798889  - accuracy: 0.8125\n",
      "At: 784 [==========>] Loss 0.1468378348966402  - accuracy: 0.8125\n",
      "At: 785 [==========>] Loss 0.1873440521300288  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.13469799462977694  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.1546518743607848  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.06884730034011784  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.14242738780986994  - accuracy: 0.84375\n",
      "At: 790 [==========>] Loss 0.13520412583989955  - accuracy: 0.84375\n",
      "At: 791 [==========>] Loss 0.1442015078160815  - accuracy: 0.8125\n",
      "At: 792 [==========>] Loss 0.16640857324066086  - accuracy: 0.78125\n",
      "At: 793 [==========>] Loss 0.12993049524204678  - accuracy: 0.84375\n",
      "At: 794 [==========>] Loss 0.1468312365848761  - accuracy: 0.75\n",
      "At: 795 [==========>] Loss 0.11202623310223762  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.11637534705750055  - accuracy: 0.875\n",
      "At: 797 [==========>] Loss 0.1683155768977695  - accuracy: 0.75\n",
      "At: 798 [==========>] Loss 0.14513818413643023  - accuracy: 0.84375\n",
      "At: 799 [==========>] Loss 0.061729006677422635  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.15734218721766047  - accuracy: 0.75\n",
      "At: 801 [==========>] Loss 0.10427372227623499  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.1763155082031202  - accuracy: 0.78125\n",
      "At: 803 [==========>] Loss 0.162282104633559  - accuracy: 0.78125\n",
      "At: 804 [==========>] Loss 0.14729392672372632  - accuracy: 0.75\n",
      "At: 805 [==========>] Loss 0.14544392420139723  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.07816593989835327  - accuracy: 0.90625\n",
      "At: 807 [==========>] Loss 0.10746248846338041  - accuracy: 0.875\n",
      "At: 808 [==========>] Loss 0.12507715086719207  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.0821683865669491  - accuracy: 0.90625\n",
      "At: 810 [==========>] Loss 0.15804708910876353  - accuracy: 0.8125\n",
      "At: 811 [==========>] Loss 0.1118380400520273  - accuracy: 0.875\n",
      "At: 812 [==========>] Loss 0.13991334915156695  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.1985124453508609  - accuracy: 0.71875\n",
      "At: 814 [==========>] Loss 0.1962873976774115  - accuracy: 0.71875\n",
      "At: 815 [==========>] Loss 0.1454120289499254  - accuracy: 0.875\n",
      "At: 816 [==========>] Loss 0.13495914233555809  - accuracy: 0.84375\n",
      "At: 817 [==========>] Loss 0.07409719293459346  - accuracy: 0.9375\n",
      "At: 818 [==========>] Loss 0.11849353563214647  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.11762848180257313  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.14004698204490004  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.14624288641387492  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.11292377385485508  - accuracy: 0.875\n",
      "At: 823 [==========>] Loss 0.16687909211726384  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.1296312219154755  - accuracy: 0.8125\n",
      "At: 825 [==========>] Loss 0.19729326038654152  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.122351583801613  - accuracy: 0.84375\n",
      "At: 827 [==========>] Loss 0.07967977553802874  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.06914384354327713  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09000042236627184  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.07510565016671092  - accuracy: 0.875\n",
      "At: 831 [==========>] Loss 0.0754690557308378  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.11621443491298866  - accuracy: 0.84375\n",
      "At: 833 [==========>] Loss 0.1724367756537838  - accuracy: 0.78125\n",
      "At: 834 [==========>] Loss 0.1125409593452866  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.09430580056356075  - accuracy: 0.84375\n",
      "At: 836 [==========>] Loss 0.1311224347487081  - accuracy: 0.78125\n",
      "At: 837 [==========>] Loss 0.1435011042726392  - accuracy: 0.78125\n",
      "At: 838 [==========>] Loss 0.10512270643814459  - accuracy: 0.8125\n",
      "At: 839 [==========>] Loss 0.10929122789063664  - accuracy: 0.84375\n",
      "At: 840 [==========>] Loss 0.10169242796774638  - accuracy: 0.875\n",
      "At: 841 [==========>] Loss 0.07261596813902065  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.06743555892039635  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.12396096935469951  - accuracy: 0.875\n",
      "At: 844 [==========>] Loss 0.11732530308177141  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.150726121456969  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.12368652539419776  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.0780327190106123  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.15773640084647864  - accuracy: 0.75\n",
      "At: 849 [==========>] Loss 0.13851609128914305  - accuracy: 0.84375\n",
      "At: 850 [==========>] Loss 0.11849560360701974  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.07793458974496463  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.14836412055493112  - accuracy: 0.75\n",
      "At: 853 [==========>] Loss 0.1552961256781567  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.20449353852047497  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.07577409224761404  - accuracy: 0.90625\n",
      "At: 856 [==========>] Loss 0.10036913289953732  - accuracy: 0.84375\n",
      "At: 857 [==========>] Loss 0.07607986690847851  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.24311299872765627  - accuracy: 0.6875\n",
      "At: 859 [==========>] Loss 0.08524355681039877  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.1378693035988391  - accuracy: 0.75\n",
      "At: 861 [==========>] Loss 0.11579617784257129  - accuracy: 0.78125\n",
      "At: 862 [==========>] Loss 0.0850208908940568  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.15856698215279288  - accuracy: 0.75\n",
      "At: 864 [==========>] Loss 0.17178877252919714  - accuracy: 0.6875\n",
      "At: 865 [==========>] Loss 0.19853541855795953  - accuracy: 0.6875\n",
      "At: 866 [==========>] Loss 0.157185477321458  - accuracy: 0.8125\n",
      "At: 867 [==========>] Loss 0.07796902230504311  - accuracy: 0.90625\n",
      "At: 868 [==========>] Loss 0.1654556768540285  - accuracy: 0.78125\n",
      "At: 869 [==========>] Loss 0.16768723613710218  - accuracy: 0.78125\n",
      "At: 870 [==========>] Loss 0.13205779448250612  - accuracy: 0.84375\n",
      "At: 871 [==========>] Loss 0.0781691686658641  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.07395513684717983  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.17514565136995996  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.15982333000316723  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.1031864268181499  - accuracy: 0.90625\n",
      "At: 876 [==========>] Loss 0.10994888450243753  - accuracy: 0.8125\n",
      "At: 877 [==========>] Loss 0.16728566095825922  - accuracy: 0.78125\n",
      "At: 878 [==========>] Loss 0.05259918786562967  - accuracy: 1.0\n",
      "At: 879 [==========>] Loss 0.13520642298238728  - accuracy: 0.78125\n",
      "At: 880 [==========>] Loss 0.09853465352614658  - accuracy: 0.90625\n",
      "At: 881 [==========>] Loss 0.14467034871873552  - accuracy: 0.8125\n",
      "At: 882 [==========>] Loss 0.11381806044170893  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.12341870265031445  - accuracy: 0.8125\n",
      "At: 884 [==========>] Loss 0.12914783651176873  - accuracy: 0.8125\n",
      "At: 885 [==========>] Loss 0.12595873091794038  - accuracy: 0.8125\n",
      "At: 886 [==========>] Loss 0.08188991790216188  - accuracy: 0.875\n",
      "At: 887 [==========>] Loss 0.13552853955472924  - accuracy: 0.8125\n",
      "At: 888 [==========>] Loss 0.16720971077248603  - accuracy: 0.6875\n",
      "At: 889 [==========>] Loss 0.10170150901752437  - accuracy: 0.875\n",
      "At: 890 [==========>] Loss 0.12815763119307194  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.09288157133637086  - accuracy: 0.9375\n",
      "At: 892 [==========>] Loss 0.11482719460256871  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.14517808973129703  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.14859283543712803  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.08532324659475289  - accuracy: 0.90625\n",
      "At: 896 [==========>] Loss 0.08559403784117353  - accuracy: 0.9375\n",
      "At: 897 [==========>] Loss 0.11611984461593702  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.1430628539983585  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.07294759593720351  - accuracy: 0.9375\n",
      "At: 900 [==========>] Loss 0.13875116529607137  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.15994018598508014  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.09006265870656471  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.12278138809740054  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.10944608428700636  - accuracy: 0.84375\n",
      "At: 905 [==========>] Loss 0.07262623003782785  - accuracy: 0.90625\n",
      "At: 906 [==========>] Loss 0.08460994208005423  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.15190117268744507  - accuracy: 0.84375\n",
      "At: 908 [==========>] Loss 0.11204281118506679  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.07662355819887598  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.11377665092623881  - accuracy: 0.84375\n",
      "At: 911 [==========>] Loss 0.17173180201933802  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.12707980230471405  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.10914636191354751  - accuracy: 0.84375\n",
      "At: 914 [==========>] Loss 0.11048409635247725  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.12996957243382207  - accuracy: 0.84375\n",
      "At: 916 [==========>] Loss 0.19629607628562135  - accuracy: 0.65625\n",
      "At: 917 [==========>] Loss 0.16024715928453098  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.16238822469472822  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.11040845839475677  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.11459652654498642  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.12222759956403421  - accuracy: 0.875\n",
      "At: 922 [==========>] Loss 0.13465650430782966  - accuracy: 0.78125\n",
      "At: 923 [==========>] Loss 0.08934594069652642  - accuracy: 0.84375\n",
      "At: 924 [==========>] Loss 0.19628820217810702  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.1366529026508964  - accuracy: 0.84375\n",
      "At: 926 [==========>] Loss 0.14098095544247286  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.09509512358553204  - accuracy: 0.90625\n",
      "At: 928 [==========>] Loss 0.13597801493334494  - accuracy: 0.78125\n",
      "At: 929 [==========>] Loss 0.13949699468787094  - accuracy: 0.84375\n",
      "At: 930 [==========>] Loss 0.11162668081028851  - accuracy: 0.84375\n",
      "At: 931 [==========>] Loss 0.1735099266385918  - accuracy: 0.71875\n",
      "At: 932 [==========>] Loss 0.08455178206864411  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.08051179751841855  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.14853630108821883  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.07309664349528269  - accuracy: 0.90625\n",
      "At: 936 [==========>] Loss 0.13605998938181132  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.1450728834975451  - accuracy: 0.84375\n",
      "At: 938 [==========>] Loss 0.12457442683633949  - accuracy: 0.875\n",
      "At: 939 [==========>] Loss 0.10848467692766403  - accuracy: 0.78125\n",
      "At: 940 [==========>] Loss 0.1985781706699061  - accuracy: 0.71875\n",
      "At: 941 [==========>] Loss 0.11303221910918282  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.12607791484393405  - accuracy: 0.84375\n",
      "At: 943 [==========>] Loss 0.12350792333777277  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.10784675040097352  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.09403197049910188  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.11902633032462227  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.13570444789606734  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.17166905262254642  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.06383280954259221  - accuracy: 0.9375\n",
      "At: 950 [==========>] Loss 0.10381369876588259  - accuracy: 0.8125\n",
      "At: 951 [==========>] Loss 0.09915147027771573  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.06145295099010357  - accuracy: 0.9375\n",
      "At: 953 [==========>] Loss 0.061032943786699205  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.10587786504421012  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.14300286190410025  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.09313553605536429  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.12015611506643789  - accuracy: 0.875\n",
      "At: 958 [==========>] Loss 0.07897568914564168  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.12163850963340324  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.10080548715483875  - accuracy: 0.8125\n",
      "At: 961 [==========>] Loss 0.12062395450879027  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.09407556982624922  - accuracy: 0.90625\n",
      "At: 963 [==========>] Loss 0.08986128132454618  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.15995473660879084  - accuracy: 0.71875\n",
      "At: 965 [==========>] Loss 0.11819617803357417  - accuracy: 0.90625\n",
      "At: 966 [==========>] Loss 0.1599536805358715  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.11037837135178769  - accuracy: 0.875\n",
      "At: 968 [==========>] Loss 0.16440801963027196  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.12690417673018062  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.12192906652284541  - accuracy: 0.78125\n",
      "At: 971 [==========>] Loss 0.11252448882418703  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.06544133565680828  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.1216929745809937  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.09633843400957243  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.1276971974468024  - accuracy: 0.78125\n",
      "At: 976 [==========>] Loss 0.10541385149495983  - accuracy: 0.875\n",
      "At: 977 [==========>] Loss 0.09540544120396366  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.15850249095032812  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.09439853849089204  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.1424605620103086  - accuracy: 0.6875\n",
      "At: 981 [==========>] Loss 0.1611516638878941  - accuracy: 0.78125\n",
      "At: 982 [==========>] Loss 0.07242029239589885  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.1036239742215434  - accuracy: 0.90625\n",
      "At: 984 [==========>] Loss 0.10421081816886345  - accuracy: 0.8125\n",
      "At: 985 [==========>] Loss 0.16174700121195223  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.10923088328286917  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.12525376105646185  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.12895847679415276  - accuracy: 0.84375\n",
      "At: 989 [==========>] Loss 0.1295389140522313  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.14414655687825736  - accuracy: 0.8125\n",
      "At: 991 [==========>] Loss 0.14648949765997776  - accuracy: 0.78125\n",
      "At: 992 [==========>] Loss 0.16584494619504583  - accuracy: 0.75\n",
      "At: 993 [==========>] Loss 0.13521107071706376  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.16278638699715214  - accuracy: 0.6875\n",
      "At: 995 [==========>] Loss 0.15262638861950356  - accuracy: 0.8125\n",
      "At: 996 [==========>] Loss 0.07185427498321893  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.14749140423254184  - accuracy: 0.78125\n",
      "At: 998 [==========>] Loss 0.11981035968105705  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.1250214838187092  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.22387740159986902  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.15479065770689343  - accuracy: 0.8125\n",
      "At: 1002 [==========>] Loss 0.21600736405460913  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.11355137078725089  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.10910569628856548  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.09745484979879478  - accuracy: 0.9375\n",
      "At: 1006 [==========>] Loss 0.09071942443847547  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.140888792636639  - accuracy: 0.84375\n",
      "At: 1008 [==========>] Loss 0.18328864429893837  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.1346427836577454  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.12955652410826338  - accuracy: 0.8125\n",
      "At: 1011 [==========>] Loss 0.14191226437270665  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.10389234120162125  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.07704536069028331  - accuracy: 0.875\n",
      "At: 1014 [==========>] Loss 0.09317444959596984  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.16899396967837563  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.16686037970390227  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.17487346430978  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.16812512503098975  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.17470071003347576  - accuracy: 0.71875\n",
      "At: 1020 [==========>] Loss 0.11257974155854866  - accuracy: 0.84375\n",
      "At: 1021 [==========>] Loss 0.1303388940095726  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.1086053489567826  - accuracy: 0.875\n",
      "At: 1023 [==========>] Loss 0.1710105400790095  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.16854447080383222  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.19218725746379975  - accuracy: 0.75\n",
      "At: 1026 [==========>] Loss 0.08961430907874302  - accuracy: 0.875\n",
      "At: 1027 [==========>] Loss 0.11022394541952975  - accuracy: 0.90625\n",
      "At: 1028 [==========>] Loss 0.2344785529316396  - accuracy: 0.59375\n",
      "At: 1029 [==========>] Loss 0.09702055463766973  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.10196761682433209  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.15895434677836462  - accuracy: 0.6875\n",
      "At: 1032 [==========>] Loss 0.12218809533271359  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.12129622872878947  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.08202180226655777  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.10037031308648747  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.15786929031682265  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.13544036261468295  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.10582053735120459  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.08256021600776525  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.11320426183978782  - accuracy: 0.8125\n",
      "At: 1041 [==========>] Loss 0.1517694131518536  - accuracy: 0.71875\n",
      "At: 1042 [==========>] Loss 0.10752291654559473  - accuracy: 0.90625\n",
      "At: 1043 [==========>] Loss 0.20872348152124698  - accuracy: 0.71875\n",
      "At: 1044 [==========>] Loss 0.10454769139213299  - accuracy: 0.875\n",
      "At: 1045 [==========>] Loss 0.160853174759101  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.16253578971260102  - accuracy: 0.78125\n",
      "At: 1047 [==========>] Loss 0.15154981727404077  - accuracy: 0.8125\n",
      "At: 1048 [==========>] Loss 0.18000000244897713  - accuracy: 0.84375\n",
      "At: 1049 [==========>] Loss 0.15666613758381642  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.14835072926213863  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.06443353371771322  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.10651332676320995  - accuracy: 0.90625\n",
      "At: 1053 [==========>] Loss 0.09660715000933431  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.1110921232607252  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.19291232252756302  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.11951991403299403  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.15388013435464615  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.05769983232972694  - accuracy: 0.9375\n",
      "At: 1059 [==========>] Loss 0.0686733414427545  - accuracy: 0.9375\n",
      "At: 1060 [==========>] Loss 0.0784541313886794  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.09692174265474167  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.1583608309041457  - accuracy: 0.71875\n",
      "At: 1063 [==========>] Loss 0.12305739765947935  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.11685249826269128  - accuracy: 0.90625\n",
      "At: 1065 [==========>] Loss 0.08826217607626863  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.10297105060720459  - accuracy: 0.78125\n",
      "At: 1067 [==========>] Loss 0.11545962205726473  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.10926137492737356  - accuracy: 0.875\n",
      "At: 1069 [==========>] Loss 0.13046430063758466  - accuracy: 0.84375\n",
      "At: 1070 [==========>] Loss 0.10626357011801038  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.1303016095851669  - accuracy: 0.78125\n",
      "At: 1072 [==========>] Loss 0.12114189081834693  - accuracy: 0.875\n",
      "At: 1073 [==========>] Loss 0.1589629561691785  - accuracy: 0.84375\n",
      "At: 1074 [==========>] Loss 0.16894666146131204  - accuracy: 0.71875\n",
      "At: 1075 [==========>] Loss 0.12261547314056914  - accuracy: 0.8125\n",
      "At: 1076 [==========>] Loss 0.11573267619304145  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.07109739679805166  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.07467236624055137  - accuracy: 0.9375\n",
      "At: 1079 [==========>] Loss 0.1261243108957868  - accuracy: 0.8125\n",
      "At: 1080 [==========>] Loss 0.13712475532698015  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.10638906873938253  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.11855356936198241  - accuracy: 0.875\n",
      "At: 1083 [==========>] Loss 0.09712552578690858  - accuracy: 0.875\n",
      "At: 1084 [==========>] Loss 0.1144088403391992  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.1271526483771035  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.11482659743609888  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.12693678880305212  - accuracy: 0.90625\n",
      "At: 1088 [==========>] Loss 0.16129798595848313  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.07661542279358147  - accuracy: 0.96875\n",
      "At: 1090 [==========>] Loss 0.08285955400201359  - accuracy: 0.9375\n",
      "At: 1091 [==========>] Loss 0.19582779487417046  - accuracy: 0.71875\n",
      "At: 1092 [==========>] Loss 0.1288365925199486  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.12927455063937032  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.1258987019253559  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.12304438244697147  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.11276958636628022  - accuracy: 0.84375\n",
      "At: 1097 [==========>] Loss 0.07909526615582829  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.12695454356301888  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.14655663777048092  - accuracy: 0.78125\n",
      "At: 1100 [==========>] Loss 0.0824921541261106  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.1096197833378898  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.1266726974289059  - accuracy: 0.78125\n",
      "At: 1103 [==========>] Loss 0.08056280487310319  - accuracy: 0.875\n",
      "At: 1104 [==========>] Loss 0.10170860388009169  - accuracy: 0.875\n",
      "At: 1105 [==========>] Loss 0.08977573351411512  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.04908266080241866  - accuracy: 0.96875\n",
      "At: 1107 [==========>] Loss 0.17809024415897345  - accuracy: 0.71875\n",
      "At: 1108 [==========>] Loss 0.10477365901260903  - accuracy: 0.875\n",
      "At: 1109 [==========>] Loss 0.0759824742535109  - accuracy: 0.875\n",
      "At: 1110 [==========>] Loss 0.09790719845015369  - accuracy: 0.875\n",
      "At: 1111 [==========>] Loss 0.18237944763900535  - accuracy: 0.71875\n",
      "At: 1112 [==========>] Loss 0.14459706899477098  - accuracy: 0.8125\n",
      "At: 1113 [==========>] Loss 0.1261636224423949  - accuracy: 0.8125\n",
      "At: 1114 [==========>] Loss 0.07931166254509668  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.09735667539201834  - accuracy: 0.90625\n",
      "At: 1116 [==========>] Loss 0.12680335118848574  - accuracy: 0.8125\n",
      "At: 1117 [==========>] Loss 0.05560601718228447  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.12132831647605816  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.139204382873803  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.07650992937568667  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.11609233199712087  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.07282393727031947  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.11561781572530871  - accuracy: 0.84375\n",
      "At: 1124 [==========>] Loss 0.10936247897690228  - accuracy: 0.875\n",
      "At: 1125 [==========>] Loss 0.14619021795980408  - accuracy: 0.8125\n",
      "At: 1126 [==========>] Loss 0.09928240928939466  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.12857523218137235  - accuracy: 0.8125\n",
      "At: 1128 [==========>] Loss 0.08797922939512631  - accuracy: 0.875\n",
      "At: 1129 [==========>] Loss 0.1592334823598603  - accuracy: 0.75\n",
      "At: 1130 [==========>] Loss 0.11581094587373195  - accuracy: 0.90625\n",
      "At: 1131 [==========>] Loss 0.09301070055457822  - accuracy: 0.90625\n",
      "At: 1132 [==========>] Loss 0.11840463070267819  - accuracy: 0.8125\n",
      "At: 1133 [==========>] Loss 0.1242476504178597  - accuracy: 0.75\n",
      "At: 1134 [==========>] Loss 0.09872125911359247  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.12405364205589883  - accuracy: 0.84375\n",
      "At: 1136 [==========>] Loss 0.17401727120525884  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.06670260555850518  - accuracy: 0.96875\n",
      "At: 1138 [==========>] Loss 0.09520024362107939  - accuracy: 0.875\n",
      "At: 1139 [==========>] Loss 0.08679179486153757  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.1524766434552923  - accuracy: 0.78125\n",
      "At: 1141 [==========>] Loss 0.1221958499828511  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.11884060564738537  - accuracy: 0.875\n",
      "At: 1143 [==========>] Loss 0.06493601947232491  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.1239855312974928  - accuracy: 0.78125\n",
      "At: 1145 [==========>] Loss 0.14834217329616212  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.11624771733235148  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.20971906014581593  - accuracy: 0.6875\n",
      "At: 1148 [==========>] Loss 0.07493126450460308  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.10001196189932769  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.10216786305769553  - accuracy: 0.84375\n",
      "At: 1151 [==========>] Loss 0.14569823882603108  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.12864458701992573  - accuracy: 0.8125\n",
      "At: 1153 [==========>] Loss 0.16859196117198352  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.10989363287592742  - accuracy: 0.84375\n",
      "At: 1155 [==========>] Loss 0.11821094199316398  - accuracy: 0.8125\n",
      "At: 1156 [==========>] Loss 0.14558177213352708  - accuracy: 0.78125\n",
      "At: 1157 [==========>] Loss 0.12482185624519038  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.17555970159103745  - accuracy: 0.71875\n",
      "At: 1159 [==========>] Loss 0.09299922640417352  - accuracy: 0.9375\n",
      "At: 1160 [==========>] Loss 0.08454567229543253  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.09820217650410529  - accuracy: 0.90625\n",
      "At: 1162 [==========>] Loss 0.120776218118415  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.19734800822623177  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.06952406190268454  - accuracy: 0.9375\n",
      "At: 1165 [==========>] Loss 0.15949066411352703  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.07634926426848174  - accuracy: 0.90625\n",
      "At: 1167 [==========>] Loss 0.13870191781009977  - accuracy: 0.8125\n",
      "At: 1168 [==========>] Loss 0.12823020370815996  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.08133745508579478  - accuracy: 0.9375\n",
      "At: 1170 [==========>] Loss 0.15388266203279466  - accuracy: 0.71875\n",
      "At: 1171 [==========>] Loss 0.08706724220820747  - accuracy: 0.875\n",
      "At: 1172 [==========>] Loss 0.11348081432242423  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.12158937252993246  - accuracy: 0.84375\n",
      "At: 1174 [==========>] Loss 0.16099127823183634  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.13271467447494548  - accuracy: 0.75\n",
      "At: 1176 [==========>] Loss 0.1152710907811165  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.06862059660194768  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.15900313239130676  - accuracy: 0.71875\n",
      "At: 1179 [==========>] Loss 0.13893339948033856  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.21026967463903923  - accuracy: 0.6875\n",
      "At: 1181 [==========>] Loss 0.0887105751956775  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.09157532929349596  - accuracy: 0.84375\n",
      "At: 1183 [==========>] Loss 0.17958094317514367  - accuracy: 0.78125\n",
      "At: 1184 [==========>] Loss 0.16174450960876408  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.10978874507798182  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.13016485295660532  - accuracy: 0.84375\n",
      "At: 1187 [==========>] Loss 0.11670489102331422  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.08499687887427165  - accuracy: 0.90625\n",
      "At: 1189 [==========>] Loss 0.15464181942796257  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.08890682507522299  - accuracy: 0.9375\n",
      "At: 1191 [==========>] Loss 0.16430398082040765  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.07428933902686581  - accuracy: 0.9375\n",
      "At: 1193 [==========>] Loss 0.15282271509428855  - accuracy: 0.71875\n",
      "At: 1194 [==========>] Loss 0.13092372510657038  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.12329265733981645  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.1248433882872622  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.09681743708950019  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.08884966034365675  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.2025087882113789  - accuracy: 0.78125\n",
      "At: 1200 [==========>] Loss 0.053877445905290575  - accuracy: 0.96875\n",
      "At: 1201 [==========>] Loss 0.14014689314024553  - accuracy: 0.78125\n",
      "At: 1202 [==========>] Loss 0.17442679597987304  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.14103076597479064  - accuracy: 0.8125\n",
      "At: 1204 [==========>] Loss 0.06680580807241998  - accuracy: 0.90625\n",
      "At: 1205 [==========>] Loss 0.06929506903156019  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.0938183320261694  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.14315128810646613  - accuracy: 0.8125\n",
      "At: 1208 [==========>] Loss 0.09007527157953177  - accuracy: 0.9375\n",
      "At: 1209 [==========>] Loss 0.1254184707541749  - accuracy: 0.75\n",
      "At: 1210 [==========>] Loss 0.10537750848486485  - accuracy: 0.875\n",
      "At: 1211 [==========>] Loss 0.1609175299789956  - accuracy: 0.84375\n",
      "At: 1212 [==========>] Loss 0.09866365649370173  - accuracy: 0.875\n",
      "At: 1213 [==========>] Loss 0.19893990700945394  - accuracy: 0.71875\n",
      "At: 1214 [==========>] Loss 0.16140350791848482  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.1494534038876959  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.10448418622732902  - accuracy: 0.84375\n",
      "At: 1217 [==========>] Loss 0.08385414957801157  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.08580238200874543  - accuracy: 0.875\n",
      "At: 1219 [==========>] Loss 0.10386613915549799  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.10913321154468344  - accuracy: 0.90625\n",
      "At: 1221 [==========>] Loss 0.09409952476405768  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.14035040879113647  - accuracy: 0.78125\n",
      "At: 1223 [==========>] Loss 0.10296850649957999  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.07747028668126266  - accuracy: 0.9375\n",
      "At: 1225 [==========>] Loss 0.06890945725052901  - accuracy: 0.96875\n",
      "At: 1226 [==========>] Loss 0.09526599571427719  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.16398433157182346  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.1260752497826993  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.12791638302779376  - accuracy: 0.78125\n",
      "At: 1230 [==========>] Loss 0.17417952990279412  - accuracy: 0.75\n",
      "At: 1231 [==========>] Loss 0.14415778655649464  - accuracy: 0.75\n",
      "At: 1232 [==========>] Loss 0.06677729606825544  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.11450008674195139  - accuracy: 0.84375\n",
      "At: 1234 [==========>] Loss 0.12285449097592084  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.04784235025899915  - accuracy: 0.96875\n",
      "At: 1236 [==========>] Loss 0.14191045550217818  - accuracy: 0.8125\n",
      "At: 1237 [==========>] Loss 0.07193382097108586  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.1191578510985042  - accuracy: 0.84375\n",
      "At: 1239 [==========>] Loss 0.177133982568416  - accuracy: 0.71875\n",
      "At: 1240 [==========>] Loss 0.09868654040358561  - accuracy: 0.8125\n",
      "At: 1241 [==========>] Loss 0.13303402998561714  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.14451450770018912  - accuracy: 0.75\n",
      "At: 1243 [==========>] Loss 0.12366339524841738  - accuracy: 0.8125\n",
      "At: 1244 [==========>] Loss 0.1460726714018184  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.10061031187854082  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.0819814625618364  - accuracy: 0.875\n",
      "At: 1247 [==========>] Loss 0.17374853562413095  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.10015211346208616  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.14194038008677834  - accuracy: 0.78125\n",
      "At: 1250 [==========>] Loss 0.12686882858060428  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.1009345145965076  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.08553552955703811  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.11454607629584884  - accuracy: 0.8125\n",
      "At: 1254 [==========>] Loss 0.17694967512935328  - accuracy: 0.6875\n",
      "At: 1255 [==========>] Loss 0.08541087434008521  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.1343498021330058  - accuracy: 0.8125\n",
      "At: 1257 [==========>] Loss 0.13595882017585814  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.08033264163197869  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.1164474206237571  - accuracy: 0.84375\n",
      "At: 1260 [==========>] Loss 0.12570653101664936  - accuracy: 0.84375\n",
      "At: 1261 [==========>] Loss 0.1396558435983291  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.1334593195181243  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.11151597057429774  - accuracy: 0.84375\n",
      "At: 1264 [==========>] Loss 0.08834344533738224  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.11254116404610752  - accuracy: 0.875\n",
      "At: 1266 [==========>] Loss 0.13121969155903498  - accuracy: 0.84375\n",
      "At: 1267 [==========>] Loss 0.12069666573122928  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.12835591081796208  - accuracy: 0.75\n",
      "At: 1269 [==========>] Loss 0.09393628817081698  - accuracy: 0.90625\n",
      "At: 1270 [==========>] Loss 0.12446029278690529  - accuracy: 0.875\n",
      "At: 1271 [==========>] Loss 0.1411885032540801  - accuracy: 0.75\n",
      "At: 1272 [==========>] Loss 0.054286593918349925  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.2051830039776138  - accuracy: 0.75\n",
      "At: 1274 [==========>] Loss 0.12157627594610762  - accuracy: 0.8125\n",
      "At: 1275 [==========>] Loss 0.0769917424579679  - accuracy: 0.90625\n",
      "At: 1276 [==========>] Loss 0.09441963154767932  - accuracy: 0.875\n",
      "At: 1277 [==========>] Loss 0.09147011079801823  - accuracy: 0.875\n",
      "At: 1278 [==========>] Loss 0.1551102672628221  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.08649463670945284  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.11291087831749179  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.1284085361887944  - accuracy: 0.84375\n",
      "At: 1282 [==========>] Loss 0.1310049742558187  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.1271345852403351  - accuracy: 0.8125\n",
      "At: 1284 [==========>] Loss 0.16822084390233943  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.07348109861431576  - accuracy: 0.875\n",
      "At: 1286 [==========>] Loss 0.10128956377684818  - accuracy: 0.90625\n",
      "At: 1287 [==========>] Loss 0.12946727892803261  - accuracy: 0.8125\n",
      "At: 1288 [==========>] Loss 0.15657585449225792  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.0998902397069705  - accuracy: 0.8125\n",
      "At: 1290 [==========>] Loss 0.12789491034333314  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.17984870634179695  - accuracy: 0.71875\n",
      "At: 1292 [==========>] Loss 0.11491501675677508  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.13463633004476078  - accuracy: 0.8125\n",
      "At: 1294 [==========>] Loss 0.16136453954542987  - accuracy: 0.71875\n",
      "At: 1295 [==========>] Loss 0.1634058883465981  - accuracy: 0.84375\n",
      "At: 1296 [==========>] Loss 0.14192256332434633  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.14651057883963747  - accuracy: 0.78125\n",
      "At: 1298 [==========>] Loss 0.10065067304080265  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.1767500874689638  - accuracy: 0.75\n",
      "At: 1300 [==========>] Loss 0.13285047060813976  - accuracy: 0.8125\n",
      "At: 1301 [==========>] Loss 0.1251145123340489  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.07080700539746965  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.08014424336550435  - accuracy: 0.9375\n",
      "At: 1304 [==========>] Loss 0.1062808993458691  - accuracy: 0.84375\n",
      "At: 1305 [==========>] Loss 0.1289527555339084  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.0698533802451736  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.13061343973447104  - accuracy: 0.78125\n",
      "At: 1308 [==========>] Loss 0.06337138910022028  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.16596161209903848  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.13388590431420205  - accuracy: 0.8125\n",
      "At: 1311 [==========>] Loss 0.15590873128190283  - accuracy: 0.75\n",
      "At: 1312 [==========>] Loss 0.08527012650991393  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.16294275891802096  - accuracy: 0.8125\n",
      "At: 1314 [==========>] Loss 0.06631641588440884  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.12418273420863021  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.14031864905201924  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.06744016464798054  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.11448269473752788  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.11326570537772085  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.13397628251547578  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.10631737963034406  - accuracy: 0.84375\n",
      "At: 1322 [==========>] Loss 0.14991054044761112  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.08761138335869545  - accuracy: 0.9375\n",
      "At: 1324 [==========>] Loss 0.15352382566525613  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.105720021096718  - accuracy: 0.8125\n",
      "At: 1326 [==========>] Loss 0.07477332104890849  - accuracy: 0.9375\n",
      "At: 1327 [==========>] Loss 0.15628328089733917  - accuracy: 0.84375\n",
      "At: 1328 [==========>] Loss 0.10819594114835321  - accuracy: 0.8125\n",
      "At: 1329 [==========>] Loss 0.07004063180731947  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.11073397998703603  - accuracy: 0.90625\n",
      "At: 1331 [==========>] Loss 0.16322511605503692  - accuracy: 0.71875\n",
      "At: 1332 [==========>] Loss 0.11017266563293288  - accuracy: 0.84375\n",
      "At: 1333 [==========>] Loss 0.14303555515167454  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.09261406469847372  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.09290800493080212  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.08723819376446455  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.14293075405023287  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.1469278711371314  - accuracy: 0.75\n",
      "At: 1339 [==========>] Loss 0.11426280890724996  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.14954902307754717  - accuracy: 0.875\n",
      "At: 1341 [==========>] Loss 0.09738881902133825  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.13655184949978116  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.18744339484981437  - accuracy: 0.78125\n",
      "At: 1344 [==========>] Loss 0.19850124966560917  - accuracy: 0.6875\n",
      "At: 1345 [==========>] Loss 0.09821888373809792  - accuracy: 0.8125\n",
      "At: 1346 [==========>] Loss 0.10135412437572994  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.10784269545388059  - accuracy: 0.84375\n",
      "At: 1348 [==========>] Loss 0.1017876879851756  - accuracy: 0.8125\n",
      "At: 1349 [==========>] Loss 0.14267087198488315  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.1271107344080192  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.0921770808813019  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.0964092980587193  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.16349061116111313  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.15843795435736274  - accuracy: 0.75\n",
      "At: 1355 [==========>] Loss 0.06262459903723509  - accuracy: 0.9375\n",
      "At: 1356 [==========>] Loss 0.09650269724163682  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.12784216181816954  - accuracy: 0.78125\n",
      "At: 1358 [==========>] Loss 0.12506979579995015  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.05557091101371482  - accuracy: 0.96875\n",
      "At: 1360 [==========>] Loss 0.16217286923754115  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.08522086621027268  - accuracy: 0.9375\n",
      "At: 1362 [==========>] Loss 0.11964394729370123  - accuracy: 0.875\n",
      "At: 1363 [==========>] Loss 0.09400700433468274  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.15285963386256235  - accuracy: 0.75\n",
      "At: 1365 [==========>] Loss 0.14023523814249647  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.13600290735704315  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.10168903591316214  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.14561673109450407  - accuracy: 0.71875\n",
      "At: 1369 [==========>] Loss 0.08522191182480285  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.09018380918987562  - accuracy: 0.9375\n",
      "At: 1371 [==========>] Loss 0.1999988813185632  - accuracy: 0.78125\n",
      "At: 1372 [==========>] Loss 0.08829035399735136  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.11367953413224031  - accuracy: 0.875\n",
      "At: 1374 [==========>] Loss 0.13492319576720874  - accuracy: 0.75\n",
      "At: 1375 [==========>] Loss 0.12083776680203456  - accuracy: 0.84375\n",
      "At: 1376 [==========>] Loss 0.08500894611670881  - accuracy: 0.875\n",
      "At: 1377 [==========>] Loss 0.16244937985870206  - accuracy: 0.8125\n",
      "At: 1378 [==========>] Loss 0.11495168634534395  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.14590010990027186  - accuracy: 0.84375\n",
      "At: 1380 [==========>] Loss 0.14684596031566302  - accuracy: 0.78125\n",
      "At: 1381 [==========>] Loss 0.10594694542001223  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.13004902118858008  - accuracy: 0.84375\n",
      "At: 1383 [==========>] Loss 0.1133649158511154  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.10347148556518218  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.17490890714183227  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.15488972709620913  - accuracy: 0.75\n",
      "At: 1387 [==========>] Loss 0.06503804837462632  - accuracy: 0.9375\n",
      "At: 1388 [==========>] Loss 0.1630189437946804  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.10577614796866802  - accuracy: 0.8125\n",
      "At: 1390 [==========>] Loss 0.12431155457351  - accuracy: 0.90625\n",
      "At: 1391 [==========>] Loss 0.12957352657429339  - accuracy: 0.84375\n",
      "At: 1392 [==========>] Loss 0.08570515765612602  - accuracy: 0.8125\n",
      "At: 1393 [==========>] Loss 0.1293401233208762  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.09362592748221085  - accuracy: 0.90625\n",
      "At: 1395 [==========>] Loss 0.21118160881828518  - accuracy: 0.6875\n",
      "At: 1396 [==========>] Loss 0.06610634573076503  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.1337222506585132  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.11936523174884096  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.10923172346799781  - accuracy: 0.90625\n",
      "At: 1400 [==========>] Loss 0.1290501414862429  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.08852702413266478  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.1440703936367302  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.10784488873832743  - accuracy: 0.84375\n",
      "At: 1404 [==========>] Loss 0.09128814143242817  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.0925956410386261  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.1519953690295609  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.08937436203152252  - accuracy: 0.875\n",
      "At: 1408 [==========>] Loss 0.13738869024532482  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.050245866001707995  - accuracy: 0.90625\n",
      "At: 1410 [==========>] Loss 0.1302861513252295  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.13592776384833338  - accuracy: 0.8125\n",
      "At: 1412 [==========>] Loss 0.12899049365445114  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.09746879436756502  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.16556955070868618  - accuracy: 0.75\n",
      "At: 1415 [==========>] Loss 0.06519638484156556  - accuracy: 0.96875\n",
      "At: 1416 [==========>] Loss 0.1448406708105408  - accuracy: 0.8125\n",
      "At: 1417 [==========>] Loss 0.10435409637393382  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.12841670562314275  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.12090948184239997  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.10004389423136179  - accuracy: 0.84375\n",
      "At: 1421 [==========>] Loss 0.10372805749350986  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.14324093292761994  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.11907108144975369  - accuracy: 0.84375\n",
      "At: 1424 [==========>] Loss 0.14122430667234465  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.08907166307831266  - accuracy: 0.875\n",
      "At: 1426 [==========>] Loss 0.12153607801384725  - accuracy: 0.84375\n",
      "At: 1427 [==========>] Loss 0.12940630290223648  - accuracy: 0.8125\n",
      "At: 1428 [==========>] Loss 0.10638915284354128  - accuracy: 0.84375\n",
      "At: 1429 [==========>] Loss 0.16197963479275906  - accuracy: 0.75\n",
      "At: 1430 [==========>] Loss 0.07484018564332144  - accuracy: 0.9375\n",
      "At: 1431 [==========>] Loss 0.10361497084725363  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.10669339072542042  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.10608562435983546  - accuracy: 0.875\n",
      "At: 1434 [==========>] Loss 0.15057461692887553  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.13596972139100139  - accuracy: 0.78125\n",
      "At: 1436 [==========>] Loss 0.0774819536429856  - accuracy: 0.875\n",
      "At: 1437 [==========>] Loss 0.10566370608927383  - accuracy: 0.9375\n",
      "At: 1438 [==========>] Loss 0.15897917983492582  - accuracy: 0.78125\n",
      "At: 1439 [==========>] Loss 0.10500328115494051  - accuracy: 0.84375\n",
      "At: 1440 [==========>] Loss 0.13100283803481727  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.07147285481471494  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.1068510559294992  - accuracy: 0.84375\n",
      "At: 1443 [==========>] Loss 0.14569731736612473  - accuracy: 0.78125\n",
      "At: 1444 [==========>] Loss 0.12040450618098722  - accuracy: 0.84375\n",
      "At: 1445 [==========>] Loss 0.17285541942043792  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.18828523754736853  - accuracy: 0.65625\n",
      "At: 1447 [==========>] Loss 0.1613895699636389  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.08024128347883325  - accuracy: 0.8125\n",
      "At: 1449 [==========>] Loss 0.1538257786055754  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.12219319679839521  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.11725333978824036  - accuracy: 0.78125\n",
      "At: 1452 [==========>] Loss 0.10776282638068452  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.06109999951865107  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.17799245815403067  - accuracy: 0.84375\n",
      "At: 1455 [==========>] Loss 0.10793018308818668  - accuracy: 0.875\n",
      "At: 1456 [==========>] Loss 0.11115439653600868  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.11068339243173855  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.16574437811524378  - accuracy: 0.71875\n",
      "At: 1459 [==========>] Loss 0.11925261435921672  - accuracy: 0.90625\n",
      "At: 1460 [==========>] Loss 0.20164197573223075  - accuracy: 0.71875\n",
      "At: 1461 [==========>] Loss 0.10798386458601894  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.151179497348025  - accuracy: 0.78125\n",
      "At: 1463 [==========>] Loss 0.1054553232661112  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.17289078910633116  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.11239101984234931  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.07192222903396653  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.1956332174813969  - accuracy: 0.6875\n",
      "At: 1468 [==========>] Loss 0.15110549184533253  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.1737966930196974  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.15658530760111133  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.15350983287798672  - accuracy: 0.78125\n",
      "At: 1472 [==========>] Loss 0.0783544938255791  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.11156534058593448  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.1932635300466371  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.1308303163380582  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.14122412673170742  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.09236537073436724  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.08109679767925225  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.13607857625686748  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.0732592896666841  - accuracy: 0.9375\n",
      "At: 1481 [==========>] Loss 0.13619518613600598  - accuracy: 0.75\n",
      "At: 1482 [==========>] Loss 0.1077449701707549  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.1898058462299438  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.13400767126141022  - accuracy: 0.8125\n",
      "At: 1485 [==========>] Loss 0.15345499966102327  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.08248543272038884  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.06496261325987346  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.14454877024524376  - accuracy: 0.78125\n",
      "At: 1489 [==========>] Loss 0.20616627820580258  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.09702616499951  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.15218941239418962  - accuracy: 0.8125\n",
      "At: 1492 [==========>] Loss 0.15181778341279567  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.18805799516588656  - accuracy: 0.6875\n",
      "At: 1494 [==========>] Loss 0.14632174927654487  - accuracy: 0.84375\n",
      "At: 1495 [==========>] Loss 0.11124831869392382  - accuracy: 0.875\n",
      "At: 1496 [==========>] Loss 0.09456736733807741  - accuracy: 0.90625\n",
      "At: 1497 [==========>] Loss 0.1619734058890453  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.14671283942664926  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.1171528001514226  - accuracy: 0.8125\n",
      "At: 1500 [==========>] Loss 0.0795091188749063  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.08491928510493818  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.14120403999073314  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.11725919191134837  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.12380377102385146  - accuracy: 0.8125\n",
      "At: 1505 [==========>] Loss 0.11049608379015735  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.14352804801071656  - accuracy: 0.78125\n",
      "At: 1507 [==========>] Loss 0.1315125070923031  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.2382069196697686  - accuracy: 0.625\n",
      "At: 1509 [==========>] Loss 0.11177893658140814  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.11226115784639498  - accuracy: 0.875\n",
      "At: 1511 [==========>] Loss 0.12802815534737938  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.0868679755898975  - accuracy: 0.875\n",
      "At: 1513 [==========>] Loss 0.1546685367897862  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.14006610577256284  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.1045522589144031  - accuracy: 0.875\n",
      "At: 1516 [==========>] Loss 0.10384192612495975  - accuracy: 0.84375\n",
      "At: 1517 [==========>] Loss 0.1204106202016527  - accuracy: 0.875\n",
      "At: 1518 [==========>] Loss 0.11921753331898513  - accuracy: 0.8125\n",
      "At: 1519 [==========>] Loss 0.1455020595317689  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.11780818758945502  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.07300904845762471  - accuracy: 0.9375\n",
      "At: 1522 [==========>] Loss 0.1722857457298887  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.11221306110756707  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.13678987613854263  - accuracy: 0.8125\n",
      "At: 1525 [==========>] Loss 0.10708249188805787  - accuracy: 0.90625\n",
      "At: 1526 [==========>] Loss 0.09450995062399277  - accuracy: 0.875\n",
      "At: 1527 [==========>] Loss 0.14381126865356747  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.1383892903694362  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.08665494303391147  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.04875603512507977  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.10304846677519457  - accuracy: 0.9375\n",
      "At: 1532 [==========>] Loss 0.1569661135417024  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.14499022827063543  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.10788201230937934  - accuracy: 0.875\n",
      "At: 1535 [==========>] Loss 0.14573573141937807  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.15970472512279005  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10633685789623709  - accuracy: 0.8125\n",
      "At: 1538 [==========>] Loss 0.1240678208729749  - accuracy: 0.84375\n",
      "At: 1539 [==========>] Loss 0.08616143942660989  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1485276054217719  - accuracy: 0.78125\n",
      "At: 1541 [==========>] Loss 0.13116779542054277  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.0817472921960764  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.13576524497611722  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.15539934083213783  - accuracy: 0.8125\n",
      "At: 1545 [==========>] Loss 0.1986090918091103  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.11966881346092531  - accuracy: 0.8125\n",
      "At: 1547 [==========>] Loss 0.15240506286741337  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.14355906603442514  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.12504696760922152  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.07359028247954782  - accuracy: 0.9375\n",
      "At: 1551 [==========>] Loss 0.17833626526204177  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.09408962085362249  - accuracy: 0.8125\n",
      "At: 1553 [==========>] Loss 0.08527187194824265  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.14835582909410394  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.12995957704140787  - accuracy: 0.8125\n",
      "At: 1556 [==========>] Loss 0.16867736097062083  - accuracy: 0.71875\n",
      "At: 1557 [==========>] Loss 0.08948465590722596  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.12878380085772562  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.09726074130298731  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.09744174590257308  - accuracy: 0.84375\n",
      "At: 1561 [==========>] Loss 0.15559735916992132  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.09504420737712742  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.0985025737733216  - accuracy: 0.9375\n",
      "At: 1564 [==========>] Loss 0.0983073782693773  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.10152447027572717  - accuracy: 0.875\n",
      "At: 1566 [==========>] Loss 0.1709346623603453  - accuracy: 0.78125\n",
      "At: 1567 [==========>] Loss 0.14186301550440633  - accuracy: 0.8125\n",
      "At: 1568 [==========>] Loss 0.09565140427531015  - accuracy: 0.84375\n",
      "At: 1569 [==========>] Loss 0.09725847851984926  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.09195149510925671  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.16590196374151278  - accuracy: 0.75\n",
      "At: 1572 [==========>] Loss 0.14582796982470708  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.04141453323569484  - accuracy: 0.96875\n",
      "At: 1574 [==========>] Loss 0.12616566861235184  - accuracy: 0.875\n",
      "At: 1575 [==========>] Loss 0.11588986735091378  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.1327247915642328  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.07309629950911155  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.07043893428828726  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.10762330231029749  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.11333097984095236  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.09007569120747119  - accuracy: 0.90625\n",
      "At: 1582 [==========>] Loss 0.15510720969274672  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.10047747589591136  - accuracy: 0.90625\n",
      "At: 1584 [==========>] Loss 0.12734715070941854  - accuracy: 0.84375\n",
      "At: 1585 [==========>] Loss 0.1115908075346197  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.12408835406439711  - accuracy: 0.875\n",
      "At: 1587 [==========>] Loss 0.08379815772346114  - accuracy: 0.90625\n",
      "At: 1588 [==========>] Loss 0.12300826093615934  - accuracy: 0.875\n",
      "At: 1589 [==========>] Loss 0.1412275370345782  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.12618658668413194  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.09427000738016297  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.07194984784521094  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.18741959784143702  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.08860062468181736  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.14257913687238397  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.17688499675883773  - accuracy: 0.71875\n",
      "At: 1597 [==========>] Loss 0.15181594098337317  - accuracy: 0.75\n",
      "At: 1598 [==========>] Loss 0.1601787365445307  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.2051305841296946  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.15635292317187702  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.08357522107156101  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.1243578207161872  - accuracy: 0.8125\n",
      "At: 1603 [==========>] Loss 0.18694935753166678  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.22694119083711278  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.0858114121249959  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.10783087970027595  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.1741843098293786  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.15108304987454996  - accuracy: 0.75\n",
      "At: 1609 [==========>] Loss 0.16967463352017095  - accuracy: 0.75\n",
      "At: 1610 [==========>] Loss 0.17420542845852313  - accuracy: 0.84375\n",
      "At: 1611 [==========>] Loss 0.07432914378864079  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.06083976006622341  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.1293889922968443  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.13108969373804547  - accuracy: 0.84375\n",
      "At: 1615 [==========>] Loss 0.08719809574275461  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.14011699492879348  - accuracy: 0.75\n",
      "At: 1617 [==========>] Loss 0.09269924056596471  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11148040201324275  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.19353874241733665  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.11161745064068579  - accuracy: 0.8125\n",
      "At: 1621 [==========>] Loss 0.12429706160052768  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.14611411631245388  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.09166967634881364  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.17274405420819128  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.1532220235891641  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.08995785609666254  - accuracy: 0.875\n",
      "At: 1627 [==========>] Loss 0.10252514645858617  - accuracy: 0.84375\n",
      "At: 1628 [==========>] Loss 0.18119276146886443  - accuracy: 0.75\n",
      "At: 1629 [==========>] Loss 0.1296824482029023  - accuracy: 0.75\n",
      "At: 1630 [==========>] Loss 0.10205412040676953  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.1234870122757473  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.08582241256190125  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.05848483979329387  - accuracy: 0.96875\n",
      "At: 1634 [==========>] Loss 0.11034359089747395  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.1997879054148262  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.12964012305093772  - accuracy: 0.78125\n",
      "At: 1637 [==========>] Loss 0.07935580745207825  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.1676364824715093  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.15913311269088193  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.13143516446249498  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.08792069837424553  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.0939547639157999  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.1099936689730311  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.12356800562745827  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.07714974865372597  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.1270009401800392  - accuracy: 0.875\n",
      "At: 1647 [==========>] Loss 0.16402088076093826  - accuracy: 0.75\n",
      "At: 1648 [==========>] Loss 0.12520832340565427  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.0953440539186699  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.0928808711848394  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.1204585069988734  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.08148024424254777  - accuracy: 0.875\n",
      "At: 1653 [==========>] Loss 0.11429393420757591  - accuracy: 0.8125\n",
      "At: 1654 [==========>] Loss 0.057503506635522304  - accuracy: 0.96875\n",
      "At: 1655 [==========>] Loss 0.15462989610216832  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.12083144581025632  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.1659094019244458  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.2016948404163133  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.07995245323549767  - accuracy: 0.90625\n",
      "At: 1660 [==========>] Loss 0.03542162883976712  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.09403905451193457  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.12918063768287874  - accuracy: 0.8125\n",
      "At: 1663 [==========>] Loss 0.15094559208165892  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.10076513500589168  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.10067001581442127  - accuracy: 0.84375\n",
      "At: 1666 [==========>] Loss 0.10158630975086394  - accuracy: 0.8125\n",
      "At: 1667 [==========>] Loss 0.1538086102254292  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.16250410043678923  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.11471878882964827  - accuracy: 0.84375\n",
      "At: 1670 [==========>] Loss 0.06973394255821724  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.11883973028421699  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.14198192148196176  - accuracy: 0.75\n",
      "At: 1673 [==========>] Loss 0.05520698743681303  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.1796714811095228  - accuracy: 0.75\n",
      "At: 1675 [==========>] Loss 0.17942642595508207  - accuracy: 0.78125\n",
      "At: 1676 [==========>] Loss 0.22261307308751171  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.09739826316482753  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.14857112417677337  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.09346078610693853  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.1542111588293949  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.13359866467200343  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.09673011860390784  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.18937276285037727  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.1073568757075565  - accuracy: 0.875\n",
      "At: 1685 [==========>] Loss 0.12169851063779125  - accuracy: 0.8125\n",
      "At: 1686 [==========>] Loss 0.1142666721934186  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.1702769500967029  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.05066217369028039  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.11703442490884197  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.1130887352396809  - accuracy: 0.875\n",
      "At: 1691 [==========>] Loss 0.10002044799692783  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.16046159836203183  - accuracy: 0.78125\n",
      "At: 1693 [==========>] Loss 0.10170404620421195  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.08625788484450339  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.13286431319349395  - accuracy: 0.75\n",
      "At: 1696 [==========>] Loss 0.1786074614504208  - accuracy: 0.71875\n",
      "At: 1697 [==========>] Loss 0.09856758782497267  - accuracy: 0.84375\n",
      "At: 1698 [==========>] Loss 0.083055363929177  - accuracy: 0.8125\n",
      "At: 1699 [==========>] Loss 0.09972011053775581  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.10345574035606175  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.10175982181287692  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.10078438325059341  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.2020506777105422  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.0696870286812964  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.12318389971989416  - accuracy: 0.875\n",
      "At: 1706 [==========>] Loss 0.15688236151211685  - accuracy: 0.84375\n",
      "At: 1707 [==========>] Loss 0.19292458675858493  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.1011976897525287  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.17389217922402972  - accuracy: 0.71875\n",
      "At: 1710 [==========>] Loss 0.14833839163887375  - accuracy: 0.6875\n",
      "At: 1711 [==========>] Loss 0.08111220576456807  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.11577230350466591  - accuracy: 0.875\n",
      "At: 1713 [==========>] Loss 0.10969457933725105  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.15868317877221289  - accuracy: 0.78125\n",
      "At: 1715 [==========>] Loss 0.10874813784818058  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.05426392361374482  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08213090445022642  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.12837268518864076  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.10360700966972256  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.06227609532840786  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.15859709658883747  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.061544592705376655  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.2201926557203902  - accuracy: 0.625\n",
      "At: 1724 [==========>] Loss 0.09180478134395889  - accuracy: 0.84375\n",
      "At: 1725 [==========>] Loss 0.14980447495559066  - accuracy: 0.78125\n",
      "At: 1726 [==========>] Loss 0.11622472169550457  - accuracy: 0.84375\n",
      "At: 1727 [==========>] Loss 0.14674137703728873  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.09670652619350119  - accuracy: 0.84375\n",
      "At: 1729 [==========>] Loss 0.17264745186023708  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.12666878385344765  - accuracy: 0.84375\n",
      "At: 1731 [==========>] Loss 0.13090699451713717  - accuracy: 0.8125\n",
      "At: 1732 [==========>] Loss 0.08093356507308655  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.1694103628569189  - accuracy: 0.71875\n",
      "At: 1734 [==========>] Loss 0.11710449049053481  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.1566819993300511  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.09954017915790657  - accuracy: 0.8125\n",
      "At: 1737 [==========>] Loss 0.15231931865784964  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.11480180356154131  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.11114050609313639  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.123683163010533  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.12780360673046082  - accuracy: 0.875\n",
      "At: 1742 [==========>] Loss 0.05596473197566451  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.15564520132955809  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.08345128063864823  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.1117896511645982  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.15973048122175337  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.1004702319311572  - accuracy: 0.875\n",
      "At: 1748 [==========>] Loss 0.13592546782355042  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.10384804282436659  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.10617537727378833  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.14730663330213645  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.1123174458822466  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.08923420453738459  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.1095586894535597  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.05565904385131608  - accuracy: 0.96875\n",
      "At: 1756 [==========>] Loss 0.15168508165493197  - accuracy: 0.75\n",
      "At: 1757 [==========>] Loss 0.14413164912580637  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.0753241796643403  - accuracy: 0.84375\n",
      "At: 1759 [==========>] Loss 0.09309235164163548  - accuracy: 0.84375\n",
      "At: 1760 [==========>] Loss 0.08628890640122941  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.08973640526833701  - accuracy: 0.84375\n",
      "At: 1762 [==========>] Loss 0.152175755631199  - accuracy: 0.78125\n",
      "At: 1763 [==========>] Loss 0.09510012296138722  - accuracy: 0.875\n",
      "At: 1764 [==========>] Loss 0.13731934397224102  - accuracy: 0.84375\n",
      "At: 1765 [==========>] Loss 0.12771721698542596  - accuracy: 0.8125\n",
      "At: 1766 [==========>] Loss 0.06760008835049433  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.08529419814388162  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.11304348055348573  - accuracy: 0.8125\n",
      "At: 1769 [==========>] Loss 0.07063794783102917  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.07853809277961482  - accuracy: 0.90625\n",
      "At: 1771 [==========>] Loss 0.17119062817320502  - accuracy: 0.6875\n",
      "At: 1772 [==========>] Loss 0.12373553728736797  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.11565843551918303  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.13691546617616257  - accuracy: 0.84375\n",
      "At: 1775 [==========>] Loss 0.1181560915355566  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.09646951073368823  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.10976638414773673  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.09788938826253114  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.0793279218102474  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.11725688515528979  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.19012468031452606  - accuracy: 0.6875\n",
      "At: 1782 [==========>] Loss 0.08898248991614213  - accuracy: 0.875\n",
      "At: 1783 [==========>] Loss 0.1458714771355181  - accuracy: 0.78125\n",
      "At: 1784 [==========>] Loss 0.07731172594741632  - accuracy: 0.875\n",
      "At: 1785 [==========>] Loss 0.10488743620825189  - accuracy: 0.8125\n",
      "At: 1786 [==========>] Loss 0.1260360083250432  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.1323335375855481  - accuracy: 0.84375\n",
      "At: 1788 [==========>] Loss 0.09382329412966411  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.11235818713154477  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.1493710436550702  - accuracy: 0.78125\n",
      "At: 1791 [==========>] Loss 0.07612918940116717  - accuracy: 0.875\n",
      "At: 1792 [==========>] Loss 0.12105262471557202  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.10240144427892652  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.14547025596972607  - accuracy: 0.78125\n",
      "At: 1795 [==========>] Loss 0.07602578686556494  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.14580140576838543  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.09814368720313324  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.1108435765505277  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.07902032196544323  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.1260943311570208  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.1801158899476989  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.13564888445520068  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.18503975853759977  - accuracy: 0.71875\n",
      "At: 1804 [==========>] Loss 0.12795063641886287  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.04254033801438513  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.16154361316588117  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.18351392958849158  - accuracy: 0.71875\n",
      "At: 1808 [==========>] Loss 0.153706238762017  - accuracy: 0.84375\n",
      "At: 1809 [==========>] Loss 0.08480949616102462  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.15567795475750598  - accuracy: 0.84375\n",
      "At: 1811 [==========>] Loss 0.1301498622813998  - accuracy: 0.875\n",
      "At: 1812 [==========>] Loss 0.11442429301131023  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.12049259800510356  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.11407456854090521  - accuracy: 0.875\n",
      "At: 1815 [==========>] Loss 0.15943752381126297  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.04726313978795627  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.1560535642346975  - accuracy: 0.71875\n",
      "At: 1818 [==========>] Loss 0.13733823932130595  - accuracy: 0.78125\n",
      "At: 1819 [==========>] Loss 0.17795319888409616  - accuracy: 0.71875\n",
      "At: 1820 [==========>] Loss 0.10362948229921906  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.10237959081039177  - accuracy: 0.90625\n",
      "At: 1822 [==========>] Loss 0.1343268429257218  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.17825995076459195  - accuracy: 0.6875\n",
      "At: 1824 [==========>] Loss 0.15321351920432472  - accuracy: 0.8125\n",
      "At: 1825 [==========>] Loss 0.17075882638314432  - accuracy: 0.71875\n",
      "At: 1826 [==========>] Loss 0.07086262426235718  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.09920290239894097  - accuracy: 0.90625\n",
      "At: 1828 [==========>] Loss 0.15968296561989045  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.1594344006411153  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.13485261767721582  - accuracy: 0.8125\n",
      "At: 1831 [==========>] Loss 0.12015417123888525  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.12785647109309242  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.11340616582545945  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.0770553170508945  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.13737901682635106  - accuracy: 0.84375\n",
      "At: 1836 [==========>] Loss 0.10770965008780199  - accuracy: 0.90625\n",
      "At: 1837 [==========>] Loss 0.04511311239962702  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.08699165692615124  - accuracy: 0.875\n",
      "At: 1839 [==========>] Loss 0.08288796275479346  - accuracy: 0.84375\n",
      "At: 1840 [==========>] Loss 0.13328403754601828  - accuracy: 0.8125\n",
      "At: 1841 [==========>] Loss 0.0944683650343561  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.1517266680960573  - accuracy: 0.8125\n",
      "At: 1843 [==========>] Loss 0.08006074962399579  - accuracy: 0.90625\n",
      "At: 1844 [==========>] Loss 0.12523802476178997  - accuracy: 0.84375\n",
      "At: 1845 [==========>] Loss 0.17289198408055578  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.1382707841342728  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05808658751030754  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.05506468475299848  - accuracy: 0.9375\n",
      "At: 1849 [==========>] Loss 0.17306078194418426  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.03982263703651785  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.15973252664438875  - accuracy: 0.8125\n",
      "At: 1852 [==========>] Loss 0.07671331601900798  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.12111191560731628  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.12817764178746566  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.17238271323865484  - accuracy: 0.75\n",
      "At: 1856 [==========>] Loss 0.1340902739872913  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.1627617971417445  - accuracy: 0.78125\n",
      "At: 1858 [==========>] Loss 0.10238763701890599  - accuracy: 0.90625\n",
      "At: 1859 [==========>] Loss 0.15319264486274586  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.1313165883567325  - accuracy: 0.84375\n",
      "At: 1861 [==========>] Loss 0.09788473892052899  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.19274387185367498  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.13980963752771075  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.15467210467517806  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.09327870086386539  - accuracy: 0.8125\n",
      "At: 1866 [==========>] Loss 0.1996995351074687  - accuracy: 0.59375\n",
      "At: 1867 [==========>] Loss 0.11405633415789754  - accuracy: 0.90625\n",
      "At: 1868 [==========>] Loss 0.14114108329443112  - accuracy: 0.8125\n",
      "At: 1869 [==========>] Loss 0.18936512004696043  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.13076253590685735  - accuracy: 0.8125\n",
      "At: 1871 [==========>] Loss 0.13940608628595327  - accuracy: 0.8125\n",
      "At: 1872 [==========>] Loss 0.14502106898170142  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.11758172400777335  - accuracy: 0.78125\n",
      "At: 1874 [==========>] Loss 0.13419393462791482  - accuracy: 0.875\n",
      "At: 1875 [==========>] Loss 0.09875296545906231  - accuracy: 0.84375\n",
      "At: 1876 [==========>] Loss 0.2074692528144838  - accuracy: 0.71875\n",
      "At: 1877 [==========>] Loss 0.08636056881916948  - accuracy: 0.90625\n",
      "At: 1878 [==========>] Loss 0.12137321860121107  - accuracy: 0.8125\n",
      "At: 1879 [==========>] Loss 0.12021566499448468  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.08927278627627885  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.11600790812864804  - accuracy: 0.8125\n",
      "At: 1882 [==========>] Loss 0.10299216843299594  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.1431748684060699  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.09943089506310776  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.11783000383163694  - accuracy: 0.78125\n",
      "At: 1886 [==========>] Loss 0.15222658055450194  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.08391845432660489  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.12974966329633572  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.09972006719354129  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.18461947836811818  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.06709778164160216  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.0918058960143143  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.08141830068501874  - accuracy: 0.9375\n",
      "At: 1894 [==========>] Loss 0.09204200127935591  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.07103074640769466  - accuracy: 0.90625\n",
      "At: 1896 [==========>] Loss 0.10886612376417827  - accuracy: 0.875\n",
      "At: 1897 [==========>] Loss 0.06966950107148552  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.10140044432210374  - accuracy: 0.84375\n",
      "At: 1899 [==========>] Loss 0.07047660761558147  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.15136473047493346  - accuracy: 0.78125\n",
      "At: 1901 [==========>] Loss 0.10205681474091824  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.13974740052882856  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.12071667212832995  - accuracy: 0.84375\n",
      "At: 1904 [==========>] Loss 0.07506949306458871  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.142248197422584  - accuracy: 0.71875\n",
      "At: 1906 [==========>] Loss 0.10101281774098032  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.0795305085980828  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.10178165731444389  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.0928515354073037  - accuracy: 0.90625\n",
      "At: 1910 [==========>] Loss 0.05369245807258982  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.1392262951597924  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.12315410945140831  - accuracy: 0.78125\n",
      "At: 1913 [==========>] Loss 0.15313190524667128  - accuracy: 0.8125\n",
      "At: 1914 [==========>] Loss 0.0926918129870451  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.12152232757986846  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.1474113403054919  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.17175432993748668  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.15247826914249157  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.092285883791122  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.11557278971581685  - accuracy: 0.8125\n",
      "At: 1921 [==========>] Loss 0.14914280051497225  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.1511389145092095  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.18688979351800097  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.14083069119919822  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.13992768860012963  - accuracy: 0.78125\n",
      "At: 1926 [==========>] Loss 0.11099952489470433  - accuracy: 0.8125\n",
      "At: 1927 [==========>] Loss 0.07411998949835757  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12206869107886749  - accuracy: 0.8125\n",
      "At: 1929 [==========>] Loss 0.18515777095962233  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.16110211772204913  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.10440840757085815  - accuracy: 0.875\n",
      "At: 1932 [==========>] Loss 0.15073993348407225  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.09676939911968585  - accuracy: 0.8125\n",
      "At: 1934 [==========>] Loss 0.15839623180432102  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.13742846191625674  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.10580061326430927  - accuracy: 0.78125\n",
      "At: 1937 [==========>] Loss 0.1415851957027892  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.1608271702411856  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.11563583017876117  - accuracy: 0.8125\n",
      "At: 1940 [==========>] Loss 0.12502714691885308  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.11179204358180225  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.1332216459272434  - accuracy: 0.8125\n",
      "At: 1943 [==========>] Loss 0.12490712453406294  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.11133289516054162  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.16477278844017157  - accuracy: 0.75\n",
      "At: 1946 [==========>] Loss 0.0969019707678371  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.10620867814013674  - accuracy: 0.84375\n",
      "At: 1948 [==========>] Loss 0.11088532253865203  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.0739165071278694  - accuracy: 0.96875\n",
      "At: 1950 [==========>] Loss 0.14583384453124526  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.14603816103477724  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.10856573822511863  - accuracy: 0.78125\n",
      "At: 1953 [==========>] Loss 0.06817940389346634  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.21091966500953657  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.06348356575362188  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.10932917691954648  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.0782981789886257  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.08262046824167962  - accuracy: 0.90625\n",
      "At: 1959 [==========>] Loss 0.10525940292793391  - accuracy: 0.84375\n",
      "At: 1960 [==========>] Loss 0.07020721967682345  - accuracy: 0.84375\n",
      "At: 1961 [==========>] Loss 0.18473191042343726  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.17266147969025458  - accuracy: 0.78125\n",
      "At: 1963 [==========>] Loss 0.07039589990460307  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.17524794182462494  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.14414655944688892  - accuracy: 0.84375\n",
      "At: 1966 [==========>] Loss 0.1130348514456663  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.1325394343460139  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.18199943548195427  - accuracy: 0.65625\n",
      "At: 1969 [==========>] Loss 0.1621510230301197  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.11665940212308012  - accuracy: 0.84375\n",
      "At: 1971 [==========>] Loss 0.23685570098542036  - accuracy: 0.65625\n",
      "At: 1972 [==========>] Loss 0.07540224759137894  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.11916683773708703  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.129519081662967  - accuracy: 0.78125\n",
      "At: 1975 [==========>] Loss 0.16163866751433942  - accuracy: 0.8125\n",
      "At: 1976 [==========>] Loss 0.05756549094659082  - accuracy: 0.9375\n",
      "At: 1977 [==========>] Loss 0.09772737416946028  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.12754613111435167  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.11696148868503023  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.11701432081560167  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.16615126320046125  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06218530727687732  - accuracy: 0.96875\n",
      "At: 1983 [==========>] Loss 0.14670898263866522  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.09718678648740249  - accuracy: 0.84375\n",
      "At: 1985 [==========>] Loss 0.14066034213826234  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.18452408618122806  - accuracy: 0.71875\n",
      "At: 1987 [==========>] Loss 0.1133766302858723  - accuracy: 0.875\n",
      "At: 1988 [==========>] Loss 0.09216798247015655  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.11049232343356163  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.10132871060918225  - accuracy: 0.90625\n",
      "At: 1991 [==========>] Loss 0.13544740459449262  - accuracy: 0.78125\n",
      "At: 1992 [==========>] Loss 0.10344035401831833  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.14543671857516982  - accuracy: 0.75\n",
      "At: 1994 [==========>] Loss 0.09081146292988675  - accuracy: 0.90625\n",
      "At: 1995 [==========>] Loss 0.19167409715951272  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.1109776134018639  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.23040336544626222  - accuracy: 0.65625\n",
      "At: 1998 [==========>] Loss 0.16876115625036642  - accuracy: 0.71875\n",
      "At: 1999 [==========>] Loss 0.08622173199461122  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.14325649301443216  - accuracy: 0.75\n",
      "At: 2001 [==========>] Loss 0.07148740398701788  - accuracy: 0.9375\n",
      "At: 2002 [==========>] Loss 0.08454444625795568  - accuracy: 0.90625\n",
      "At: 2003 [==========>] Loss 0.13386181850062084  - accuracy: 0.78125\n",
      "At: 2004 [==========>] Loss 0.1375287563927423  - accuracy: 0.90625\n",
      "At: 2005 [==========>] Loss 0.11484438082114913  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.12051585377724153  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.10729028593601384  - accuracy: 0.8125\n",
      "At: 2008 [==========>] Loss 0.14034180183517445  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13965575916713074  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.11591193263107219  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.11694271884174046  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.1200705059780882  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.07896688004113112  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.23390059394654378  - accuracy: 0.59375\n",
      "At: 2015 [==========>] Loss 0.07799417871899446  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.10567993211333954  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.0636308783042756  - accuracy: 0.9375\n",
      "At: 2018 [==========>] Loss 0.09033806100828126  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.13684825011000865  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.06727299175481144  - accuracy: 0.9375\n",
      "At: 2021 [==========>] Loss 0.10137493530914382  - accuracy: 0.90625\n",
      "At: 2022 [==========>] Loss 0.13451661258400402  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.08192245870520729  - accuracy: 0.84375\n",
      "At: 2024 [==========>] Loss 0.10775853016753026  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.16151371406285892  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.10846793715234232  - accuracy: 0.84375\n",
      "At: 2027 [==========>] Loss 0.15965633696364512  - accuracy: 0.75\n",
      "At: 2028 [==========>] Loss 0.13630990293728062  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.12369307060109219  - accuracy: 0.78125\n",
      "At: 2030 [==========>] Loss 0.14448882014850112  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.15956966535113348  - accuracy: 0.71875\n",
      "At: 2032 [==========>] Loss 0.14938867816169799  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.16631091788129015  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.21225499730608827  - accuracy: 0.65625\n",
      "At: 2035 [==========>] Loss 0.11873489767368571  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09466337462640699  - accuracy: 0.9375\n",
      "At: 2037 [==========>] Loss 0.14223950754027898  - accuracy: 0.78125\n",
      "At: 2038 [==========>] Loss 0.12077297327929332  - accuracy: 0.75\n",
      "At: 2039 [==========>] Loss 0.0959562315310834  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.08645722696732844  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.05643137087081586  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.12361632875701584  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.1193955236428914  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.0958872915774584  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.22622045249219316  - accuracy: 0.71875\n",
      "At: 2046 [==========>] Loss 0.06959177275949674  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.10743830553022726  - accuracy: 0.84375\n",
      "At: 2048 [==========>] Loss 0.09719079299314343  - accuracy: 0.90625\n",
      "At: 2049 [==========>] Loss 0.1438957075356599  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.12862379458508  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1876446889173518  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.0650650645973922  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.10098526994172277  - accuracy: 0.875\n",
      "At: 2054 [==========>] Loss 0.11544825221208775  - accuracy: 0.84375\n",
      "At: 2055 [==========>] Loss 0.0597711887052453  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.12044837588933437  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.14768984938743346  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.12749051642004033  - accuracy: 0.78125\n",
      "At: 2059 [==========>] Loss 0.17821409304003916  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.13204636559396685  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.14327641908218455  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.15679601615979707  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.08896664556553929  - accuracy: 0.90625\n",
      "At: 2064 [==========>] Loss 0.15209388021105014  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.0380741874431142  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.16966032262096495  - accuracy: 0.71875\n",
      "At: 2067 [==========>] Loss 0.09826058348335745  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.10727141433321083  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.11777162294593368  - accuracy: 0.8125\n",
      "At: 2070 [==========>] Loss 0.15308660782040778  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.11991342840543749  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.0765488148151161  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.1079430642157077  - accuracy: 0.8125\n",
      "At: 2074 [==========>] Loss 0.11425027148613917  - accuracy: 0.84375\n",
      "At: 2075 [==========>] Loss 0.12282436651828163  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.12779981055486628  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.15148392190766152  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.09933420910461578  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.06689844033873113  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.1077236123784044  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.12027535309113203  - accuracy: 0.84375\n",
      "At: 2082 [==========>] Loss 0.11754916374207512  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.20396767436747879  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.09464761162806623  - accuracy: 0.90625\n",
      "At: 2085 [==========>] Loss 0.10153629476856235  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.08068699697695711  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.14661795428379443  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.10648951958523113  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.12801030435453736  - accuracy: 0.78125\n",
      "At: 2090 [==========>] Loss 0.08943194657388209  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.11883584234543162  - accuracy: 0.8125\n",
      "At: 2092 [==========>] Loss 0.08476233384230125  - accuracy: 0.875\n",
      "At: 2093 [==========>] Loss 0.14120673035131154  - accuracy: 0.8125\n",
      "At: 2094 [==========>] Loss 0.1284044952660168  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.12353295542391213  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.15576641388095766  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.10199548639130203  - accuracy: 0.90625\n",
      "At: 2098 [==========>] Loss 0.12240431704685031  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.11957660365947083  - accuracy: 0.875\n",
      "At: 2100 [==========>] Loss 0.05787167177618116  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.1366140454801575  - accuracy: 0.875\n",
      "At: 2102 [==========>] Loss 0.07883238598201447  - accuracy: 0.90625\n",
      "At: 2103 [==========>] Loss 0.14967313693624723  - accuracy: 0.75\n",
      "At: 2104 [==========>] Loss 0.12689040280108238  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.14567113606399915  - accuracy: 0.78125\n",
      "At: 2106 [==========>] Loss 0.15266088066677644  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.10283314529627473  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.15048901586091054  - accuracy: 0.78125\n",
      "At: 2109 [==========>] Loss 0.1225364438429967  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.06781723442724227  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.13386108988536932  - accuracy: 0.8125\n",
      "At: 2112 [==========>] Loss 0.11345289150756946  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.08364636814491605  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.14074894634142032  - accuracy: 0.8125\n",
      "At: 2115 [==========>] Loss 0.10666220882755992  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.1117585689179013  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.14114115074494374  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.13789154200218745  - accuracy: 0.8125\n",
      "At: 2119 [==========>] Loss 0.07239846082957173  - accuracy: 0.90625\n",
      "At: 2120 [==========>] Loss 0.14235516367352574  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.12561992355409632  - accuracy: 0.84375\n",
      "At: 2122 [==========>] Loss 0.14323976085627135  - accuracy: 0.75\n",
      "At: 2123 [==========>] Loss 0.20480870028538906  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.12725762109473648  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10863434871463315  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.0546857593687568  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.10692908609564933  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.10196287433250142  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.14240360095988744  - accuracy: 0.875\n",
      "At: 2130 [==========>] Loss 0.04651460931784856  - accuracy: 0.9375\n",
      "At: 2131 [==========>] Loss 0.1081286729863834  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.23491242764595055  - accuracy: 0.65625\n",
      "At: 2133 [==========>] Loss 0.1505323517149066  - accuracy: 0.84375\n",
      "At: 2134 [==========>] Loss 0.11901975116934488  - accuracy: 0.84375\n",
      "At: 2135 [==========>] Loss 0.09624811824814233  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.11971092063789672  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.12242912506459969  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.12171618348879801  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.163069510451748  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.10113421330501857  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.12596209757071886  - accuracy: 0.78125\n",
      "At: 2142 [==========>] Loss 0.12331435343737444  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.08205975185438952  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.07359163942512822  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.10247494434845436  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.15194704673519868  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.12639000656342936  - accuracy: 0.8125\n",
      "At: 2148 [==========>] Loss 0.17596273788302097  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.11335162796252349  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.10284698811404892  - accuracy: 0.90625\n",
      "At: 2151 [==========>] Loss 0.09418719139559424  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.19131749166036133  - accuracy: 0.75\n",
      "At: 2153 [==========>] Loss 0.17185699107158459  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.13335627525126117  - accuracy: 0.84375\n",
      "At: 2155 [==========>] Loss 0.13554503130305176  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.1219380532702734  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.11758978324371827  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.15405120914557163  - accuracy: 0.78125\n",
      "At: 2159 [==========>] Loss 0.05578702393445446  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.14572302705351986  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.10604629730389221  - accuracy: 0.84375\n",
      "At: 2162 [==========>] Loss 0.09989615550676345  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.12610545561461828  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.18694616535982786  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.10189943137972707  - accuracy: 0.84375\n",
      "At: 2166 [==========>] Loss 0.10140907194128992  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.09122099310748694  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.07168659245201728  - accuracy: 0.90625\n",
      "At: 2169 [==========>] Loss 0.10308635579334409  - accuracy: 0.875\n",
      "At: 2170 [==========>] Loss 0.11628701731773564  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.15199558837851881  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.09051018252273527  - accuracy: 0.90625\n",
      "At: 2173 [==========>] Loss 0.11965829004543255  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.11873289815927782  - accuracy: 0.8125\n",
      "At: 2175 [==========>] Loss 0.14384160344026858  - accuracy: 0.8125\n",
      "At: 2176 [==========>] Loss 0.13657764888466997  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.14406205638801844  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.10307546874510001  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.10997830763068354  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.08073814623641923  - accuracy: 0.9375\n",
      "At: 2181 [==========>] Loss 0.1539779793409776  - accuracy: 0.78125\n",
      "At: 2182 [==========>] Loss 0.10038590306969009  - accuracy: 0.84375\n",
      "At: 2183 [==========>] Loss 0.2118073467493042  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.07106108615975146  - accuracy: 0.9375\n",
      "At: 2185 [==========>] Loss 0.12346627779974473  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.1631308386497782  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.16732675990560514  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.080937982371616  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.060400843823930114  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.1120455304743952  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.13491534515516748  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.10639075880971993  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.15591623513306355  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.10474794284492717  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.16570957023417981  - accuracy: 0.6875\n",
      "At: 2196 [==========>] Loss 0.13597826768174193  - accuracy: 0.8125\n",
      "At: 2197 [==========>] Loss 0.07901913683737338  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.09242671478448587  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.04992553675667304  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.051938072611572926  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.09415951547335805  - accuracy: 0.90625\n",
      "At: 2202 [==========>] Loss 0.09079479418099537  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.10248176181084194  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.15677394431771224  - accuracy: 0.75\n",
      "At: 2205 [==========>] Loss 0.1087195131387399  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.09631111298300446  - accuracy: 0.84375\n",
      "At: 2207 [==========>] Loss 0.09208258157238845  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.17508348311577204  - accuracy: 0.78125\n",
      "At: 2209 [==========>] Loss 0.20071699364239312  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.11727155666315037  - accuracy: 0.875\n",
      "At: 2211 [==========>] Loss 0.12910927258126692  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.0742792297300067  - accuracy: 0.90625\n",
      "At: 2213 [==========>] Loss 0.12765809755603916  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.10397862094891742  - accuracy: 0.875\n",
      "At: 2215 [==========>] Loss 0.11570724870560564  - accuracy: 0.875\n",
      "At: 2216 [==========>] Loss 0.12581432097496004  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.11920137818576654  - accuracy: 0.78125\n",
      "At: 2218 [==========>] Loss 0.1514042332521448  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.06775570835912713  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.10777768679820168  - accuracy: 0.875\n",
      "At: 2221 [==========>] Loss 0.16082664143222636  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.11273215065325498  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.17441098652597858  - accuracy: 0.6875\n",
      "At: 2224 [==========>] Loss 0.12169377995469158  - accuracy: 0.875\n",
      "At: 2225 [==========>] Loss 0.14761896356937518  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.12760070782832222  - accuracy: 0.78125\n",
      "At: 2227 [==========>] Loss 0.1914626176143845  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.08908089618647153  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.18370457185774008  - accuracy: 0.71875\n",
      "At: 2230 [==========>] Loss 0.10456808384440786  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.17031128607501328  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.16986778801804708  - accuracy: 0.75\n",
      "At: 2233 [==========>] Loss 0.15919942461386444  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.14123231345994108  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.12047993022647885  - accuracy: 0.84375\n",
      "At: 2236 [==========>] Loss 0.08214687488050909  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.11282708718894524  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.14608347084142964  - accuracy: 0.84375\n",
      "At: 2239 [==========>] Loss 0.16943150731723552  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.16124435684867527  - accuracy: 0.8125\n",
      "At: 2241 [==========>] Loss 0.14014235195966407  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.15753961282502454  - accuracy: 0.78125\n",
      "At: 2243 [==========>] Loss 0.06562449679162947  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.08344159474446228  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.08535774664057552  - accuracy: 0.84375\n",
      "At: 2246 [==========>] Loss 0.13868209960567118  - accuracy: 0.84375\n",
      "At: 2247 [==========>] Loss 0.10774223221851759  - accuracy: 0.875\n",
      "At: 2248 [==========>] Loss 0.16231765656780756  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.11264234644030319  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.07099521139037011  - accuracy: 0.9375\n",
      "At: 2251 [==========>] Loss 0.07316855867433059  - accuracy: 0.90625\n",
      "At: 2252 [==========>] Loss 0.12254964740265398  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.11026540722814443  - accuracy: 0.84375\n",
      "At: 2254 [==========>] Loss 0.1243653911818333  - accuracy: 0.75\n",
      "At: 2255 [==========>] Loss 0.1442995078638259  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.1464503911938129  - accuracy: 0.71875\n",
      "At: 2257 [==========>] Loss 0.10537022883614362  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.14185434464433738  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.1265952587012322  - accuracy: 0.78125\n",
      "At: 2260 [==========>] Loss 0.16226087977572795  - accuracy: 0.75\n",
      "At: 2261 [==========>] Loss 0.09242225502750384  - accuracy: 0.9375\n",
      "At: 2262 [==========>] Loss 0.1473907602151665  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.13426031678762843  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.09606694803285233  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.11266550278461257  - accuracy: 0.8125\n",
      "At: 2266 [==========>] Loss 0.12366405549227681  - accuracy: 0.8125\n",
      "At: 2267 [==========>] Loss 0.0704010608188485  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.09054390733582597  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.0381437611205228  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.11422153111057151  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.14300364384715034  - accuracy: 0.78125\n",
      "At: 2272 [==========>] Loss 0.07919728083880921  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.10941161591437253  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.0836455045088563  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.07807764051918123  - accuracy: 0.90625\n",
      "At: 2276 [==========>] Loss 0.07567444354575076  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.16696341223462935  - accuracy: 0.78125\n",
      "At: 2278 [==========>] Loss 0.08807401514245547  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.14728519625100311  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.11378522672127152  - accuracy: 0.8125\n",
      "At: 2281 [==========>] Loss 0.11149889215061562  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.07313487800598917  - accuracy: 0.90625\n",
      "At: 2283 [==========>] Loss 0.14557655485378146  - accuracy: 0.8125\n",
      "At: 2284 [==========>] Loss 0.12404715267136954  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.10763969328930525  - accuracy: 0.84375\n",
      "At: 2286 [==========>] Loss 0.1435529962048066  - accuracy: 0.78125\n",
      "At: 2287 [==========>] Loss 0.1293516202547987  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.0896159687749623  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.13887706958989782  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.06439913002516619  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.12010879568462962  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.08470451957477634  - accuracy: 0.875\n",
      "At: 2293 [==========>] Loss 0.05929439738021495  - accuracy: 0.875\n",
      "At: 2294 [==========>] Loss 0.055526272745183236  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.1622103667520053  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.1731348525821998  - accuracy: 0.71875\n",
      "At: 2297 [==========>] Loss 0.07445497403277701  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.10858793904381764  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.1169386954955293  - accuracy: 0.78125\n",
      "At: 2300 [==========>] Loss 0.1432846514305422  - accuracy: 0.8125\n",
      "At: 2301 [==========>] Loss 0.20524661802604743  - accuracy: 0.71875\n",
      "At: 2302 [==========>] Loss 0.14583906279445052  - accuracy: 0.78125\n",
      "At: 2303 [==========>] Loss 0.08302927447972487  - accuracy: 0.875\n",
      "At: 2304 [==========>] Loss 0.08788581106061523  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.08426201555108839  - accuracy: 0.90625\n",
      "At: 2306 [==========>] Loss 0.1203497521526026  - accuracy: 0.875\n",
      "At: 2307 [==========>] Loss 0.17502233994716587  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.1844841856722988  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.14113837771095872  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.15593347113639255  - accuracy: 0.78125\n",
      "At: 2311 [==========>] Loss 0.1364983973736903  - accuracy: 0.75\n",
      "At: 2312 [==========>] Loss 0.11438384475770652  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.0770917211105486  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.1225206336693187  - accuracy: 0.78125\n",
      "At: 2315 [==========>] Loss 0.11583242295036106  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.1285237681330681  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.16913003543651486  - accuracy: 0.78125\n",
      "At: 2318 [==========>] Loss 0.16716288898319487  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.09985203647470234  - accuracy: 0.78125\n",
      "At: 2320 [==========>] Loss 0.09020172214634711  - accuracy: 0.90625\n",
      "At: 2321 [==========>] Loss 0.14542638572527306  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.17632114557544293  - accuracy: 0.75\n",
      "At: 2323 [==========>] Loss 0.1280792822502631  - accuracy: 0.8125\n",
      "At: 2324 [==========>] Loss 0.15386467364793854  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.11892639214963469  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.06241617610934125  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.08072207926080785  - accuracy: 0.90625\n",
      "At: 2328 [==========>] Loss 0.118227065631094  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.10480709374253228  - accuracy: 0.90625\n",
      "At: 2330 [==========>] Loss 0.14659318915067693  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.09623466207848844  - accuracy: 0.9375\n",
      "At: 2332 [==========>] Loss 0.09884128978079945  - accuracy: 0.84375\n",
      "At: 2333 [==========>] Loss 0.08035363045412301  - accuracy: 0.90625\n",
      "At: 2334 [==========>] Loss 0.1834813382617152  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.10928681287736638  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.1251496912180922  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.14662950794023594  - accuracy: 0.84375\n",
      "At: 2338 [==========>] Loss 0.11579081355732981  - accuracy: 0.78125\n",
      "At: 2339 [==========>] Loss 0.07824992151106941  - accuracy: 0.90625\n",
      "At: 2340 [==========>] Loss 0.1517988733528597  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.13409442872832017  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.1611813211404292  - accuracy: 0.78125\n",
      "At: 2343 [==========>] Loss 0.08849465971758905  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.20658245990200452  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.1483835037976991  - accuracy: 0.75\n",
      "At: 2346 [==========>] Loss 0.08443127430091463  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.13401435469362166  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.07548945266259652  - accuracy: 0.90625\n",
      "At: 2349 [==========>] Loss 0.09691963101050315  - accuracy: 0.84375\n",
      "At: 2350 [==========>] Loss 0.1203161523071706  - accuracy: 0.84375\n",
      "At: 2351 [==========>] Loss 0.09421838440609817  - accuracy: 0.84375\n",
      "At: 2352 [==========>] Loss 0.0887532778127817  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.10757563047969004  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.15365461780785403  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.055928793004446795  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.13142625136393987  - accuracy: 0.78125\n",
      "At: 2357 [==========>] Loss 0.14491607541887822  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.14546210173631793  - accuracy: 0.8125\n",
      "At: 2359 [==========>] Loss 0.1856460373252647  - accuracy: 0.6875\n",
      "At: 2360 [==========>] Loss 0.13774237751524598  - accuracy: 0.78125\n",
      "At: 2361 [==========>] Loss 0.17415077535665371  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.08008008609094056  - accuracy: 0.875\n",
      "At: 2363 [==========>] Loss 0.19210745987319103  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.07891487773132136  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.07222691531832295  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.15100160356280345  - accuracy: 0.75\n",
      "At: 2367 [==========>] Loss 0.10921902715994608  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.10716470684027815  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.11561404202833558  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.07971692337236952  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.10968705300259533  - accuracy: 0.78125\n",
      "At: 2372 [==========>] Loss 0.14803090729992194  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.10810270496495131  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.09641456714979772  - accuracy: 0.875\n",
      "At: 2375 [==========>] Loss 0.05406048282083424  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.10256002659460856  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.0802658024339414  - accuracy: 0.90625\n",
      "At: 2378 [==========>] Loss 0.12324939020403605  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.17955192576031143  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.08074054004088455  - accuracy: 0.84375\n",
      "At: 2381 [==========>] Loss 0.09610435251883165  - accuracy: 0.875\n",
      "At: 2382 [==========>] Loss 0.09843811495190748  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1321902143692606  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.10360452616982407  - accuracy: 0.875\n",
      "At: 2385 [==========>] Loss 0.13182170824124306  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.08770702649329096  - accuracy: 0.90625\n",
      "At: 2387 [==========>] Loss 0.0651891557305079  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.13899009447190103  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.05542092952230509  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.07022087105318871  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.16843753643305406  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.15377216090452162  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.11285077590217224  - accuracy: 0.8125\n",
      "At: 2394 [==========>] Loss 0.056648387903641864  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.0943785306125309  - accuracy: 0.875\n",
      "At: 2396 [==========>] Loss 0.0740286087889967  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.09101349608285988  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.10477544023268992  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.14126834057030047  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.08351370203004141  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.09754739889819061  - accuracy: 0.8125\n",
      "At: 2402 [==========>] Loss 0.10217520854477354  - accuracy: 0.84375\n",
      "At: 2403 [==========>] Loss 0.19280965690070148  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.13402213013212846  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.06331730687572884  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.14274604517991302  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.11787356959446188  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.09468408095849484  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.13919063948583094  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.13779756583626787  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.10203135574505046  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.08821591982527044  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.09356688296378868  - accuracy: 0.9375\n",
      "At: 2414 [==========>] Loss 0.0596667495981997  - accuracy: 0.9375\n",
      "At: 2415 [==========>] Loss 0.09398223973966299  - accuracy: 0.90625\n",
      "At: 2416 [==========>] Loss 0.06823732787242784  - accuracy: 0.90625\n",
      "At: 2417 [==========>] Loss 0.14896885125822462  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.11708761398419926  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.1612566480257044  - accuracy: 0.75\n",
      "At: 2420 [==========>] Loss 0.13082785305691066  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.07262693677054491  - accuracy: 0.9375\n",
      "At: 2422 [==========>] Loss 0.1541517883196016  - accuracy: 0.75\n",
      "At: 2423 [==========>] Loss 0.1391420237975845  - accuracy: 0.75\n",
      "At: 2424 [==========>] Loss 0.10136602456762064  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.09106364232318832  - accuracy: 0.875\n",
      "At: 2426 [==========>] Loss 0.20494331368526797  - accuracy: 0.65625\n",
      "At: 2427 [==========>] Loss 0.13014667369309263  - accuracy: 0.84375\n",
      "At: 2428 [==========>] Loss 0.07830700491390719  - accuracy: 0.9375\n",
      "At: 2429 [==========>] Loss 0.0971820415661758  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.15720813637076286  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.14382333806507042  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.06125226030852977  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.06392664878748372  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.06362228130642614  - accuracy: 0.90625\n",
      "At: 2435 [==========>] Loss 0.14550227560085538  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.0961175832595777  - accuracy: 0.90625\n",
      "At: 2437 [==========>] Loss 0.17005583184815362  - accuracy: 0.8125\n",
      "At: 2438 [==========>] Loss 0.09314753132116632  - accuracy: 0.9375\n",
      "At: 2439 [==========>] Loss 0.13732478135450293  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.10860643293758038  - accuracy: 0.8125\n",
      "At: 2441 [==========>] Loss 0.11125969236827893  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.14806617357374  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.09849342379986747  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.08132292652645952  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.06483333763149485  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.16931505231973792  - accuracy: 0.75\n",
      "At: 2447 [==========>] Loss 0.15274840068108403  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.11377580762021369  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.0808606109501625  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.06760096166944801  - accuracy: 0.9375\n",
      "At: 2451 [==========>] Loss 0.06849081426855547  - accuracy: 0.84375\n",
      "At: 2452 [==========>] Loss 0.1403573323534161  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.1145454409463215  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.16599906606385123  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.13201002673333492  - accuracy: 0.78125\n",
      "At: 2456 [==========>] Loss 0.14635349950462706  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.17137508818239483  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.08310448155256514  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.14453147543539874  - accuracy: 0.78125\n",
      "At: 2460 [==========>] Loss 0.05969527477466601  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.061710925114092874  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.16425914979508305  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.11237153346020051  - accuracy: 0.875\n",
      "At: 2464 [==========>] Loss 0.1477363634587019  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.13297780584334992  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.07967357777365956  - accuracy: 0.9375\n",
      "At: 2467 [==========>] Loss 0.11559912785572272  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.0978553906013086  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.13480095914343299  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.12002928726301046  - accuracy: 0.8125\n",
      "At: 2471 [==========>] Loss 0.1171291337380666  - accuracy: 0.8125\n",
      "At: 2472 [==========>] Loss 0.09617772263324148  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.11355832040404382  - accuracy: 0.875\n",
      "At: 2474 [==========>] Loss 0.07647955893824161  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10484064815276917  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.07017455863557454  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.15036076653702318  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.1223556595937603  - accuracy: 0.875\n",
      "At: 2479 [==========>] Loss 0.08054125450732877  - accuracy: 0.9375\n",
      "At: 2480 [==========>] Loss 0.14324149488464  - accuracy: 0.78125\n",
      "At: 2481 [==========>] Loss 0.06301295092446871  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.14784920160708848  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.11558631742868461  - accuracy: 0.78125\n",
      "At: 2484 [==========>] Loss 0.10166625525258514  - accuracy: 0.78125\n",
      "At: 2485 [==========>] Loss 0.11791201455056449  - accuracy: 0.8125\n",
      "At: 2486 [==========>] Loss 0.1330122145892203  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.12549158855226708  - accuracy: 0.78125\n",
      "At: 2488 [==========>] Loss 0.18446308657309174  - accuracy: 0.75\n",
      "At: 2489 [==========>] Loss 0.1935852012848574  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.11922455293467299  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.1406420317658298  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.12502292361569117  - accuracy: 0.875\n",
      "At: 2493 [==========>] Loss 0.08912510624694364  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.0902938497549999  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.08551513508880737  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.05315027800532639  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.20283647810784883  - accuracy: 0.65625\n",
      "At: 2498 [==========>] Loss 0.09397713387885803  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.07521000200940946  - accuracy: 0.875\n",
      "At: 2500 [==========>] Loss 0.1680643937709796  - accuracy: 0.6875\n",
      "At: 2501 [==========>] Loss 0.1714795075830511  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.13450736355321785  - accuracy: 0.8125\n",
      "At: 2503 [==========>] Loss 0.12370969461698228  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.15438549008287838  - accuracy: 0.75\n",
      "At: 2505 [==========>] Loss 0.09851400062429888  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.10094651973007114  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.14803679894474106  - accuracy: 0.8125\n",
      "At: 2508 [==========>] Loss 0.11839611331376508  - accuracy: 0.84375\n",
      "At: 2509 [==========>] Loss 0.14021104220432926  - accuracy: 0.84375\n",
      "At: 2510 [==========>] Loss 0.10941727802448153  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14789687496912543  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.10236883206371017  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.14272997233254003  - accuracy: 0.84375\n",
      "At: 2514 [==========>] Loss 0.14302043993133703  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.19123886067632762  - accuracy: 0.75\n",
      "At: 2516 [==========>] Loss 0.19671992357726817  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.12545731200889607  - accuracy: 0.84375\n",
      "At: 2518 [==========>] Loss 0.1262688163674591  - accuracy: 0.78125\n",
      "At: 2519 [==========>] Loss 0.12508799291355552  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.13057219241384974  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.11342641430569034  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.2202190360160114  - accuracy: 0.6875\n",
      "At: 2523 [==========>] Loss 0.11834294129449868  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.17357035254622855  - accuracy: 0.71875\n",
      "At: 2525 [==========>] Loss 0.06654250522502436  - accuracy: 0.875\n",
      "At: 2526 [==========>] Loss 0.10255416621762846  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.17369536673938657  - accuracy: 0.75\n",
      "At: 2528 [==========>] Loss 0.0680404133901268  - accuracy: 0.875\n",
      "At: 2529 [==========>] Loss 0.1538942144031134  - accuracy: 0.75\n",
      "At: 2530 [==========>] Loss 0.12526129761806537  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.04275389929284  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.1156941161429675  - accuracy: 0.8125\n",
      "At: 2533 [==========>] Loss 0.10164187673806868  - accuracy: 0.9375\n",
      "At: 2534 [==========>] Loss 0.04413216182439095  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.07690360120628464  - accuracy: 0.875\n",
      "At: 2536 [==========>] Loss 0.16711562994361157  - accuracy: 0.71875\n",
      "At: 2537 [==========>] Loss 0.11210792233738721  - accuracy: 0.8125\n",
      "At: 2538 [==========>] Loss 0.14033123924968693  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.07893767851645561  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.13668476340122612  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.060154829136069426  - accuracy: 0.9375\n",
      "At: 2542 [==========>] Loss 0.06528828626050526  - accuracy: 0.90625\n",
      "At: 2543 [==========>] Loss 0.16975808195546666  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.1129676873521023  - accuracy: 0.84375\n",
      "At: 2545 [==========>] Loss 0.056788922312508275  - accuracy: 0.96875\n",
      "At: 2546 [==========>] Loss 0.13163181262786983  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.09425153924797375  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.07348129729740446  - accuracy: 0.9375\n",
      "At: 2549 [==========>] Loss 0.09546993222044838  - accuracy: 0.8125\n",
      "At: 2550 [==========>] Loss 0.13312044196036898  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.12149339622210785  - accuracy: 0.84375\n",
      "At: 2552 [==========>] Loss 0.09751125083062126  - accuracy: 0.90625\n",
      "At: 2553 [==========>] Loss 0.07797202045065493  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.12595421167649468  - accuracy: 0.84375\n",
      "At: 2555 [==========>] Loss 0.15114620063179107  - accuracy: 0.75\n",
      "At: 2556 [==========>] Loss 0.09179577139070878  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.13166023581918415  - accuracy: 0.78125\n",
      "At: 2558 [==========>] Loss 0.10806937151971206  - accuracy: 0.8125\n",
      "At: 2559 [==========>] Loss 0.09153292906164454  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.2015205689568117  - accuracy: 0.6875\n",
      "At: 2561 [==========>] Loss 0.09336575203278914  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.08543861030877337  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.13308626381286415  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.07977853007762639  - accuracy: 0.90625\n",
      "At: 2565 [==========>] Loss 0.11454669702336329  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.180868177415808  - accuracy: 0.75\n",
      "At: 2567 [==========>] Loss 0.10042888369886778  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.12247285204288316  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.07371832544939019  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.20620468639873157  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.09892669629749794  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.1860843911896497  - accuracy: 0.71875\n",
      "At: 2573 [==========>] Loss 0.16894965163382675  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.13756905866379995  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.07008349911724632  - accuracy: 0.875\n",
      "At: 2576 [==========>] Loss 0.10251666725933804  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.2120577395021568  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.1891803795134412  - accuracy: 0.6875\n",
      "At: 2579 [==========>] Loss 0.18397045508688686  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.158805277644619  - accuracy: 0.71875\n",
      "At: 2581 [==========>] Loss 0.06937367768037228  - accuracy: 0.96875\n",
      "At: 2582 [==========>] Loss 0.17961712722969012  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.06244768616975664  - accuracy: 0.96875\n",
      "At: 2584 [==========>] Loss 0.17389971909510932  - accuracy: 0.75\n",
      "At: 2585 [==========>] Loss 0.10584542380986663  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.07920579061305683  - accuracy: 0.875\n",
      "At: 2587 [==========>] Loss 0.17068140292017778  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.12807133863337042  - accuracy: 0.875\n",
      "At: 2589 [==========>] Loss 0.10004955399256903  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.2007255268752232  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.10964642585739666  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.12853493388351317  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.09891405267756619  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.07556397766766933  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.08954097607876535  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.10308237469069806  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.08787759780177773  - accuracy: 0.84375\n",
      "At: 2598 [==========>] Loss 0.12038955453329608  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.15438666201756523  - accuracy: 0.71875\n",
      "At: 2600 [==========>] Loss 0.14943430419636142  - accuracy: 0.75\n",
      "At: 2601 [==========>] Loss 0.1216491749710404  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.08248528619939344  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.15674667400845071  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.10339293274203348  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.11003123092556683  - accuracy: 0.875\n",
      "At: 2606 [==========>] Loss 0.11145819190824802  - accuracy: 0.90625\n",
      "At: 2607 [==========>] Loss 0.13251294099559952  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.06867693755420112  - accuracy: 0.96875\n",
      "At: 2609 [==========>] Loss 0.09125146759558721  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.15753280677907247  - accuracy: 0.75\n",
      "At: 2611 [==========>] Loss 0.15167917650149537  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.0754374054926989  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.0958505750680935  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.11362555034222424  - accuracy: 0.84375\n",
      "At: 2615 [==========>] Loss 0.08673623574986272  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.06296949821052915  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.09919346502935632  - accuracy: 0.875\n",
      "At: 2618 [==========>] Loss 0.06641568968277507  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.10662719551367836  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.10843211067975961  - accuracy: 0.875\n",
      "At: 2621 [==========>] Loss 0.11197962031088815  - accuracy: 0.875\n",
      "At: 2622 [==========>] Loss 0.09806020710914104  - accuracy: 0.875\n",
      "At: 2623 [==========>] Loss 0.1238303396589521  - accuracy: 0.84375\n",
      "At: 2624 [==========>] Loss 0.1277258383554255  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.06330631728171504  - accuracy: 0.90625\n",
      "At: 2626 [==========>] Loss 0.045592308665172346  - accuracy: 1.0\n",
      "At: 2627 [==========>] Loss 0.13809578773015393  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.07796053552968504  - accuracy: 0.9375\n",
      "At: 2629 [==========>] Loss 0.1067099201356688  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.08972537549203732  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.11471930321494356  - accuracy: 0.84375\n",
      "At: 2632 [==========>] Loss 0.11295573415964119  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.09597427298289668  - accuracy: 0.9375\n",
      "At: 2634 [==========>] Loss 0.074455623928282  - accuracy: 0.875\n",
      "At: 2635 [==========>] Loss 0.19613362086206831  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.1343487027445241  - accuracy: 0.75\n",
      "At: 2637 [==========>] Loss 0.15581118915976683  - accuracy: 0.75\n",
      "At: 2638 [==========>] Loss 0.06914455798824595  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.06376350987430571  - accuracy: 0.9375\n",
      "At: 2640 [==========>] Loss 0.12627808788089184  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.15127157887402093  - accuracy: 0.84375\n",
      "At: 2642 [==========>] Loss 0.18321405391635198  - accuracy: 0.6875\n",
      "At: 2643 [==========>] Loss 0.09189541243389121  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.16773351858249705  - accuracy: 0.71875\n",
      "At: 2645 [==========>] Loss 0.07627637811196455  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.14597844933166448  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.11355528994631543  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.15872677937773075  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.13577197414102204  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.12840298394916919  - accuracy: 0.78125\n",
      "At: 2651 [==========>] Loss 0.15834689289023088  - accuracy: 0.75\n",
      "At: 2652 [==========>] Loss 0.10389057843720245  - accuracy: 0.78125\n",
      "At: 2653 [==========>] Loss 0.1299680510726467  - accuracy: 0.8125\n",
      "At: 2654 [==========>] Loss 0.1017164775042214  - accuracy: 0.875\n",
      "At: 2655 [==========>] Loss 0.23264146542422626  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.02757736956662288  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.11046370120205387  - accuracy: 0.84375\n",
      "At: 2658 [==========>] Loss 0.0981923821232917  - accuracy: 0.84375\n",
      "At: 2659 [==========>] Loss 0.06491491996338775  - accuracy: 0.90625\n",
      "At: 2660 [==========>] Loss 0.12311216579820523  - accuracy: 0.84375\n",
      "At: 2661 [==========>] Loss 0.08214996883615795  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.06526174324512529  - accuracy: 0.90625\n",
      "At: 2663 [==========>] Loss 0.09049689730978767  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.0729908025358923  - accuracy: 0.9375\n",
      "At: 2665 [==========>] Loss 0.1152015630977467  - accuracy: 0.84375\n",
      "At: 2666 [==========>] Loss 0.1387727948847059  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.14406936989750668  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.13958672525587917  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.12949631680161056  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.10410588436934445  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.05462976270644783  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.11404068731975872  - accuracy: 0.8125\n",
      "At: 2673 [==========>] Loss 0.13196662470831166  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.10370152454964121  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.11664265535240134  - accuracy: 0.8125\n",
      "At: 2676 [==========>] Loss 0.12419282226788733  - accuracy: 0.78125\n",
      "At: 2677 [==========>] Loss 0.10975332756149125  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.07369474657478362  - accuracy: 0.84375\n",
      "At: 2679 [==========>] Loss 0.07878034715860888  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.08706216644929066  - accuracy: 0.84375\n",
      "At: 2681 [==========>] Loss 0.07778026300054727  - accuracy: 0.9375\n",
      "At: 2682 [==========>] Loss 0.15381849512514234  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.190896746208025  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.08497737785475262  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.10508568899542929  - accuracy: 0.78125\n",
      "At: 2686 [==========>] Loss 0.06458301126701058  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.14108180992731964  - accuracy: 0.78125\n",
      "At: 2688 [==========>] Loss 0.1509823135753672  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.11726766258778698  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.10642417785160277  - accuracy: 0.84375\n",
      "Epochs  2 / 10\n",
      "At: 1 [==========>] Loss 0.18301936248950232  - accuracy: 0.75\n",
      "At: 2 [==========>] Loss 0.18995758779525218  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.16605148588206095  - accuracy: 0.78125\n",
      "At: 4 [==========>] Loss 0.16163552636567569  - accuracy: 0.78125\n",
      "At: 5 [==========>] Loss 0.11897127300629105  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.13980476781048948  - accuracy: 0.8125\n",
      "At: 7 [==========>] Loss 0.17248449815519934  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.21125431972260736  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.3031393625687866  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.2195869580667041  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.15282219196590552  - accuracy: 0.78125\n",
      "At: 12 [==========>] Loss 0.2074020232893249  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.1965998478383843  - accuracy: 0.78125\n",
      "At: 14 [==========>] Loss 0.10057198549341248  - accuracy: 0.84375\n",
      "At: 15 [==========>] Loss 0.18546312992384187  - accuracy: 0.75\n",
      "At: 16 [==========>] Loss 0.17751607587727614  - accuracy: 0.71875\n",
      "At: 17 [==========>] Loss 0.215507399680483  - accuracy: 0.6875\n",
      "At: 18 [==========>] Loss 0.22837096605879076  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.1547641148929471  - accuracy: 0.84375\n",
      "At: 20 [==========>] Loss 0.16419674195439565  - accuracy: 0.78125\n",
      "At: 21 [==========>] Loss 0.232660334557457  - accuracy: 0.65625\n",
      "At: 22 [==========>] Loss 0.1394502844292923  - accuracy: 0.875\n",
      "At: 23 [==========>] Loss 0.09598435144499165  - accuracy: 0.875\n",
      "At: 24 [==========>] Loss 0.2584525179278143  - accuracy: 0.65625\n",
      "At: 25 [==========>] Loss 0.22052192736255294  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.2230522889888417  - accuracy: 0.625\n",
      "At: 27 [==========>] Loss 0.2543507350361813  - accuracy: 0.625\n",
      "At: 28 [==========>] Loss 0.1758312204142242  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.2156174052811124  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.2870396034157412  - accuracy: 0.625\n",
      "At: 31 [==========>] Loss 0.22714781757132127  - accuracy: 0.59375\n",
      "At: 32 [==========>] Loss 0.2312355566948966  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.09868019018634158  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.1758713237847658  - accuracy: 0.78125\n",
      "At: 35 [==========>] Loss 0.16187028233901235  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.19121477510284937  - accuracy: 0.8125\n",
      "At: 37 [==========>] Loss 0.20953151446010226  - accuracy: 0.75\n",
      "At: 38 [==========>] Loss 0.1986651339624027  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.20541154321917338  - accuracy: 0.75\n",
      "At: 40 [==========>] Loss 0.17771815383457482  - accuracy: 0.71875\n",
      "At: 41 [==========>] Loss 0.09289870973522728  - accuracy: 0.84375\n",
      "At: 42 [==========>] Loss 0.13997999851766496  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.13729182065391365  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.15638351835428574  - accuracy: 0.75\n",
      "At: 45 [==========>] Loss 0.0693701562380912  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.18367622554753915  - accuracy: 0.75\n",
      "At: 47 [==========>] Loss 0.17875240061217695  - accuracy: 0.78125\n",
      "At: 48 [==========>] Loss 0.1809990550699453  - accuracy: 0.75\n",
      "At: 49 [==========>] Loss 0.13908818259686268  - accuracy: 0.8125\n",
      "At: 50 [==========>] Loss 0.16544269265489225  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.22717448590572595  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.1954999275876005  - accuracy: 0.71875\n",
      "At: 53 [==========>] Loss 0.19749767409917626  - accuracy: 0.75\n",
      "At: 54 [==========>] Loss 0.152065510588899  - accuracy: 0.75\n",
      "At: 55 [==========>] Loss 0.23423382754943597  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.18609105726230657  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.14158653617881342  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.21806696835235762  - accuracy: 0.65625\n",
      "At: 59 [==========>] Loss 0.16603304187386325  - accuracy: 0.78125\n",
      "At: 60 [==========>] Loss 0.18474264574091437  - accuracy: 0.8125\n",
      "At: 61 [==========>] Loss 0.2056589298995445  - accuracy: 0.75\n",
      "At: 62 [==========>] Loss 0.20308571048900376  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.20434454822933065  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.16801517272119093  - accuracy: 0.78125\n",
      "At: 65 [==========>] Loss 0.20728863955582785  - accuracy: 0.6875\n",
      "At: 66 [==========>] Loss 0.24721354347194044  - accuracy: 0.59375\n",
      "At: 67 [==========>] Loss 0.19180420091885975  - accuracy: 0.65625\n",
      "At: 68 [==========>] Loss 0.12437586009367864  - accuracy: 0.84375\n",
      "At: 69 [==========>] Loss 0.16303715338226693  - accuracy: 0.78125\n",
      "At: 70 [==========>] Loss 0.13141471616126285  - accuracy: 0.78125\n",
      "At: 71 [==========>] Loss 0.18517921599075782  - accuracy: 0.71875\n",
      "At: 72 [==========>] Loss 0.17468106829423397  - accuracy: 0.75\n",
      "At: 73 [==========>] Loss 0.17290321615904441  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.2455620064437153  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.14940208220187454  - accuracy: 0.8125\n",
      "At: 76 [==========>] Loss 0.20548795005564596  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.2317487154199663  - accuracy: 0.6875\n",
      "At: 78 [==========>] Loss 0.15228017601903113  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.1942879767552104  - accuracy: 0.6875\n",
      "At: 80 [==========>] Loss 0.23799449594966993  - accuracy: 0.75\n",
      "At: 81 [==========>] Loss 0.1792674294613829  - accuracy: 0.71875\n",
      "At: 82 [==========>] Loss 0.18202778025454336  - accuracy: 0.75\n",
      "At: 83 [==========>] Loss 0.1764513141979349  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.18997819736431232  - accuracy: 0.6875\n",
      "At: 85 [==========>] Loss 0.2234280745554355  - accuracy: 0.6875\n",
      "At: 86 [==========>] Loss 0.17937030290179684  - accuracy: 0.75\n",
      "At: 87 [==========>] Loss 0.16370728649624938  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.3205445820049756  - accuracy: 0.5\n",
      "At: 89 [==========>] Loss 0.18266152910322608  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.17250612903510879  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.16840546866782455  - accuracy: 0.8125\n",
      "At: 92 [==========>] Loss 0.13524290706607073  - accuracy: 0.84375\n",
      "At: 93 [==========>] Loss 0.10320913058027055  - accuracy: 0.90625\n",
      "At: 94 [==========>] Loss 0.15475552725052233  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.17211338653111735  - accuracy: 0.8125\n",
      "At: 96 [==========>] Loss 0.17916400396349547  - accuracy: 0.78125\n",
      "At: 97 [==========>] Loss 0.13122526961540495  - accuracy: 0.8125\n",
      "At: 98 [==========>] Loss 0.2285367188279085  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.17599313145025977  - accuracy: 0.78125\n",
      "At: 100 [==========>] Loss 0.11820216245387276  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.11688188757975937  - accuracy: 0.875\n",
      "At: 102 [==========>] Loss 0.15477558692811294  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.15737036197615284  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.11679028808119596  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.1505036463875979  - accuracy: 0.8125\n",
      "At: 106 [==========>] Loss 0.17171451722796854  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.20173404857526064  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.20102628129925249  - accuracy: 0.71875\n",
      "At: 109 [==========>] Loss 0.12919413940391172  - accuracy: 0.8125\n",
      "At: 110 [==========>] Loss 0.24120870883526535  - accuracy: 0.65625\n",
      "At: 111 [==========>] Loss 0.10751369532595348  - accuracy: 0.8125\n",
      "At: 112 [==========>] Loss 0.11761710425958513  - accuracy: 0.875\n",
      "At: 113 [==========>] Loss 0.1973698736495099  - accuracy: 0.78125\n",
      "At: 114 [==========>] Loss 0.1613058083163244  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.15948776610053983  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.193318529560497  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.1758733958138802  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.259752683499586  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.12905300370405753  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.1326076029709265  - accuracy: 0.84375\n",
      "At: 121 [==========>] Loss 0.19516901783203028  - accuracy: 0.75\n",
      "At: 122 [==========>] Loss 0.16243524592980568  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.1494349390373477  - accuracy: 0.875\n",
      "At: 124 [==========>] Loss 0.2457552829765211  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.17364956460974512  - accuracy: 0.8125\n",
      "At: 126 [==========>] Loss 0.22523006208354815  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.21146570967906328  - accuracy: 0.6875\n",
      "At: 128 [==========>] Loss 0.2525086177281991  - accuracy: 0.6875\n",
      "At: 129 [==========>] Loss 0.09743011722099526  - accuracy: 0.90625\n",
      "At: 130 [==========>] Loss 0.2134727596792132  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.15978282320644333  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.21719324132711232  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.2015406628921308  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.1834875006023576  - accuracy: 0.78125\n",
      "At: 135 [==========>] Loss 0.21486548287792484  - accuracy: 0.71875\n",
      "At: 136 [==========>] Loss 0.13021360000244506  - accuracy: 0.8125\n",
      "At: 137 [==========>] Loss 0.05627350319327426  - accuracy: 0.9375\n",
      "At: 138 [==========>] Loss 0.193018380517697  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.1306430096797664  - accuracy: 0.90625\n",
      "At: 140 [==========>] Loss 0.1459328430653829  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.25265067103695327  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.18747800920994193  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.20366828675953003  - accuracy: 0.6875\n",
      "At: 144 [==========>] Loss 0.10279574199136524  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.11534935851496732  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.10378455737441655  - accuracy: 0.90625\n",
      "At: 147 [==========>] Loss 0.2117577919909804  - accuracy: 0.71875\n",
      "At: 148 [==========>] Loss 0.15649960031283022  - accuracy: 0.78125\n",
      "At: 149 [==========>] Loss 0.17495650956002054  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.16196947231296704  - accuracy: 0.84375\n",
      "At: 151 [==========>] Loss 0.1701983823691497  - accuracy: 0.75\n",
      "At: 152 [==========>] Loss 0.14632783000417704  - accuracy: 0.875\n",
      "At: 153 [==========>] Loss 0.20335482061825283  - accuracy: 0.6875\n",
      "At: 154 [==========>] Loss 0.11545013421932226  - accuracy: 0.8125\n",
      "At: 155 [==========>] Loss 0.19489708876256068  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.13312767486752858  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.24313658022358908  - accuracy: 0.625\n",
      "At: 158 [==========>] Loss 0.1789920102060728  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.12603169366088743  - accuracy: 0.875\n",
      "At: 160 [==========>] Loss 0.1469342534198398  - accuracy: 0.875\n",
      "At: 161 [==========>] Loss 0.1450061972526677  - accuracy: 0.8125\n",
      "At: 162 [==========>] Loss 0.24610887094501796  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.14828298389478903  - accuracy: 0.78125\n",
      "At: 164 [==========>] Loss 0.16818789280775223  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.1794306521947985  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.16783911479396568  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.14650415816077691  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.14790880722509972  - accuracy: 0.8125\n",
      "At: 169 [==========>] Loss 0.15548972665868777  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.1407795217584761  - accuracy: 0.84375\n",
      "At: 171 [==========>] Loss 0.20761604821421312  - accuracy: 0.65625\n",
      "At: 172 [==========>] Loss 0.17628235325279124  - accuracy: 0.8125\n",
      "At: 173 [==========>] Loss 0.19714414295612784  - accuracy: 0.6875\n",
      "At: 174 [==========>] Loss 0.21491018415827917  - accuracy: 0.65625\n",
      "At: 175 [==========>] Loss 0.1809080008227123  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.16320231498118826  - accuracy: 0.8125\n",
      "At: 177 [==========>] Loss 0.11770119702737883  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.1541809835482586  - accuracy: 0.78125\n",
      "At: 179 [==========>] Loss 0.16890043804630803  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.1697828627898207  - accuracy: 0.75\n",
      "At: 181 [==========>] Loss 0.056088562429039104  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.1623071681624266  - accuracy: 0.75\n",
      "At: 183 [==========>] Loss 0.13106397933305186  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.18251895054311099  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.12385957509786005  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.16726845864217815  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.2069391142185699  - accuracy: 0.75\n",
      "At: 188 [==========>] Loss 0.15561517680789003  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.19943459239840827  - accuracy: 0.75\n",
      "At: 190 [==========>] Loss 0.1102660310587625  - accuracy: 0.90625\n",
      "At: 191 [==========>] Loss 0.3089297658159107  - accuracy: 0.5625\n",
      "At: 192 [==========>] Loss 0.16344958353082636  - accuracy: 0.78125\n",
      "At: 193 [==========>] Loss 0.17954687587237744  - accuracy: 0.78125\n",
      "At: 194 [==========>] Loss 0.16145293696112756  - accuracy: 0.8125\n",
      "At: 195 [==========>] Loss 0.17702610727992948  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.14486250744695275  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.13360652786268248  - accuracy: 0.84375\n",
      "At: 198 [==========>] Loss 0.13307782186268352  - accuracy: 0.75\n",
      "At: 199 [==========>] Loss 0.07272998360937555  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.19010051315252208  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.1497673737203732  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.10098521636828414  - accuracy: 0.90625\n",
      "At: 203 [==========>] Loss 0.1706863958157399  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.19603235668128016  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.12243930680969914  - accuracy: 0.875\n",
      "At: 206 [==========>] Loss 0.10966547869335917  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.09979102030508755  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.22361248124703278  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.18519481371347238  - accuracy: 0.6875\n",
      "At: 210 [==========>] Loss 0.10966608339954431  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.16788536482772817  - accuracy: 0.75\n",
      "At: 212 [==========>] Loss 0.17962962437208707  - accuracy: 0.78125\n",
      "At: 213 [==========>] Loss 0.18683832329105987  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.18317280839757738  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.1301160380245036  - accuracy: 0.8125\n",
      "At: 216 [==========>] Loss 0.14173252847686163  - accuracy: 0.8125\n",
      "At: 217 [==========>] Loss 0.2172781146528457  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.1361314462966867  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.21196756362912356  - accuracy: 0.6875\n",
      "At: 220 [==========>] Loss 0.14628372357797959  - accuracy: 0.78125\n",
      "At: 221 [==========>] Loss 0.1532554147823217  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.09865466824740482  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.2525162345058592  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.16682425148018223  - accuracy: 0.75\n",
      "At: 225 [==========>] Loss 0.16824187352351083  - accuracy: 0.75\n",
      "At: 226 [==========>] Loss 0.13045807914054608  - accuracy: 0.84375\n",
      "At: 227 [==========>] Loss 0.1744381171841037  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.19629983332718717  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.19549238844680222  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.17957605302589302  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.2328611899118777  - accuracy: 0.625\n",
      "At: 232 [==========>] Loss 0.2168611247166552  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.19894943090115222  - accuracy: 0.6875\n",
      "At: 234 [==========>] Loss 0.14276242664434266  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.1371972143773279  - accuracy: 0.8125\n",
      "At: 236 [==========>] Loss 0.2222043503319718  - accuracy: 0.65625\n",
      "At: 237 [==========>] Loss 0.11263796639760593  - accuracy: 0.8125\n",
      "At: 238 [==========>] Loss 0.11696387664596056  - accuracy: 0.875\n",
      "At: 239 [==========>] Loss 0.1569867267226372  - accuracy: 0.75\n",
      "At: 240 [==========>] Loss 0.2109319983560636  - accuracy: 0.78125\n",
      "At: 241 [==========>] Loss 0.12903803613590392  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.15005641256013613  - accuracy: 0.78125\n",
      "At: 243 [==========>] Loss 0.1281736281926001  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.14939812623594578  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.10601760884886555  - accuracy: 0.875\n",
      "At: 246 [==========>] Loss 0.12011720463196522  - accuracy: 0.875\n",
      "At: 247 [==========>] Loss 0.18439324733930254  - accuracy: 0.75\n",
      "At: 248 [==========>] Loss 0.12723297911729864  - accuracy: 0.8125\n",
      "At: 249 [==========>] Loss 0.1027302846448975  - accuracy: 0.9375\n",
      "At: 250 [==========>] Loss 0.2100715959489471  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.19656925887774887  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.10416059796595835  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.17279552831876208  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.09504687201404143  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.16803224329728889  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.18339957024261883  - accuracy: 0.75\n",
      "At: 257 [==========>] Loss 0.08555270400253127  - accuracy: 0.875\n",
      "At: 258 [==========>] Loss 0.18511263834291092  - accuracy: 0.8125\n",
      "At: 259 [==========>] Loss 0.16362244608136395  - accuracy: 0.84375\n",
      "At: 260 [==========>] Loss 0.15154034195095856  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.09598773037098578  - accuracy: 0.875\n",
      "At: 262 [==========>] Loss 0.11296715695488024  - accuracy: 0.84375\n",
      "At: 263 [==========>] Loss 0.10869546159011664  - accuracy: 0.90625\n",
      "At: 264 [==========>] Loss 0.11876352898992684  - accuracy: 0.8125\n",
      "At: 265 [==========>] Loss 0.16432023316460775  - accuracy: 0.75\n",
      "At: 266 [==========>] Loss 0.25144685498166996  - accuracy: 0.6875\n",
      "At: 267 [==========>] Loss 0.14364305404550015  - accuracy: 0.84375\n",
      "At: 268 [==========>] Loss 0.18029624295007843  - accuracy: 0.78125\n",
      "At: 269 [==========>] Loss 0.08300720237810964  - accuracy: 0.875\n",
      "At: 270 [==========>] Loss 0.2775383479315176  - accuracy: 0.5625\n",
      "At: 271 [==========>] Loss 0.1676010324325335  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.12190683820854165  - accuracy: 0.84375\n",
      "At: 273 [==========>] Loss 0.16073479065019958  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.1716740044334804  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.07272613590565336  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.20812776561393603  - accuracy: 0.65625\n",
      "At: 277 [==========>] Loss 0.13567007233348405  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.10728644103905843  - accuracy: 0.875\n",
      "At: 279 [==========>] Loss 0.13534797667556847  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.15394012114290417  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.1480457054076652  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.18727617295238658  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.14873422804746528  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.0892434358859719  - accuracy: 0.9375\n",
      "At: 285 [==========>] Loss 0.1614289048850494  - accuracy: 0.84375\n",
      "At: 286 [==========>] Loss 0.07115878960009568  - accuracy: 0.9375\n",
      "At: 287 [==========>] Loss 0.1819620862683681  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.1161947738448976  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.12813781608275115  - accuracy: 0.84375\n",
      "At: 290 [==========>] Loss 0.08510983927079668  - accuracy: 0.90625\n",
      "At: 291 [==========>] Loss 0.13693088499521577  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.15675359229332392  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.22250622301715475  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.14285798528813773  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.18505774313341739  - accuracy: 0.75\n",
      "At: 296 [==========>] Loss 0.13412979543706563  - accuracy: 0.8125\n",
      "At: 297 [==========>] Loss 0.16082955452391726  - accuracy: 0.78125\n",
      "At: 298 [==========>] Loss 0.12858969126670905  - accuracy: 0.875\n",
      "At: 299 [==========>] Loss 0.1832366527094947  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.1776362579091435  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.16146248098376195  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.14674281315739757  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.09126484819371755  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.1537387070292601  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.23816898107686754  - accuracy: 0.59375\n",
      "At: 306 [==========>] Loss 0.1346426274853941  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.273354625115157  - accuracy: 0.5625\n",
      "At: 308 [==========>] Loss 0.13776410928694072  - accuracy: 0.8125\n",
      "At: 309 [==========>] Loss 0.11391618079572263  - accuracy: 0.875\n",
      "At: 310 [==========>] Loss 0.19138628939794416  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.05841783823077991  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.15085937056491155  - accuracy: 0.78125\n",
      "At: 313 [==========>] Loss 0.107630093961645  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.16830442994457467  - accuracy: 0.65625\n",
      "At: 315 [==========>] Loss 0.11601248653640478  - accuracy: 0.90625\n",
      "At: 316 [==========>] Loss 0.24217217245791  - accuracy: 0.6875\n",
      "At: 317 [==========>] Loss 0.3076409503064556  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.157009786211877  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.11285498271210084  - accuracy: 0.8125\n",
      "At: 320 [==========>] Loss 0.18284126527048994  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.17211194369344643  - accuracy: 0.65625\n",
      "At: 322 [==========>] Loss 0.07675804615461201  - accuracy: 0.90625\n",
      "At: 323 [==========>] Loss 0.10142777289889013  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.14114661247411453  - accuracy: 0.8125\n",
      "At: 325 [==========>] Loss 0.10411826540173602  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.21414752957235073  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.11889315051929525  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.12988057389038898  - accuracy: 0.875\n",
      "At: 329 [==========>] Loss 0.1515420192619997  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.13209505249996872  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.15943426872497535  - accuracy: 0.78125\n",
      "At: 332 [==========>] Loss 0.23093628475143774  - accuracy: 0.71875\n",
      "At: 333 [==========>] Loss 0.11866070124908754  - accuracy: 0.84375\n",
      "At: 334 [==========>] Loss 0.10642021176768521  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.13410476668087581  - accuracy: 0.875\n",
      "At: 336 [==========>] Loss 0.14040860683144363  - accuracy: 0.875\n",
      "At: 337 [==========>] Loss 0.17474834122485589  - accuracy: 0.78125\n",
      "At: 338 [==========>] Loss 0.16288858887049015  - accuracy: 0.75\n",
      "At: 339 [==========>] Loss 0.14314049799040848  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.14655142830418383  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.15647861385783332  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.16960523341943518  - accuracy: 0.75\n",
      "At: 343 [==========>] Loss 0.28840202279428523  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.20786294356676133  - accuracy: 0.75\n",
      "At: 345 [==========>] Loss 0.16455121175318393  - accuracy: 0.8125\n",
      "At: 346 [==========>] Loss 0.1250529535412715  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.11137149681666525  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.1087660878710867  - accuracy: 0.84375\n",
      "At: 349 [==========>] Loss 0.1585112451825358  - accuracy: 0.8125\n",
      "At: 350 [==========>] Loss 0.11348324782309431  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.28659127359379544  - accuracy: 0.65625\n",
      "At: 352 [==========>] Loss 0.10123033057954933  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.13104024873000833  - accuracy: 0.84375\n",
      "At: 354 [==========>] Loss 0.14373971870640012  - accuracy: 0.8125\n",
      "At: 355 [==========>] Loss 0.08509837386299465  - accuracy: 0.90625\n",
      "At: 356 [==========>] Loss 0.18235831919972845  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.14488739802172973  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.10156720863930743  - accuracy: 0.875\n",
      "At: 359 [==========>] Loss 0.09807977770376071  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.11318579817619312  - accuracy: 0.84375\n",
      "At: 361 [==========>] Loss 0.09414238532753369  - accuracy: 0.875\n",
      "At: 362 [==========>] Loss 0.15833208470197452  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.11023400503890114  - accuracy: 0.90625\n",
      "At: 364 [==========>] Loss 0.24362860719782703  - accuracy: 0.6875\n",
      "At: 365 [==========>] Loss 0.16222279957345168  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.21857371216030358  - accuracy: 0.6875\n",
      "At: 367 [==========>] Loss 0.1719494712371774  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.18895684185900072  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.1339399306960778  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.21226240247792078  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.1006383432191996  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.10912918035910127  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.16730212106286954  - accuracy: 0.75\n",
      "At: 374 [==========>] Loss 0.061347487550743506  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.14486817926217638  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.07205805722054091  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.20255763109852104  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.15755397300555984  - accuracy: 0.78125\n",
      "At: 379 [==========>] Loss 0.1472977661418089  - accuracy: 0.75\n",
      "At: 380 [==========>] Loss 0.1893816313902002  - accuracy: 0.71875\n",
      "At: 381 [==========>] Loss 0.1846777225350565  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.09721032957654369  - accuracy: 0.875\n",
      "At: 383 [==========>] Loss 0.15639340179280817  - accuracy: 0.78125\n",
      "At: 384 [==========>] Loss 0.1746118999220377  - accuracy: 0.8125\n",
      "At: 385 [==========>] Loss 0.13960403676360603  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.2144925716838117  - accuracy: 0.75\n",
      "At: 387 [==========>] Loss 0.08235857911277064  - accuracy: 0.96875\n",
      "At: 388 [==========>] Loss 0.20588945871936234  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.14885577802623082  - accuracy: 0.78125\n",
      "At: 390 [==========>] Loss 0.12872438368269157  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.10282474291156067  - accuracy: 0.875\n",
      "At: 392 [==========>] Loss 0.1516868719275077  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.2409741048419908  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.08552926517087481  - accuracy: 0.9375\n",
      "At: 395 [==========>] Loss 0.1913095386027905  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.1470172753372524  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.14807997613401055  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.18445751853302572  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.19571763645861784  - accuracy: 0.71875\n",
      "At: 400 [==========>] Loss 0.2073022229679353  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.13295338243414376  - accuracy: 0.8125\n",
      "At: 402 [==========>] Loss 0.13358918622450763  - accuracy: 0.8125\n",
      "At: 403 [==========>] Loss 0.055347055238860306  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.09325785936781467  - accuracy: 0.90625\n",
      "At: 405 [==========>] Loss 0.20168598418716105  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.15161529298392445  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.19891009924114705  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.19254177295257263  - accuracy: 0.75\n",
      "At: 409 [==========>] Loss 0.2301432385747059  - accuracy: 0.71875\n",
      "At: 410 [==========>] Loss 0.16119805157233788  - accuracy: 0.78125\n",
      "At: 411 [==========>] Loss 0.1280828230429757  - accuracy: 0.78125\n",
      "At: 412 [==========>] Loss 0.17616066705089212  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.1069872319717875  - accuracy: 0.90625\n",
      "At: 414 [==========>] Loss 0.18863903917091066  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.12158829670444127  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.19877674653122976  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.14132946107332628  - accuracy: 0.8125\n",
      "At: 418 [==========>] Loss 0.1440867314513919  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.11723449135791247  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.13426905718680146  - accuracy: 0.78125\n",
      "At: 421 [==========>] Loss 0.14777394135216737  - accuracy: 0.78125\n",
      "At: 422 [==========>] Loss 0.13292099584082578  - accuracy: 0.8125\n",
      "At: 423 [==========>] Loss 0.14884205893967223  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.20739100328889326  - accuracy: 0.71875\n",
      "At: 425 [==========>] Loss 0.17623504399491707  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.1422645062738692  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.16423456673117004  - accuracy: 0.8125\n",
      "At: 428 [==========>] Loss 0.24957999339609457  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.18416681669687612  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.0925578624765162  - accuracy: 0.8125\n",
      "At: 431 [==========>] Loss 0.11596372684366965  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.13366917561355113  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.08150266655023192  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.13216500832292888  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.1867586301267472  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.14190158271580638  - accuracy: 0.84375\n",
      "At: 437 [==========>] Loss 0.11572442824599648  - accuracy: 0.84375\n",
      "At: 438 [==========>] Loss 0.1416260147200239  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.10648121288713605  - accuracy: 0.84375\n",
      "At: 440 [==========>] Loss 0.07561006287342895  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.1879486119144207  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.15748531117297096  - accuracy: 0.84375\n",
      "At: 443 [==========>] Loss 0.14649894878085895  - accuracy: 0.78125\n",
      "At: 444 [==========>] Loss 0.16920897871212615  - accuracy: 0.78125\n",
      "At: 445 [==========>] Loss 0.14621125200842916  - accuracy: 0.84375\n",
      "At: 446 [==========>] Loss 0.2224981539633919  - accuracy: 0.6875\n",
      "At: 447 [==========>] Loss 0.12682275350003447  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.1589272026580278  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.08389162371385833  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.1451557042921735  - accuracy: 0.78125\n",
      "At: 451 [==========>] Loss 0.13255799090522274  - accuracy: 0.84375\n",
      "At: 452 [==========>] Loss 0.15886223607818348  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.1460369517710563  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.21883966703665386  - accuracy: 0.6875\n",
      "At: 455 [==========>] Loss 0.18429447038105295  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.1471008757411701  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.14988916770542365  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.08845658619983393  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.20824482132081382  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.10869209914518196  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.21024391987730365  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.20996665863781794  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.14850302074875124  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.20482374530711212  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.20411212122259834  - accuracy: 0.65625\n",
      "At: 466 [==========>] Loss 0.10915612054484022  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.17152530489330928  - accuracy: 0.8125\n",
      "At: 468 [==========>] Loss 0.11795313747974681  - accuracy: 0.8125\n",
      "At: 469 [==========>] Loss 0.1415603574587762  - accuracy: 0.75\n",
      "At: 470 [==========>] Loss 0.12334664059858823  - accuracy: 0.8125\n",
      "At: 471 [==========>] Loss 0.15381421291727246  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.12068961795632899  - accuracy: 0.84375\n",
      "At: 473 [==========>] Loss 0.17918993118247412  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.16433382828834908  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.14614626988077017  - accuracy: 0.84375\n",
      "At: 476 [==========>] Loss 0.17700937007221362  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.13547502347284618  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.14469150358897548  - accuracy: 0.8125\n",
      "At: 479 [==========>] Loss 0.13690747167890535  - accuracy: 0.84375\n",
      "At: 480 [==========>] Loss 0.1838326122872606  - accuracy: 0.78125\n",
      "At: 481 [==========>] Loss 0.13951131456641308  - accuracy: 0.8125\n",
      "At: 482 [==========>] Loss 0.08280516427691431  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.08242777844261787  - accuracy: 0.90625\n",
      "At: 484 [==========>] Loss 0.1197498341925915  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.10230340334094601  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.19222421228100411  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.13766860799752417  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.12092827454469737  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.16825049556585403  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.14681292564019371  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.1705457470282703  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.19976156709882542  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.13971415226695855  - accuracy: 0.8125\n",
      "At: 494 [==========>] Loss 0.15293190014330083  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.11766638949015992  - accuracy: 0.84375\n",
      "At: 496 [==========>] Loss 0.19143383631135064  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.16983046339494015  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.10261213672179936  - accuracy: 0.84375\n",
      "At: 499 [==========>] Loss 0.19901254055435214  - accuracy: 0.71875\n",
      "At: 500 [==========>] Loss 0.1483030566710165  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.15587563253598802  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.14072007869669179  - accuracy: 0.78125\n",
      "At: 503 [==========>] Loss 0.13306906838407961  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.15275989986440552  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.20955251179478515  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.21913293136436338  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.09794931790934394  - accuracy: 0.8125\n",
      "At: 508 [==========>] Loss 0.11552368497019513  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.20451258415297274  - accuracy: 0.6875\n",
      "At: 510 [==========>] Loss 0.1845019149132522  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.12918789949522183  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.19489673869585247  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.215289657591902  - accuracy: 0.6875\n",
      "At: 514 [==========>] Loss 0.1526753202721329  - accuracy: 0.8125\n",
      "At: 515 [==========>] Loss 0.13086406494756947  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.19053607586557889  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.12962152188278708  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.15049455711807358  - accuracy: 0.75\n",
      "At: 519 [==========>] Loss 0.08464341099746238  - accuracy: 0.875\n",
      "At: 520 [==========>] Loss 0.1287049680528234  - accuracy: 0.78125\n",
      "At: 521 [==========>] Loss 0.1373806013005225  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.14775365763191226  - accuracy: 0.84375\n",
      "At: 523 [==========>] Loss 0.14246352666176743  - accuracy: 0.6875\n",
      "At: 524 [==========>] Loss 0.08525326802791264  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.14918081932674243  - accuracy: 0.75\n",
      "At: 526 [==========>] Loss 0.20207956270565053  - accuracy: 0.71875\n",
      "At: 527 [==========>] Loss 0.2729035254776985  - accuracy: 0.5625\n",
      "At: 528 [==========>] Loss 0.1993968562741915  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.137996827584479  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.13538717926157973  - accuracy: 0.875\n",
      "At: 531 [==========>] Loss 0.14733846733910763  - accuracy: 0.8125\n",
      "At: 532 [==========>] Loss 0.12865087304384404  - accuracy: 0.8125\n",
      "At: 533 [==========>] Loss 0.1013346776134117  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.15215473742625557  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.13789632298059448  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.17540120712366936  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.0840762066473105  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.17206256019188468  - accuracy: 0.75\n",
      "At: 539 [==========>] Loss 0.11520306754548154  - accuracy: 0.84375\n",
      "At: 540 [==========>] Loss 0.18893466920316818  - accuracy: 0.75\n",
      "At: 541 [==========>] Loss 0.17969069520284014  - accuracy: 0.78125\n",
      "At: 542 [==========>] Loss 0.17677785643429178  - accuracy: 0.75\n",
      "At: 543 [==========>] Loss 0.13852786558046257  - accuracy: 0.8125\n",
      "At: 544 [==========>] Loss 0.23273670400058055  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.0990011497625911  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.1954243331462477  - accuracy: 0.71875\n",
      "At: 547 [==========>] Loss 0.13223064335323986  - accuracy: 0.84375\n",
      "At: 548 [==========>] Loss 0.10100821934647353  - accuracy: 0.90625\n",
      "At: 549 [==========>] Loss 0.1307035849507532  - accuracy: 0.8125\n",
      "At: 550 [==========>] Loss 0.11237679670532874  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.14158175692624395  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.15959485696385858  - accuracy: 0.75\n",
      "At: 553 [==========>] Loss 0.10371323602331622  - accuracy: 0.84375\n",
      "At: 554 [==========>] Loss 0.09963308853093253  - accuracy: 0.90625\n",
      "At: 555 [==========>] Loss 0.15114783757217692  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.16512421911564432  - accuracy: 0.84375\n",
      "At: 557 [==========>] Loss 0.12131232861075615  - accuracy: 0.8125\n",
      "At: 558 [==========>] Loss 0.17098620929608815  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.2035999582712466  - accuracy: 0.75\n",
      "At: 560 [==========>] Loss 0.12872975644777646  - accuracy: 0.78125\n",
      "At: 561 [==========>] Loss 0.1340065004418974  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.058151664616431106  - accuracy: 0.96875\n",
      "At: 563 [==========>] Loss 0.11846839118306872  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.15625377046010192  - accuracy: 0.8125\n",
      "At: 565 [==========>] Loss 0.09593622380258879  - accuracy: 0.875\n",
      "At: 566 [==========>] Loss 0.15390763823098585  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.1844257365613891  - accuracy: 0.8125\n",
      "At: 568 [==========>] Loss 0.2368853705582855  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.1836315087963799  - accuracy: 0.71875\n",
      "At: 570 [==========>] Loss 0.1346952284777844  - accuracy: 0.8125\n",
      "At: 571 [==========>] Loss 0.11768955444669191  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.1380764413099112  - accuracy: 0.78125\n",
      "At: 573 [==========>] Loss 0.09985057352124263  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.17166335565832697  - accuracy: 0.78125\n",
      "At: 575 [==========>] Loss 0.13806256952949403  - accuracy: 0.84375\n",
      "At: 576 [==========>] Loss 0.11076399099754983  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.20513077518856765  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.1829519360067614  - accuracy: 0.78125\n",
      "At: 579 [==========>] Loss 0.07957550160034754  - accuracy: 0.90625\n",
      "At: 580 [==========>] Loss 0.1755565978768339  - accuracy: 0.71875\n",
      "At: 581 [==========>] Loss 0.15387605075444172  - accuracy: 0.75\n",
      "At: 582 [==========>] Loss 0.1155701579446359  - accuracy: 0.875\n",
      "At: 583 [==========>] Loss 0.1789695042898108  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.09880599357776994  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.1435504076729311  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.10083534329941356  - accuracy: 0.90625\n",
      "At: 587 [==========>] Loss 0.13380561584301137  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.14496666462280064  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.19174664375745615  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.08586174758433252  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.1479734642386913  - accuracy: 0.8125\n",
      "At: 592 [==========>] Loss 0.10915741943385077  - accuracy: 0.875\n",
      "At: 593 [==========>] Loss 0.1785373169455683  - accuracy: 0.71875\n",
      "At: 594 [==========>] Loss 0.13945520627192937  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.18168635175130582  - accuracy: 0.78125\n",
      "At: 596 [==========>] Loss 0.13809208161696596  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.23586246754998497  - accuracy: 0.6875\n",
      "At: 598 [==========>] Loss 0.1471878408132617  - accuracy: 0.78125\n",
      "At: 599 [==========>] Loss 0.17795514594818657  - accuracy: 0.71875\n",
      "At: 600 [==========>] Loss 0.1215889749627478  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.1130115195878856  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.09861013033864686  - accuracy: 0.9375\n",
      "At: 603 [==========>] Loss 0.16885114915580676  - accuracy: 0.71875\n",
      "At: 604 [==========>] Loss 0.2416326436932277  - accuracy: 0.59375\n",
      "At: 605 [==========>] Loss 0.10168256699361969  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.19568567128209996  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.1526931989637264  - accuracy: 0.75\n",
      "At: 608 [==========>] Loss 0.14609770973444852  - accuracy: 0.8125\n",
      "At: 609 [==========>] Loss 0.11896775707525413  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.14279420474369203  - accuracy: 0.8125\n",
      "At: 611 [==========>] Loss 0.11302730777985698  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.12472691325765826  - accuracy: 0.875\n",
      "At: 613 [==========>] Loss 0.15793133965851897  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.1414730100637461  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.19953099123864093  - accuracy: 0.71875\n",
      "At: 616 [==========>] Loss 0.15533396789614204  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.14040090799704505  - accuracy: 0.84375\n",
      "At: 618 [==========>] Loss 0.24111582005838378  - accuracy: 0.625\n",
      "At: 619 [==========>] Loss 0.13072269460532732  - accuracy: 0.84375\n",
      "At: 620 [==========>] Loss 0.15785480906219915  - accuracy: 0.78125\n",
      "At: 621 [==========>] Loss 0.07195038656497954  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.21306548253133817  - accuracy: 0.71875\n",
      "At: 623 [==========>] Loss 0.11111723510780133  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.10735398858335367  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.148058101173384  - accuracy: 0.78125\n",
      "At: 626 [==========>] Loss 0.12956646101321168  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.11421951061071786  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.1246036345430673  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.2295734269257131  - accuracy: 0.6875\n",
      "At: 630 [==========>] Loss 0.24746622841432225  - accuracy: 0.59375\n",
      "At: 631 [==========>] Loss 0.21511793904318682  - accuracy: 0.71875\n",
      "At: 632 [==========>] Loss 0.13140190291119905  - accuracy: 0.84375\n",
      "At: 633 [==========>] Loss 0.1791428327943699  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.17780675168175708  - accuracy: 0.71875\n",
      "At: 635 [==========>] Loss 0.1282133147174341  - accuracy: 0.8125\n",
      "At: 636 [==========>] Loss 0.16392765149236246  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.1437806706071315  - accuracy: 0.78125\n",
      "At: 638 [==========>] Loss 0.13735817782526663  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.13908186822793095  - accuracy: 0.75\n",
      "At: 640 [==========>] Loss 0.23350478316768808  - accuracy: 0.59375\n",
      "At: 641 [==========>] Loss 0.1577259273903723  - accuracy: 0.8125\n",
      "At: 642 [==========>] Loss 0.1665288362418467  - accuracy: 0.78125\n",
      "At: 643 [==========>] Loss 0.13761426151609135  - accuracy: 0.8125\n",
      "At: 644 [==========>] Loss 0.08163174790266133  - accuracy: 0.90625\n",
      "At: 645 [==========>] Loss 0.13382043118061981  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.10959232237338602  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.17131339335378856  - accuracy: 0.6875\n",
      "At: 648 [==========>] Loss 0.20701598207458338  - accuracy: 0.71875\n",
      "At: 649 [==========>] Loss 0.1810240447957019  - accuracy: 0.75\n",
      "At: 650 [==========>] Loss 0.08754089351031775  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.13759260935120443  - accuracy: 0.84375\n",
      "At: 652 [==========>] Loss 0.09569114226197697  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.13166557690897393  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.1423387844286839  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.15331722357578131  - accuracy: 0.75\n",
      "At: 656 [==========>] Loss 0.11275862441542511  - accuracy: 0.875\n",
      "At: 657 [==========>] Loss 0.11272899117246205  - accuracy: 0.84375\n",
      "At: 658 [==========>] Loss 0.14497239997631903  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.13871762434185406  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.1195496250099635  - accuracy: 0.875\n",
      "At: 661 [==========>] Loss 0.13254527362464713  - accuracy: 0.84375\n",
      "At: 662 [==========>] Loss 0.10925450113543453  - accuracy: 0.875\n",
      "At: 663 [==========>] Loss 0.09511387173659283  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.11078426535023911  - accuracy: 0.90625\n",
      "At: 665 [==========>] Loss 0.15290787399078581  - accuracy: 0.78125\n",
      "At: 666 [==========>] Loss 0.19968045300078818  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.11678338565418571  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.16855260085805962  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.1450077766982594  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.1792642257935418  - accuracy: 0.71875\n",
      "At: 671 [==========>] Loss 0.12324045724635597  - accuracy: 0.84375\n",
      "At: 672 [==========>] Loss 0.15284218559831947  - accuracy: 0.71875\n",
      "At: 673 [==========>] Loss 0.08401383267643837  - accuracy: 0.875\n",
      "At: 674 [==========>] Loss 0.13930340529556132  - accuracy: 0.75\n",
      "At: 675 [==========>] Loss 0.10652779534764813  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.13983057747143218  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.12693179721752262  - accuracy: 0.8125\n",
      "At: 678 [==========>] Loss 0.13276141484346343  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.09402595317333425  - accuracy: 0.90625\n",
      "At: 680 [==========>] Loss 0.11489673759556442  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.1329988002756288  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.129158285290565  - accuracy: 0.8125\n",
      "At: 683 [==========>] Loss 0.1476435019453637  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.11532231943488777  - accuracy: 0.8125\n",
      "At: 685 [==========>] Loss 0.15905575512007447  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.09488197382684367  - accuracy: 0.90625\n",
      "At: 687 [==========>] Loss 0.07987417667267918  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.09667195391206913  - accuracy: 0.875\n",
      "At: 689 [==========>] Loss 0.17449656272700984  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.16565052712037454  - accuracy: 0.8125\n",
      "At: 691 [==========>] Loss 0.1103022150204271  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.1123339779059835  - accuracy: 0.90625\n",
      "At: 693 [==========>] Loss 0.12042585752363966  - accuracy: 0.875\n",
      "At: 694 [==========>] Loss 0.17557805477437868  - accuracy: 0.71875\n",
      "At: 695 [==========>] Loss 0.13812985120257004  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.1475630608578829  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.1767461113073708  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.11366081995005523  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.11048950449081561  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.09012127666839448  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.17212146105241646  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.08861062319068966  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.1810074108891829  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.16069699271622861  - accuracy: 0.8125\n",
      "At: 705 [==========>] Loss 0.23035349840450103  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.16494825336399133  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.1015418145532617  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.1522548685855312  - accuracy: 0.78125\n",
      "At: 709 [==========>] Loss 0.1821515724508006  - accuracy: 0.8125\n",
      "At: 710 [==========>] Loss 0.15485824404260717  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.1904123148085735  - accuracy: 0.8125\n",
      "At: 712 [==========>] Loss 0.1461571899515765  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.21289004163409134  - accuracy: 0.6875\n",
      "At: 714 [==========>] Loss 0.19423233285773933  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.11358703425671839  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.11163156690061483  - accuracy: 0.875\n",
      "At: 717 [==========>] Loss 0.09753030405971251  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.20216031718155691  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.10917597389761094  - accuracy: 0.8125\n",
      "At: 720 [==========>] Loss 0.13958995389293105  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.09761429289375174  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.17963298504294267  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.11807795462095799  - accuracy: 0.78125\n",
      "At: 724 [==========>] Loss 0.08593141798641868  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.16419944108868034  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.1646627589250388  - accuracy: 0.8125\n",
      "At: 727 [==========>] Loss 0.12310229208049808  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.18236491638467967  - accuracy: 0.71875\n",
      "At: 729 [==========>] Loss 0.2001422873997461  - accuracy: 0.71875\n",
      "At: 730 [==========>] Loss 0.14939454705046928  - accuracy: 0.71875\n",
      "At: 731 [==========>] Loss 0.15964453426510422  - accuracy: 0.78125\n",
      "At: 732 [==========>] Loss 0.15803271542983896  - accuracy: 0.78125\n",
      "At: 733 [==========>] Loss 0.11158490787135503  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.2423412546727326  - accuracy: 0.625\n",
      "At: 735 [==========>] Loss 0.11656320047286625  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.11390304999285183  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.15413964449244943  - accuracy: 0.71875\n",
      "At: 738 [==========>] Loss 0.09195416606333494  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.08485465457561028  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.1662580621924732  - accuracy: 0.75\n",
      "At: 741 [==========>] Loss 0.13016643862564606  - accuracy: 0.875\n",
      "At: 742 [==========>] Loss 0.10255494796346955  - accuracy: 0.875\n",
      "At: 743 [==========>] Loss 0.1690070368237824  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.16023893012251125  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.17277811518831826  - accuracy: 0.6875\n",
      "At: 746 [==========>] Loss 0.15778230399946694  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.12283951945191388  - accuracy: 0.875\n",
      "At: 748 [==========>] Loss 0.18640914096528632  - accuracy: 0.6875\n",
      "At: 749 [==========>] Loss 0.18373512940540035  - accuracy: 0.75\n",
      "At: 750 [==========>] Loss 0.1168670218390574  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.1620810879055055  - accuracy: 0.71875\n",
      "At: 752 [==========>] Loss 0.09304494666936322  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.17510071991327572  - accuracy: 0.75\n",
      "At: 754 [==========>] Loss 0.20340207271604913  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.08134366575927428  - accuracy: 0.9375\n",
      "At: 756 [==========>] Loss 0.22355467181606326  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.11886571764060926  - accuracy: 0.8125\n",
      "At: 758 [==========>] Loss 0.13515842793165148  - accuracy: 0.8125\n",
      "At: 759 [==========>] Loss 0.06005401355035217  - accuracy: 0.9375\n",
      "At: 760 [==========>] Loss 0.15561597859157422  - accuracy: 0.6875\n",
      "At: 761 [==========>] Loss 0.13513523098870384  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.1388294101295009  - accuracy: 0.78125\n",
      "At: 763 [==========>] Loss 0.15807371060972186  - accuracy: 0.8125\n",
      "At: 764 [==========>] Loss 0.14415673212831037  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.19055352753149707  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.12392423932971089  - accuracy: 0.8125\n",
      "At: 767 [==========>] Loss 0.1356537090897087  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.1423881992740173  - accuracy: 0.84375\n",
      "At: 769 [==========>] Loss 0.12506130054845002  - accuracy: 0.8125\n",
      "At: 770 [==========>] Loss 0.1093924395663594  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.20165643508296793  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.13288111676609082  - accuracy: 0.84375\n",
      "At: 773 [==========>] Loss 0.10420607865445397  - accuracy: 0.875\n",
      "At: 774 [==========>] Loss 0.14514984023902966  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.25321489575008177  - accuracy: 0.625\n",
      "At: 776 [==========>] Loss 0.17770920897844214  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.09868303424411942  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.1826402646861362  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.09486943475234891  - accuracy: 0.875\n",
      "At: 780 [==========>] Loss 0.09500944976619563  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.154044938365066  - accuracy: 0.8125\n",
      "At: 782 [==========>] Loss 0.173440669755926  - accuracy: 0.75\n",
      "At: 783 [==========>] Loss 0.18823229173281175  - accuracy: 0.6875\n",
      "At: 784 [==========>] Loss 0.159111510720133  - accuracy: 0.75\n",
      "At: 785 [==========>] Loss 0.20148070339065374  - accuracy: 0.75\n",
      "At: 786 [==========>] Loss 0.15269959037984449  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.17323867013207933  - accuracy: 0.75\n",
      "At: 788 [==========>] Loss 0.08688093583192384  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.13204167365224573  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.13602329814036018  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.19589236567773638  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.18426654767092293  - accuracy: 0.65625\n",
      "At: 793 [==========>] Loss 0.11100881492907441  - accuracy: 0.9375\n",
      "At: 794 [==========>] Loss 0.15972605104714255  - accuracy: 0.6875\n",
      "At: 795 [==========>] Loss 0.12155685183491566  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.1259418309547221  - accuracy: 0.90625\n",
      "At: 797 [==========>] Loss 0.18629404476253436  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.16894454658674485  - accuracy: 0.78125\n",
      "At: 799 [==========>] Loss 0.055200736002804104  - accuracy: 0.9375\n",
      "At: 800 [==========>] Loss 0.15133407495979853  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.10082449138181615  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.19616528046309525  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.17494191320523683  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.14728804744116047  - accuracy: 0.78125\n",
      "At: 805 [==========>] Loss 0.16477538353350743  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.09051231838266027  - accuracy: 0.875\n",
      "At: 807 [==========>] Loss 0.09133933109413173  - accuracy: 0.875\n",
      "At: 808 [==========>] Loss 0.13391731376789962  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.11610906273472282  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.1725593978837876  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.12136343021465668  - accuracy: 0.8125\n",
      "At: 812 [==========>] Loss 0.13709084350969758  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.20340976109404035  - accuracy: 0.65625\n",
      "At: 814 [==========>] Loss 0.1807079611069174  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.11230806378445918  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.16591460783031725  - accuracy: 0.75\n",
      "At: 817 [==========>] Loss 0.056899687883575245  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.1506241704840353  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.10511277383563697  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.1221070030517805  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.1392582788949183  - accuracy: 0.8125\n",
      "At: 822 [==========>] Loss 0.15255771120735834  - accuracy: 0.75\n",
      "At: 823 [==========>] Loss 0.1616794532106381  - accuracy: 0.78125\n",
      "At: 824 [==========>] Loss 0.17046607149349852  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.207126865873331  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.13363161980755428  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.09412229690688706  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.0823175380810432  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.09109743317948354  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.10746918766964025  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.08507337372315088  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.13288698326874476  - accuracy: 0.78125\n",
      "At: 833 [==========>] Loss 0.18055152702085556  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.12679433090544862  - accuracy: 0.84375\n",
      "At: 835 [==========>] Loss 0.10500787573999293  - accuracy: 0.84375\n",
      "At: 836 [==========>] Loss 0.14297024315920495  - accuracy: 0.8125\n",
      "At: 837 [==========>] Loss 0.14006028007489893  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.11709357259651951  - accuracy: 0.875\n",
      "At: 839 [==========>] Loss 0.12983517337577083  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.10566806634727599  - accuracy: 0.84375\n",
      "At: 841 [==========>] Loss 0.06951484942584507  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.06996715818487562  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.1408425991631213  - accuracy: 0.8125\n",
      "At: 844 [==========>] Loss 0.15869939422759657  - accuracy: 0.78125\n",
      "At: 845 [==========>] Loss 0.1481926184716258  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.12777973982019483  - accuracy: 0.78125\n",
      "At: 847 [==========>] Loss 0.09570037148419068  - accuracy: 0.84375\n",
      "At: 848 [==========>] Loss 0.17284842353289312  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.1500681277563236  - accuracy: 0.75\n",
      "At: 850 [==========>] Loss 0.101283083013015  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.11220511352123125  - accuracy: 0.84375\n",
      "At: 852 [==========>] Loss 0.1457200309062254  - accuracy: 0.78125\n",
      "At: 853 [==========>] Loss 0.14159106925312173  - accuracy: 0.84375\n",
      "At: 854 [==========>] Loss 0.22067817348055863  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.0797023479743724  - accuracy: 0.9375\n",
      "At: 856 [==========>] Loss 0.10209673693814222  - accuracy: 0.84375\n",
      "At: 857 [==========>] Loss 0.06755873974541937  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.25446654176220873  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.13137276703022988  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.128062191077078  - accuracy: 0.84375\n",
      "At: 861 [==========>] Loss 0.09766544425997989  - accuracy: 0.84375\n",
      "At: 862 [==========>] Loss 0.13240191183441852  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.1584610175468466  - accuracy: 0.8125\n",
      "At: 864 [==========>] Loss 0.1710204374245613  - accuracy: 0.75\n",
      "At: 865 [==========>] Loss 0.1741334756027258  - accuracy: 0.75\n",
      "At: 866 [==========>] Loss 0.168769963455276  - accuracy: 0.75\n",
      "At: 867 [==========>] Loss 0.10476516797586764  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.18505315295746672  - accuracy: 0.75\n",
      "At: 869 [==========>] Loss 0.1540583226614487  - accuracy: 0.78125\n",
      "At: 870 [==========>] Loss 0.15669800647950713  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.08792898686704356  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.08654642237167609  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.2366273176379592  - accuracy: 0.71875\n",
      "At: 874 [==========>] Loss 0.14605975349209493  - accuracy: 0.75\n",
      "At: 875 [==========>] Loss 0.1292035482540299  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.14603377840232443  - accuracy: 0.78125\n",
      "At: 877 [==========>] Loss 0.15376959570216314  - accuracy: 0.78125\n",
      "At: 878 [==========>] Loss 0.04438111191846545  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.15842064011137308  - accuracy: 0.6875\n",
      "At: 880 [==========>] Loss 0.11273338532060165  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.14542510567758093  - accuracy: 0.8125\n",
      "At: 882 [==========>] Loss 0.11495683149992036  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.11823077060279699  - accuracy: 0.84375\n",
      "At: 884 [==========>] Loss 0.15179334902480998  - accuracy: 0.75\n",
      "At: 885 [==========>] Loss 0.11753434256837256  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.10044602729099375  - accuracy: 0.875\n",
      "At: 887 [==========>] Loss 0.14158265277082324  - accuracy: 0.78125\n",
      "At: 888 [==========>] Loss 0.1659318894760863  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.11450797715806435  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.19211916936544654  - accuracy: 0.71875\n",
      "At: 891 [==========>] Loss 0.10122686318412627  - accuracy: 0.8125\n",
      "At: 892 [==========>] Loss 0.12819887668658436  - accuracy: 0.84375\n",
      "At: 893 [==========>] Loss 0.16616192738531751  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.15370453629586056  - accuracy: 0.78125\n",
      "At: 895 [==========>] Loss 0.1468381223058971  - accuracy: 0.8125\n",
      "At: 896 [==========>] Loss 0.11821154621639712  - accuracy: 0.78125\n",
      "At: 897 [==========>] Loss 0.13690634470799107  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.1164559128368964  - accuracy: 0.84375\n",
      "At: 899 [==========>] Loss 0.11454883063837093  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.17949680157469589  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.169617288192639  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.11754912405976578  - accuracy: 0.8125\n",
      "At: 903 [==========>] Loss 0.13680708132421587  - accuracy: 0.78125\n",
      "At: 904 [==========>] Loss 0.1390863027032877  - accuracy: 0.8125\n",
      "At: 905 [==========>] Loss 0.0596946190808203  - accuracy: 0.9375\n",
      "At: 906 [==========>] Loss 0.07024794329605268  - accuracy: 0.9375\n",
      "At: 907 [==========>] Loss 0.17153535153680424  - accuracy: 0.71875\n",
      "At: 908 [==========>] Loss 0.11419403992988049  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.11002905623968252  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.12637679870359025  - accuracy: 0.84375\n",
      "At: 911 [==========>] Loss 0.16972834528361147  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.16054479071937416  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.12020258133321254  - accuracy: 0.90625\n",
      "At: 914 [==========>] Loss 0.13428415992606085  - accuracy: 0.75\n",
      "At: 915 [==========>] Loss 0.14731321999629424  - accuracy: 0.8125\n",
      "At: 916 [==========>] Loss 0.17558775043579494  - accuracy: 0.75\n",
      "At: 917 [==========>] Loss 0.17637794275934743  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.19329074519592881  - accuracy: 0.75\n",
      "At: 919 [==========>] Loss 0.1429470131611174  - accuracy: 0.78125\n",
      "At: 920 [==========>] Loss 0.11845884565412912  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.17281125564912878  - accuracy: 0.71875\n",
      "At: 922 [==========>] Loss 0.12205845527981889  - accuracy: 0.84375\n",
      "At: 923 [==========>] Loss 0.11919538129527549  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.19622429320519602  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.16712696281355113  - accuracy: 0.75\n",
      "At: 926 [==========>] Loss 0.14584945376828146  - accuracy: 0.8125\n",
      "At: 927 [==========>] Loss 0.12043198560802411  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.11728410912896159  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.16278385225219366  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.10394912811347726  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.17609057359845365  - accuracy: 0.8125\n",
      "At: 932 [==========>] Loss 0.08663109083855987  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.07523654692602114  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.1117444411423715  - accuracy: 0.78125\n",
      "At: 935 [==========>] Loss 0.04302180536661572  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.1609271960735858  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.16359331688166417  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.15137686953497587  - accuracy: 0.78125\n",
      "At: 939 [==========>] Loss 0.10720556334559019  - accuracy: 0.84375\n",
      "At: 940 [==========>] Loss 0.21765476975059572  - accuracy: 0.65625\n",
      "At: 941 [==========>] Loss 0.15051322316186208  - accuracy: 0.75\n",
      "At: 942 [==========>] Loss 0.1450749953302114  - accuracy: 0.875\n",
      "At: 943 [==========>] Loss 0.08420200357161786  - accuracy: 0.875\n",
      "At: 944 [==========>] Loss 0.12228513475381184  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.09287887346346163  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.15930570549685166  - accuracy: 0.8125\n",
      "At: 947 [==========>] Loss 0.12842522192136602  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.14904083610428026  - accuracy: 0.8125\n",
      "At: 949 [==========>] Loss 0.05862682761714162  - accuracy: 0.96875\n",
      "At: 950 [==========>] Loss 0.09660173639649375  - accuracy: 0.875\n",
      "At: 951 [==========>] Loss 0.09626754508476391  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.05701541732550372  - accuracy: 0.96875\n",
      "At: 953 [==========>] Loss 0.08729409571223229  - accuracy: 0.875\n",
      "At: 954 [==========>] Loss 0.10952391197745118  - accuracy: 0.90625\n",
      "At: 955 [==========>] Loss 0.1319214334080496  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.08841982318018507  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.1200987181370694  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.10045054831547826  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.15050467597781006  - accuracy: 0.78125\n",
      "At: 960 [==========>] Loss 0.11318937293976508  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.11950340791576351  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.08426014783137772  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.11682035204264048  - accuracy: 0.8125\n",
      "At: 964 [==========>] Loss 0.15948948348279773  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.1470365197951609  - accuracy: 0.78125\n",
      "At: 966 [==========>] Loss 0.12791303596897452  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.11278527082680331  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.1406693786704608  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.16538611876393905  - accuracy: 0.75\n",
      "At: 970 [==========>] Loss 0.11493920606595141  - accuracy: 0.78125\n",
      "At: 971 [==========>] Loss 0.11213564412239066  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.08458854618528895  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.13441724172196406  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.0835344253747457  - accuracy: 0.9375\n",
      "At: 975 [==========>] Loss 0.12545366104623532  - accuracy: 0.78125\n",
      "At: 976 [==========>] Loss 0.11576782234459292  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.0967694978588819  - accuracy: 0.90625\n",
      "At: 978 [==========>] Loss 0.15128264619624401  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.07723958084839769  - accuracy: 0.9375\n",
      "At: 980 [==========>] Loss 0.1575476402048162  - accuracy: 0.84375\n",
      "At: 981 [==========>] Loss 0.20192031450922185  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.06270020686484115  - accuracy: 0.96875\n",
      "At: 983 [==========>] Loss 0.14117090039058233  - accuracy: 0.78125\n",
      "At: 984 [==========>] Loss 0.11502884071687813  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.20827095381827118  - accuracy: 0.71875\n",
      "At: 986 [==========>] Loss 0.12253004870725515  - accuracy: 0.84375\n",
      "At: 987 [==========>] Loss 0.11554259739904713  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.08669925455592181  - accuracy: 0.90625\n",
      "At: 989 [==========>] Loss 0.14076949998440721  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.1320317244883979  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.13776554791387796  - accuracy: 0.875\n",
      "At: 992 [==========>] Loss 0.2231635273300369  - accuracy: 0.6875\n",
      "At: 993 [==========>] Loss 0.13417663263313495  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.1641238498277578  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.15124998638234136  - accuracy: 0.71875\n",
      "At: 996 [==========>] Loss 0.07192580543557518  - accuracy: 0.90625\n",
      "At: 997 [==========>] Loss 0.16037404746066303  - accuracy: 0.71875\n",
      "At: 998 [==========>] Loss 0.09183121826006796  - accuracy: 0.90625\n",
      "At: 999 [==========>] Loss 0.15539114032092344  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.25182957000477746  - accuracy: 0.625\n",
      "At: 1001 [==========>] Loss 0.1287008213608487  - accuracy: 0.84375\n",
      "At: 1002 [==========>] Loss 0.21198340316775552  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.09103744451371265  - accuracy: 0.90625\n",
      "At: 1004 [==========>] Loss 0.1479485861907468  - accuracy: 0.75\n",
      "At: 1005 [==========>] Loss 0.09201267753484167  - accuracy: 0.84375\n",
      "At: 1006 [==========>] Loss 0.12903803615313217  - accuracy: 0.84375\n",
      "At: 1007 [==========>] Loss 0.12214850905016071  - accuracy: 0.875\n",
      "At: 1008 [==========>] Loss 0.1884323703264636  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.11362275967589905  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.16952640464618324  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.13396871802465152  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.10660115180375943  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.06353928444202542  - accuracy: 0.96875\n",
      "At: 1014 [==========>] Loss 0.07627515283459516  - accuracy: 0.9375\n",
      "At: 1015 [==========>] Loss 0.18992169535186387  - accuracy: 0.6875\n",
      "At: 1016 [==========>] Loss 0.1209789800871953  - accuracy: 0.84375\n",
      "At: 1017 [==========>] Loss 0.16492905373232858  - accuracy: 0.71875\n",
      "At: 1018 [==========>] Loss 0.14130123691283092  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.2038745069175263  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.14084144731119755  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.11902487934104011  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.1533883035594899  - accuracy: 0.75\n",
      "At: 1023 [==========>] Loss 0.16121444046457006  - accuracy: 0.75\n",
      "At: 1024 [==========>] Loss 0.20275837037559355  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.18398840650749698  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.11709205098199141  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.11873129548152013  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.25161480648654344  - accuracy: 0.625\n",
      "At: 1029 [==========>] Loss 0.11169922310950377  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.1334311121270924  - accuracy: 0.8125\n",
      "At: 1031 [==========>] Loss 0.1743024028830985  - accuracy: 0.71875\n",
      "At: 1032 [==========>] Loss 0.14307267025525938  - accuracy: 0.8125\n",
      "At: 1033 [==========>] Loss 0.13239893026415414  - accuracy: 0.78125\n",
      "At: 1034 [==========>] Loss 0.09276358541214164  - accuracy: 0.875\n",
      "At: 1035 [==========>] Loss 0.09654474769697392  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.16287545056279373  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.15931982447271426  - accuracy: 0.6875\n",
      "At: 1038 [==========>] Loss 0.10489556031149004  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.108498982049473  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.10202918058009916  - accuracy: 0.875\n",
      "At: 1041 [==========>] Loss 0.17598462603613013  - accuracy: 0.71875\n",
      "At: 1042 [==========>] Loss 0.11682955550408755  - accuracy: 0.875\n",
      "At: 1043 [==========>] Loss 0.19261641953942382  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.1268870038898891  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.16055511482188622  - accuracy: 0.78125\n",
      "At: 1046 [==========>] Loss 0.12996786047440503  - accuracy: 0.84375\n",
      "At: 1047 [==========>] Loss 0.12198180147875343  - accuracy: 0.8125\n",
      "At: 1048 [==========>] Loss 0.17771011797265418  - accuracy: 0.6875\n",
      "At: 1049 [==========>] Loss 0.15038477818982338  - accuracy: 0.84375\n",
      "At: 1050 [==========>] Loss 0.1467010171932071  - accuracy: 0.78125\n",
      "At: 1051 [==========>] Loss 0.08397894068788678  - accuracy: 0.90625\n",
      "At: 1052 [==========>] Loss 0.1296534963363651  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.127343395530619  - accuracy: 0.75\n",
      "At: 1054 [==========>] Loss 0.12252239074290894  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.16738690668808753  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.1174337280267883  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.13932301101891406  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.05633773365789273  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.08203921244913082  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.09427174780160195  - accuracy: 0.90625\n",
      "At: 1061 [==========>] Loss 0.09897583260376121  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.16846491256602408  - accuracy: 0.75\n",
      "At: 1063 [==========>] Loss 0.14760377416379464  - accuracy: 0.78125\n",
      "At: 1064 [==========>] Loss 0.14300936646167742  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.07904407757039938  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.11856549441175225  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.14208477967994232  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.13056011838660084  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.12350242923764396  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.13079033329268216  - accuracy: 0.8125\n",
      "At: 1071 [==========>] Loss 0.11492189334121916  - accuracy: 0.8125\n",
      "At: 1072 [==========>] Loss 0.13840214720051125  - accuracy: 0.84375\n",
      "At: 1073 [==========>] Loss 0.16619491000488412  - accuracy: 0.75\n",
      "At: 1074 [==========>] Loss 0.1873360606600309  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.12762667243775505  - accuracy: 0.875\n",
      "At: 1076 [==========>] Loss 0.12563701894932666  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.0835443691977911  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.08206378688036517  - accuracy: 0.90625\n",
      "At: 1079 [==========>] Loss 0.11976019292875373  - accuracy: 0.875\n",
      "At: 1080 [==========>] Loss 0.1570642763135809  - accuracy: 0.71875\n",
      "At: 1081 [==========>] Loss 0.11801718667726638  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.12820905115177544  - accuracy: 0.8125\n",
      "At: 1083 [==========>] Loss 0.11532784619141931  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.07092921940943328  - accuracy: 0.96875\n",
      "At: 1085 [==========>] Loss 0.11940280309364032  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.10052419784593127  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.11890907162843262  - accuracy: 0.90625\n",
      "At: 1088 [==========>] Loss 0.1687811023644229  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.09141934294016961  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.08795407001961404  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.19557552475069823  - accuracy: 0.75\n",
      "At: 1092 [==========>] Loss 0.11103799014472235  - accuracy: 0.875\n",
      "At: 1093 [==========>] Loss 0.15273076614580194  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.1549073138786004  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.13250087689381584  - accuracy: 0.875\n",
      "At: 1096 [==========>] Loss 0.08982549662711872  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.07874102091225826  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.1503824972249712  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.13702189557238287  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.08556983709315572  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.12318763353816801  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.136089767558139  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.11614847413788365  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.09636865138732899  - accuracy: 0.875\n",
      "At: 1105 [==========>] Loss 0.08961538980871131  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.08893335147139456  - accuracy: 0.84375\n",
      "At: 1107 [==========>] Loss 0.1984265740791739  - accuracy: 0.71875\n",
      "At: 1108 [==========>] Loss 0.08944639078269725  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.06029007854711055  - accuracy: 0.90625\n",
      "At: 1110 [==========>] Loss 0.12706384375398538  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.19573853811419428  - accuracy: 0.78125\n",
      "At: 1112 [==========>] Loss 0.1445844233379886  - accuracy: 0.78125\n",
      "At: 1113 [==========>] Loss 0.14196187995522605  - accuracy: 0.75\n",
      "At: 1114 [==========>] Loss 0.06581889845344535  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.12298354656169991  - accuracy: 0.78125\n",
      "At: 1116 [==========>] Loss 0.14817019121297456  - accuracy: 0.78125\n",
      "At: 1117 [==========>] Loss 0.07845013375626848  - accuracy: 0.9375\n",
      "At: 1118 [==========>] Loss 0.11555534586879411  - accuracy: 0.8125\n",
      "At: 1119 [==========>] Loss 0.15218052989209727  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.06241888450075028  - accuracy: 0.9375\n",
      "At: 1121 [==========>] Loss 0.128150620418279  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.07922497894102723  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.14840784764843562  - accuracy: 0.78125\n",
      "At: 1124 [==========>] Loss 0.1423505693393325  - accuracy: 0.78125\n",
      "At: 1125 [==========>] Loss 0.16482827017501467  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.09277934856865949  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.1347803958243166  - accuracy: 0.75\n",
      "At: 1128 [==========>] Loss 0.0708407882521785  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.14759447391089825  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.13631593596758995  - accuracy: 0.8125\n",
      "At: 1131 [==========>] Loss 0.10811219285388435  - accuracy: 0.84375\n",
      "At: 1132 [==========>] Loss 0.11484154023574258  - accuracy: 0.8125\n",
      "At: 1133 [==========>] Loss 0.1401190342882396  - accuracy: 0.75\n",
      "At: 1134 [==========>] Loss 0.08924883031963851  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.08581397222305778  - accuracy: 0.90625\n",
      "At: 1136 [==========>] Loss 0.15604573987629072  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.13085399239366038  - accuracy: 0.8125\n",
      "At: 1138 [==========>] Loss 0.0950896476945029  - accuracy: 0.875\n",
      "At: 1139 [==========>] Loss 0.08817077749699001  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.17658468276154587  - accuracy: 0.65625\n",
      "At: 1141 [==========>] Loss 0.15087242114673644  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.12944974477561816  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.07894616919623904  - accuracy: 0.875\n",
      "At: 1144 [==========>] Loss 0.12466540037510127  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.11782618106631362  - accuracy: 0.84375\n",
      "At: 1146 [==========>] Loss 0.12538452594768645  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.2351037738317276  - accuracy: 0.65625\n",
      "At: 1148 [==========>] Loss 0.06270071357855836  - accuracy: 0.9375\n",
      "At: 1149 [==========>] Loss 0.10770657654973072  - accuracy: 0.8125\n",
      "At: 1150 [==========>] Loss 0.13044232765453964  - accuracy: 0.8125\n",
      "At: 1151 [==========>] Loss 0.17413228294381453  - accuracy: 0.71875\n",
      "At: 1152 [==========>] Loss 0.09645239533494193  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.18150464545004666  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.1472919110532763  - accuracy: 0.8125\n",
      "At: 1155 [==========>] Loss 0.11665231353752445  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.12373966893512527  - accuracy: 0.84375\n",
      "At: 1157 [==========>] Loss 0.10142922467336886  - accuracy: 0.875\n",
      "At: 1158 [==========>] Loss 0.14447815561360697  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.11078463989388077  - accuracy: 0.9375\n",
      "At: 1160 [==========>] Loss 0.1014137198789678  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.11204549624465189  - accuracy: 0.84375\n",
      "At: 1162 [==========>] Loss 0.14901670600228822  - accuracy: 0.78125\n",
      "At: 1163 [==========>] Loss 0.20423136117507312  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.07490765515097192  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.14553775036804362  - accuracy: 0.8125\n",
      "At: 1166 [==========>] Loss 0.09915504004529284  - accuracy: 0.84375\n",
      "At: 1167 [==========>] Loss 0.13438956578906447  - accuracy: 0.8125\n",
      "At: 1168 [==========>] Loss 0.123811803412593  - accuracy: 0.78125\n",
      "At: 1169 [==========>] Loss 0.11262843359401804  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.16101519119708058  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.06586865691180258  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.13402401192389532  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.14906506264231079  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.19683961607952272  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.1326957944466834  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.12411900071304596  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.08096460695684325  - accuracy: 0.84375\n",
      "At: 1178 [==========>] Loss 0.1507376349385316  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.1305878591485793  - accuracy: 0.75\n",
      "At: 1180 [==========>] Loss 0.1691598247935382  - accuracy: 0.8125\n",
      "At: 1181 [==========>] Loss 0.09144939590252771  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.0996510290002328  - accuracy: 0.875\n",
      "At: 1183 [==========>] Loss 0.1597585980247956  - accuracy: 0.8125\n",
      "At: 1184 [==========>] Loss 0.14397386165060438  - accuracy: 0.75\n",
      "At: 1185 [==========>] Loss 0.09008373334886002  - accuracy: 0.90625\n",
      "At: 1186 [==========>] Loss 0.15589797167369118  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.1239551497611698  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.07171112709223998  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.15337867705821692  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.07910603090934967  - accuracy: 0.9375\n",
      "At: 1191 [==========>] Loss 0.1835462350570909  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.09049564082531267  - accuracy: 0.9375\n",
      "At: 1193 [==========>] Loss 0.12278678096815593  - accuracy: 0.78125\n",
      "At: 1194 [==========>] Loss 0.132705858969354  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.13463372668940704  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.11429980955219968  - accuracy: 0.8125\n",
      "At: 1197 [==========>] Loss 0.11007585348284316  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.07032620866515826  - accuracy: 0.9375\n",
      "At: 1199 [==========>] Loss 0.12506841824517634  - accuracy: 0.78125\n",
      "At: 1200 [==========>] Loss 0.06827658038380187  - accuracy: 0.90625\n",
      "At: 1201 [==========>] Loss 0.09869802940048927  - accuracy: 0.9375\n",
      "At: 1202 [==========>] Loss 0.15940390195892495  - accuracy: 0.8125\n",
      "At: 1203 [==========>] Loss 0.14049765658527114  - accuracy: 0.71875\n",
      "At: 1204 [==========>] Loss 0.08264173017574869  - accuracy: 0.875\n",
      "At: 1205 [==========>] Loss 0.06952195232714989  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.09377278126767731  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.15029326448576194  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.1043657042443083  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.11394258216113967  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.13706373872587152  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.15643504100129885  - accuracy: 0.75\n",
      "At: 1212 [==========>] Loss 0.10531282259986147  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.21000325486438776  - accuracy: 0.625\n",
      "At: 1214 [==========>] Loss 0.15557725100105851  - accuracy: 0.84375\n",
      "At: 1215 [==========>] Loss 0.14892406232388095  - accuracy: 0.875\n",
      "At: 1216 [==========>] Loss 0.1071316638276496  - accuracy: 0.84375\n",
      "At: 1217 [==========>] Loss 0.08816552001980618  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.1300811805009448  - accuracy: 0.8125\n",
      "At: 1219 [==========>] Loss 0.13408784418560596  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.11832101502361454  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.09093957750736573  - accuracy: 0.90625\n",
      "At: 1222 [==========>] Loss 0.1827644946391897  - accuracy: 0.65625\n",
      "At: 1223 [==========>] Loss 0.11447304749509961  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08794158776601754  - accuracy: 0.96875\n",
      "At: 1225 [==========>] Loss 0.1063127554770536  - accuracy: 0.90625\n",
      "At: 1226 [==========>] Loss 0.1101903016925111  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.13837644557454293  - accuracy: 0.84375\n",
      "At: 1228 [==========>] Loss 0.15229036790755401  - accuracy: 0.78125\n",
      "At: 1229 [==========>] Loss 0.1355820706218482  - accuracy: 0.75\n",
      "At: 1230 [==========>] Loss 0.12964157390895453  - accuracy: 0.8125\n",
      "At: 1231 [==========>] Loss 0.11242450607320897  - accuracy: 0.84375\n",
      "At: 1232 [==========>] Loss 0.09027903773858617  - accuracy: 0.875\n",
      "At: 1233 [==========>] Loss 0.11803491247886969  - accuracy: 0.75\n",
      "At: 1234 [==========>] Loss 0.15140977313455492  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.06832976598622735  - accuracy: 0.96875\n",
      "At: 1236 [==========>] Loss 0.16373180157626588  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.08671742707801278  - accuracy: 0.84375\n",
      "At: 1238 [==========>] Loss 0.09801126426684101  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.21618566643209136  - accuracy: 0.71875\n",
      "At: 1240 [==========>] Loss 0.12477075554413157  - accuracy: 0.78125\n",
      "At: 1241 [==========>] Loss 0.13550190563890052  - accuracy: 0.78125\n",
      "At: 1242 [==========>] Loss 0.15021287176463846  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.15386331288366895  - accuracy: 0.75\n",
      "At: 1244 [==========>] Loss 0.16649425793036374  - accuracy: 0.75\n",
      "At: 1245 [==========>] Loss 0.11466671207182948  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.07203394376205842  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.1731758181165415  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.10143367835387859  - accuracy: 0.84375\n",
      "At: 1249 [==========>] Loss 0.1174840117647426  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.1429984330396929  - accuracy: 0.8125\n",
      "At: 1251 [==========>] Loss 0.12112040073156695  - accuracy: 0.84375\n",
      "At: 1252 [==========>] Loss 0.10698158213515527  - accuracy: 0.84375\n",
      "At: 1253 [==========>] Loss 0.08954566389609427  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.1749642880905003  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.08327319969879982  - accuracy: 0.875\n",
      "At: 1256 [==========>] Loss 0.12243503703023528  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.14782416553995298  - accuracy: 0.78125\n",
      "At: 1258 [==========>] Loss 0.08456828984853429  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.15031940358457446  - accuracy: 0.71875\n",
      "At: 1260 [==========>] Loss 0.10472445606692764  - accuracy: 0.84375\n",
      "At: 1261 [==========>] Loss 0.14682355580841666  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.13710847159085013  - accuracy: 0.78125\n",
      "At: 1263 [==========>] Loss 0.10336639106674023  - accuracy: 0.90625\n",
      "At: 1264 [==========>] Loss 0.09316620473540896  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.13695585569836854  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.11547575701999563  - accuracy: 0.78125\n",
      "At: 1267 [==========>] Loss 0.13625866448943313  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.16673439023925313  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.15042678164373918  - accuracy: 0.75\n",
      "At: 1270 [==========>] Loss 0.12192890682396731  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.16446726701285797  - accuracy: 0.75\n",
      "At: 1272 [==========>] Loss 0.08740351526369991  - accuracy: 0.875\n",
      "At: 1273 [==========>] Loss 0.23168572130436832  - accuracy: 0.65625\n",
      "At: 1274 [==========>] Loss 0.13330195085476954  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.10474246923225605  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.09027226896532958  - accuracy: 0.9375\n",
      "At: 1277 [==========>] Loss 0.09167610516426493  - accuracy: 0.8125\n",
      "At: 1278 [==========>] Loss 0.13858050357930693  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.10688976783261256  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.10315718088988957  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.1484947887344225  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.12366763729213981  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.1345775078451219  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.16753868296797916  - accuracy: 0.71875\n",
      "At: 1285 [==========>] Loss 0.08267918540656843  - accuracy: 0.84375\n",
      "At: 1286 [==========>] Loss 0.13113092730155002  - accuracy: 0.84375\n",
      "At: 1287 [==========>] Loss 0.1166318171739766  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.16650758683696779  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.1150737769962492  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.16258252342589624  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.1639832517271413  - accuracy: 0.71875\n",
      "At: 1292 [==========>] Loss 0.10850689619085245  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.14767738138541536  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.15118124244634865  - accuracy: 0.78125\n",
      "At: 1295 [==========>] Loss 0.1870199429890616  - accuracy: 0.78125\n",
      "At: 1296 [==========>] Loss 0.15283079142819278  - accuracy: 0.75\n",
      "At: 1297 [==========>] Loss 0.1230686672771485  - accuracy: 0.875\n",
      "At: 1298 [==========>] Loss 0.10232939967237709  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.1256123770528143  - accuracy: 0.84375\n",
      "At: 1300 [==========>] Loss 0.1397356899348377  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.11920233485150578  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.07971015135080713  - accuracy: 0.90625\n",
      "At: 1303 [==========>] Loss 0.12436439607607483  - accuracy: 0.8125\n",
      "At: 1304 [==========>] Loss 0.12638686821368567  - accuracy: 0.78125\n",
      "At: 1305 [==========>] Loss 0.13475750529721725  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.07175475898753722  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.14873515499451856  - accuracy: 0.8125\n",
      "At: 1308 [==========>] Loss 0.06665526970805288  - accuracy: 0.9375\n",
      "At: 1309 [==========>] Loss 0.19028713212016313  - accuracy: 0.75\n",
      "At: 1310 [==========>] Loss 0.1462035342463014  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.16305482396433302  - accuracy: 0.75\n",
      "At: 1312 [==========>] Loss 0.06309444835370703  - accuracy: 0.9375\n",
      "At: 1313 [==========>] Loss 0.16865545973487514  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.05484464164711206  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.16176116883420572  - accuracy: 0.6875\n",
      "At: 1316 [==========>] Loss 0.1283732270421808  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.11655881380417522  - accuracy: 0.875\n",
      "At: 1318 [==========>] Loss 0.11301838121826112  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.13723294173070577  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.09976953389814033  - accuracy: 0.875\n",
      "At: 1321 [==========>] Loss 0.06653647625427944  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.15944089431629638  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.0924307758633485  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.16366648931042396  - accuracy: 0.75\n",
      "At: 1325 [==========>] Loss 0.1211568728295763  - accuracy: 0.84375\n",
      "At: 1326 [==========>] Loss 0.08988465477098592  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.11609282840134083  - accuracy: 0.90625\n",
      "At: 1328 [==========>] Loss 0.07527624248879597  - accuracy: 0.90625\n",
      "At: 1329 [==========>] Loss 0.05509795977612123  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.12113016767070067  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.1554237532106833  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.11583511384671548  - accuracy: 0.875\n",
      "At: 1333 [==========>] Loss 0.15447744706880967  - accuracy: 0.71875\n",
      "At: 1334 [==========>] Loss 0.12687257658098058  - accuracy: 0.8125\n",
      "At: 1335 [==========>] Loss 0.10626074537441715  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.12710661329135003  - accuracy: 0.8125\n",
      "At: 1337 [==========>] Loss 0.1840105316144886  - accuracy: 0.78125\n",
      "At: 1338 [==========>] Loss 0.15975768200195484  - accuracy: 0.8125\n",
      "At: 1339 [==========>] Loss 0.15093495192297568  - accuracy: 0.71875\n",
      "At: 1340 [==========>] Loss 0.13080820531642284  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.07881202066907231  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.12395530287193898  - accuracy: 0.875\n",
      "At: 1343 [==========>] Loss 0.19116920047858188  - accuracy: 0.6875\n",
      "At: 1344 [==========>] Loss 0.20883840039966922  - accuracy: 0.6875\n",
      "At: 1345 [==========>] Loss 0.1000820099708588  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.09244742927065287  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.10125302506390785  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.10680629760357266  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.16060632109831463  - accuracy: 0.71875\n",
      "At: 1350 [==========>] Loss 0.15269905789865096  - accuracy: 0.75\n",
      "At: 1351 [==========>] Loss 0.09902033913458003  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.09181012004863398  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.16665146223225732  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.16398467987767473  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.08799073289355674  - accuracy: 0.875\n",
      "At: 1356 [==========>] Loss 0.11461827924625773  - accuracy: 0.8125\n",
      "At: 1357 [==========>] Loss 0.11760770709629895  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.1188139862993835  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.06618890910231995  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.16004545644258644  - accuracy: 0.84375\n",
      "At: 1361 [==========>] Loss 0.09747220785489885  - accuracy: 0.84375\n",
      "At: 1362 [==========>] Loss 0.1459212578245539  - accuracy: 0.75\n",
      "At: 1363 [==========>] Loss 0.11080556356335937  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.15337252492211853  - accuracy: 0.75\n",
      "At: 1365 [==========>] Loss 0.108410407582148  - accuracy: 0.84375\n",
      "At: 1366 [==========>] Loss 0.17183040083318482  - accuracy: 0.71875\n",
      "At: 1367 [==========>] Loss 0.107191784112137  - accuracy: 0.84375\n",
      "At: 1368 [==========>] Loss 0.17699908625482905  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.09890170020020807  - accuracy: 0.875\n",
      "At: 1370 [==========>] Loss 0.08600623638711823  - accuracy: 0.875\n",
      "At: 1371 [==========>] Loss 0.1783252186227931  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.09780227016253809  - accuracy: 0.875\n",
      "At: 1373 [==========>] Loss 0.10966965431451546  - accuracy: 0.90625\n",
      "At: 1374 [==========>] Loss 0.16148488688850898  - accuracy: 0.78125\n",
      "At: 1375 [==========>] Loss 0.11973961375428825  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.10403973291649075  - accuracy: 0.875\n",
      "At: 1377 [==========>] Loss 0.17169151511232789  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.1414444390420055  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.15050267927916927  - accuracy: 0.75\n",
      "At: 1380 [==========>] Loss 0.14738550907968784  - accuracy: 0.78125\n",
      "At: 1381 [==========>] Loss 0.08365707189007762  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.13390767699728415  - accuracy: 0.78125\n",
      "At: 1383 [==========>] Loss 0.09320587313474397  - accuracy: 0.90625\n",
      "At: 1384 [==========>] Loss 0.1090743969145909  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.1269294532473497  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.14917732575069031  - accuracy: 0.84375\n",
      "At: 1387 [==========>] Loss 0.07044002785098026  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.16892368034424635  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.1082001556684449  - accuracy: 0.84375\n",
      "At: 1390 [==========>] Loss 0.16353384358937262  - accuracy: 0.8125\n",
      "At: 1391 [==========>] Loss 0.10710326474788492  - accuracy: 0.8125\n",
      "At: 1392 [==========>] Loss 0.067984798256228  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.11941584407718109  - accuracy: 0.875\n",
      "At: 1394 [==========>] Loss 0.09144758720024991  - accuracy: 0.8125\n",
      "At: 1395 [==========>] Loss 0.221825832778452  - accuracy: 0.65625\n",
      "At: 1396 [==========>] Loss 0.05502080679672036  - accuracy: 0.90625\n",
      "At: 1397 [==========>] Loss 0.12406798091468339  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.10537908223605705  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.12895652698143562  - accuracy: 0.8125\n",
      "At: 1400 [==========>] Loss 0.15545936912880115  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.0970932261397637  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.15029298434584548  - accuracy: 0.6875\n",
      "At: 1403 [==========>] Loss 0.1568682005918846  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.11023874403815688  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.09181111091467553  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.18327978010440976  - accuracy: 0.71875\n",
      "At: 1407 [==========>] Loss 0.10334432761094978  - accuracy: 0.8125\n",
      "At: 1408 [==========>] Loss 0.11966518760269287  - accuracy: 0.84375\n",
      "At: 1409 [==========>] Loss 0.04325711983983539  - accuracy: 0.96875\n",
      "At: 1410 [==========>] Loss 0.15372009006783588  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.1512588048077677  - accuracy: 0.75\n",
      "At: 1412 [==========>] Loss 0.12587955456574174  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.07784554244998698  - accuracy: 0.90625\n",
      "At: 1414 [==========>] Loss 0.1692399640646409  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.08495795976806153  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.16083630660641024  - accuracy: 0.71875\n",
      "At: 1417 [==========>] Loss 0.11294907404608744  - accuracy: 0.8125\n",
      "At: 1418 [==========>] Loss 0.11528540486824641  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.13491544487937124  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.11115479707950479  - accuracy: 0.84375\n",
      "At: 1421 [==========>] Loss 0.10270453943894393  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.13233445953663653  - accuracy: 0.78125\n",
      "At: 1423 [==========>] Loss 0.1467748009141226  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.14769679196459803  - accuracy: 0.78125\n",
      "At: 1425 [==========>] Loss 0.11597101044711326  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.1547592844528088  - accuracy: 0.78125\n",
      "At: 1427 [==========>] Loss 0.14240643810483833  - accuracy: 0.78125\n",
      "At: 1428 [==========>] Loss 0.10898629895598447  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.13139262509457067  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.09439609499688024  - accuracy: 0.84375\n",
      "At: 1431 [==========>] Loss 0.11760171203905412  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.11482406392539848  - accuracy: 0.8125\n",
      "At: 1433 [==========>] Loss 0.09071390069477243  - accuracy: 0.875\n",
      "At: 1434 [==========>] Loss 0.17436125712834424  - accuracy: 0.78125\n",
      "At: 1435 [==========>] Loss 0.11330059452417868  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.08363226465426887  - accuracy: 0.84375\n",
      "At: 1437 [==========>] Loss 0.12207694114066825  - accuracy: 0.875\n",
      "At: 1438 [==========>] Loss 0.14320944000481786  - accuracy: 0.84375\n",
      "At: 1439 [==========>] Loss 0.11291083504597904  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.12568572936352582  - accuracy: 0.8125\n",
      "At: 1441 [==========>] Loss 0.08070712610349806  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.10746133489545501  - accuracy: 0.84375\n",
      "At: 1443 [==========>] Loss 0.13037435760861393  - accuracy: 0.875\n",
      "At: 1444 [==========>] Loss 0.12767097858770238  - accuracy: 0.8125\n",
      "At: 1445 [==========>] Loss 0.16372885329977185  - accuracy: 0.71875\n",
      "At: 1446 [==========>] Loss 0.146614284296405  - accuracy: 0.75\n",
      "At: 1447 [==========>] Loss 0.17141446168750196  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.08475999196120457  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.12986387412726771  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.1402087837624012  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.12908253960847343  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.10213182942806143  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.05339022988777456  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.13293529995087733  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.12136940536632786  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.099634197634031  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.09000739099414737  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.1444119863000934  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.13565494981002924  - accuracy: 0.8125\n",
      "At: 1460 [==========>] Loss 0.15081468874128834  - accuracy: 0.78125\n",
      "At: 1461 [==========>] Loss 0.10389750229080766  - accuracy: 0.875\n",
      "At: 1462 [==========>] Loss 0.16360878690564948  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.09740423967501023  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.17916072248089937  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.12403412858640163  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.10474663850990823  - accuracy: 0.84375\n",
      "At: 1467 [==========>] Loss 0.18190527638554255  - accuracy: 0.6875\n",
      "At: 1468 [==========>] Loss 0.16412160247268295  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.16888416266286557  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.12484786792305816  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.14607039372508282  - accuracy: 0.71875\n",
      "At: 1472 [==========>] Loss 0.06844270462747062  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.12499493244831573  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.20199974246474683  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.16151581236783957  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.1176428183544194  - accuracy: 0.78125\n",
      "At: 1477 [==========>] Loss 0.11669363587631744  - accuracy: 0.78125\n",
      "At: 1478 [==========>] Loss 0.1047469996015537  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.14059844403194732  - accuracy: 0.78125\n",
      "At: 1480 [==========>] Loss 0.08460984065586381  - accuracy: 0.9375\n",
      "At: 1481 [==========>] Loss 0.10807011361418922  - accuracy: 0.8125\n",
      "At: 1482 [==========>] Loss 0.10927359757188362  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.17936606624729823  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.14586919496753392  - accuracy: 0.8125\n",
      "At: 1485 [==========>] Loss 0.1659593986371428  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.09033051969308961  - accuracy: 0.84375\n",
      "At: 1487 [==========>] Loss 0.06557541935030677  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.13999900420437034  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.2220354048071129  - accuracy: 0.65625\n",
      "At: 1490 [==========>] Loss 0.12342168211988697  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.17655583085635862  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.14751174952576165  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.20290085415663045  - accuracy: 0.65625\n",
      "At: 1494 [==========>] Loss 0.15201584903307747  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.14413344099073977  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.09631097065687697  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.17401800065825224  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.17155142066157805  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.09805990308771459  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.11164797684720858  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.09229259766047478  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.12917627792550207  - accuracy: 0.84375\n",
      "At: 1503 [==========>] Loss 0.10615387303096543  - accuracy: 0.875\n",
      "At: 1504 [==========>] Loss 0.13702412871081432  - accuracy: 0.78125\n",
      "At: 1505 [==========>] Loss 0.13050939534014755  - accuracy: 0.75\n",
      "At: 1506 [==========>] Loss 0.15617367068690913  - accuracy: 0.71875\n",
      "At: 1507 [==========>] Loss 0.13393669400967506  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.21709474475778945  - accuracy: 0.6875\n",
      "At: 1509 [==========>] Loss 0.10000166365654647  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.1252890552180161  - accuracy: 0.78125\n",
      "At: 1511 [==========>] Loss 0.12296560456544689  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.09638835314845576  - accuracy: 0.875\n",
      "At: 1513 [==========>] Loss 0.15069528892509837  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.14212226097625802  - accuracy: 0.875\n",
      "At: 1515 [==========>] Loss 0.13273349098039883  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.09591550456672183  - accuracy: 0.90625\n",
      "At: 1517 [==========>] Loss 0.16110740909681806  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.09556152613244703  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.15609891743214865  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.1071570451817481  - accuracy: 0.875\n",
      "At: 1521 [==========>] Loss 0.07442131388678747  - accuracy: 0.9375\n",
      "At: 1522 [==========>] Loss 0.15883450634880822  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.13620927026245896  - accuracy: 0.8125\n",
      "At: 1524 [==========>] Loss 0.1415176112111381  - accuracy: 0.71875\n",
      "At: 1525 [==========>] Loss 0.1524302627251369  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.12430345710145319  - accuracy: 0.8125\n",
      "At: 1527 [==========>] Loss 0.12998140149164394  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.12156528357932864  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.08322463927270812  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.053293115438938374  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.141290357534866  - accuracy: 0.78125\n",
      "At: 1532 [==========>] Loss 0.18207903020393587  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.14954807475724483  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.10667510343769243  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.17616383849313771  - accuracy: 0.71875\n",
      "At: 1536 [==========>] Loss 0.18246729259612374  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.1024639667770828  - accuracy: 0.875\n",
      "At: 1538 [==========>] Loss 0.1301463234958434  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.05671897938153615  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.12652620313503715  - accuracy: 0.78125\n",
      "At: 1541 [==========>] Loss 0.1254709840276617  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.08198124818141726  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.1473891409526977  - accuracy: 0.75\n",
      "At: 1544 [==========>] Loss 0.14408204261119617  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.2054369778787164  - accuracy: 0.6875\n",
      "At: 1546 [==========>] Loss 0.13178718123635344  - accuracy: 0.8125\n",
      "At: 1547 [==========>] Loss 0.14223852441697687  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.1371024257373257  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.13295111199556958  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.09713011922374548  - accuracy: 0.875\n",
      "At: 1551 [==========>] Loss 0.15628881829695657  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.09544926227115819  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.08245937906732753  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.1289825792213507  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.12634001710945086  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1332505747100603  - accuracy: 0.84375\n",
      "At: 1557 [==========>] Loss 0.07564139278931845  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.14846360315237828  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.08111651227780893  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.14804123277809889  - accuracy: 0.71875\n",
      "At: 1561 [==========>] Loss 0.16541432285327978  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.10171234908548463  - accuracy: 0.8125\n",
      "At: 1563 [==========>] Loss 0.09808205295671707  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.11680818268557967  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.13597130384786152  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.14132881383807758  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.1356514352551942  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.07005570638341356  - accuracy: 0.9375\n",
      "At: 1569 [==========>] Loss 0.1067052329140093  - accuracy: 0.8125\n",
      "At: 1570 [==========>] Loss 0.09077605954960181  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.14673638231878933  - accuracy: 0.75\n",
      "At: 1572 [==========>] Loss 0.14539037024466597  - accuracy: 0.78125\n",
      "At: 1573 [==========>] Loss 0.06049132729588293  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.1395986009509478  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.11582039771169567  - accuracy: 0.78125\n",
      "At: 1576 [==========>] Loss 0.11467659606280305  - accuracy: 0.875\n",
      "At: 1577 [==========>] Loss 0.10093135316423903  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.07448580038448396  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.08267848144461747  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.09928066516755424  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.11763047001000695  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.1784866282529753  - accuracy: 0.71875\n",
      "At: 1583 [==========>] Loss 0.09466551647061897  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.1286426986732772  - accuracy: 0.78125\n",
      "At: 1585 [==========>] Loss 0.09329062391417639  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.14475080770064735  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.12263203428949493  - accuracy: 0.8125\n",
      "At: 1588 [==========>] Loss 0.1192665988657873  - accuracy: 0.875\n",
      "At: 1589 [==========>] Loss 0.1270857992981494  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.13208713716798187  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.14096950660271712  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.08709933006290284  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.1649498681145892  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.0751127025777247  - accuracy: 0.9375\n",
      "At: 1595 [==========>] Loss 0.12672458191297273  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.18535237436004962  - accuracy: 0.6875\n",
      "At: 1597 [==========>] Loss 0.15159446417993777  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.1757990241072246  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.2153189897648855  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.1533453118918059  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.06919376003287292  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.11289267942737079  - accuracy: 0.875\n",
      "At: 1603 [==========>] Loss 0.18167180783359618  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.24399056458653595  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.09795952512016573  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.12755061617991884  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.17778463714582807  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.12984998151966337  - accuracy: 0.78125\n",
      "At: 1609 [==========>] Loss 0.15732441106287812  - accuracy: 0.75\n",
      "At: 1610 [==========>] Loss 0.16365821994709243  - accuracy: 0.8125\n",
      "At: 1611 [==========>] Loss 0.09156432590360972  - accuracy: 0.8125\n",
      "At: 1612 [==========>] Loss 0.0886371203633454  - accuracy: 0.875\n",
      "At: 1613 [==========>] Loss 0.13890749589208484  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.1262659782297835  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.08863990686077551  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.1462583270690094  - accuracy: 0.75\n",
      "At: 1617 [==========>] Loss 0.08528161681643145  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.10249728337827171  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.19508958968265927  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.12285718412476021  - accuracy: 0.78125\n",
      "At: 1621 [==========>] Loss 0.12071419876468269  - accuracy: 0.84375\n",
      "At: 1622 [==========>] Loss 0.1691277888028725  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.12110054334379541  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.16629616974936107  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.13141053817758575  - accuracy: 0.875\n",
      "At: 1626 [==========>] Loss 0.10450732368854856  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.1018734032036065  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.1696401072515874  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.1281195564251662  - accuracy: 0.84375\n",
      "At: 1630 [==========>] Loss 0.0810497504346846  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.12194999721550648  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.10475900443226818  - accuracy: 0.84375\n",
      "At: 1633 [==========>] Loss 0.09427752201194206  - accuracy: 0.875\n",
      "At: 1634 [==========>] Loss 0.10529086054472933  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.2335451558707835  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.12322925261121401  - accuracy: 0.84375\n",
      "At: 1637 [==========>] Loss 0.09372392902101945  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.16047048263982577  - accuracy: 0.71875\n",
      "At: 1639 [==========>] Loss 0.16390543597823526  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.13941885530901632  - accuracy: 0.78125\n",
      "At: 1641 [==========>] Loss 0.11015576583791034  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.10825096813152316  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.10719501796472342  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.12902454694956564  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.07718458008821179  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.14976133795418373  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.16219834277174683  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.15409541859632597  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.0901282278632951  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.10794355882586945  - accuracy: 0.8125\n",
      "At: 1651 [==========>] Loss 0.1563265712954542  - accuracy: 0.78125\n",
      "At: 1652 [==========>] Loss 0.060857104372982716  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.08529013092408383  - accuracy: 0.84375\n",
      "At: 1654 [==========>] Loss 0.06557930205224738  - accuracy: 0.90625\n",
      "At: 1655 [==========>] Loss 0.15952103148820682  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.08862061014303659  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.12753098787618844  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.18775007672982366  - accuracy: 0.75\n",
      "At: 1659 [==========>] Loss 0.12030216332823623  - accuracy: 0.71875\n",
      "At: 1660 [==========>] Loss 0.05515981645365481  - accuracy: 0.9375\n",
      "At: 1661 [==========>] Loss 0.08328588702543076  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.09530274381805941  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16015058135117705  - accuracy: 0.78125\n",
      "At: 1664 [==========>] Loss 0.09947148931853438  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.09753066979598805  - accuracy: 0.84375\n",
      "At: 1666 [==========>] Loss 0.09836384491596668  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.16940269605611832  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.12572842449030935  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.12052054021843484  - accuracy: 0.84375\n",
      "At: 1670 [==========>] Loss 0.06707399491857126  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.11891249295838018  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.13609228205868984  - accuracy: 0.8125\n",
      "At: 1673 [==========>] Loss 0.10244080403373054  - accuracy: 0.84375\n",
      "At: 1674 [==========>] Loss 0.1467352949733213  - accuracy: 0.8125\n",
      "At: 1675 [==========>] Loss 0.19526511527758494  - accuracy: 0.71875\n",
      "At: 1676 [==========>] Loss 0.20086174433880394  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.12023459905386841  - accuracy: 0.8125\n",
      "At: 1678 [==========>] Loss 0.1841502418025066  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.11300932697136246  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.16402759006033607  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.14664567435830292  - accuracy: 0.8125\n",
      "At: 1682 [==========>] Loss 0.09471621808105128  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.1481708699557598  - accuracy: 0.84375\n",
      "At: 1684 [==========>] Loss 0.14677082021221316  - accuracy: 0.78125\n",
      "At: 1685 [==========>] Loss 0.12983061404546276  - accuracy: 0.78125\n",
      "At: 1686 [==========>] Loss 0.13609898590452732  - accuracy: 0.8125\n",
      "At: 1687 [==========>] Loss 0.1430921096517476  - accuracy: 0.8125\n",
      "At: 1688 [==========>] Loss 0.06137362562588934  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.11674755875428423  - accuracy: 0.875\n",
      "At: 1690 [==========>] Loss 0.11597827397837077  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.11061602604480195  - accuracy: 0.90625\n",
      "At: 1692 [==========>] Loss 0.15108615248869484  - accuracy: 0.8125\n",
      "At: 1693 [==========>] Loss 0.10397190920040453  - accuracy: 0.8125\n",
      "At: 1694 [==========>] Loss 0.09549630959331346  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.11690015209874477  - accuracy: 0.8125\n",
      "At: 1696 [==========>] Loss 0.1667229690421494  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.14150375741850718  - accuracy: 0.8125\n",
      "At: 1698 [==========>] Loss 0.09156304762026131  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.09384075878478881  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.11380773938624199  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.08945942121074905  - accuracy: 0.90625\n",
      "At: 1702 [==========>] Loss 0.09931321497610249  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.1834686711244154  - accuracy: 0.6875\n",
      "At: 1704 [==========>] Loss 0.08075665113455402  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.11194983573996622  - accuracy: 0.84375\n",
      "At: 1706 [==========>] Loss 0.15468006806247062  - accuracy: 0.75\n",
      "At: 1707 [==========>] Loss 0.18804326024821177  - accuracy: 0.78125\n",
      "At: 1708 [==========>] Loss 0.09561153106495646  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.1671938638361904  - accuracy: 0.75\n",
      "At: 1710 [==========>] Loss 0.16340898511938806  - accuracy: 0.71875\n",
      "At: 1711 [==========>] Loss 0.06841491438651559  - accuracy: 0.9375\n",
      "At: 1712 [==========>] Loss 0.10879120697204478  - accuracy: 0.84375\n",
      "At: 1713 [==========>] Loss 0.09626303052914338  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.17302130867928112  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.109342922754192  - accuracy: 0.875\n",
      "At: 1716 [==========>] Loss 0.06747838566946336  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.09462761741637724  - accuracy: 0.84375\n",
      "At: 1718 [==========>] Loss 0.13593855636006064  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.11905835598879788  - accuracy: 0.8125\n",
      "At: 1720 [==========>] Loss 0.06538152724021164  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.15247044085991968  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.07248330355672097  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.20583956425867886  - accuracy: 0.65625\n",
      "At: 1724 [==========>] Loss 0.08882380361772949  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.15759380740681067  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.10756214007729932  - accuracy: 0.8125\n",
      "At: 1727 [==========>] Loss 0.11633534953779183  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.09049552654945586  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.18278501701836758  - accuracy: 0.6875\n",
      "At: 1730 [==========>] Loss 0.13078357457864304  - accuracy: 0.78125\n",
      "At: 1731 [==========>] Loss 0.10237766572760484  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.09725042111616589  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.19362039548846505  - accuracy: 0.65625\n",
      "At: 1734 [==========>] Loss 0.11303120174212368  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.17702397012863524  - accuracy: 0.65625\n",
      "At: 1736 [==========>] Loss 0.12032380609487475  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.16159006216068791  - accuracy: 0.75\n",
      "At: 1738 [==========>] Loss 0.12281455639165269  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.13049402700758814  - accuracy: 0.8125\n",
      "At: 1740 [==========>] Loss 0.1259078310373834  - accuracy: 0.875\n",
      "At: 1741 [==========>] Loss 0.12428963347174812  - accuracy: 0.875\n",
      "At: 1742 [==========>] Loss 0.06105881375889409  - accuracy: 0.875\n",
      "At: 1743 [==========>] Loss 0.14174622153825503  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.10346204264309251  - accuracy: 0.84375\n",
      "At: 1745 [==========>] Loss 0.12988442265299405  - accuracy: 0.71875\n",
      "At: 1746 [==========>] Loss 0.18371958457534315  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.10389910813980086  - accuracy: 0.90625\n",
      "At: 1748 [==========>] Loss 0.1205619623890694  - accuracy: 0.8125\n",
      "At: 1749 [==========>] Loss 0.10028368630275594  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.10497126917291082  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.16303627689795455  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.09500057134265544  - accuracy: 0.90625\n",
      "At: 1753 [==========>] Loss 0.08681300579632553  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.10021098636491849  - accuracy: 0.90625\n",
      "At: 1755 [==========>] Loss 0.07472709148907222  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.14838959144453823  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.14535188935247584  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.07303110716271455  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.10822647842515104  - accuracy: 0.84375\n",
      "At: 1760 [==========>] Loss 0.11638103844664135  - accuracy: 0.8125\n",
      "At: 1761 [==========>] Loss 0.12379488104016112  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.15329636748855924  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.12844533521070958  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.1001655079838807  - accuracy: 0.84375\n",
      "At: 1765 [==========>] Loss 0.1377768007263228  - accuracy: 0.8125\n",
      "At: 1766 [==========>] Loss 0.07085454446897663  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.07087872671514617  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.09486289198789437  - accuracy: 0.90625\n",
      "At: 1769 [==========>] Loss 0.08256367446617315  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.08267956357148257  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.17298049194779844  - accuracy: 0.75\n",
      "At: 1772 [==========>] Loss 0.13455594876618274  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.09356766739096892  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.16119960694547675  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.0930409533368241  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.1207517745058283  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11543464070012022  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.13525019559154117  - accuracy: 0.8125\n",
      "At: 1779 [==========>] Loss 0.11377502754291632  - accuracy: 0.84375\n",
      "At: 1780 [==========>] Loss 0.09118506726153522  - accuracy: 0.90625\n",
      "At: 1781 [==========>] Loss 0.18312372851070507  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.09013408882407994  - accuracy: 0.875\n",
      "At: 1783 [==========>] Loss 0.12051589042807874  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.09200950115340437  - accuracy: 0.84375\n",
      "At: 1785 [==========>] Loss 0.09151129302246705  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.12891321931416413  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.12891769832729738  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.11283662499139915  - accuracy: 0.875\n",
      "At: 1789 [==========>] Loss 0.10479282662215136  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.16218740378325014  - accuracy: 0.71875\n",
      "At: 1791 [==========>] Loss 0.09108297580899519  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.1225058536600064  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.09647167786201322  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.18143308165403751  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.07364760375951555  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.14381469018075685  - accuracy: 0.78125\n",
      "At: 1797 [==========>] Loss 0.1231136858765399  - accuracy: 0.84375\n",
      "At: 1798 [==========>] Loss 0.11190886622070731  - accuracy: 0.875\n",
      "At: 1799 [==========>] Loss 0.09530651417257542  - accuracy: 0.875\n",
      "At: 1800 [==========>] Loss 0.11784981355231039  - accuracy: 0.8125\n",
      "At: 1801 [==========>] Loss 0.20406223077587732  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.1428270337421747  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.20448475163812035  - accuracy: 0.6875\n",
      "At: 1804 [==========>] Loss 0.12804795488604429  - accuracy: 0.84375\n",
      "At: 1805 [==========>] Loss 0.040861245788831935  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.16322714956959497  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.17794833078965216  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.16515184174258146  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.09567774672301753  - accuracy: 0.875\n",
      "At: 1810 [==========>] Loss 0.12961181798535149  - accuracy: 0.75\n",
      "At: 1811 [==========>] Loss 0.12991896781431803  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.12586839996895247  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.14140331758197344  - accuracy: 0.78125\n",
      "At: 1814 [==========>] Loss 0.14679647400870433  - accuracy: 0.71875\n",
      "At: 1815 [==========>] Loss 0.19905284166213744  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.04684297617281928  - accuracy: 0.9375\n",
      "At: 1817 [==========>] Loss 0.15553131803844958  - accuracy: 0.75\n",
      "At: 1818 [==========>] Loss 0.13387859323771967  - accuracy: 0.75\n",
      "At: 1819 [==========>] Loss 0.18030174401015434  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.12281418935713576  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.09918403173139037  - accuracy: 0.875\n",
      "At: 1822 [==========>] Loss 0.16280777557896803  - accuracy: 0.75\n",
      "At: 1823 [==========>] Loss 0.165488192505707  - accuracy: 0.78125\n",
      "At: 1824 [==========>] Loss 0.16762862436143333  - accuracy: 0.71875\n",
      "At: 1825 [==========>] Loss 0.10063341247943007  - accuracy: 0.875\n",
      "At: 1826 [==========>] Loss 0.08626488487149014  - accuracy: 0.875\n",
      "At: 1827 [==========>] Loss 0.11832559465245555  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.14341792961677513  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.16157007138185053  - accuracy: 0.75\n",
      "At: 1830 [==========>] Loss 0.1550995866862712  - accuracy: 0.75\n",
      "At: 1831 [==========>] Loss 0.1427852410335757  - accuracy: 0.78125\n",
      "At: 1832 [==========>] Loss 0.08843903186511937  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.11787600005980926  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.08394450039386929  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.13869948093990533  - accuracy: 0.84375\n",
      "At: 1836 [==========>] Loss 0.106398688637107  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.03902118919947101  - accuracy: 1.0\n",
      "At: 1838 [==========>] Loss 0.07431723489666112  - accuracy: 0.90625\n",
      "At: 1839 [==========>] Loss 0.09516104378304023  - accuracy: 0.90625\n",
      "At: 1840 [==========>] Loss 0.10480671316942675  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.1031436247409972  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.11569282752242885  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.12440522142273235  - accuracy: 0.84375\n",
      "At: 1844 [==========>] Loss 0.09153948939664816  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.15551051930091486  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.13325998969777314  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05707416588546857  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.04968120909855821  - accuracy: 0.9375\n",
      "At: 1849 [==========>] Loss 0.1902569276108695  - accuracy: 0.78125\n",
      "At: 1850 [==========>] Loss 0.06029535253076349  - accuracy: 0.9375\n",
      "At: 1851 [==========>] Loss 0.1702536435134259  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.08431044504137117  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.1112092205219441  - accuracy: 0.84375\n",
      "At: 1854 [==========>] Loss 0.12942025411635386  - accuracy: 0.78125\n",
      "At: 1855 [==========>] Loss 0.1705782158733174  - accuracy: 0.78125\n",
      "At: 1856 [==========>] Loss 0.13135095711531797  - accuracy: 0.8125\n",
      "At: 1857 [==========>] Loss 0.16193048318706038  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.115744447631287  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.15613128888061123  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.13065363361649762  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.09399313723795644  - accuracy: 0.90625\n",
      "At: 1862 [==========>] Loss 0.1835841170091086  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.13244507869926103  - accuracy: 0.8125\n",
      "At: 1864 [==========>] Loss 0.13778337792517117  - accuracy: 0.84375\n",
      "At: 1865 [==========>] Loss 0.09268622609623248  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.2190179564753375  - accuracy: 0.6875\n",
      "At: 1867 [==========>] Loss 0.11595067368884056  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.15409100259959077  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.17450406495748078  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.08658626876520625  - accuracy: 0.90625\n",
      "At: 1871 [==========>] Loss 0.13841094936323936  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.16328753886534209  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.10875568359444342  - accuracy: 0.8125\n",
      "At: 1874 [==========>] Loss 0.15890949622605627  - accuracy: 0.78125\n",
      "At: 1875 [==========>] Loss 0.0853154426696731  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.2087315450103232  - accuracy: 0.6875\n",
      "At: 1877 [==========>] Loss 0.10569258245551968  - accuracy: 0.875\n",
      "At: 1878 [==========>] Loss 0.11837267065261556  - accuracy: 0.875\n",
      "At: 1879 [==========>] Loss 0.12797401627896315  - accuracy: 0.78125\n",
      "At: 1880 [==========>] Loss 0.06798015899445038  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.10670985957744272  - accuracy: 0.90625\n",
      "At: 1882 [==========>] Loss 0.10169000260034317  - accuracy: 0.9375\n",
      "At: 1883 [==========>] Loss 0.155591084450399  - accuracy: 0.78125\n",
      "At: 1884 [==========>] Loss 0.09991272258476254  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.1231678744647349  - accuracy: 0.8125\n",
      "At: 1886 [==========>] Loss 0.1517221656186945  - accuracy: 0.75\n",
      "At: 1887 [==========>] Loss 0.09109542464794479  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.13196941338262153  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.11325927164416472  - accuracy: 0.875\n",
      "At: 1890 [==========>] Loss 0.17548436161435443  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.06531532498475065  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.08132752450942157  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.06734321002628854  - accuracy: 0.9375\n",
      "At: 1894 [==========>] Loss 0.09939410744170661  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.07679131042654083  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.10944231849489516  - accuracy: 0.875\n",
      "At: 1897 [==========>] Loss 0.06653200543817804  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.14241171601810235  - accuracy: 0.8125\n",
      "At: 1899 [==========>] Loss 0.11571043200629666  - accuracy: 0.8125\n",
      "At: 1900 [==========>] Loss 0.11932529640681597  - accuracy: 0.8125\n",
      "At: 1901 [==========>] Loss 0.11878632675802109  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.12718671600077433  - accuracy: 0.84375\n",
      "At: 1903 [==========>] Loss 0.1369955615383977  - accuracy: 0.78125\n",
      "At: 1904 [==========>] Loss 0.06435927358638456  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.14244140305254827  - accuracy: 0.78125\n",
      "At: 1906 [==========>] Loss 0.09807605246123162  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.08708427470660825  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.11964028056223963  - accuracy: 0.78125\n",
      "At: 1909 [==========>] Loss 0.09599589377598222  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.06076558385157167  - accuracy: 0.875\n",
      "At: 1911 [==========>] Loss 0.12456316903889951  - accuracy: 0.78125\n",
      "At: 1912 [==========>] Loss 0.12597568328845854  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.16504999674695225  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.07831931904307085  - accuracy: 0.9375\n",
      "At: 1915 [==========>] Loss 0.10252411551856974  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.13807162509235582  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.17472382036732068  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.12946838150702417  - accuracy: 0.8125\n",
      "At: 1919 [==========>] Loss 0.088459418345576  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10399564013356628  - accuracy: 0.90625\n",
      "At: 1921 [==========>] Loss 0.1267209743317973  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.1302750625719302  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.20229421060569536  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.13498575671720692  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.17190422619930068  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.08693899505059066  - accuracy: 0.90625\n",
      "At: 1927 [==========>] Loss 0.09641941314692093  - accuracy: 0.8125\n",
      "At: 1928 [==========>] Loss 0.1415976285531475  - accuracy: 0.8125\n",
      "At: 1929 [==========>] Loss 0.16763646969626145  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.15521118808775997  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.11240050260420707  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.17897701076078654  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.09861205473834082  - accuracy: 0.84375\n",
      "At: 1934 [==========>] Loss 0.16150173279125007  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.11510031468228962  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.10874961611291396  - accuracy: 0.8125\n",
      "At: 1937 [==========>] Loss 0.14745998817258002  - accuracy: 0.75\n",
      "At: 1938 [==========>] Loss 0.18584559383121554  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.10993592952428875  - accuracy: 0.875\n",
      "At: 1940 [==========>] Loss 0.11864760313383683  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.12625468198717651  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.14902041083847098  - accuracy: 0.75\n",
      "At: 1943 [==========>] Loss 0.13339179817864805  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.1014905523741286  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.17762185910085287  - accuracy: 0.78125\n",
      "At: 1946 [==========>] Loss 0.1156893473819671  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.10413142866157926  - accuracy: 0.8125\n",
      "At: 1948 [==========>] Loss 0.13167846298887986  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.07368822725069989  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.11581842305101556  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.13833207206780807  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.08529711981641871  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.086782660792147  - accuracy: 0.84375\n",
      "At: 1954 [==========>] Loss 0.1788017980865929  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.06658471193774934  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.12742806753301783  - accuracy: 0.78125\n",
      "At: 1957 [==========>] Loss 0.08790338564453977  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.09714502473168288  - accuracy: 0.84375\n",
      "At: 1959 [==========>] Loss 0.1557793139298404  - accuracy: 0.75\n",
      "At: 1960 [==========>] Loss 0.05539059777614244  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.18124505475649472  - accuracy: 0.65625\n",
      "At: 1962 [==========>] Loss 0.19500029085626858  - accuracy: 0.6875\n",
      "At: 1963 [==========>] Loss 0.08441271542226561  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.17919236053138773  - accuracy: 0.75\n",
      "At: 1965 [==========>] Loss 0.14281023359865247  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.13559368934748095  - accuracy: 0.78125\n",
      "At: 1967 [==========>] Loss 0.14966890138306913  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.19423879885475123  - accuracy: 0.75\n",
      "At: 1969 [==========>] Loss 0.1711212794544008  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.09786409344155947  - accuracy: 0.84375\n",
      "At: 1971 [==========>] Loss 0.1966125906499237  - accuracy: 0.75\n",
      "At: 1972 [==========>] Loss 0.08765026771286558  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.11752322553828509  - accuracy: 0.84375\n",
      "At: 1974 [==========>] Loss 0.1368844389638273  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.163714376395051  - accuracy: 0.8125\n",
      "At: 1976 [==========>] Loss 0.06926517828717707  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.09142210390413788  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.12975778017749207  - accuracy: 0.875\n",
      "At: 1979 [==========>] Loss 0.10031196174639084  - accuracy: 0.90625\n",
      "At: 1980 [==========>] Loss 0.13500695921646275  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.13815897083399498  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.07303164787650115  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.13569171417333661  - accuracy: 0.78125\n",
      "At: 1984 [==========>] Loss 0.11604164447223064  - accuracy: 0.84375\n",
      "At: 1985 [==========>] Loss 0.14056833921987671  - accuracy: 0.8125\n",
      "At: 1986 [==========>] Loss 0.20183364600252707  - accuracy: 0.65625\n",
      "At: 1987 [==========>] Loss 0.1031888602686325  - accuracy: 0.90625\n",
      "At: 1988 [==========>] Loss 0.08005754014242374  - accuracy: 0.96875\n",
      "At: 1989 [==========>] Loss 0.10239666509064213  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.12471607627515649  - accuracy: 0.78125\n",
      "At: 1991 [==========>] Loss 0.14551961382506307  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.12059179370719961  - accuracy: 0.8125\n",
      "At: 1993 [==========>] Loss 0.15349308151633223  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.12578895852449345  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.16799635018759113  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.09206426685916164  - accuracy: 0.875\n",
      "At: 1997 [==========>] Loss 0.17924407592984523  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.1588869140101821  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.09216703071779872  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.13833856466052563  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.10763888637411123  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.09231146388099122  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.13904785340017348  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.16710783293459364  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.10713880372813149  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.12086098263804124  - accuracy: 0.84375\n",
      "At: 2007 [==========>] Loss 0.10681831990093425  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.1224912531337166  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13023476252369837  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.12051404273614846  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.10726912545702674  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.1054079087384158  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.08782615798127097  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.20528173691924467  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.051191302029993314  - accuracy: 0.9375\n",
      "At: 2016 [==========>] Loss 0.13318941946206586  - accuracy: 0.8125\n",
      "At: 2017 [==========>] Loss 0.08347372086014279  - accuracy: 0.9375\n",
      "At: 2018 [==========>] Loss 0.07322880070853884  - accuracy: 0.90625\n",
      "At: 2019 [==========>] Loss 0.11987953280114294  - accuracy: 0.875\n",
      "At: 2020 [==========>] Loss 0.04809159306035629  - accuracy: 0.9375\n",
      "At: 2021 [==========>] Loss 0.08944042536392977  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.11556863389688951  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.107201262499048  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.0950407084325221  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.17289165622163025  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.09247403396387523  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.14007295465308162  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.12810083414141663  - accuracy: 0.8125\n",
      "At: 2029 [==========>] Loss 0.12902790876440823  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.1468970794179545  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.17000969248683473  - accuracy: 0.78125\n",
      "At: 2032 [==========>] Loss 0.12405183130988046  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.16471147706623046  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.2238775115792173  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.11027719804807168  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.11022934275657788  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.09756570492753848  - accuracy: 0.875\n",
      "At: 2038 [==========>] Loss 0.12912312066738643  - accuracy: 0.75\n",
      "At: 2039 [==========>] Loss 0.10228209530068927  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.11853141780063384  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.06369490947344927  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.10726523610774086  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.1096198475643801  - accuracy: 0.8125\n",
      "At: 2044 [==========>] Loss 0.0966960891238336  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.20965518912808553  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.07752208650239047  - accuracy: 0.84375\n",
      "At: 2047 [==========>] Loss 0.08630911911670089  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.08685450365395661  - accuracy: 0.90625\n",
      "At: 2049 [==========>] Loss 0.14114831164895714  - accuracy: 0.8125\n",
      "At: 2050 [==========>] Loss 0.16304899907716763  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.18818857207675047  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.050033663693782066  - accuracy: 1.0\n",
      "At: 2053 [==========>] Loss 0.11663211358805864  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.12133818134056379  - accuracy: 0.8125\n",
      "At: 2055 [==========>] Loss 0.09011472847086732  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.13312482804940695  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.12611424112288794  - accuracy: 0.84375\n",
      "At: 2058 [==========>] Loss 0.12467322601321079  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.20246323361053126  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.1421578737131466  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.13687277242439141  - accuracy: 0.78125\n",
      "At: 2062 [==========>] Loss 0.13188932266140788  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.10427715557982656  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.2150297924488911  - accuracy: 0.65625\n",
      "At: 2065 [==========>] Loss 0.03119611863445735  - accuracy: 1.0\n",
      "At: 2066 [==========>] Loss 0.18246884003142844  - accuracy: 0.75\n",
      "At: 2067 [==========>] Loss 0.07512920814870942  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.12350824755655557  - accuracy: 0.78125\n",
      "At: 2069 [==========>] Loss 0.09039706363475694  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.14659065930209844  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.12270555523581858  - accuracy: 0.84375\n",
      "At: 2072 [==========>] Loss 0.08161280178014216  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.1228077029490335  - accuracy: 0.8125\n",
      "At: 2074 [==========>] Loss 0.08812834366445946  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.1230424079717427  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.13720424875045034  - accuracy: 0.75\n",
      "At: 2077 [==========>] Loss 0.14230850173002962  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.09406422750483093  - accuracy: 0.875\n",
      "At: 2079 [==========>] Loss 0.098149528493298  - accuracy: 0.84375\n",
      "At: 2080 [==========>] Loss 0.08142535127872387  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.15022959555081025  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.11932970034012658  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.19148140661603902  - accuracy: 0.75\n",
      "At: 2084 [==========>] Loss 0.08913281062227467  - accuracy: 0.90625\n",
      "At: 2085 [==========>] Loss 0.11107570560110638  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.1097011734441565  - accuracy: 0.84375\n",
      "At: 2087 [==========>] Loss 0.15586521268625958  - accuracy: 0.8125\n",
      "At: 2088 [==========>] Loss 0.10795860780893671  - accuracy: 0.8125\n",
      "At: 2089 [==========>] Loss 0.13276051931759625  - accuracy: 0.8125\n",
      "At: 2090 [==========>] Loss 0.09045489768916982  - accuracy: 0.90625\n",
      "At: 2091 [==========>] Loss 0.16763286785047415  - accuracy: 0.75\n",
      "At: 2092 [==========>] Loss 0.10928203442487518  - accuracy: 0.84375\n",
      "At: 2093 [==========>] Loss 0.1422467368953627  - accuracy: 0.875\n",
      "At: 2094 [==========>] Loss 0.17205754718031596  - accuracy: 0.71875\n",
      "At: 2095 [==========>] Loss 0.11493571459246779  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.15005380688429582  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.11603749890132445  - accuracy: 0.90625\n",
      "At: 2098 [==========>] Loss 0.12484866727692626  - accuracy: 0.78125\n",
      "At: 2099 [==========>] Loss 0.10602233951315984  - accuracy: 0.84375\n",
      "At: 2100 [==========>] Loss 0.06535267145897858  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.16045668741326502  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.09782630452842739  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.18279341406319166  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.13209071036855347  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.14570632369083336  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.1614743459947734  - accuracy: 0.71875\n",
      "At: 2107 [==========>] Loss 0.08280643196523577  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.13451470927140374  - accuracy: 0.78125\n",
      "At: 2109 [==========>] Loss 0.1271442766571841  - accuracy: 0.78125\n",
      "At: 2110 [==========>] Loss 0.067342900257993  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.10880557042583103  - accuracy: 0.90625\n",
      "At: 2112 [==========>] Loss 0.10692739122248723  - accuracy: 0.84375\n",
      "At: 2113 [==========>] Loss 0.1202306327581224  - accuracy: 0.78125\n",
      "At: 2114 [==========>] Loss 0.11916143661291902  - accuracy: 0.90625\n",
      "At: 2115 [==========>] Loss 0.10926042109427098  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.0975965907154604  - accuracy: 0.90625\n",
      "At: 2117 [==========>] Loss 0.1330242266047565  - accuracy: 0.8125\n",
      "At: 2118 [==========>] Loss 0.11058023261901469  - accuracy: 0.875\n",
      "At: 2119 [==========>] Loss 0.08593154698099342  - accuracy: 0.84375\n",
      "At: 2120 [==========>] Loss 0.14126382630634204  - accuracy: 0.8125\n",
      "At: 2121 [==========>] Loss 0.16095010401410775  - accuracy: 0.75\n",
      "At: 2122 [==========>] Loss 0.14721137935442535  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.16432406518800566  - accuracy: 0.8125\n",
      "At: 2124 [==========>] Loss 0.15467572549120026  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10866612295359461  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.06562122601894077  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.12272638673590788  - accuracy: 0.78125\n",
      "At: 2128 [==========>] Loss 0.1081232368774799  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.15818500903581967  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.054593847360992535  - accuracy: 0.96875\n",
      "At: 2131 [==========>] Loss 0.10649405797028651  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.20463271346445047  - accuracy: 0.71875\n",
      "At: 2133 [==========>] Loss 0.17153967074350376  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.11224302302410882  - accuracy: 0.84375\n",
      "At: 2135 [==========>] Loss 0.10737981369236864  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.13752667652881362  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.09878927133165252  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.12164708536568052  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.1731426999656058  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.07639128484823082  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.1320621397475782  - accuracy: 0.75\n",
      "At: 2142 [==========>] Loss 0.12821984934046443  - accuracy: 0.8125\n",
      "At: 2143 [==========>] Loss 0.08283341378936014  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.07577412163742255  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.1019220977587565  - accuracy: 0.875\n",
      "At: 2146 [==========>] Loss 0.18638208871342196  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.11819672363727093  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.18949127983758407  - accuracy: 0.71875\n",
      "At: 2149 [==========>] Loss 0.09456868935748719  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.09854119396024033  - accuracy: 0.8125\n",
      "At: 2151 [==========>] Loss 0.10199491052351298  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.22260155482302765  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.1531969256704666  - accuracy: 0.75\n",
      "At: 2154 [==========>] Loss 0.14760227525936462  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.12875297917250184  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.11970676930400655  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.11300640968326112  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.13499870984949325  - accuracy: 0.8125\n",
      "At: 2159 [==========>] Loss 0.06692463065651671  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.13342298411173314  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.0932327236640819  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.10516705881062065  - accuracy: 0.84375\n",
      "At: 2163 [==========>] Loss 0.12932497928493203  - accuracy: 0.8125\n",
      "At: 2164 [==========>] Loss 0.1639433936660397  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.10263931136160188  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.1303460184561302  - accuracy: 0.8125\n",
      "At: 2167 [==========>] Loss 0.08512239871923931  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.07982021669650974  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.11225444639652879  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.10723507545006035  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.173556734668522  - accuracy: 0.75\n",
      "At: 2172 [==========>] Loss 0.11151102337140158  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.13365988568001289  - accuracy: 0.8125\n",
      "At: 2174 [==========>] Loss 0.12052429444350318  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.12127553401576907  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.11602987452924515  - accuracy: 0.8125\n",
      "At: 2177 [==========>] Loss 0.13317304244432043  - accuracy: 0.84375\n",
      "At: 2178 [==========>] Loss 0.07928054289774335  - accuracy: 0.90625\n",
      "At: 2179 [==========>] Loss 0.14566671966675437  - accuracy: 0.78125\n",
      "At: 2180 [==========>] Loss 0.11576373294881191  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.14750851852555813  - accuracy: 0.75\n",
      "At: 2182 [==========>] Loss 0.12410284897900034  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.2009035501276369  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.09017369519719352  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.12090017313937577  - accuracy: 0.84375\n",
      "At: 2186 [==========>] Loss 0.18730139581462474  - accuracy: 0.6875\n",
      "At: 2187 [==========>] Loss 0.1740061449942859  - accuracy: 0.6875\n",
      "At: 2188 [==========>] Loss 0.08780754821636316  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.0651612970709107  - accuracy: 0.96875\n",
      "At: 2190 [==========>] Loss 0.13261091200777209  - accuracy: 0.78125\n",
      "At: 2191 [==========>] Loss 0.1068179396642973  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.09675158901962895  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.15095155538954735  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.10471913244482697  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.16778603225092067  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.16033225472615162  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.09496447252064182  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.09591490121334323  - accuracy: 0.9375\n",
      "At: 2199 [==========>] Loss 0.04893753474584463  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.06268874765264351  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.12724940530963885  - accuracy: 0.8125\n",
      "At: 2202 [==========>] Loss 0.07136408122446677  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.09540281843326817  - accuracy: 0.84375\n",
      "At: 2204 [==========>] Loss 0.11383945333686785  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.11065354114971525  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.09395863088118044  - accuracy: 0.84375\n",
      "At: 2207 [==========>] Loss 0.07577975089088601  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.14700756915592195  - accuracy: 0.8125\n",
      "At: 2209 [==========>] Loss 0.21540648823953276  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.12308404239025195  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.14296676961070942  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.0987951330130872  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.12026353526383599  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.13623585056266316  - accuracy: 0.78125\n",
      "At: 2215 [==========>] Loss 0.1356702888062084  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.13260826544083967  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.1343439355996351  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.1508734467404071  - accuracy: 0.84375\n",
      "At: 2219 [==========>] Loss 0.08531229129703102  - accuracy: 0.875\n",
      "At: 2220 [==========>] Loss 0.08894862924290811  - accuracy: 0.9375\n",
      "At: 2221 [==========>] Loss 0.17695895183264573  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.10691621065127183  - accuracy: 0.90625\n",
      "At: 2223 [==========>] Loss 0.15488499015152637  - accuracy: 0.71875\n",
      "At: 2224 [==========>] Loss 0.12175278245160702  - accuracy: 0.78125\n",
      "At: 2225 [==========>] Loss 0.153076711604878  - accuracy: 0.78125\n",
      "At: 2226 [==========>] Loss 0.12783613492149762  - accuracy: 0.78125\n",
      "At: 2227 [==========>] Loss 0.1646388656856692  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.09457855794429436  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.1780154769291476  - accuracy: 0.6875\n",
      "At: 2230 [==========>] Loss 0.12758449701005664  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.14585205248205618  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.15811572153018985  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.16558913926763402  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.1325817877384325  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.10985463415341645  - accuracy: 0.90625\n",
      "At: 2236 [==========>] Loss 0.07547093937272621  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.13408080736216113  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.1566479419445389  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.19133739831273555  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.14322415141172623  - accuracy: 0.8125\n",
      "At: 2241 [==========>] Loss 0.13658585442016682  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.16450390977137797  - accuracy: 0.75\n",
      "At: 2243 [==========>] Loss 0.08632472325251413  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.10343159178549306  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.08927251688059146  - accuracy: 0.84375\n",
      "At: 2246 [==========>] Loss 0.14816280530491582  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.14937242932493877  - accuracy: 0.78125\n",
      "At: 2248 [==========>] Loss 0.16052530266435577  - accuracy: 0.84375\n",
      "At: 2249 [==========>] Loss 0.10078082809695826  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.10343576839107595  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.07381294700523461  - accuracy: 0.9375\n",
      "At: 2252 [==========>] Loss 0.10609555186226619  - accuracy: 0.84375\n",
      "At: 2253 [==========>] Loss 0.12927666446918723  - accuracy: 0.8125\n",
      "At: 2254 [==========>] Loss 0.13902730187393747  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.15564363830453792  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.14995361039230667  - accuracy: 0.8125\n",
      "At: 2257 [==========>] Loss 0.09668039066334268  - accuracy: 0.875\n",
      "At: 2258 [==========>] Loss 0.1502907179723456  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.14578806587151566  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.1830989909162947  - accuracy: 0.71875\n",
      "At: 2261 [==========>] Loss 0.11655644165580223  - accuracy: 0.84375\n",
      "At: 2262 [==========>] Loss 0.170495362355333  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.13273748312324707  - accuracy: 0.84375\n",
      "At: 2264 [==========>] Loss 0.1098251435946313  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.16240583524302601  - accuracy: 0.71875\n",
      "At: 2266 [==========>] Loss 0.1156133982899665  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.0778265416371203  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.10482332102423925  - accuracy: 0.875\n",
      "At: 2269 [==========>] Loss 0.057546395251803925  - accuracy: 0.90625\n",
      "At: 2270 [==========>] Loss 0.11124568152944248  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.16472054466552827  - accuracy: 0.6875\n",
      "At: 2272 [==========>] Loss 0.09492152117594164  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.11561132286874463  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.08579923633286402  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.10249663104318603  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.09929241855812287  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.10887140021510115  - accuracy: 0.90625\n",
      "At: 2278 [==========>] Loss 0.10309468010883915  - accuracy: 0.78125\n",
      "At: 2279 [==========>] Loss 0.15466770213286402  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.1533810380196004  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.1268020755595437  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.06486028275291525  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.1804693291661341  - accuracy: 0.75\n",
      "At: 2284 [==========>] Loss 0.1174938379149582  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.12125990409721427  - accuracy: 0.78125\n",
      "At: 2286 [==========>] Loss 0.12454010971759269  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.1437302802975307  - accuracy: 0.75\n",
      "At: 2288 [==========>] Loss 0.0941862562606155  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.1453449446400002  - accuracy: 0.84375\n",
      "At: 2290 [==========>] Loss 0.061876859509352504  - accuracy: 0.90625\n",
      "At: 2291 [==========>] Loss 0.12192790057031833  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.08661803773240243  - accuracy: 0.9375\n",
      "At: 2293 [==========>] Loss 0.07303135802748661  - accuracy: 0.90625\n",
      "At: 2294 [==========>] Loss 0.054880659289689174  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.13584520548848303  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.14609358496385344  - accuracy: 0.84375\n",
      "At: 2297 [==========>] Loss 0.08047193489827799  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.11720971834443229  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.11499264102716157  - accuracy: 0.75\n",
      "At: 2300 [==========>] Loss 0.10579722669473127  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.1944514337805166  - accuracy: 0.75\n",
      "At: 2302 [==========>] Loss 0.15938659112894515  - accuracy: 0.78125\n",
      "At: 2303 [==========>] Loss 0.0631106998846524  - accuracy: 0.9375\n",
      "At: 2304 [==========>] Loss 0.08121472585193072  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10045873499604968  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.14441811632443  - accuracy: 0.78125\n",
      "At: 2307 [==========>] Loss 0.16849139453389284  - accuracy: 0.71875\n",
      "At: 2308 [==========>] Loss 0.19107465820218594  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.14597842542922024  - accuracy: 0.78125\n",
      "At: 2310 [==========>] Loss 0.15045330296742887  - accuracy: 0.78125\n",
      "At: 2311 [==========>] Loss 0.12452997404671519  - accuracy: 0.84375\n",
      "At: 2312 [==========>] Loss 0.11549072468618907  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.07079449565964718  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.11022814492676627  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.1297174177658121  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.1506505046620949  - accuracy: 0.75\n",
      "At: 2317 [==========>] Loss 0.14751434246221118  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.21355566183812824  - accuracy: 0.65625\n",
      "At: 2319 [==========>] Loss 0.11357868771821343  - accuracy: 0.8125\n",
      "At: 2320 [==========>] Loss 0.09026953083720043  - accuracy: 0.90625\n",
      "At: 2321 [==========>] Loss 0.13883826231562651  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.19208559775104483  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.13227810295299705  - accuracy: 0.875\n",
      "At: 2324 [==========>] Loss 0.13915357917952156  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.13127345617181702  - accuracy: 0.75\n",
      "At: 2326 [==========>] Loss 0.06989013111982084  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.06023656121167924  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12170758792045208  - accuracy: 0.78125\n",
      "At: 2329 [==========>] Loss 0.10923271891055374  - accuracy: 0.8125\n",
      "At: 2330 [==========>] Loss 0.16756866745341797  - accuracy: 0.6875\n",
      "At: 2331 [==========>] Loss 0.0952147911414086  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.12407419075140479  - accuracy: 0.8125\n",
      "At: 2333 [==========>] Loss 0.1201464758756791  - accuracy: 0.8125\n",
      "At: 2334 [==========>] Loss 0.17967858578914211  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.10016401969570093  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.11485170788699582  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.1719037921483787  - accuracy: 0.78125\n",
      "At: 2338 [==========>] Loss 0.10733876974857423  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.09339175208472111  - accuracy: 0.8125\n",
      "At: 2340 [==========>] Loss 0.15910766333519868  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.1443803936833424  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.17615035123316083  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.10261677224035753  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.19018095441705266  - accuracy: 0.75\n",
      "At: 2345 [==========>] Loss 0.16341381014853967  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.0776913367624531  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.1227650622524416  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.06656503518925372  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.07565455645100565  - accuracy: 0.90625\n",
      "At: 2350 [==========>] Loss 0.09863021248845795  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.0850807948786692  - accuracy: 0.84375\n",
      "At: 2352 [==========>] Loss 0.11121875556997854  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.09438221651374129  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.1600978118713622  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.0589227477718294  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.12230676698636642  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.14464693363691827  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.1759039125870948  - accuracy: 0.75\n",
      "At: 2359 [==========>] Loss 0.16631566410561063  - accuracy: 0.71875\n",
      "At: 2360 [==========>] Loss 0.1400278192734413  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.16296264931226678  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.08722006895147948  - accuracy: 0.84375\n",
      "At: 2363 [==========>] Loss 0.16280185924257562  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.07389560715939271  - accuracy: 0.9375\n",
      "At: 2365 [==========>] Loss 0.09963368500222677  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.17553047980975683  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.13500288079512102  - accuracy: 0.78125\n",
      "At: 2368 [==========>] Loss 0.1064182927044672  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.0907248006282162  - accuracy: 0.875\n",
      "At: 2370 [==========>] Loss 0.08643732613815394  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.11138014210189931  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.13668792995040818  - accuracy: 0.8125\n",
      "At: 2373 [==========>] Loss 0.11892494157265085  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.11387625076352215  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.06121188906676471  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.1137417728171782  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.09754312663627723  - accuracy: 0.90625\n",
      "At: 2378 [==========>] Loss 0.1316080207713132  - accuracy: 0.8125\n",
      "At: 2379 [==========>] Loss 0.1730263400957333  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.07267415949683068  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.0817899811768123  - accuracy: 0.9375\n",
      "At: 2382 [==========>] Loss 0.12289725944421825  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.11189349202527288  - accuracy: 0.84375\n",
      "At: 2384 [==========>] Loss 0.09538481565302284  - accuracy: 0.875\n",
      "At: 2385 [==========>] Loss 0.13243290033149996  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.13281338423780364  - accuracy: 0.8125\n",
      "At: 2387 [==========>] Loss 0.06517398830920781  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.1119527673420494  - accuracy: 0.875\n",
      "At: 2389 [==========>] Loss 0.0484646361421507  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.04791674018278953  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.1613739979502518  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.13891691680655094  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.10471282805948348  - accuracy: 0.84375\n",
      "At: 2394 [==========>] Loss 0.05748021803794232  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.11065749418621111  - accuracy: 0.78125\n",
      "At: 2396 [==========>] Loss 0.07093227075999625  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.0971788975820317  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.11671376476241987  - accuracy: 0.78125\n",
      "At: 2399 [==========>] Loss 0.1251332593120603  - accuracy: 0.90625\n",
      "At: 2400 [==========>] Loss 0.0903340482150895  - accuracy: 0.84375\n",
      "At: 2401 [==========>] Loss 0.09724105333793576  - accuracy: 0.84375\n",
      "At: 2402 [==========>] Loss 0.10120829344442651  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.1732579818686839  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.14187500116339694  - accuracy: 0.78125\n",
      "At: 2405 [==========>] Loss 0.07932371641406116  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.12488554433200875  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.10697547280255021  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.09696978157060474  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.14026033830248485  - accuracy: 0.875\n",
      "At: 2410 [==========>] Loss 0.14988472435090153  - accuracy: 0.84375\n",
      "At: 2411 [==========>] Loss 0.09236615580429976  - accuracy: 0.90625\n",
      "At: 2412 [==========>] Loss 0.09179500683182734  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.09419276031855905  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.07956323180034842  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.07929481027645026  - accuracy: 0.90625\n",
      "At: 2416 [==========>] Loss 0.11307048837863903  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.1722826172260477  - accuracy: 0.75\n",
      "At: 2418 [==========>] Loss 0.14063453244338547  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.1411151914405888  - accuracy: 0.78125\n",
      "At: 2420 [==========>] Loss 0.13195398257497631  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.09312533752490988  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.16104526488840687  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.11036510501740876  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.09319360318440152  - accuracy: 0.84375\n",
      "At: 2425 [==========>] Loss 0.09027698538140264  - accuracy: 0.875\n",
      "At: 2426 [==========>] Loss 0.21660637127739446  - accuracy: 0.625\n",
      "At: 2427 [==========>] Loss 0.12274444280952558  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.08210563347876852  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.1322617020051212  - accuracy: 0.8125\n",
      "At: 2430 [==========>] Loss 0.11726818423174698  - accuracy: 0.84375\n",
      "At: 2431 [==========>] Loss 0.14311299625758916  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.06503831680606095  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.0602188624853278  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.04332505248410086  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.17158308500713976  - accuracy: 0.75\n",
      "At: 2436 [==========>] Loss 0.12405632338388985  - accuracy: 0.78125\n",
      "At: 2437 [==========>] Loss 0.1956678757555116  - accuracy: 0.6875\n",
      "At: 2438 [==========>] Loss 0.10803713356763647  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.1408617268898013  - accuracy: 0.71875\n",
      "At: 2440 [==========>] Loss 0.11479628811252818  - accuracy: 0.875\n",
      "At: 2441 [==========>] Loss 0.1270371639474196  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.13965638543122683  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.11102506254133074  - accuracy: 0.84375\n",
      "At: 2444 [==========>] Loss 0.0875098866874973  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.07336452495160548  - accuracy: 0.875\n",
      "At: 2446 [==========>] Loss 0.143383632428139  - accuracy: 0.875\n",
      "At: 2447 [==========>] Loss 0.1504496084948947  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.13692802203301524  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.087760110924329  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.04356661694885034  - accuracy: 0.96875\n",
      "At: 2451 [==========>] Loss 0.06700843052949923  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.13174936767798304  - accuracy: 0.875\n",
      "At: 2453 [==========>] Loss 0.11852833865588114  - accuracy: 0.875\n",
      "At: 2454 [==========>] Loss 0.15732764307058925  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.12355644808488661  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.13386364032439124  - accuracy: 0.75\n",
      "At: 2457 [==========>] Loss 0.1621968173540724  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.0728194864097088  - accuracy: 0.9375\n",
      "At: 2459 [==========>] Loss 0.14167874304719127  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.06687670954217231  - accuracy: 0.9375\n",
      "At: 2461 [==========>] Loss 0.05768776581310189  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.16148509863394256  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.10192307405877898  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.16009025315186381  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.1284782249199111  - accuracy: 0.84375\n",
      "At: 2466 [==========>] Loss 0.08244402860645113  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.10160698825184816  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.0943328977569357  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.15288304520531731  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.11910674082637847  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.12033428197204218  - accuracy: 0.8125\n",
      "At: 2472 [==========>] Loss 0.09663950234347851  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.11956685487609858  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.07431038066420179  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10860817200241632  - accuracy: 0.8125\n",
      "At: 2476 [==========>] Loss 0.08070199633992542  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.14387252500679332  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.11155570688772687  - accuracy: 0.8125\n",
      "At: 2479 [==========>] Loss 0.08987074949366453  - accuracy: 0.84375\n",
      "At: 2480 [==========>] Loss 0.11712234489160336  - accuracy: 0.84375\n",
      "At: 2481 [==========>] Loss 0.0648433591990856  - accuracy: 1.0\n",
      "At: 2482 [==========>] Loss 0.16583290630757253  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.10956249173559053  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.08592083656159222  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.11053808040678281  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.11276459664718011  - accuracy: 0.90625\n",
      "At: 2487 [==========>] Loss 0.1332137109208063  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.1501879125372846  - accuracy: 0.8125\n",
      "At: 2489 [==========>] Loss 0.2003019460550702  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.09610604442051557  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.11154467934379483  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.131801361688869  - accuracy: 0.78125\n",
      "At: 2493 [==========>] Loss 0.12969613721492373  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.09494831612918325  - accuracy: 0.9375\n",
      "At: 2495 [==========>] Loss 0.09321908459575196  - accuracy: 0.84375\n",
      "At: 2496 [==========>] Loss 0.06915020879392912  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.22513781306183855  - accuracy: 0.625\n",
      "At: 2498 [==========>] Loss 0.14015567263396217  - accuracy: 0.78125\n",
      "At: 2499 [==========>] Loss 0.060577244248963526  - accuracy: 0.9375\n",
      "At: 2500 [==========>] Loss 0.19977894984364436  - accuracy: 0.6875\n",
      "At: 2501 [==========>] Loss 0.16899893366466412  - accuracy: 0.6875\n",
      "At: 2502 [==========>] Loss 0.13329572892371716  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.12656243684510962  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.16112695283793071  - accuracy: 0.75\n",
      "At: 2505 [==========>] Loss 0.1108920526265165  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.114534440508833  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.16898140035245965  - accuracy: 0.71875\n",
      "At: 2508 [==========>] Loss 0.1322399963904557  - accuracy: 0.78125\n",
      "At: 2509 [==========>] Loss 0.1301417164821835  - accuracy: 0.84375\n",
      "At: 2510 [==========>] Loss 0.13115059277357277  - accuracy: 0.875\n",
      "At: 2511 [==========>] Loss 0.16347447260834091  - accuracy: 0.75\n",
      "At: 2512 [==========>] Loss 0.08939728966400842  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.1443080253511978  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.1547590592403209  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.16097078206185236  - accuracy: 0.78125\n",
      "At: 2516 [==========>] Loss 0.18581178310641264  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.1230811968250002  - accuracy: 0.78125\n",
      "At: 2518 [==========>] Loss 0.1340295838777154  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.14442390586744233  - accuracy: 0.78125\n",
      "At: 2520 [==========>] Loss 0.1297056924883882  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.1002954968883337  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.20706959527175656  - accuracy: 0.71875\n",
      "At: 2523 [==========>] Loss 0.13292052504544127  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.1908325414729281  - accuracy: 0.78125\n",
      "At: 2525 [==========>] Loss 0.08905654967251096  - accuracy: 0.875\n",
      "At: 2526 [==========>] Loss 0.11621942144814112  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.12972255088088086  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.0976545909369207  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.13008231760346586  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.12890479544585354  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.041436950685662666  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.11365826229873713  - accuracy: 0.875\n",
      "At: 2533 [==========>] Loss 0.11445385043684406  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.07179051654899604  - accuracy: 0.90625\n",
      "At: 2535 [==========>] Loss 0.08157686047670101  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.1565786035904725  - accuracy: 0.84375\n",
      "At: 2537 [==========>] Loss 0.09676078675676734  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.15382151772525293  - accuracy: 0.8125\n",
      "At: 2539 [==========>] Loss 0.08538193281370342  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.13206019161923158  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.07941285843537048  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.0540905746392738  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16383550915255807  - accuracy: 0.8125\n",
      "At: 2544 [==========>] Loss 0.13477773982520638  - accuracy: 0.84375\n",
      "At: 2545 [==========>] Loss 0.07989823513257518  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.1568222856348092  - accuracy: 0.78125\n",
      "At: 2547 [==========>] Loss 0.0913966630073421  - accuracy: 0.90625\n",
      "At: 2548 [==========>] Loss 0.09712801605597142  - accuracy: 0.84375\n",
      "At: 2549 [==========>] Loss 0.1018184015654298  - accuracy: 0.84375\n",
      "At: 2550 [==========>] Loss 0.11115974673560017  - accuracy: 0.90625\n",
      "At: 2551 [==========>] Loss 0.13419528398438205  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.1219678425972257  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.07502217077664153  - accuracy: 0.9375\n",
      "At: 2554 [==========>] Loss 0.1594483691596277  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.1593514289494885  - accuracy: 0.78125\n",
      "At: 2556 [==========>] Loss 0.0794593997102989  - accuracy: 0.9375\n",
      "At: 2557 [==========>] Loss 0.10702065995075861  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.08669764134315  - accuracy: 0.875\n",
      "At: 2559 [==========>] Loss 0.12316297242497834  - accuracy: 0.78125\n",
      "At: 2560 [==========>] Loss 0.17080741208662983  - accuracy: 0.65625\n",
      "At: 2561 [==========>] Loss 0.101230565885062  - accuracy: 0.8125\n",
      "At: 2562 [==========>] Loss 0.11935340481915395  - accuracy: 0.8125\n",
      "At: 2563 [==========>] Loss 0.12852477843838042  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09222723180224214  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.11000825876831918  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.18115307807650946  - accuracy: 0.75\n",
      "At: 2567 [==========>] Loss 0.15428427290460148  - accuracy: 0.75\n",
      "At: 2568 [==========>] Loss 0.09030696593240614  - accuracy: 0.875\n",
      "At: 2569 [==========>] Loss 0.08075288508844988  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.17157184103661138  - accuracy: 0.78125\n",
      "At: 2571 [==========>] Loss 0.08510958332802551  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.1817751965706988  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.18569342056091703  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.13865140990726893  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.0707439756352502  - accuracy: 0.90625\n",
      "At: 2576 [==========>] Loss 0.11010908021203095  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.201138688433423  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.1790768158637842  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.20027429837667687  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.13229342694041624  - accuracy: 0.875\n",
      "At: 2581 [==========>] Loss 0.05304053799957353  - accuracy: 0.96875\n",
      "At: 2582 [==========>] Loss 0.2083318086758046  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.06895979358842831  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.2209736656926552  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.0982097060490525  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.10300453817542649  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.15625709842262583  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.14188278455649855  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.10699376399864866  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.1949978892531728  - accuracy: 0.71875\n",
      "At: 2591 [==========>] Loss 0.12288062826452173  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.0961730875019681  - accuracy: 0.875\n",
      "At: 2593 [==========>] Loss 0.07884515835047445  - accuracy: 0.90625\n",
      "At: 2594 [==========>] Loss 0.07045485949970207  - accuracy: 0.9375\n",
      "At: 2595 [==========>] Loss 0.10683343642792527  - accuracy: 0.84375\n",
      "At: 2596 [==========>] Loss 0.1088941022727464  - accuracy: 0.90625\n",
      "At: 2597 [==========>] Loss 0.07068935235966917  - accuracy: 0.9375\n",
      "At: 2598 [==========>] Loss 0.12224804242629293  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.17457939973530506  - accuracy: 0.71875\n",
      "At: 2600 [==========>] Loss 0.14685894310319  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.14382341973697227  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.08493346435022767  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.13430814761052112  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.08693276540953916  - accuracy: 0.90625\n",
      "At: 2605 [==========>] Loss 0.13319479291062075  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.12993706465467814  - accuracy: 0.8125\n",
      "At: 2607 [==========>] Loss 0.1504688952252678  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.07077468339555999  - accuracy: 0.96875\n",
      "At: 2609 [==========>] Loss 0.0984149155544398  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.14614935093290343  - accuracy: 0.78125\n",
      "At: 2611 [==========>] Loss 0.15878349708629697  - accuracy: 0.75\n",
      "At: 2612 [==========>] Loss 0.09218062665096778  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.11151419410459376  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.12957224269572154  - accuracy: 0.78125\n",
      "At: 2615 [==========>] Loss 0.08334371968988415  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.05424378698036651  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.07871688049800413  - accuracy: 0.90625\n",
      "At: 2618 [==========>] Loss 0.08570342714185662  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.10155625828914645  - accuracy: 0.84375\n",
      "At: 2620 [==========>] Loss 0.11751511939167149  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.11524389983387934  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.11738052946124236  - accuracy: 0.84375\n",
      "At: 2623 [==========>] Loss 0.10902126254391548  - accuracy: 0.84375\n",
      "At: 2624 [==========>] Loss 0.16093889723606858  - accuracy: 0.75\n",
      "At: 2625 [==========>] Loss 0.06746017901769333  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.07119825374947247  - accuracy: 0.875\n",
      "At: 2627 [==========>] Loss 0.14900588297365203  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.0874341865662345  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.09945684975510702  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.10956690533510667  - accuracy: 0.8125\n",
      "At: 2631 [==========>] Loss 0.12982865121023884  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.14260413442534697  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.08863279430600571  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.070341516563974  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.2198703931264921  - accuracy: 0.65625\n",
      "At: 2636 [==========>] Loss 0.1363114169214435  - accuracy: 0.8125\n",
      "At: 2637 [==========>] Loss 0.15334974024190953  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.0845701054159109  - accuracy: 0.90625\n",
      "At: 2639 [==========>] Loss 0.0790715179632461  - accuracy: 0.9375\n",
      "At: 2640 [==========>] Loss 0.11904051271444405  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.14559667795257192  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.18692204085080077  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.09656585067116738  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.165094654907027  - accuracy: 0.75\n",
      "At: 2645 [==========>] Loss 0.07285856119009763  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.14100132007028904  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.11386209639981158  - accuracy: 0.78125\n",
      "At: 2648 [==========>] Loss 0.12910918727563198  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.14273131575838688  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.10959855702999519  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.18635440542603537  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.10184012055227781  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.1063760832356523  - accuracy: 0.875\n",
      "At: 2654 [==========>] Loss 0.13662331540682704  - accuracy: 0.78125\n",
      "At: 2655 [==========>] Loss 0.22211040942672777  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.03560864722187803  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.11185113331884355  - accuracy: 0.84375\n",
      "At: 2658 [==========>] Loss 0.10411386698843382  - accuracy: 0.84375\n",
      "At: 2659 [==========>] Loss 0.06296896747696867  - accuracy: 0.96875\n",
      "At: 2660 [==========>] Loss 0.11271474963024636  - accuracy: 0.84375\n",
      "At: 2661 [==========>] Loss 0.08446657178813076  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.06615216081707337  - accuracy: 0.96875\n",
      "At: 2663 [==========>] Loss 0.06721203144703614  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.11910583797153072  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.09561689005200272  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.14488363194586992  - accuracy: 0.84375\n",
      "At: 2667 [==========>] Loss 0.13138601115393  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.1304460729855153  - accuracy: 0.84375\n",
      "At: 2669 [==========>] Loss 0.14387349101466507  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.10568880317433418  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.07584387290081532  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.11253227525049854  - accuracy: 0.8125\n",
      "At: 2673 [==========>] Loss 0.12023530936968568  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.12186584612371984  - accuracy: 0.8125\n",
      "At: 2675 [==========>] Loss 0.1265519539351871  - accuracy: 0.8125\n",
      "At: 2676 [==========>] Loss 0.12888882453809788  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.1128363294929191  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.07916711407502572  - accuracy: 0.84375\n",
      "At: 2679 [==========>] Loss 0.08847621143842312  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.09008169311126588  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.09473355862707267  - accuracy: 0.875\n",
      "At: 2682 [==========>] Loss 0.1523651285356132  - accuracy: 0.75\n",
      "At: 2683 [==========>] Loss 0.1826396394580928  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09626318936985225  - accuracy: 0.84375\n",
      "At: 2685 [==========>] Loss 0.12369628402672889  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.07173607890663079  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.13367392408371942  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.14970804013840203  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.12550218253995424  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.12497282061810798  - accuracy: 0.75\n",
      "Epochs  3 / 10\n",
      "At: 1 [==========>] Loss 0.1676947749584983  - accuracy: 0.75\n",
      "At: 2 [==========>] Loss 0.20921356471164845  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.15193980560640652  - accuracy: 0.8125\n",
      "At: 4 [==========>] Loss 0.19192708927752633  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.09621836989388995  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.10643818191151627  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.2077310225782643  - accuracy: 0.65625\n",
      "At: 8 [==========>] Loss 0.24583023934607112  - accuracy: 0.6875\n",
      "At: 9 [==========>] Loss 0.3073021528113569  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.26604482337721036  - accuracy: 0.625\n",
      "At: 11 [==========>] Loss 0.2819156312804704  - accuracy: 0.6875\n",
      "At: 12 [==========>] Loss 0.256152338804677  - accuracy: 0.6875\n",
      "At: 13 [==========>] Loss 0.21807729422930855  - accuracy: 0.75\n",
      "At: 14 [==========>] Loss 0.11700389825457626  - accuracy: 0.8125\n",
      "At: 15 [==========>] Loss 0.1732484651123602  - accuracy: 0.6875\n",
      "At: 16 [==========>] Loss 0.21479793924630564  - accuracy: 0.71875\n",
      "At: 17 [==========>] Loss 0.24499560257636  - accuracy: 0.59375\n",
      "At: 18 [==========>] Loss 0.25575639694229263  - accuracy: 0.71875\n",
      "At: 19 [==========>] Loss 0.20825013054745078  - accuracy: 0.71875\n",
      "At: 20 [==========>] Loss 0.14564191069910926  - accuracy: 0.8125\n",
      "At: 21 [==========>] Loss 0.2627989112423687  - accuracy: 0.65625\n",
      "At: 22 [==========>] Loss 0.2219526082940022  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.11410177745173927  - accuracy: 0.84375\n",
      "At: 24 [==========>] Loss 0.1891641204369337  - accuracy: 0.78125\n",
      "At: 25 [==========>] Loss 0.26602424960392723  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.24179938510608295  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.2140808209884676  - accuracy: 0.75\n",
      "At: 28 [==========>] Loss 0.17502655951998802  - accuracy: 0.84375\n",
      "At: 29 [==========>] Loss 0.15715631072700015  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.18986414987164774  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.2771127845715002  - accuracy: 0.6875\n",
      "At: 32 [==========>] Loss 0.20895745422224626  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.1299441945433698  - accuracy: 0.84375\n",
      "At: 34 [==========>] Loss 0.14890779599943235  - accuracy: 0.78125\n",
      "At: 35 [==========>] Loss 0.17290270312993408  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.21040539244039463  - accuracy: 0.78125\n",
      "At: 37 [==========>] Loss 0.28767709769671407  - accuracy: 0.59375\n",
      "At: 38 [==========>] Loss 0.2927108525594424  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.17249315113289812  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.24059286915364064  - accuracy: 0.78125\n",
      "At: 41 [==========>] Loss 0.10098702132442847  - accuracy: 0.875\n",
      "At: 42 [==========>] Loss 0.15254257640752714  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.20004582188812342  - accuracy: 0.75\n",
      "At: 44 [==========>] Loss 0.18694200018032672  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.11046450832572295  - accuracy: 0.875\n",
      "At: 46 [==========>] Loss 0.1897538059734724  - accuracy: 0.8125\n",
      "At: 47 [==========>] Loss 0.27185073743766847  - accuracy: 0.71875\n",
      "At: 48 [==========>] Loss 0.19221822387796744  - accuracy: 0.78125\n",
      "At: 49 [==========>] Loss 0.12387572735632488  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.2528058012275092  - accuracy: 0.65625\n",
      "At: 51 [==========>] Loss 0.26578778490668936  - accuracy: 0.6875\n",
      "At: 52 [==========>] Loss 0.23396099784017604  - accuracy: 0.75\n",
      "At: 53 [==========>] Loss 0.16618091578813815  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.1604910129766199  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.24190909116469217  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.19071179957116552  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.17141784279413794  - accuracy: 0.78125\n",
      "At: 58 [==========>] Loss 0.2166098915550986  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.25344815297989093  - accuracy: 0.6875\n",
      "At: 60 [==========>] Loss 0.17284064460626897  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.30963729576107646  - accuracy: 0.625\n",
      "At: 62 [==========>] Loss 0.17616551085408963  - accuracy: 0.84375\n",
      "At: 63 [==========>] Loss 0.1770449038911821  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.2177259738759734  - accuracy: 0.6875\n",
      "At: 65 [==========>] Loss 0.2796047785120486  - accuracy: 0.625\n",
      "At: 66 [==========>] Loss 0.2658748411757056  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.2721139487008933  - accuracy: 0.625\n",
      "At: 68 [==========>] Loss 0.15315415072517347  - accuracy: 0.8125\n",
      "At: 69 [==========>] Loss 0.1936332565736627  - accuracy: 0.75\n",
      "At: 70 [==========>] Loss 0.2119587659838224  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.17403224020787272  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.16301657729623364  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.1261102860899841  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.21636622704308178  - accuracy: 0.75\n",
      "At: 75 [==========>] Loss 0.22835224720060915  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.2851450443304546  - accuracy: 0.625\n",
      "At: 77 [==========>] Loss 0.25161640352041476  - accuracy: 0.75\n",
      "At: 78 [==========>] Loss 0.149938403461978  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.16866893074553593  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.22713642352235877  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.2104560697516786  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.2394739852162185  - accuracy: 0.75\n",
      "At: 83 [==========>] Loss 0.21849104586402968  - accuracy: 0.75\n",
      "At: 84 [==========>] Loss 0.18153308991413114  - accuracy: 0.8125\n",
      "At: 85 [==========>] Loss 0.21790159259788244  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.15757894303163072  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.15844214517787203  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.3613284445589164  - accuracy: 0.5\n",
      "At: 89 [==========>] Loss 0.19028944789279909  - accuracy: 0.6875\n",
      "At: 90 [==========>] Loss 0.2662960216621043  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.21302320076324266  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.11101980988997268  - accuracy: 0.90625\n",
      "At: 93 [==========>] Loss 0.16534308527389913  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.10984349942098476  - accuracy: 0.875\n",
      "At: 95 [==========>] Loss 0.20635515918257438  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.13334033764401254  - accuracy: 0.84375\n",
      "At: 97 [==========>] Loss 0.09252612959067047  - accuracy: 0.84375\n",
      "At: 98 [==========>] Loss 0.30334614231618745  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.1387450889952447  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.14415309020360395  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.1692456717622612  - accuracy: 0.8125\n",
      "At: 102 [==========>] Loss 0.18375747162308131  - accuracy: 0.8125\n",
      "At: 103 [==========>] Loss 0.1645429733661638  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.16927186086940954  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.20204681571691457  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.25439755112734586  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.21264243961547508  - accuracy: 0.78125\n",
      "At: 108 [==========>] Loss 0.22799514031211773  - accuracy: 0.75\n",
      "At: 109 [==========>] Loss 0.1206306098196567  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.281672459232729  - accuracy: 0.59375\n",
      "At: 111 [==========>] Loss 0.1341255726098896  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.1774178222122061  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.2263919974103758  - accuracy: 0.6875\n",
      "At: 114 [==========>] Loss 0.16821349395995866  - accuracy: 0.78125\n",
      "At: 115 [==========>] Loss 0.2116455932189412  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.19109081545497864  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.13050378098006188  - accuracy: 0.84375\n",
      "At: 118 [==========>] Loss 0.23264017389476963  - accuracy: 0.6875\n",
      "At: 119 [==========>] Loss 0.14307205162424944  - accuracy: 0.8125\n",
      "At: 120 [==========>] Loss 0.19900783664734611  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.1511986999782991  - accuracy: 0.8125\n",
      "At: 122 [==========>] Loss 0.18334195847224563  - accuracy: 0.75\n",
      "At: 123 [==========>] Loss 0.2156592997132759  - accuracy: 0.71875\n",
      "At: 124 [==========>] Loss 0.25631183806516195  - accuracy: 0.65625\n",
      "At: 125 [==========>] Loss 0.1907974876381745  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.2538964685070295  - accuracy: 0.6875\n",
      "At: 127 [==========>] Loss 0.18070044612234856  - accuracy: 0.78125\n",
      "At: 128 [==========>] Loss 0.2530328844680665  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.19395983828305163  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.23584646182760713  - accuracy: 0.6875\n",
      "At: 131 [==========>] Loss 0.1763109118991624  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.16275820368227187  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.22496428718361813  - accuracy: 0.6875\n",
      "At: 134 [==========>] Loss 0.15576026298605739  - accuracy: 0.84375\n",
      "At: 135 [==========>] Loss 0.17781112700639026  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.1781555224708421  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.13026415288722917  - accuracy: 0.8125\n",
      "At: 138 [==========>] Loss 0.1570682600647411  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.10840764086473764  - accuracy: 0.90625\n",
      "At: 140 [==========>] Loss 0.16568909791368672  - accuracy: 0.78125\n",
      "At: 141 [==========>] Loss 0.33560652354109655  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.1952199539216448  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.20663155827796897  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.1561072192898747  - accuracy: 0.78125\n",
      "At: 145 [==========>] Loss 0.17802318067386763  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.1706122111023068  - accuracy: 0.78125\n",
      "At: 147 [==========>] Loss 0.18578753451914087  - accuracy: 0.78125\n",
      "At: 148 [==========>] Loss 0.16253293371890754  - accuracy: 0.8125\n",
      "At: 149 [==========>] Loss 0.1760250410112631  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.1621468863731249  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.19668002427479347  - accuracy: 0.75\n",
      "At: 152 [==========>] Loss 0.17859289531303793  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.22308030998566483  - accuracy: 0.65625\n",
      "At: 154 [==========>] Loss 0.1986624972273806  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.21725972717584047  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.14932738580390734  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.2986169152349041  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.14006546329497327  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.15370942529475928  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.1354502571528908  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.12510866996853776  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.18164807664723742  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.20689530386904254  - accuracy: 0.71875\n",
      "At: 164 [==========>] Loss 0.20414172019874488  - accuracy: 0.71875\n",
      "At: 165 [==========>] Loss 0.23328100157830967  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.16343179672035169  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.1863185644749044  - accuracy: 0.6875\n",
      "At: 168 [==========>] Loss 0.24262648663265549  - accuracy: 0.6875\n",
      "At: 169 [==========>] Loss 0.1931399984430277  - accuracy: 0.75\n",
      "At: 170 [==========>] Loss 0.16192763479959976  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.22704677115450606  - accuracy: 0.75\n",
      "At: 172 [==========>] Loss 0.182993331345939  - accuracy: 0.78125\n",
      "At: 173 [==========>] Loss 0.259215757925588  - accuracy: 0.6875\n",
      "At: 174 [==========>] Loss 0.19948730004284077  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.1793523643032191  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.2199971009874176  - accuracy: 0.71875\n",
      "At: 177 [==========>] Loss 0.11920532158203638  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.21759930112363032  - accuracy: 0.71875\n",
      "At: 179 [==========>] Loss 0.18125010026108052  - accuracy: 0.75\n",
      "At: 180 [==========>] Loss 0.14760466328382477  - accuracy: 0.84375\n",
      "At: 181 [==========>] Loss 0.07690776509276623  - accuracy: 0.90625\n",
      "At: 182 [==========>] Loss 0.26392643795230175  - accuracy: 0.6875\n",
      "At: 183 [==========>] Loss 0.18520426556151665  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.2042310007728086  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.0887842769104256  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.1830504390832591  - accuracy: 0.75\n",
      "At: 187 [==========>] Loss 0.1867350252876172  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.2451897922557967  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.20564612871720084  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.12631097262409943  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.3241566200582687  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.1367916553341293  - accuracy: 0.875\n",
      "At: 193 [==========>] Loss 0.24858646745363416  - accuracy: 0.59375\n",
      "At: 194 [==========>] Loss 0.21795346915825276  - accuracy: 0.75\n",
      "At: 195 [==========>] Loss 0.14354321632484546  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.1686227571555253  - accuracy: 0.84375\n",
      "At: 197 [==========>] Loss 0.2234416700598809  - accuracy: 0.75\n",
      "At: 198 [==========>] Loss 0.1417936116457449  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.1367220978709019  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.21363355843178156  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.13137339830966221  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.10120099579384788  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.154818782028802  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.23316967507019032  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.17676327220114874  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.09855814393005845  - accuracy: 0.84375\n",
      "At: 207 [==========>] Loss 0.1349555949749864  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.23237664989702303  - accuracy: 0.6875\n",
      "At: 209 [==========>] Loss 0.1841706841524035  - accuracy: 0.71875\n",
      "At: 210 [==========>] Loss 0.1212467418231822  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.2182750464505433  - accuracy: 0.75\n",
      "At: 212 [==========>] Loss 0.25110873461371963  - accuracy: 0.6875\n",
      "At: 213 [==========>] Loss 0.20393937238259774  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.236295215731998  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.14186687786404834  - accuracy: 0.78125\n",
      "At: 216 [==========>] Loss 0.19785168483715215  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.21911182550629416  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.15632896377584365  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.2182396385895114  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.20977472285064588  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.19125809105002378  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.09889482138789099  - accuracy: 0.9375\n",
      "At: 223 [==========>] Loss 0.3188526187248716  - accuracy: 0.625\n",
      "At: 224 [==========>] Loss 0.22291439617868491  - accuracy: 0.75\n",
      "At: 225 [==========>] Loss 0.13474206960539448  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.19184537058135798  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.1573860159669756  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.20257544302870553  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.18454660973573464  - accuracy: 0.71875\n",
      "At: 230 [==========>] Loss 0.22275621975477816  - accuracy: 0.75\n",
      "At: 231 [==========>] Loss 0.22369332718959314  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.24235089158423867  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.2071032495909868  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.1285128309463716  - accuracy: 0.84375\n",
      "At: 235 [==========>] Loss 0.2123878585162365  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.19191193810821044  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.15731260448504888  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.15718522161238332  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.1259039362786371  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.2796900168315217  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.12015278683173597  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.18669977976911303  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.12575242970942113  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.18236850149346523  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.14655916792632173  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.15698675894246938  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.1985535794790082  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.1125806122390762  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.09412811825119893  - accuracy: 0.9375\n",
      "At: 250 [==========>] Loss 0.23035499043271648  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.23878508932388826  - accuracy: 0.6875\n",
      "At: 252 [==========>] Loss 0.1460198747589737  - accuracy: 0.8125\n",
      "At: 253 [==========>] Loss 0.20354028941027108  - accuracy: 0.75\n",
      "At: 254 [==========>] Loss 0.06177396788465355  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.18785572507413  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.1969610008776215  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.16092808494752875  - accuracy: 0.8125\n",
      "At: 258 [==========>] Loss 0.18065297742012099  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.13508511386730968  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.12717098711179364  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.07895547447908442  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.1884621821355018  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.12378009389256257  - accuracy: 0.84375\n",
      "At: 264 [==========>] Loss 0.13340755238227492  - accuracy: 0.875\n",
      "At: 265 [==========>] Loss 0.2612171774440165  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.26843543943910286  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1699039227405561  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.23054569382194143  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.19686222483323101  - accuracy: 0.8125\n",
      "At: 270 [==========>] Loss 0.27805403516949523  - accuracy: 0.59375\n",
      "At: 271 [==========>] Loss 0.20330131045273464  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.1004361998055487  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.19521901262077412  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.21536620577176396  - accuracy: 0.75\n",
      "At: 275 [==========>] Loss 0.09422537036409856  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.2148703400631196  - accuracy: 0.65625\n",
      "At: 277 [==========>] Loss 0.13664395062073148  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.14163599618308698  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.17529476959040846  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.1482940063997939  - accuracy: 0.84375\n",
      "At: 281 [==========>] Loss 0.1582563018729498  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.22328312923629906  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.15396631730866125  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.08806903701863876  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.16868066211794502  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.1120670277297511  - accuracy: 0.84375\n",
      "At: 287 [==========>] Loss 0.1970308105048734  - accuracy: 0.71875\n",
      "At: 288 [==========>] Loss 0.1281070347409156  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.10694935350435335  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.12075437039007096  - accuracy: 0.84375\n",
      "At: 291 [==========>] Loss 0.09235603502180867  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.162254152494698  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.22865708923173828  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.22044843971448322  - accuracy: 0.71875\n",
      "At: 295 [==========>] Loss 0.1575475055323857  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.13763087494526152  - accuracy: 0.84375\n",
      "At: 297 [==========>] Loss 0.15036178094882202  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.14585132557895514  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.21189850813299305  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.1790011301749181  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.17368150627321308  - accuracy: 0.75\n",
      "At: 302 [==========>] Loss 0.10977009100094355  - accuracy: 0.84375\n",
      "At: 303 [==========>] Loss 0.11057845756704643  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.2489404882637204  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.2663670145802758  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.12137637817337857  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.2923798133512671  - accuracy: 0.53125\n",
      "At: 308 [==========>] Loss 0.17866544614086222  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.09808203973738927  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.2156422257526838  - accuracy: 0.6875\n",
      "At: 311 [==========>] Loss 0.07572459032196463  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.13675894024265725  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.16412820704618003  - accuracy: 0.8125\n",
      "At: 314 [==========>] Loss 0.24417329369440344  - accuracy: 0.6875\n",
      "At: 315 [==========>] Loss 0.15392771856311582  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.20362627794129556  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.25470176167005515  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.1512473202433546  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.10866931571765663  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.21620593714593372  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.27540880305640253  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.12439365141417026  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.11390735262614088  - accuracy: 0.875\n",
      "At: 324 [==========>] Loss 0.16914040760205096  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.11385614121790232  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.16584092879284298  - accuracy: 0.78125\n",
      "At: 327 [==========>] Loss 0.13157359551591527  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.15836369072390338  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.13655062840492682  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.1432345061958257  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.2373748814425018  - accuracy: 0.71875\n",
      "At: 332 [==========>] Loss 0.2740508033948873  - accuracy: 0.59375\n",
      "At: 333 [==========>] Loss 0.1547738490612403  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.12297674351651545  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.14650621192365415  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.13014076670962565  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.19840445037358168  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.18199227658976022  - accuracy: 0.78125\n",
      "At: 339 [==========>] Loss 0.1873443763176827  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.1266673752630621  - accuracy: 0.75\n",
      "At: 341 [==========>] Loss 0.11923054695576377  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.18442666301016095  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.27277885578765915  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.22060263805024855  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.23865935107288322  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.1548058073721414  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.1233878340175293  - accuracy: 0.875\n",
      "At: 348 [==========>] Loss 0.15171418093051758  - accuracy: 0.78125\n",
      "At: 349 [==========>] Loss 0.20442484556063095  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.13136290951975316  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.23279588900069476  - accuracy: 0.6875\n",
      "At: 352 [==========>] Loss 0.1038344257002989  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.1698321216298835  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.2208108410375167  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.12268617761603044  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.18690331770468185  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.14005713411243442  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.1537166829257689  - accuracy: 0.78125\n",
      "At: 359 [==========>] Loss 0.077232528743171  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.1505872772555214  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.07792759605345162  - accuracy: 0.9375\n",
      "At: 362 [==========>] Loss 0.19699558728642647  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.09046190230085914  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.23396822487839863  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.08508731014802985  - accuracy: 0.875\n",
      "At: 366 [==========>] Loss 0.1950698869446631  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.19131649875732934  - accuracy: 0.6875\n",
      "At: 368 [==========>] Loss 0.2055660938676157  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.177278195931731  - accuracy: 0.78125\n",
      "At: 370 [==========>] Loss 0.14479142734646577  - accuracy: 0.78125\n",
      "At: 371 [==========>] Loss 0.08704185947753534  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.10899502340006065  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.22925044255120414  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.08513330110377641  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1308865535273459  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.11034842096996118  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.2449841592449952  - accuracy: 0.6875\n",
      "At: 378 [==========>] Loss 0.21784739061344266  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.19720495923993311  - accuracy: 0.75\n",
      "At: 380 [==========>] Loss 0.15736646759229303  - accuracy: 0.78125\n",
      "At: 381 [==========>] Loss 0.18054540897056554  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.11087894238243752  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.21205727663796695  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.1745931070647396  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.15593346770864464  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.2397405416842192  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.09887669166481172  - accuracy: 0.875\n",
      "At: 388 [==========>] Loss 0.23236015285404646  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.23553281748267718  - accuracy: 0.71875\n",
      "At: 390 [==========>] Loss 0.10989075195587025  - accuracy: 0.875\n",
      "At: 391 [==========>] Loss 0.16699247350114726  - accuracy: 0.78125\n",
      "At: 392 [==========>] Loss 0.15962555402722925  - accuracy: 0.78125\n",
      "At: 393 [==========>] Loss 0.2222975291628409  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.10422691190423539  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.188123897712028  - accuracy: 0.71875\n",
      "At: 396 [==========>] Loss 0.1638563820127041  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.1381676076155714  - accuracy: 0.84375\n",
      "At: 398 [==========>] Loss 0.19920285966041246  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.27858532437165856  - accuracy: 0.6875\n",
      "At: 400 [==========>] Loss 0.18419939482307918  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.154926010482497  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.11963194835798284  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.03035627286820869  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.08600766399732204  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.2238088318556539  - accuracy: 0.6875\n",
      "At: 406 [==========>] Loss 0.16319670903893246  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.16405857539410348  - accuracy: 0.78125\n",
      "At: 408 [==========>] Loss 0.182777741412435  - accuracy: 0.75\n",
      "At: 409 [==========>] Loss 0.2573147383141452  - accuracy: 0.625\n",
      "At: 410 [==========>] Loss 0.12317784110514637  - accuracy: 0.875\n",
      "At: 411 [==========>] Loss 0.13663042315079396  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.19149721149583  - accuracy: 0.8125\n",
      "At: 413 [==========>] Loss 0.1298292930559979  - accuracy: 0.8125\n",
      "At: 414 [==========>] Loss 0.19881628027422443  - accuracy: 0.6875\n",
      "At: 415 [==========>] Loss 0.1480104817527075  - accuracy: 0.78125\n",
      "At: 416 [==========>] Loss 0.2234859589800119  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.19080227985319634  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.15899617812846775  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.17503299505764836  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.18463958015300042  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.1426167384904543  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.11958702277825442  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.145717380798209  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.2597873686798504  - accuracy: 0.65625\n",
      "At: 425 [==========>] Loss 0.20311812493216347  - accuracy: 0.78125\n",
      "At: 426 [==========>] Loss 0.13428909891298832  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.21691113515293312  - accuracy: 0.71875\n",
      "At: 428 [==========>] Loss 0.2517126480012459  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.13263006645449887  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.13302671737180377  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.16141718336669525  - accuracy: 0.8125\n",
      "At: 432 [==========>] Loss 0.16017800456768688  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.10767874593359338  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.10471805283445121  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.20012508270093748  - accuracy: 0.71875\n",
      "At: 436 [==========>] Loss 0.1429760420730317  - accuracy: 0.78125\n",
      "At: 437 [==========>] Loss 0.16159474562973727  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.17222649820019148  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.10490220376146929  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.08798144351226492  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.2165642107527983  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.21684204343365585  - accuracy: 0.6875\n",
      "At: 443 [==========>] Loss 0.14231639058984905  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.17312598754398284  - accuracy: 0.71875\n",
      "At: 445 [==========>] Loss 0.158416711777071  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.2635474787371398  - accuracy: 0.65625\n",
      "At: 447 [==========>] Loss 0.12686260715656328  - accuracy: 0.84375\n",
      "At: 448 [==========>] Loss 0.16656811954504064  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.10117626717566756  - accuracy: 0.875\n",
      "At: 450 [==========>] Loss 0.1908423797123371  - accuracy: 0.78125\n",
      "At: 451 [==========>] Loss 0.1909337687716926  - accuracy: 0.71875\n",
      "At: 452 [==========>] Loss 0.15791020770147593  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.19107227639067115  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.2584659511250579  - accuracy: 0.65625\n",
      "At: 455 [==========>] Loss 0.1979935598538707  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.15796158343240782  - accuracy: 0.78125\n",
      "At: 457 [==========>] Loss 0.17689464610036526  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.132668020360423  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.24929387203427583  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.13637629331707535  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.21881651585416312  - accuracy: 0.65625\n",
      "At: 462 [==========>] Loss 0.15710870254082593  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.16813388039186686  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.18314333670981725  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.21291766874348794  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.08836753826031771  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.17217343744517527  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.10476042209198183  - accuracy: 0.90625\n",
      "At: 469 [==========>] Loss 0.14881782268481364  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.12271778888879958  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.1751717064235991  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.10711427454007949  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.16101880362520948  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.11589204055384308  - accuracy: 0.84375\n",
      "At: 475 [==========>] Loss 0.1272792409370035  - accuracy: 0.875\n",
      "At: 476 [==========>] Loss 0.1666354535034658  - accuracy: 0.75\n",
      "At: 477 [==========>] Loss 0.12176059496082411  - accuracy: 0.875\n",
      "At: 478 [==========>] Loss 0.13165681081374703  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.18284725320247025  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.22665399406084202  - accuracy: 0.6875\n",
      "At: 481 [==========>] Loss 0.14829950322692942  - accuracy: 0.8125\n",
      "At: 482 [==========>] Loss 0.10971993553829855  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.11610916250231623  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.0894514667138705  - accuracy: 0.90625\n",
      "At: 485 [==========>] Loss 0.12983723822177043  - accuracy: 0.78125\n",
      "At: 486 [==========>] Loss 0.2286590012952914  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.20350395472722999  - accuracy: 0.75\n",
      "At: 488 [==========>] Loss 0.14955481644620183  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.1718733659071717  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.19806070768933592  - accuracy: 0.71875\n",
      "At: 491 [==========>] Loss 0.21219707531249687  - accuracy: 0.71875\n",
      "At: 492 [==========>] Loss 0.2385578097549429  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.13999969260215084  - accuracy: 0.78125\n",
      "At: 494 [==========>] Loss 0.21303037410314363  - accuracy: 0.75\n",
      "At: 495 [==========>] Loss 0.17691199209615344  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.16883264755134483  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.20690885540934661  - accuracy: 0.71875\n",
      "At: 498 [==========>] Loss 0.09566775152265228  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.22254359966987805  - accuracy: 0.71875\n",
      "At: 500 [==========>] Loss 0.1591373152916204  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.16904058804505434  - accuracy: 0.8125\n",
      "At: 502 [==========>] Loss 0.12440358910243057  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.1327522701286586  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.1261053860596704  - accuracy: 0.84375\n",
      "At: 505 [==========>] Loss 0.23448523556578857  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.281581435206221  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.1014886090081949  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.11944328352912907  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.20415495569716074  - accuracy: 0.75\n",
      "At: 510 [==========>] Loss 0.2058655900324498  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.1473344013775828  - accuracy: 0.75\n",
      "At: 512 [==========>] Loss 0.15075985896217026  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.24680461841315005  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.17514683259107494  - accuracy: 0.78125\n",
      "At: 515 [==========>] Loss 0.12677968220244368  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.1915430598280634  - accuracy: 0.6875\n",
      "At: 517 [==========>] Loss 0.17790457509797242  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.18403541448811567  - accuracy: 0.75\n",
      "At: 519 [==========>] Loss 0.1321132378576214  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.11329977202126326  - accuracy: 0.84375\n",
      "At: 521 [==========>] Loss 0.18430980914396417  - accuracy: 0.78125\n",
      "At: 522 [==========>] Loss 0.1725473319501834  - accuracy: 0.75\n",
      "At: 523 [==========>] Loss 0.18316051809306447  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.13500201980903298  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.18452121279022202  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.17834984319578334  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.33459528722362764  - accuracy: 0.53125\n",
      "At: 528 [==========>] Loss 0.21535597316342225  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.11004762305286028  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.2594603570726426  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.15062146950648725  - accuracy: 0.75\n",
      "At: 532 [==========>] Loss 0.15804123161894768  - accuracy: 0.8125\n",
      "At: 533 [==========>] Loss 0.09291526552272852  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.1797625711333514  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.19713943726004646  - accuracy: 0.65625\n",
      "At: 536 [==========>] Loss 0.19744003205559235  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.1625579649072233  - accuracy: 0.8125\n",
      "At: 538 [==========>] Loss 0.13539470023634034  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.11692243277657655  - accuracy: 0.875\n",
      "At: 540 [==========>] Loss 0.23253979016510146  - accuracy: 0.6875\n",
      "At: 541 [==========>] Loss 0.19278305749985475  - accuracy: 0.6875\n",
      "At: 542 [==========>] Loss 0.13196008773513634  - accuracy: 0.84375\n",
      "At: 543 [==========>] Loss 0.20218449879806813  - accuracy: 0.71875\n",
      "At: 544 [==========>] Loss 0.21272413772796686  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.10623425765541113  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.18716939815437833  - accuracy: 0.71875\n",
      "At: 547 [==========>] Loss 0.1595672697213085  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.1336587796673401  - accuracy: 0.84375\n",
      "At: 549 [==========>] Loss 0.1297733686722281  - accuracy: 0.8125\n",
      "At: 550 [==========>] Loss 0.09346033592714255  - accuracy: 0.9375\n",
      "At: 551 [==========>] Loss 0.13888662839823235  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.1688027440123977  - accuracy: 0.78125\n",
      "At: 553 [==========>] Loss 0.12314648605883954  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.10375388708751963  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.18858584028912326  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.1588900816798789  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.1586767493290489  - accuracy: 0.8125\n",
      "At: 558 [==========>] Loss 0.2003828752787143  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.19934546563958577  - accuracy: 0.71875\n",
      "At: 560 [==========>] Loss 0.16055159626899018  - accuracy: 0.8125\n",
      "At: 561 [==========>] Loss 0.1426532342352752  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.08630875265485027  - accuracy: 0.875\n",
      "At: 563 [==========>] Loss 0.12205924229870758  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.18565926173331415  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.15316857783869348  - accuracy: 0.78125\n",
      "At: 566 [==========>] Loss 0.18348274748835977  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.18475805571665294  - accuracy: 0.71875\n",
      "At: 568 [==========>] Loss 0.2322365647240351  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.1936266848470135  - accuracy: 0.6875\n",
      "At: 570 [==========>] Loss 0.15263433828876138  - accuracy: 0.8125\n",
      "At: 571 [==========>] Loss 0.10954678147081136  - accuracy: 0.875\n",
      "At: 572 [==========>] Loss 0.1410845111214065  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.10934711460099442  - accuracy: 0.84375\n",
      "At: 574 [==========>] Loss 0.20257095231300526  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.16018523285861763  - accuracy: 0.78125\n",
      "At: 576 [==========>] Loss 0.069079649153715  - accuracy: 0.9375\n",
      "At: 577 [==========>] Loss 0.201157796205654  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.19289946583548045  - accuracy: 0.6875\n",
      "At: 579 [==========>] Loss 0.12552583477035445  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.126758307354613  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.17333130469960897  - accuracy: 0.71875\n",
      "At: 582 [==========>] Loss 0.17854457655658662  - accuracy: 0.71875\n",
      "At: 583 [==========>] Loss 0.18656001520705862  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.11114020833583041  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.2000474249386639  - accuracy: 0.75\n",
      "At: 586 [==========>] Loss 0.05991962616764382  - accuracy: 0.90625\n",
      "At: 587 [==========>] Loss 0.17663303567245897  - accuracy: 0.6875\n",
      "At: 588 [==========>] Loss 0.14814001458219078  - accuracy: 0.8125\n",
      "At: 589 [==========>] Loss 0.1391728655443691  - accuracy: 0.875\n",
      "At: 590 [==========>] Loss 0.09278443330951117  - accuracy: 0.875\n",
      "At: 591 [==========>] Loss 0.1496528583833271  - accuracy: 0.8125\n",
      "At: 592 [==========>] Loss 0.12334971016163981  - accuracy: 0.84375\n",
      "At: 593 [==========>] Loss 0.1993905286667966  - accuracy: 0.65625\n",
      "At: 594 [==========>] Loss 0.15271794130146088  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.12530107449350053  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.14193561561350354  - accuracy: 0.78125\n",
      "At: 597 [==========>] Loss 0.19992710258791702  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.1436156683653971  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.1339146590900787  - accuracy: 0.84375\n",
      "At: 600 [==========>] Loss 0.10266473488815112  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.1631391292300251  - accuracy: 0.8125\n",
      "At: 602 [==========>] Loss 0.1444081662531098  - accuracy: 0.84375\n",
      "At: 603 [==========>] Loss 0.23511033518598742  - accuracy: 0.65625\n",
      "At: 604 [==========>] Loss 0.20113307919474027  - accuracy: 0.65625\n",
      "At: 605 [==========>] Loss 0.11312884706599635  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.17022843134276291  - accuracy: 0.78125\n",
      "At: 607 [==========>] Loss 0.1288846394767213  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.14664208617941832  - accuracy: 0.84375\n",
      "At: 609 [==========>] Loss 0.11789305161894739  - accuracy: 0.875\n",
      "At: 610 [==========>] Loss 0.15973976246495014  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.1086546665383952  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.14555002668079664  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.16190450632482534  - accuracy: 0.75\n",
      "At: 614 [==========>] Loss 0.15314666923956083  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.15550099540206808  - accuracy: 0.8125\n",
      "At: 616 [==========>] Loss 0.18408919012086888  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.1421188914934285  - accuracy: 0.75\n",
      "At: 618 [==========>] Loss 0.2309647184201235  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.16682999602898974  - accuracy: 0.78125\n",
      "At: 620 [==========>] Loss 0.20279874223708513  - accuracy: 0.78125\n",
      "At: 621 [==========>] Loss 0.08057224904611493  - accuracy: 0.875\n",
      "At: 622 [==========>] Loss 0.18735650993415803  - accuracy: 0.78125\n",
      "At: 623 [==========>] Loss 0.13645775391449333  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.1174826919954787  - accuracy: 0.84375\n",
      "At: 625 [==========>] Loss 0.14658182931097472  - accuracy: 0.78125\n",
      "At: 626 [==========>] Loss 0.178160606064691  - accuracy: 0.75\n",
      "At: 627 [==========>] Loss 0.09558167060817976  - accuracy: 0.90625\n",
      "At: 628 [==========>] Loss 0.10115683486295828  - accuracy: 0.90625\n",
      "At: 629 [==========>] Loss 0.17637297644781277  - accuracy: 0.78125\n",
      "At: 630 [==========>] Loss 0.2922719054199846  - accuracy: 0.5625\n",
      "At: 631 [==========>] Loss 0.19304706961583207  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.15912094733634935  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.1798482758213944  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.15588379237872146  - accuracy: 0.8125\n",
      "At: 635 [==========>] Loss 0.12152994336243461  - accuracy: 0.875\n",
      "At: 636 [==========>] Loss 0.14971714618191329  - accuracy: 0.78125\n",
      "At: 637 [==========>] Loss 0.15725117521619542  - accuracy: 0.78125\n",
      "At: 638 [==========>] Loss 0.13492027338418783  - accuracy: 0.8125\n",
      "At: 639 [==========>] Loss 0.151501377072155  - accuracy: 0.75\n",
      "At: 640 [==========>] Loss 0.24515411533924125  - accuracy: 0.59375\n",
      "At: 641 [==========>] Loss 0.15169883047472638  - accuracy: 0.78125\n",
      "At: 642 [==========>] Loss 0.16769428950241402  - accuracy: 0.78125\n",
      "At: 643 [==========>] Loss 0.18281420315850805  - accuracy: 0.78125\n",
      "At: 644 [==========>] Loss 0.0920344648839613  - accuracy: 0.90625\n",
      "At: 645 [==========>] Loss 0.15254060519848917  - accuracy: 0.78125\n",
      "At: 646 [==========>] Loss 0.11231663749240826  - accuracy: 0.8125\n",
      "At: 647 [==========>] Loss 0.19201989483390597  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.13994799207572622  - accuracy: 0.84375\n",
      "At: 649 [==========>] Loss 0.2390474960858878  - accuracy: 0.6875\n",
      "At: 650 [==========>] Loss 0.10395555537471027  - accuracy: 0.90625\n",
      "At: 651 [==========>] Loss 0.1872791107345935  - accuracy: 0.6875\n",
      "At: 652 [==========>] Loss 0.1086430944649687  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.12325223547113476  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.08320595930258051  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.15453325831519352  - accuracy: 0.78125\n",
      "At: 656 [==========>] Loss 0.11113576797423774  - accuracy: 0.90625\n",
      "At: 657 [==========>] Loss 0.18502796023605494  - accuracy: 0.6875\n",
      "At: 658 [==========>] Loss 0.12141355079965474  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.1581126293398969  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.19181167709951413  - accuracy: 0.71875\n",
      "At: 661 [==========>] Loss 0.1706975605390297  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.10465040312186737  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.10805121000010506  - accuracy: 0.9375\n",
      "At: 664 [==========>] Loss 0.17467655101749036  - accuracy: 0.75\n",
      "At: 665 [==========>] Loss 0.21417731932459638  - accuracy: 0.6875\n",
      "At: 666 [==========>] Loss 0.2053746096069648  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.17265689470339426  - accuracy: 0.78125\n",
      "At: 668 [==========>] Loss 0.15997043575420053  - accuracy: 0.8125\n",
      "At: 669 [==========>] Loss 0.16321218482111505  - accuracy: 0.78125\n",
      "At: 670 [==========>] Loss 0.22729839971006716  - accuracy: 0.65625\n",
      "At: 671 [==========>] Loss 0.0993122317303245  - accuracy: 0.9375\n",
      "At: 672 [==========>] Loss 0.1415175690363007  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.06615123064426437  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.10449624662951877  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.13300397585294027  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.15623600956449707  - accuracy: 0.78125\n",
      "At: 677 [==========>] Loss 0.1494740253182486  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.13599396309302353  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.11227894297452876  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.12642594020522116  - accuracy: 0.8125\n",
      "At: 681 [==========>] Loss 0.11880440609165635  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.13019465708973363  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.13543895993909247  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.12307079136360867  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.1290028261832728  - accuracy: 0.84375\n",
      "At: 686 [==========>] Loss 0.09918069351037186  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.06340568139307104  - accuracy: 0.96875\n",
      "At: 688 [==========>] Loss 0.10343505443052899  - accuracy: 0.84375\n",
      "At: 689 [==========>] Loss 0.17323167544303547  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.1416717847046791  - accuracy: 0.75\n",
      "At: 691 [==========>] Loss 0.10297829498732251  - accuracy: 0.875\n",
      "At: 692 [==========>] Loss 0.11336113405742379  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.1345171803711912  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.17786274422134268  - accuracy: 0.6875\n",
      "At: 695 [==========>] Loss 0.19069868689488909  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.17850118008932747  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.22294584884568128  - accuracy: 0.6875\n",
      "At: 698 [==========>] Loss 0.14654780845799975  - accuracy: 0.84375\n",
      "At: 699 [==========>] Loss 0.17745268356643645  - accuracy: 0.8125\n",
      "At: 700 [==========>] Loss 0.13432808681216746  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.1567457143289312  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.068294722824922  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.1691225781296843  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.20938319565777314  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.21518313873886835  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.1246734626250279  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.16057464061373894  - accuracy: 0.78125\n",
      "At: 708 [==========>] Loss 0.15541629091557632  - accuracy: 0.78125\n",
      "At: 709 [==========>] Loss 0.15245100103588927  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.20194182319226964  - accuracy: 0.71875\n",
      "At: 711 [==========>] Loss 0.2698761251679854  - accuracy: 0.625\n",
      "At: 712 [==========>] Loss 0.2016207692906239  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.17947214630143465  - accuracy: 0.71875\n",
      "At: 714 [==========>] Loss 0.2522979044902201  - accuracy: 0.59375\n",
      "At: 715 [==========>] Loss 0.09306685606006394  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.14700175228672735  - accuracy: 0.78125\n",
      "At: 717 [==========>] Loss 0.08300435752051781  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.19010509898758385  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.1238302006603521  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.1582244311679562  - accuracy: 0.78125\n",
      "At: 721 [==========>] Loss 0.10799191978826475  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.16389789741385036  - accuracy: 0.78125\n",
      "At: 723 [==========>] Loss 0.15790450548629648  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.08281548719992925  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.17118509575146595  - accuracy: 0.78125\n",
      "At: 726 [==========>] Loss 0.20254424261026055  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.17796538034203202  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.23439871105818502  - accuracy: 0.65625\n",
      "At: 729 [==========>] Loss 0.20946773607089553  - accuracy: 0.71875\n",
      "At: 730 [==========>] Loss 0.22505876487712403  - accuracy: 0.71875\n",
      "At: 731 [==========>] Loss 0.15451931117633566  - accuracy: 0.75\n",
      "At: 732 [==========>] Loss 0.13940923903125238  - accuracy: 0.875\n",
      "At: 733 [==========>] Loss 0.12475210338749262  - accuracy: 0.8125\n",
      "At: 734 [==========>] Loss 0.19964534642156767  - accuracy: 0.71875\n",
      "At: 735 [==========>] Loss 0.13620192840910658  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.10976885957085654  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.18217938620606167  - accuracy: 0.71875\n",
      "At: 738 [==========>] Loss 0.10316455972106384  - accuracy: 0.8125\n",
      "At: 739 [==========>] Loss 0.09429329041596628  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.13041379687467108  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.14493606455956803  - accuracy: 0.8125\n",
      "At: 742 [==========>] Loss 0.1964984365077339  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.1449211652612976  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.1669375127691727  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.2230932083518638  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.11738171238819224  - accuracy: 0.875\n",
      "At: 747 [==========>] Loss 0.15146377918695786  - accuracy: 0.8125\n",
      "At: 748 [==========>] Loss 0.16259995356650703  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.19882095484485504  - accuracy: 0.6875\n",
      "At: 750 [==========>] Loss 0.10727284491210937  - accuracy: 0.875\n",
      "At: 751 [==========>] Loss 0.19856670184918854  - accuracy: 0.71875\n",
      "At: 752 [==========>] Loss 0.071004966438704  - accuracy: 0.875\n",
      "At: 753 [==========>] Loss 0.18322264419151857  - accuracy: 0.71875\n",
      "At: 754 [==========>] Loss 0.1725459166921016  - accuracy: 0.6875\n",
      "At: 755 [==========>] Loss 0.11136301043648288  - accuracy: 0.8125\n",
      "At: 756 [==========>] Loss 0.1983939002266093  - accuracy: 0.75\n",
      "At: 757 [==========>] Loss 0.0946965837845441  - accuracy: 0.9375\n",
      "At: 758 [==========>] Loss 0.13215125046566145  - accuracy: 0.78125\n",
      "At: 759 [==========>] Loss 0.08263827401540269  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.1829376789294362  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.10670189933840267  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.14367037921559947  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.12817677361030821  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.12718369095644205  - accuracy: 0.75\n",
      "At: 765 [==========>] Loss 0.1886617093272617  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.16499488721693317  - accuracy: 0.71875\n",
      "At: 767 [==========>] Loss 0.11111824717337597  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.14590410158379752  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.14126926600205872  - accuracy: 0.8125\n",
      "At: 770 [==========>] Loss 0.13988229950630415  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.21517928049812918  - accuracy: 0.75\n",
      "At: 772 [==========>] Loss 0.11585109432404231  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.09367164972396508  - accuracy: 0.875\n",
      "At: 774 [==========>] Loss 0.17584535544840044  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.21095211717107626  - accuracy: 0.65625\n",
      "At: 776 [==========>] Loss 0.19796394907181136  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.10855011660120834  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.1772597503009699  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.0966167890291138  - accuracy: 0.875\n",
      "At: 780 [==========>] Loss 0.07511757825585272  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.12665883817684054  - accuracy: 0.84375\n",
      "At: 782 [==========>] Loss 0.18557298379155154  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.18519381793963274  - accuracy: 0.71875\n",
      "At: 784 [==========>] Loss 0.1741732367214045  - accuracy: 0.6875\n",
      "At: 785 [==========>] Loss 0.23800598412921364  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.13058708358075807  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.20182424594476156  - accuracy: 0.6875\n",
      "At: 788 [==========>] Loss 0.06689283201852331  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.17284006943108804  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.16612460285644948  - accuracy: 0.78125\n",
      "At: 791 [==========>] Loss 0.14889647825131552  - accuracy: 0.8125\n",
      "At: 792 [==========>] Loss 0.17089166577331152  - accuracy: 0.78125\n",
      "At: 793 [==========>] Loss 0.12684629170827788  - accuracy: 0.84375\n",
      "At: 794 [==========>] Loss 0.18875060309183056  - accuracy: 0.71875\n",
      "At: 795 [==========>] Loss 0.15178738909013484  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.15025629040754407  - accuracy: 0.78125\n",
      "At: 797 [==========>] Loss 0.14624648863108836  - accuracy: 0.8125\n",
      "At: 798 [==========>] Loss 0.1808710471772328  - accuracy: 0.71875\n",
      "At: 799 [==========>] Loss 0.09727707879934616  - accuracy: 0.84375\n",
      "At: 800 [==========>] Loss 0.08964936186415473  - accuracy: 0.90625\n",
      "At: 801 [==========>] Loss 0.10415380932181859  - accuracy: 0.90625\n",
      "At: 802 [==========>] Loss 0.22121187445710422  - accuracy: 0.65625\n",
      "At: 803 [==========>] Loss 0.16813406608723563  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.15096989753161394  - accuracy: 0.78125\n",
      "At: 805 [==========>] Loss 0.1413483843230326  - accuracy: 0.78125\n",
      "At: 806 [==========>] Loss 0.10031348631398215  - accuracy: 0.875\n",
      "At: 807 [==========>] Loss 0.1280496785629181  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.14394928579989746  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.09083395008122061  - accuracy: 0.875\n",
      "At: 810 [==========>] Loss 0.22082032423812423  - accuracy: 0.71875\n",
      "At: 811 [==========>] Loss 0.16056398095072405  - accuracy: 0.78125\n",
      "At: 812 [==========>] Loss 0.13843896403811204  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.2248665552302029  - accuracy: 0.6875\n",
      "At: 814 [==========>] Loss 0.1566537483386875  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.13654680624885726  - accuracy: 0.875\n",
      "At: 816 [==========>] Loss 0.13668702640808592  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.07113637912874206  - accuracy: 0.875\n",
      "At: 818 [==========>] Loss 0.1478373495072619  - accuracy: 0.78125\n",
      "At: 819 [==========>] Loss 0.13122262033331625  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.12408371011796082  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.1469600204515069  - accuracy: 0.8125\n",
      "At: 822 [==========>] Loss 0.16452066904990878  - accuracy: 0.78125\n",
      "At: 823 [==========>] Loss 0.137899504198219  - accuracy: 0.84375\n",
      "At: 824 [==========>] Loss 0.17428038221817438  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.21311560537642693  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.12241511054996326  - accuracy: 0.875\n",
      "At: 827 [==========>] Loss 0.09910957677052562  - accuracy: 0.84375\n",
      "At: 828 [==========>] Loss 0.060192829353220685  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09934360984367238  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.10189273916371039  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.12218454477716739  - accuracy: 0.84375\n",
      "At: 832 [==========>] Loss 0.14442141331979833  - accuracy: 0.78125\n",
      "At: 833 [==========>] Loss 0.19286273364012013  - accuracy: 0.6875\n",
      "At: 834 [==========>] Loss 0.13301488934823286  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.07432080663310832  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.10629309718607237  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.09450999279967896  - accuracy: 0.84375\n",
      "At: 838 [==========>] Loss 0.1170160410332372  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.10667896715329538  - accuracy: 0.875\n",
      "At: 840 [==========>] Loss 0.12419340615575933  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.07082571329662074  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.06288533050905323  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.14323007935957927  - accuracy: 0.8125\n",
      "At: 844 [==========>] Loss 0.13374594654698063  - accuracy: 0.78125\n",
      "At: 845 [==========>] Loss 0.16901636328132552  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.1200084006824456  - accuracy: 0.875\n",
      "At: 847 [==========>] Loss 0.09854011660951831  - accuracy: 0.875\n",
      "At: 848 [==========>] Loss 0.11526314454195151  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.17983418095690146  - accuracy: 0.75\n",
      "At: 850 [==========>] Loss 0.0937172626366768  - accuracy: 0.90625\n",
      "At: 851 [==========>] Loss 0.09179540050522896  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.14240768404108084  - accuracy: 0.78125\n",
      "At: 853 [==========>] Loss 0.16336575170655335  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.22154198713529621  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.08366304280837021  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.09219112003142216  - accuracy: 0.84375\n",
      "At: 857 [==========>] Loss 0.10242778029886662  - accuracy: 0.875\n",
      "At: 858 [==========>] Loss 0.27513218458616717  - accuracy: 0.59375\n",
      "At: 859 [==========>] Loss 0.11363496416805462  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.0993273804264208  - accuracy: 0.875\n",
      "At: 861 [==========>] Loss 0.09053300684555765  - accuracy: 0.90625\n",
      "At: 862 [==========>] Loss 0.10126125433348811  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.19140650504707918  - accuracy: 0.71875\n",
      "At: 864 [==========>] Loss 0.1761644572697452  - accuracy: 0.75\n",
      "At: 865 [==========>] Loss 0.2259811880630167  - accuracy: 0.6875\n",
      "At: 866 [==========>] Loss 0.14018109097510117  - accuracy: 0.84375\n",
      "At: 867 [==========>] Loss 0.10709339212061712  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.1622177795999737  - accuracy: 0.75\n",
      "At: 869 [==========>] Loss 0.20212913513725317  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.12593273581914366  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.0924054942307416  - accuracy: 0.84375\n",
      "At: 872 [==========>] Loss 0.08056772338575757  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.17858913868104942  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.14415433174529246  - accuracy: 0.75\n",
      "At: 875 [==========>] Loss 0.12568513620412433  - accuracy: 0.84375\n",
      "At: 876 [==========>] Loss 0.11842109241786136  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.17745487497297802  - accuracy: 0.78125\n",
      "At: 878 [==========>] Loss 0.07907190290331038  - accuracy: 0.875\n",
      "At: 879 [==========>] Loss 0.14103848661660429  - accuracy: 0.8125\n",
      "At: 880 [==========>] Loss 0.1288209245461834  - accuracy: 0.8125\n",
      "At: 881 [==========>] Loss 0.1769351347275289  - accuracy: 0.8125\n",
      "At: 882 [==========>] Loss 0.09911460030235988  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.1471296458132449  - accuracy: 0.71875\n",
      "At: 884 [==========>] Loss 0.1424022535389065  - accuracy: 0.75\n",
      "At: 885 [==========>] Loss 0.11217282483518157  - accuracy: 0.875\n",
      "At: 886 [==========>] Loss 0.06955562930473164  - accuracy: 0.90625\n",
      "At: 887 [==========>] Loss 0.1655598546142244  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.1396975618304746  - accuracy: 0.8125\n",
      "At: 889 [==========>] Loss 0.09213553832483269  - accuracy: 0.90625\n",
      "At: 890 [==========>] Loss 0.10603648285719217  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.11066833433906054  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.12745381082855933  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.15859791752483324  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.12244763614634294  - accuracy: 0.875\n",
      "At: 895 [==========>] Loss 0.13467986143437305  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.15755661899606066  - accuracy: 0.78125\n",
      "At: 897 [==========>] Loss 0.1438135167592232  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.19271032464209553  - accuracy: 0.71875\n",
      "At: 899 [==========>] Loss 0.12098582690307141  - accuracy: 0.8125\n",
      "At: 900 [==========>] Loss 0.16629590725020524  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.1420355945225255  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.13407701231987437  - accuracy: 0.78125\n",
      "At: 903 [==========>] Loss 0.14395706771591354  - accuracy: 0.78125\n",
      "At: 904 [==========>] Loss 0.08995499050525603  - accuracy: 0.84375\n",
      "At: 905 [==========>] Loss 0.12312513623229954  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.11974517420287856  - accuracy: 0.8125\n",
      "At: 907 [==========>] Loss 0.14332832314767469  - accuracy: 0.8125\n",
      "At: 908 [==========>] Loss 0.14589285778354427  - accuracy: 0.8125\n",
      "At: 909 [==========>] Loss 0.12946107271269386  - accuracy: 0.75\n",
      "At: 910 [==========>] Loss 0.1128609008434466  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.12225775159906618  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.13320303127049243  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.1252536116155379  - accuracy: 0.875\n",
      "At: 914 [==========>] Loss 0.14209487994654224  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.14093856891890222  - accuracy: 0.75\n",
      "At: 916 [==========>] Loss 0.1873109497737756  - accuracy: 0.6875\n",
      "At: 917 [==========>] Loss 0.16958454588543975  - accuracy: 0.71875\n",
      "At: 918 [==========>] Loss 0.17035589533406925  - accuracy: 0.75\n",
      "At: 919 [==========>] Loss 0.08956937819408084  - accuracy: 0.84375\n",
      "At: 920 [==========>] Loss 0.10183544946127275  - accuracy: 0.875\n",
      "At: 921 [==========>] Loss 0.15544659217172133  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.16471051057435948  - accuracy: 0.75\n",
      "At: 923 [==========>] Loss 0.10218850235732688  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.14997209912958603  - accuracy: 0.8125\n",
      "At: 925 [==========>] Loss 0.1271525689721626  - accuracy: 0.84375\n",
      "At: 926 [==========>] Loss 0.11534412106205692  - accuracy: 0.875\n",
      "At: 927 [==========>] Loss 0.13329654018856335  - accuracy: 0.78125\n",
      "At: 928 [==========>] Loss 0.11722541229773975  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.13627557306603844  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.12486269420070178  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.12215696518652144  - accuracy: 0.8125\n",
      "At: 932 [==========>] Loss 0.10209102537899714  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.09477221688231288  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.1231386850682908  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.044536572951494796  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.15251482167082633  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.19125220560283457  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.15185887235799275  - accuracy: 0.78125\n",
      "At: 939 [==========>] Loss 0.11159247992274483  - accuracy: 0.90625\n",
      "At: 940 [==========>] Loss 0.23565768086546346  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.12492634592593291  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.19686622599463538  - accuracy: 0.71875\n",
      "At: 943 [==========>] Loss 0.10638811970602194  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.10843499430282326  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.12324089215443522  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.10702190513088569  - accuracy: 0.90625\n",
      "At: 947 [==========>] Loss 0.14885339386849916  - accuracy: 0.84375\n",
      "At: 948 [==========>] Loss 0.19857820528197748  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.08137242193808461  - accuracy: 0.875\n",
      "At: 950 [==========>] Loss 0.14535899119990886  - accuracy: 0.8125\n",
      "At: 951 [==========>] Loss 0.1222679060878864  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.1042699798550667  - accuracy: 0.875\n",
      "At: 953 [==========>] Loss 0.06164225308907249  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.10436672061304121  - accuracy: 0.875\n",
      "At: 955 [==========>] Loss 0.1622019375310632  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.09945017313729193  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.13475817874201879  - accuracy: 0.78125\n",
      "At: 958 [==========>] Loss 0.07548967397061473  - accuracy: 0.9375\n",
      "At: 959 [==========>] Loss 0.15571088650133794  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.13319331768650522  - accuracy: 0.875\n",
      "At: 961 [==========>] Loss 0.10732727349191269  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.12063441596785478  - accuracy: 0.78125\n",
      "At: 963 [==========>] Loss 0.10024866998415506  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.17288561163763178  - accuracy: 0.8125\n",
      "At: 965 [==========>] Loss 0.14535420552514752  - accuracy: 0.75\n",
      "At: 966 [==========>] Loss 0.174811519864277  - accuracy: 0.71875\n",
      "At: 967 [==========>] Loss 0.13236623186076935  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.13411616444328356  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.1520356439160892  - accuracy: 0.71875\n",
      "At: 970 [==========>] Loss 0.096228405263521  - accuracy: 0.875\n",
      "At: 971 [==========>] Loss 0.12681702964556593  - accuracy: 0.75\n",
      "At: 972 [==========>] Loss 0.04647038190282056  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.14363643131712817  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.08925909829414694  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.16198013773700914  - accuracy: 0.75\n",
      "At: 976 [==========>] Loss 0.11540856561987672  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.13725537063169713  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.1761662572614769  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.10878238906313865  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.15353560028947816  - accuracy: 0.6875\n",
      "At: 981 [==========>] Loss 0.18930252634144878  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.06478350632469851  - accuracy: 0.9375\n",
      "At: 983 [==========>] Loss 0.11236514717555515  - accuracy: 0.84375\n",
      "At: 984 [==========>] Loss 0.0958072802206889  - accuracy: 0.90625\n",
      "At: 985 [==========>] Loss 0.17877666580256923  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.114642571467824  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.1519845154993157  - accuracy: 0.78125\n",
      "At: 988 [==========>] Loss 0.10474877481210135  - accuracy: 0.90625\n",
      "At: 989 [==========>] Loss 0.1370273998968057  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.14214726884733597  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.13290739418782477  - accuracy: 0.84375\n",
      "At: 992 [==========>] Loss 0.2119619128315412  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.13895750458231718  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.15058750542581972  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.1772278383932021  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.0665633096516473  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.12999133006964161  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.145350889649616  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.16729637454454105  - accuracy: 0.71875\n",
      "At: 1000 [==========>] Loss 0.21351796354218333  - accuracy: 0.6875\n",
      "At: 1001 [==========>] Loss 0.14604378812670069  - accuracy: 0.8125\n",
      "At: 1002 [==========>] Loss 0.2279689620988291  - accuracy: 0.625\n",
      "At: 1003 [==========>] Loss 0.14210248417985052  - accuracy: 0.84375\n",
      "At: 1004 [==========>] Loss 0.1063541798071314  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.10807149828813833  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.11487031156110868  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.1390212312693093  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.1959424801800207  - accuracy: 0.6875\n",
      "At: 1009 [==========>] Loss 0.1513570963257878  - accuracy: 0.78125\n",
      "At: 1010 [==========>] Loss 0.14557727391193392  - accuracy: 0.8125\n",
      "At: 1011 [==========>] Loss 0.149108807554735  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.09887762922487081  - accuracy: 0.875\n",
      "At: 1013 [==========>] Loss 0.07938034544315237  - accuracy: 0.875\n",
      "At: 1014 [==========>] Loss 0.12381397161753836  - accuracy: 0.84375\n",
      "At: 1015 [==========>] Loss 0.199210537150225  - accuracy: 0.78125\n",
      "At: 1016 [==========>] Loss 0.14607871837674263  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.12054758265881264  - accuracy: 0.8125\n",
      "At: 1018 [==========>] Loss 0.17124035813312288  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.1903412060000063  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.1370756018134931  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.10211427367249695  - accuracy: 0.84375\n",
      "At: 1022 [==========>] Loss 0.09477017550345779  - accuracy: 0.875\n",
      "At: 1023 [==========>] Loss 0.1621669294951713  - accuracy: 0.78125\n",
      "At: 1024 [==========>] Loss 0.1975611485523106  - accuracy: 0.625\n",
      "At: 1025 [==========>] Loss 0.18564256379280258  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.09905483247478804  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.1464913216970417  - accuracy: 0.78125\n",
      "At: 1028 [==========>] Loss 0.19158655858185403  - accuracy: 0.78125\n",
      "At: 1029 [==========>] Loss 0.10068349357057065  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.10529211326480457  - accuracy: 0.90625\n",
      "At: 1031 [==========>] Loss 0.17130648397565457  - accuracy: 0.75\n",
      "At: 1032 [==========>] Loss 0.14380375794947897  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.15295351523081846  - accuracy: 0.78125\n",
      "At: 1034 [==========>] Loss 0.08342966653806731  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.07526191543636743  - accuracy: 0.9375\n",
      "At: 1036 [==========>] Loss 0.17949724759699054  - accuracy: 0.71875\n",
      "At: 1037 [==========>] Loss 0.14031534189057981  - accuracy: 0.84375\n",
      "At: 1038 [==========>] Loss 0.07765263824754436  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.10489724060734848  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.15138777101609552  - accuracy: 0.8125\n",
      "At: 1041 [==========>] Loss 0.15435517104865837  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.09878213778106587  - accuracy: 0.875\n",
      "At: 1043 [==========>] Loss 0.2204015060290097  - accuracy: 0.625\n",
      "At: 1044 [==========>] Loss 0.13861630153434173  - accuracy: 0.78125\n",
      "At: 1045 [==========>] Loss 0.15287524980438694  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.1673987847015878  - accuracy: 0.71875\n",
      "At: 1047 [==========>] Loss 0.1337760540911103  - accuracy: 0.78125\n",
      "At: 1048 [==========>] Loss 0.15754886873363666  - accuracy: 0.75\n",
      "At: 1049 [==========>] Loss 0.15032735108801193  - accuracy: 0.8125\n",
      "At: 1050 [==========>] Loss 0.16381518213367188  - accuracy: 0.75\n",
      "At: 1051 [==========>] Loss 0.08441180729608083  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.10204770076406777  - accuracy: 0.8125\n",
      "At: 1053 [==========>] Loss 0.07642799200759577  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.12724365840951712  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.19744666116178686  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.146759527638536  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.1455496453428186  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.06596596146341578  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.07282931836447888  - accuracy: 0.9375\n",
      "At: 1060 [==========>] Loss 0.10954755979063394  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.0811730376779616  - accuracy: 0.9375\n",
      "At: 1062 [==========>] Loss 0.15079708264170075  - accuracy: 0.78125\n",
      "At: 1063 [==========>] Loss 0.15151092081466028  - accuracy: 0.75\n",
      "At: 1064 [==========>] Loss 0.12919767348670183  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.11145895626806795  - accuracy: 0.8125\n",
      "At: 1066 [==========>] Loss 0.1132339770372856  - accuracy: 0.78125\n",
      "At: 1067 [==========>] Loss 0.1417304411463622  - accuracy: 0.78125\n",
      "At: 1068 [==========>] Loss 0.10059567413186782  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.13334225051021042  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.13280753129975675  - accuracy: 0.84375\n",
      "At: 1071 [==========>] Loss 0.1283648328559562  - accuracy: 0.78125\n",
      "At: 1072 [==========>] Loss 0.14571044860024912  - accuracy: 0.78125\n",
      "At: 1073 [==========>] Loss 0.13157783585323585  - accuracy: 0.78125\n",
      "At: 1074 [==========>] Loss 0.16111510971446774  - accuracy: 0.71875\n",
      "At: 1075 [==========>] Loss 0.1059815782722847  - accuracy: 0.875\n",
      "At: 1076 [==========>] Loss 0.11271085073245263  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.09221571367473447  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.09418330530498316  - accuracy: 0.84375\n",
      "At: 1079 [==========>] Loss 0.1521856723465294  - accuracy: 0.78125\n",
      "At: 1080 [==========>] Loss 0.12167959492272473  - accuracy: 0.84375\n",
      "At: 1081 [==========>] Loss 0.15571121777946068  - accuracy: 0.78125\n",
      "At: 1082 [==========>] Loss 0.12192382837998664  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.11495630735573514  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.09710712879166897  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.10134223464872096  - accuracy: 0.875\n",
      "At: 1086 [==========>] Loss 0.11844462917569486  - accuracy: 0.84375\n",
      "At: 1087 [==========>] Loss 0.11628846815224889  - accuracy: 0.90625\n",
      "At: 1088 [==========>] Loss 0.16331889848307163  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.10346671450597644  - accuracy: 0.84375\n",
      "At: 1090 [==========>] Loss 0.07675291869124579  - accuracy: 0.9375\n",
      "At: 1091 [==========>] Loss 0.18940079328218087  - accuracy: 0.75\n",
      "At: 1092 [==========>] Loss 0.12212472933319077  - accuracy: 0.78125\n",
      "At: 1093 [==========>] Loss 0.1293730362958393  - accuracy: 0.875\n",
      "At: 1094 [==========>] Loss 0.1588913641052297  - accuracy: 0.75\n",
      "At: 1095 [==========>] Loss 0.1449188481479757  - accuracy: 0.75\n",
      "At: 1096 [==========>] Loss 0.11743688795373604  - accuracy: 0.84375\n",
      "At: 1097 [==========>] Loss 0.07771566064787051  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.14826483580756059  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.19574219612173746  - accuracy: 0.71875\n",
      "At: 1100 [==========>] Loss 0.10127122813944477  - accuracy: 0.875\n",
      "At: 1101 [==========>] Loss 0.09623698560978447  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.10403910358659557  - accuracy: 0.78125\n",
      "At: 1103 [==========>] Loss 0.10609630243784605  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.09094983593942445  - accuracy: 0.875\n",
      "At: 1105 [==========>] Loss 0.09515269607398052  - accuracy: 0.875\n",
      "At: 1106 [==========>] Loss 0.0710832111726792  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.1521496817579383  - accuracy: 0.75\n",
      "At: 1108 [==========>] Loss 0.10330005198141505  - accuracy: 0.84375\n",
      "At: 1109 [==========>] Loss 0.06371359117912728  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.09345159617649695  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.16708973539016972  - accuracy: 0.78125\n",
      "At: 1112 [==========>] Loss 0.17896110452361352  - accuracy: 0.8125\n",
      "At: 1113 [==========>] Loss 0.13738639542306785  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.08941483897682925  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.15547805021855388  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.11580369166141347  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.08685561401824482  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.11476773252594231  - accuracy: 0.875\n",
      "At: 1119 [==========>] Loss 0.12888507036587188  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.0899191129912384  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.10383621084806362  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.09400552641567406  - accuracy: 0.875\n",
      "At: 1123 [==========>] Loss 0.1280322732438861  - accuracy: 0.875\n",
      "At: 1124 [==========>] Loss 0.12312498071231731  - accuracy: 0.8125\n",
      "At: 1125 [==========>] Loss 0.17967721924355293  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.12437234049160523  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.11250922907914465  - accuracy: 0.875\n",
      "At: 1128 [==========>] Loss 0.07198525649012749  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.16758958417127717  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.12024401018127785  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.0806630831473226  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.09116767632369291  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.1292431500644714  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.09364367714223738  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.12162771903273946  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.1781215869236374  - accuracy: 0.71875\n",
      "At: 1137 [==========>] Loss 0.1076580894406285  - accuracy: 0.875\n",
      "At: 1138 [==========>] Loss 0.08873030739238526  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.08684066835358727  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.15604907604510454  - accuracy: 0.75\n",
      "At: 1141 [==========>] Loss 0.14428362420951454  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.14010324666800145  - accuracy: 0.84375\n",
      "At: 1143 [==========>] Loss 0.047244362345905566  - accuracy: 0.96875\n",
      "At: 1144 [==========>] Loss 0.11513299482767084  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.14872574392404203  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.11387521426904329  - accuracy: 0.8125\n",
      "At: 1147 [==========>] Loss 0.2228973619833945  - accuracy: 0.65625\n",
      "At: 1148 [==========>] Loss 0.10757698884291444  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.10551095586638776  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.14809469097143652  - accuracy: 0.75\n",
      "At: 1151 [==========>] Loss 0.18942945357345164  - accuracy: 0.75\n",
      "At: 1152 [==========>] Loss 0.11379496651002866  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.16264870690889838  - accuracy: 0.71875\n",
      "At: 1154 [==========>] Loss 0.1207395984597785  - accuracy: 0.875\n",
      "At: 1155 [==========>] Loss 0.09437981249592069  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.18006503303531524  - accuracy: 0.75\n",
      "At: 1157 [==========>] Loss 0.1381096699468883  - accuracy: 0.90625\n",
      "At: 1158 [==========>] Loss 0.1659101380909464  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.08930088582692128  - accuracy: 0.90625\n",
      "At: 1160 [==========>] Loss 0.09326396860878591  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.09746206536357702  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.1324561306517354  - accuracy: 0.78125\n",
      "At: 1163 [==========>] Loss 0.14645635840510915  - accuracy: 0.78125\n",
      "At: 1164 [==========>] Loss 0.09183047734934058  - accuracy: 0.84375\n",
      "At: 1165 [==========>] Loss 0.1559808167820558  - accuracy: 0.75\n",
      "At: 1166 [==========>] Loss 0.07683126042726686  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.14859500262186895  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.12130916187338152  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.10650497756698524  - accuracy: 0.8125\n",
      "At: 1170 [==========>] Loss 0.15861303345949818  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.07124770857871572  - accuracy: 0.875\n",
      "At: 1172 [==========>] Loss 0.11629372628160642  - accuracy: 0.875\n",
      "At: 1173 [==========>] Loss 0.13084315941031355  - accuracy: 0.84375\n",
      "At: 1174 [==========>] Loss 0.17652182219316426  - accuracy: 0.71875\n",
      "At: 1175 [==========>] Loss 0.1144156786704496  - accuracy: 0.84375\n",
      "At: 1176 [==========>] Loss 0.1122159118164556  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.11692662105426826  - accuracy: 0.8125\n",
      "At: 1178 [==========>] Loss 0.18171888249192752  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.1448541349885284  - accuracy: 0.75\n",
      "At: 1180 [==========>] Loss 0.22440559497689444  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.11849075679060359  - accuracy: 0.8125\n",
      "At: 1182 [==========>] Loss 0.10018735924288147  - accuracy: 0.875\n",
      "At: 1183 [==========>] Loss 0.15902391555076995  - accuracy: 0.75\n",
      "At: 1184 [==========>] Loss 0.12343345620077903  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.11357352068666857  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.11442212624636042  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.10745842810217168  - accuracy: 0.875\n",
      "At: 1188 [==========>] Loss 0.08272534178075036  - accuracy: 0.90625\n",
      "At: 1189 [==========>] Loss 0.15934076616667636  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.12634006303393328  - accuracy: 0.84375\n",
      "At: 1191 [==========>] Loss 0.17555987648554872  - accuracy: 0.8125\n",
      "At: 1192 [==========>] Loss 0.08642598881645763  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.13277566360211096  - accuracy: 0.8125\n",
      "At: 1194 [==========>] Loss 0.1253684891592287  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.13772284040892463  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.08755735325269517  - accuracy: 0.9375\n",
      "At: 1197 [==========>] Loss 0.10048353780540005  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.08444385160209847  - accuracy: 0.90625\n",
      "At: 1199 [==========>] Loss 0.2145876496829796  - accuracy: 0.625\n",
      "At: 1200 [==========>] Loss 0.08932917955235192  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.12081473227165418  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.16982497773966293  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.13724395832848751  - accuracy: 0.8125\n",
      "At: 1204 [==========>] Loss 0.09536462698831714  - accuracy: 0.8125\n",
      "At: 1205 [==========>] Loss 0.09261646641201018  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.10196294899658531  - accuracy: 0.875\n",
      "At: 1207 [==========>] Loss 0.15625453484953344  - accuracy: 0.75\n",
      "At: 1208 [==========>] Loss 0.11573306692728419  - accuracy: 0.8125\n",
      "At: 1209 [==========>] Loss 0.09514760465351643  - accuracy: 0.84375\n",
      "At: 1210 [==========>] Loss 0.14668972217688359  - accuracy: 0.78125\n",
      "At: 1211 [==========>] Loss 0.17793183937035212  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.14216270811222081  - accuracy: 0.78125\n",
      "At: 1213 [==========>] Loss 0.17867670488285806  - accuracy: 0.71875\n",
      "At: 1214 [==========>] Loss 0.16280251310112742  - accuracy: 0.8125\n",
      "At: 1215 [==========>] Loss 0.1550609385393741  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.10520076827674765  - accuracy: 0.90625\n",
      "At: 1217 [==========>] Loss 0.042963065719907115  - accuracy: 0.9375\n",
      "At: 1218 [==========>] Loss 0.11720391205932007  - accuracy: 0.8125\n",
      "At: 1219 [==========>] Loss 0.1344721643712946  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.10169653443358873  - accuracy: 0.84375\n",
      "At: 1221 [==========>] Loss 0.1000208517577919  - accuracy: 0.9375\n",
      "At: 1222 [==========>] Loss 0.16570562077187823  - accuracy: 0.75\n",
      "At: 1223 [==========>] Loss 0.09847457948835303  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08406508460239714  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.1119509850980075  - accuracy: 0.84375\n",
      "At: 1226 [==========>] Loss 0.08426874523444366  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.17056003357331614  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.15691207348693612  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.12951611564921534  - accuracy: 0.8125\n",
      "At: 1230 [==========>] Loss 0.17805504701352187  - accuracy: 0.71875\n",
      "At: 1231 [==========>] Loss 0.1623150417819504  - accuracy: 0.78125\n",
      "At: 1232 [==========>] Loss 0.07883898238575801  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.11207463679577351  - accuracy: 0.78125\n",
      "At: 1234 [==========>] Loss 0.1465063072092858  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.10111886629568873  - accuracy: 0.84375\n",
      "At: 1236 [==========>] Loss 0.14442444106165903  - accuracy: 0.8125\n",
      "At: 1237 [==========>] Loss 0.07262157003860685  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.10749025856909372  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.15717430214779282  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.09887152218943748  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.09850326663281223  - accuracy: 0.84375\n",
      "At: 1242 [==========>] Loss 0.15643682191115232  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.12278943061909123  - accuracy: 0.875\n",
      "At: 1244 [==========>] Loss 0.17748743870504008  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.10783361718633336  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.07695994671214525  - accuracy: 0.84375\n",
      "At: 1247 [==========>] Loss 0.15521291178415447  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.09657994437389472  - accuracy: 0.875\n",
      "At: 1249 [==========>] Loss 0.11943374018883489  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.12794306025875343  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.11603364095014732  - accuracy: 0.875\n",
      "At: 1252 [==========>] Loss 0.08735637563398074  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.09772527526298896  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.19115888573746365  - accuracy: 0.6875\n",
      "At: 1255 [==========>] Loss 0.10859866273018787  - accuracy: 0.84375\n",
      "At: 1256 [==========>] Loss 0.11715023807554618  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.17458717183715905  - accuracy: 0.78125\n",
      "At: 1258 [==========>] Loss 0.07873942772423986  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.13719031257303146  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.13524640978330998  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.12919747696367723  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.15349374532398222  - accuracy: 0.75\n",
      "At: 1263 [==========>] Loss 0.12850578080582845  - accuracy: 0.84375\n",
      "At: 1264 [==========>] Loss 0.06584874471879439  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.1396463016313959  - accuracy: 0.78125\n",
      "At: 1266 [==========>] Loss 0.12527494227319358  - accuracy: 0.8125\n",
      "At: 1267 [==========>] Loss 0.11340596342567597  - accuracy: 0.84375\n",
      "At: 1268 [==========>] Loss 0.13624107982235406  - accuracy: 0.8125\n",
      "At: 1269 [==========>] Loss 0.11736913561539178  - accuracy: 0.90625\n",
      "At: 1270 [==========>] Loss 0.1288578541197376  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.16114433877963022  - accuracy: 0.8125\n",
      "At: 1272 [==========>] Loss 0.05006643886480952  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.20142535190593663  - accuracy: 0.6875\n",
      "At: 1274 [==========>] Loss 0.11064568512306389  - accuracy: 0.90625\n",
      "At: 1275 [==========>] Loss 0.09868496431780455  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.11114337764018965  - accuracy: 0.8125\n",
      "At: 1277 [==========>] Loss 0.06888304724719317  - accuracy: 0.90625\n",
      "At: 1278 [==========>] Loss 0.14048188422190355  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.081859600213139  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.08490698720179378  - accuracy: 0.875\n",
      "At: 1281 [==========>] Loss 0.1844007318536618  - accuracy: 0.65625\n",
      "At: 1282 [==========>] Loss 0.14078325293463814  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.12050438760268439  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.1607476551926171  - accuracy: 0.78125\n",
      "At: 1285 [==========>] Loss 0.0852364753172864  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.12036056467918131  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.12747296521034263  - accuracy: 0.8125\n",
      "At: 1288 [==========>] Loss 0.14831679148867605  - accuracy: 0.75\n",
      "At: 1289 [==========>] Loss 0.12021825195124726  - accuracy: 0.875\n",
      "At: 1290 [==========>] Loss 0.13853880298979498  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.16284086533498526  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.08618852684170071  - accuracy: 0.9375\n",
      "At: 1293 [==========>] Loss 0.1515907315533556  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.13561108276757317  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.17113077116074948  - accuracy: 0.78125\n",
      "At: 1296 [==========>] Loss 0.17193574722108118  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.14622284776243982  - accuracy: 0.78125\n",
      "At: 1298 [==========>] Loss 0.10568785550375434  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.1351616969412767  - accuracy: 0.84375\n",
      "At: 1300 [==========>] Loss 0.13210032370901398  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.1333904155490988  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.07719700630589169  - accuracy: 0.90625\n",
      "At: 1303 [==========>] Loss 0.11495949083284197  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.11001768505148092  - accuracy: 0.875\n",
      "At: 1305 [==========>] Loss 0.1349167520227974  - accuracy: 0.78125\n",
      "At: 1306 [==========>] Loss 0.07361582341304002  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.19241857794778172  - accuracy: 0.6875\n",
      "At: 1308 [==========>] Loss 0.04470183717523984  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.16025890597760406  - accuracy: 0.75\n",
      "At: 1310 [==========>] Loss 0.16889176588060642  - accuracy: 0.71875\n",
      "At: 1311 [==========>] Loss 0.12528824700715568  - accuracy: 0.84375\n",
      "At: 1312 [==========>] Loss 0.08695859879003581  - accuracy: 0.84375\n",
      "At: 1313 [==========>] Loss 0.16803245722629545  - accuracy: 0.75\n",
      "At: 1314 [==========>] Loss 0.06467158185160742  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.14049272842989183  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.1312172278427668  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.09491200242138238  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.13442294587222497  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.11778063693759705  - accuracy: 0.875\n",
      "At: 1320 [==========>] Loss 0.11641818340244557  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.08876845511647474  - accuracy: 0.90625\n",
      "At: 1322 [==========>] Loss 0.1472625952545161  - accuracy: 0.84375\n",
      "At: 1323 [==========>] Loss 0.13052027461796933  - accuracy: 0.84375\n",
      "At: 1324 [==========>] Loss 0.1358027822598658  - accuracy: 0.84375\n",
      "At: 1325 [==========>] Loss 0.09350417551658123  - accuracy: 0.84375\n",
      "At: 1326 [==========>] Loss 0.11017394377627028  - accuracy: 0.8125\n",
      "At: 1327 [==========>] Loss 0.1582963402772539  - accuracy: 0.75\n",
      "At: 1328 [==========>] Loss 0.08287432920880414  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.07494997048765803  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.134153086599717  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.1554387060656905  - accuracy: 0.75\n",
      "At: 1332 [==========>] Loss 0.12698209512999845  - accuracy: 0.75\n",
      "At: 1333 [==========>] Loss 0.15789996680056279  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.10000453807924847  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.1390128673288247  - accuracy: 0.8125\n",
      "At: 1336 [==========>] Loss 0.1286025486141288  - accuracy: 0.8125\n",
      "At: 1337 [==========>] Loss 0.1413379406695004  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.16052814554413708  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.1268581187938028  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.13759311527606793  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.08887252760971148  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.12683844679596135  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.20544365668314624  - accuracy: 0.65625\n",
      "At: 1344 [==========>] Loss 0.16818965444458556  - accuracy: 0.8125\n",
      "At: 1345 [==========>] Loss 0.08319965488587863  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.10073150187421334  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.12149435420482052  - accuracy: 0.84375\n",
      "At: 1348 [==========>] Loss 0.09284081657349061  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.14603180711584957  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.10383307116543566  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.09071527551271585  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.09405441711401584  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.15313128687087774  - accuracy: 0.78125\n",
      "At: 1354 [==========>] Loss 0.1401521942403854  - accuracy: 0.8125\n",
      "At: 1355 [==========>] Loss 0.07097871660325872  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.10150805172819394  - accuracy: 0.875\n",
      "At: 1357 [==========>] Loss 0.14170775883002792  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.11964095637577289  - accuracy: 0.875\n",
      "At: 1359 [==========>] Loss 0.07665783513250729  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.154686990785307  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.07703631033641478  - accuracy: 0.90625\n",
      "At: 1362 [==========>] Loss 0.13560614478682545  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.12168818844333051  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.14174135575129226  - accuracy: 0.78125\n",
      "At: 1365 [==========>] Loss 0.1158762336112921  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.12350273836348992  - accuracy: 0.84375\n",
      "At: 1367 [==========>] Loss 0.09187826662174141  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.19493544591106915  - accuracy: 0.6875\n",
      "At: 1369 [==========>] Loss 0.08754163777685099  - accuracy: 0.90625\n",
      "At: 1370 [==========>] Loss 0.12637346180252187  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.20228341137040087  - accuracy: 0.6875\n",
      "At: 1372 [==========>] Loss 0.11744754327227452  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.14314358256552076  - accuracy: 0.84375\n",
      "At: 1374 [==========>] Loss 0.12720544389215188  - accuracy: 0.875\n",
      "At: 1375 [==========>] Loss 0.14082141160700473  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.0816195210351039  - accuracy: 0.90625\n",
      "At: 1377 [==========>] Loss 0.1628196120995115  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.10125772306916943  - accuracy: 0.90625\n",
      "At: 1379 [==========>] Loss 0.14756942962713848  - accuracy: 0.75\n",
      "At: 1380 [==========>] Loss 0.138169436267923  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.09915669006568609  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.14590639215058765  - accuracy: 0.78125\n",
      "At: 1383 [==========>] Loss 0.10560676814264361  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.10585401105840686  - accuracy: 0.84375\n",
      "At: 1385 [==========>] Loss 0.17321788999314713  - accuracy: 0.75\n",
      "At: 1386 [==========>] Loss 0.17006970964221216  - accuracy: 0.78125\n",
      "At: 1387 [==========>] Loss 0.07204650043743305  - accuracy: 0.875\n",
      "At: 1388 [==========>] Loss 0.1557341879228675  - accuracy: 0.84375\n",
      "At: 1389 [==========>] Loss 0.12562280143574922  - accuracy: 0.84375\n",
      "At: 1390 [==========>] Loss 0.12509570339516352  - accuracy: 0.875\n",
      "At: 1391 [==========>] Loss 0.11109029633170966  - accuracy: 0.875\n",
      "At: 1392 [==========>] Loss 0.07910774885319766  - accuracy: 0.90625\n",
      "At: 1393 [==========>] Loss 0.14893649695891112  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.12792337586795158  - accuracy: 0.84375\n",
      "At: 1395 [==========>] Loss 0.22087439592252034  - accuracy: 0.65625\n",
      "At: 1396 [==========>] Loss 0.0516444788291884  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.1189383276911846  - accuracy: 0.875\n",
      "At: 1398 [==========>] Loss 0.14143732068156448  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.11706169107742727  - accuracy: 0.875\n",
      "At: 1400 [==========>] Loss 0.14006052364347957  - accuracy: 0.84375\n",
      "At: 1401 [==========>] Loss 0.1068907812566264  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.15198145557153997  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.14017935494706585  - accuracy: 0.78125\n",
      "At: 1404 [==========>] Loss 0.10325899926155385  - accuracy: 0.875\n",
      "At: 1405 [==========>] Loss 0.10661962345428527  - accuracy: 0.84375\n",
      "At: 1406 [==========>] Loss 0.16736930788713955  - accuracy: 0.8125\n",
      "At: 1407 [==========>] Loss 0.11874144187038099  - accuracy: 0.78125\n",
      "At: 1408 [==========>] Loss 0.12726547195065266  - accuracy: 0.84375\n",
      "At: 1409 [==========>] Loss 0.042666546703813424  - accuracy: 0.9375\n",
      "At: 1410 [==========>] Loss 0.12178168291218941  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.13628683229679833  - accuracy: 0.78125\n",
      "At: 1412 [==========>] Loss 0.11402055192489385  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.1075501033073908  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.1591580586975416  - accuracy: 0.8125\n",
      "At: 1415 [==========>] Loss 0.06496933175651176  - accuracy: 0.96875\n",
      "At: 1416 [==========>] Loss 0.1289982819656863  - accuracy: 0.8125\n",
      "At: 1417 [==========>] Loss 0.12169127324228533  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.14134829187402143  - accuracy: 0.78125\n",
      "At: 1419 [==========>] Loss 0.12284952127637364  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.08147938742583347  - accuracy: 0.9375\n",
      "At: 1421 [==========>] Loss 0.084373995055849  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.1479846686881965  - accuracy: 0.78125\n",
      "At: 1423 [==========>] Loss 0.15090498944969194  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.1347970187611282  - accuracy: 0.8125\n",
      "At: 1425 [==========>] Loss 0.08875573140297836  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.11779860215296026  - accuracy: 0.84375\n",
      "At: 1427 [==========>] Loss 0.1041531229859298  - accuracy: 0.875\n",
      "At: 1428 [==========>] Loss 0.11054491829233516  - accuracy: 0.84375\n",
      "At: 1429 [==========>] Loss 0.16724664406872747  - accuracy: 0.71875\n",
      "At: 1430 [==========>] Loss 0.08279555974913656  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.14546400523170772  - accuracy: 0.8125\n",
      "At: 1432 [==========>] Loss 0.08415535052551296  - accuracy: 0.8125\n",
      "At: 1433 [==========>] Loss 0.12585584124340926  - accuracy: 0.8125\n",
      "At: 1434 [==========>] Loss 0.16790315150608356  - accuracy: 0.78125\n",
      "At: 1435 [==========>] Loss 0.13705175909484021  - accuracy: 0.78125\n",
      "At: 1436 [==========>] Loss 0.05925565661873061  - accuracy: 0.9375\n",
      "At: 1437 [==========>] Loss 0.13838855581668663  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.1782142512146781  - accuracy: 0.75\n",
      "At: 1439 [==========>] Loss 0.103046199405949  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.09610969908725625  - accuracy: 0.875\n",
      "At: 1441 [==========>] Loss 0.06350004781402206  - accuracy: 0.9375\n",
      "At: 1442 [==========>] Loss 0.06826457326187076  - accuracy: 0.875\n",
      "At: 1443 [==========>] Loss 0.13198570715005076  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.14189564379584413  - accuracy: 0.75\n",
      "At: 1445 [==========>] Loss 0.17022810405953204  - accuracy: 0.71875\n",
      "At: 1446 [==========>] Loss 0.16479495662477428  - accuracy: 0.75\n",
      "At: 1447 [==========>] Loss 0.19856758793864535  - accuracy: 0.71875\n",
      "At: 1448 [==========>] Loss 0.06513680584775203  - accuracy: 0.96875\n",
      "At: 1449 [==========>] Loss 0.16197276127147922  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.14446214830066095  - accuracy: 0.78125\n",
      "At: 1451 [==========>] Loss 0.10284364370874839  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.09965769271826563  - accuracy: 0.90625\n",
      "At: 1453 [==========>] Loss 0.0627789942004108  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.16372114745326052  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.1264872346587611  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.08735806302219609  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.0787155412259174  - accuracy: 0.90625\n",
      "At: 1458 [==========>] Loss 0.11517637322744446  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.14858321161496058  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.1717013719061905  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.1227967360139722  - accuracy: 0.78125\n",
      "At: 1462 [==========>] Loss 0.14707327831784547  - accuracy: 0.78125\n",
      "At: 1463 [==========>] Loss 0.08569286427327924  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.16575731953018447  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.10922475140041461  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.0790537123842921  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.1994912626430029  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.15695779754568  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.18249763647244532  - accuracy: 0.75\n",
      "At: 1470 [==========>] Loss 0.13847551836697675  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.13665232728241072  - accuracy: 0.8125\n",
      "At: 1472 [==========>] Loss 0.08671841420234673  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.1370819876423933  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.18223061454815812  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.15468207842299786  - accuracy: 0.8125\n",
      "At: 1476 [==========>] Loss 0.11406597722331552  - accuracy: 0.84375\n",
      "At: 1477 [==========>] Loss 0.10392870722149491  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.0922815698816774  - accuracy: 0.84375\n",
      "At: 1479 [==========>] Loss 0.11966389657667195  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.09362094196256135  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.1007290595942456  - accuracy: 0.875\n",
      "At: 1482 [==========>] Loss 0.0903476098431481  - accuracy: 0.90625\n",
      "At: 1483 [==========>] Loss 0.20437212101305577  - accuracy: 0.78125\n",
      "At: 1484 [==========>] Loss 0.14027562081567696  - accuracy: 0.8125\n",
      "At: 1485 [==========>] Loss 0.15544579581768658  - accuracy: 0.71875\n",
      "At: 1486 [==========>] Loss 0.09418653449132301  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.09107693919557254  - accuracy: 0.8125\n",
      "At: 1488 [==========>] Loss 0.16621753942160167  - accuracy: 0.75\n",
      "At: 1489 [==========>] Loss 0.1903930172984237  - accuracy: 0.78125\n",
      "At: 1490 [==========>] Loss 0.09104509383066098  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.1449130116376133  - accuracy: 0.71875\n",
      "At: 1492 [==========>] Loss 0.11949501850631025  - accuracy: 0.84375\n",
      "At: 1493 [==========>] Loss 0.1758818875822947  - accuracy: 0.6875\n",
      "At: 1494 [==========>] Loss 0.1321464974487106  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.13117394002170546  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.1016854774790437  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.17934287633365847  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.1257866709294429  - accuracy: 0.84375\n",
      "At: 1499 [==========>] Loss 0.11797422400828869  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.09594338290687518  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.08213601056679178  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.16072892568260108  - accuracy: 0.75\n",
      "At: 1503 [==========>] Loss 0.1171219815864877  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.12302833949419614  - accuracy: 0.84375\n",
      "At: 1505 [==========>] Loss 0.1386243226270104  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.14202402256048235  - accuracy: 0.75\n",
      "At: 1507 [==========>] Loss 0.11550290593816587  - accuracy: 0.875\n",
      "At: 1508 [==========>] Loss 0.20563944453666777  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.09156817454294514  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.13397767666220328  - accuracy: 0.75\n",
      "At: 1511 [==========>] Loss 0.11039021169310495  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.09965282659897205  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.14940687572464456  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.15199719978882553  - accuracy: 0.8125\n",
      "At: 1515 [==========>] Loss 0.14187839596409255  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.12018289944718631  - accuracy: 0.78125\n",
      "At: 1517 [==========>] Loss 0.15196233796236044  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.10329357222539819  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.13428762329840238  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.10662002733409355  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.07800925835423507  - accuracy: 0.90625\n",
      "At: 1522 [==========>] Loss 0.15842923814703502  - accuracy: 0.78125\n",
      "At: 1523 [==========>] Loss 0.09979263369681038  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.13852653027154851  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.13183871287302953  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.11748840611338035  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.1315369480763306  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.13960638365409941  - accuracy: 0.84375\n",
      "At: 1529 [==========>] Loss 0.0633618635380209  - accuracy: 0.96875\n",
      "At: 1530 [==========>] Loss 0.04008104423143322  - accuracy: 1.0\n",
      "At: 1531 [==========>] Loss 0.11817319835916186  - accuracy: 0.875\n",
      "At: 1532 [==========>] Loss 0.20935440734473476  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.13891897063589226  - accuracy: 0.875\n",
      "At: 1534 [==========>] Loss 0.1032794958035416  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.13490237790399623  - accuracy: 0.8125\n",
      "At: 1536 [==========>] Loss 0.15228314728101355  - accuracy: 0.78125\n",
      "At: 1537 [==========>] Loss 0.11758232023372475  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.13584096811273022  - accuracy: 0.84375\n",
      "At: 1539 [==========>] Loss 0.09567869626700781  - accuracy: 0.875\n",
      "At: 1540 [==========>] Loss 0.1098952803155209  - accuracy: 0.84375\n",
      "At: 1541 [==========>] Loss 0.10484689540619438  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.07290832367760361  - accuracy: 0.9375\n",
      "At: 1543 [==========>] Loss 0.12874477386467204  - accuracy: 0.84375\n",
      "At: 1544 [==========>] Loss 0.15814083863344913  - accuracy: 0.75\n",
      "At: 1545 [==========>] Loss 0.20752746706490077  - accuracy: 0.75\n",
      "At: 1546 [==========>] Loss 0.13591206662014046  - accuracy: 0.75\n",
      "At: 1547 [==========>] Loss 0.15712494028156904  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.11293898700504243  - accuracy: 0.90625\n",
      "At: 1549 [==========>] Loss 0.13514023614624787  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.06897554174221612  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.18654790585860795  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.08222669462630734  - accuracy: 0.90625\n",
      "At: 1553 [==========>] Loss 0.062282102063199445  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.15052287145426882  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.1333752117514974  - accuracy: 0.8125\n",
      "At: 1556 [==========>] Loss 0.1521056726977446  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.08727898841226503  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.14128001395391335  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.06699684816424825  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.13998321885850745  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.15994415863216954  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.09549663934332542  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.08535432920301904  - accuracy: 0.9375\n",
      "At: 1564 [==========>] Loss 0.10279850459814396  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.12712230356334192  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.15115619882898496  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.164152835094879  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.07618055796971508  - accuracy: 0.9375\n",
      "At: 1569 [==========>] Loss 0.11411214537648508  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.10392288292233745  - accuracy: 0.84375\n",
      "At: 1571 [==========>] Loss 0.15499738772740976  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.14990964869286982  - accuracy: 0.71875\n",
      "At: 1573 [==========>] Loss 0.03136112697907445  - accuracy: 1.0\n",
      "At: 1574 [==========>] Loss 0.11906752388647913  - accuracy: 0.875\n",
      "At: 1575 [==========>] Loss 0.11416431051107126  - accuracy: 0.8125\n",
      "At: 1576 [==========>] Loss 0.13067295942339785  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.06561956185575847  - accuracy: 0.96875\n",
      "At: 1578 [==========>] Loss 0.0803642539591593  - accuracy: 0.875\n",
      "At: 1579 [==========>] Loss 0.10804553826196334  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.11492442435786429  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.11144569166912496  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.21099272800111527  - accuracy: 0.6875\n",
      "At: 1583 [==========>] Loss 0.08800143738357172  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.11018160685611433  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.09109826935616291  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.12058740064433074  - accuracy: 0.84375\n",
      "At: 1587 [==========>] Loss 0.09348705412454361  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.1134664188003893  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.122621208758531  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.14482910523838483  - accuracy: 0.78125\n",
      "At: 1591 [==========>] Loss 0.0890149803014334  - accuracy: 0.9375\n",
      "At: 1592 [==========>] Loss 0.06639010567332318  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.2120367864444471  - accuracy: 0.6875\n",
      "At: 1594 [==========>] Loss 0.1039980877676843  - accuracy: 0.8125\n",
      "At: 1595 [==========>] Loss 0.1327433529509591  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.1849655181836776  - accuracy: 0.78125\n",
      "At: 1597 [==========>] Loss 0.14889855194815294  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.15254056729754684  - accuracy: 0.8125\n",
      "At: 1599 [==========>] Loss 0.22207087957873964  - accuracy: 0.65625\n",
      "At: 1600 [==========>] Loss 0.15575367792264733  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.09621043369665586  - accuracy: 0.90625\n",
      "At: 1602 [==========>] Loss 0.09633419770250225  - accuracy: 0.90625\n",
      "At: 1603 [==========>] Loss 0.16501022189104264  - accuracy: 0.78125\n",
      "At: 1604 [==========>] Loss 0.24564798289892978  - accuracy: 0.6875\n",
      "At: 1605 [==========>] Loss 0.07982250833872498  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.1319799605562101  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.184174188333963  - accuracy: 0.65625\n",
      "At: 1608 [==========>] Loss 0.15266595607438027  - accuracy: 0.75\n",
      "At: 1609 [==========>] Loss 0.14399692284655247  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.16736758649843408  - accuracy: 0.84375\n",
      "At: 1611 [==========>] Loss 0.07112828876680599  - accuracy: 0.90625\n",
      "At: 1612 [==========>] Loss 0.06550361326887272  - accuracy: 0.9375\n",
      "At: 1613 [==========>] Loss 0.13984245625760708  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.1466513462470862  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.07142758826550202  - accuracy: 0.96875\n",
      "At: 1616 [==========>] Loss 0.1381766073651296  - accuracy: 0.78125\n",
      "At: 1617 [==========>] Loss 0.09458334533771856  - accuracy: 0.90625\n",
      "At: 1618 [==========>] Loss 0.10379753792775437  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.15221559675309523  - accuracy: 0.75\n",
      "At: 1620 [==========>] Loss 0.10268626200674671  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.12589729407090844  - accuracy: 0.75\n",
      "At: 1622 [==========>] Loss 0.15402397304233206  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.07354405100785712  - accuracy: 0.9375\n",
      "At: 1624 [==========>] Loss 0.1654759661356912  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.13510927470857798  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.11586409668525786  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.09936072227570357  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.16594684333733722  - accuracy: 0.75\n",
      "At: 1629 [==========>] Loss 0.16026558136331825  - accuracy: 0.75\n",
      "At: 1630 [==========>] Loss 0.1004048450071858  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.14822805721608096  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.08321079672787587  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08715691800281711  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.12226566296897476  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.2222673608693041  - accuracy: 0.65625\n",
      "At: 1636 [==========>] Loss 0.09239429488512038  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.07372403263331401  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.12261667967607096  - accuracy: 0.84375\n",
      "At: 1639 [==========>] Loss 0.1656770186333984  - accuracy: 0.6875\n",
      "At: 1640 [==========>] Loss 0.09735285352354202  - accuracy: 0.90625\n",
      "At: 1641 [==========>] Loss 0.08208485070673427  - accuracy: 0.9375\n",
      "At: 1642 [==========>] Loss 0.08969153472902847  - accuracy: 0.90625\n",
      "At: 1643 [==========>] Loss 0.09662026730090517  - accuracy: 0.84375\n",
      "At: 1644 [==========>] Loss 0.11374061357946905  - accuracy: 0.8125\n",
      "At: 1645 [==========>] Loss 0.06818814166982864  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.11017157482570505  - accuracy: 0.875\n",
      "At: 1647 [==========>] Loss 0.15332893007185183  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1307223058655115  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.10508105204370258  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.10180634576271486  - accuracy: 0.90625\n",
      "At: 1651 [==========>] Loss 0.12574177850127172  - accuracy: 0.90625\n",
      "At: 1652 [==========>] Loss 0.07334185748083784  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.08584376050851852  - accuracy: 0.90625\n",
      "At: 1654 [==========>] Loss 0.06158508853455184  - accuracy: 0.875\n",
      "At: 1655 [==========>] Loss 0.1647126103342246  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.11117006745977379  - accuracy: 0.90625\n",
      "At: 1657 [==========>] Loss 0.12741572165807064  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.21485007938130146  - accuracy: 0.71875\n",
      "At: 1659 [==========>] Loss 0.10903087027253648  - accuracy: 0.78125\n",
      "At: 1660 [==========>] Loss 0.041421720223073895  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.0956286508545454  - accuracy: 0.875\n",
      "At: 1662 [==========>] Loss 0.08603919469962718  - accuracy: 0.9375\n",
      "At: 1663 [==========>] Loss 0.15148523316344747  - accuracy: 0.8125\n",
      "At: 1664 [==========>] Loss 0.13509114763432886  - accuracy: 0.8125\n",
      "At: 1665 [==========>] Loss 0.11326990297947134  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.10304603045980548  - accuracy: 0.78125\n",
      "At: 1667 [==========>] Loss 0.20805874078058936  - accuracy: 0.65625\n",
      "At: 1668 [==========>] Loss 0.17826318161219454  - accuracy: 0.75\n",
      "At: 1669 [==========>] Loss 0.13709698297385425  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.07934793764160661  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.13177233720542236  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.11759718826335595  - accuracy: 0.84375\n",
      "At: 1673 [==========>] Loss 0.058725184206244604  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.1702414655512601  - accuracy: 0.75\n",
      "At: 1675 [==========>] Loss 0.1816129239480499  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.22225012090500065  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.12394371727063329  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.1540203645620412  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.08611910310954957  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.12794354240641498  - accuracy: 0.84375\n",
      "At: 1681 [==========>] Loss 0.13645935869758233  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.08538004882146141  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.1764195962870202  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.12741600152412846  - accuracy: 0.84375\n",
      "At: 1685 [==========>] Loss 0.0963227332618607  - accuracy: 0.90625\n",
      "At: 1686 [==========>] Loss 0.13480978587981685  - accuracy: 0.8125\n",
      "At: 1687 [==========>] Loss 0.18233115274840214  - accuracy: 0.75\n",
      "At: 1688 [==========>] Loss 0.04103388775691512  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.11155585802507823  - accuracy: 0.8125\n",
      "At: 1690 [==========>] Loss 0.11996801351696827  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.10282557349413388  - accuracy: 0.90625\n",
      "At: 1692 [==========>] Loss 0.16409738210849711  - accuracy: 0.71875\n",
      "At: 1693 [==========>] Loss 0.0943405215981098  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.10583203618353922  - accuracy: 0.8125\n",
      "At: 1695 [==========>] Loss 0.1325300582473989  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.16775278390654888  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.09406225137585364  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.0590986983510582  - accuracy: 0.96875\n",
      "At: 1699 [==========>] Loss 0.11216265511376372  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.12033156333805806  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.10631766926836943  - accuracy: 0.8125\n",
      "At: 1702 [==========>] Loss 0.10179423340272173  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.17365085190428936  - accuracy: 0.75\n",
      "At: 1704 [==========>] Loss 0.10112608156659594  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.10792506100445556  - accuracy: 0.9375\n",
      "At: 1706 [==========>] Loss 0.16956602600956305  - accuracy: 0.8125\n",
      "At: 1707 [==========>] Loss 0.19807457293834901  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.06948689715671305  - accuracy: 0.9375\n",
      "At: 1709 [==========>] Loss 0.1903473939794674  - accuracy: 0.71875\n",
      "At: 1710 [==========>] Loss 0.1407074097547184  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.06710965011577104  - accuracy: 0.9375\n",
      "At: 1712 [==========>] Loss 0.09351399414113593  - accuracy: 0.9375\n",
      "At: 1713 [==========>] Loss 0.10587777802114394  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.14781427694904453  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.12271243854785882  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06556463271885427  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08380962176249408  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.14246037271543543  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.13081729224902608  - accuracy: 0.8125\n",
      "At: 1720 [==========>] Loss 0.057823242866530164  - accuracy: 0.96875\n",
      "At: 1721 [==========>] Loss 0.14036294933067203  - accuracy: 0.84375\n",
      "At: 1722 [==========>] Loss 0.08373773661398372  - accuracy: 0.875\n",
      "At: 1723 [==========>] Loss 0.20620489752226873  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.08391982708492107  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.12228679392405994  - accuracy: 0.875\n",
      "At: 1726 [==========>] Loss 0.12595423574756792  - accuracy: 0.78125\n",
      "At: 1727 [==========>] Loss 0.14082676163654986  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.12314911776092317  - accuracy: 0.78125\n",
      "At: 1729 [==========>] Loss 0.19071326318099158  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.11742337285400113  - accuracy: 0.875\n",
      "At: 1731 [==========>] Loss 0.08614702794888879  - accuracy: 0.8125\n",
      "At: 1732 [==========>] Loss 0.0729363610518356  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.14977000086988337  - accuracy: 0.71875\n",
      "At: 1734 [==========>] Loss 0.0972385529147734  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.15559243650719937  - accuracy: 0.71875\n",
      "At: 1736 [==========>] Loss 0.11834226564175106  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.1514786613255768  - accuracy: 0.75\n",
      "At: 1738 [==========>] Loss 0.11376071400203416  - accuracy: 0.90625\n",
      "At: 1739 [==========>] Loss 0.09941831998967124  - accuracy: 0.875\n",
      "At: 1740 [==========>] Loss 0.160867410608045  - accuracy: 0.78125\n",
      "At: 1741 [==========>] Loss 0.17561252488869974  - accuracy: 0.78125\n",
      "At: 1742 [==========>] Loss 0.047581670450591515  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.14458028590180946  - accuracy: 0.84375\n",
      "At: 1744 [==========>] Loss 0.0932385868243617  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.12337598812947144  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.1639538279621075  - accuracy: 0.75\n",
      "At: 1747 [==========>] Loss 0.10073833949034128  - accuracy: 0.90625\n",
      "At: 1748 [==========>] Loss 0.1312108186387608  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.11619521925375649  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.10578042641688418  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.15045840258374005  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.09985659005423324  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.08158991742336762  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.13929631447800087  - accuracy: 0.8125\n",
      "At: 1755 [==========>] Loss 0.08635894962223156  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.16618514030949233  - accuracy: 0.75\n",
      "At: 1757 [==========>] Loss 0.14607218730419705  - accuracy: 0.84375\n",
      "At: 1758 [==========>] Loss 0.07833097368141267  - accuracy: 0.875\n",
      "At: 1759 [==========>] Loss 0.08631157575309414  - accuracy: 0.875\n",
      "At: 1760 [==========>] Loss 0.10120584116697409  - accuracy: 0.84375\n",
      "At: 1761 [==========>] Loss 0.12181138184857813  - accuracy: 0.84375\n",
      "At: 1762 [==========>] Loss 0.14318246213506125  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.11267686093367944  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.15010036141555042  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.16538427867539318  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.06873689928456761  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.07755474518325486  - accuracy: 0.96875\n",
      "At: 1768 [==========>] Loss 0.11032424176716107  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.05969737904020346  - accuracy: 0.96875\n",
      "At: 1770 [==========>] Loss 0.06969309678609867  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.15789938882035665  - accuracy: 0.84375\n",
      "At: 1772 [==========>] Loss 0.13074754987847093  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.1009990750574851  - accuracy: 0.90625\n",
      "At: 1774 [==========>] Loss 0.16522521692838907  - accuracy: 0.75\n",
      "At: 1775 [==========>] Loss 0.1162116556308819  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.10845361188648452  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.0991301977899037  - accuracy: 0.90625\n",
      "At: 1778 [==========>] Loss 0.10957664412255733  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.09978320788521218  - accuracy: 0.84375\n",
      "At: 1780 [==========>] Loss 0.1039066549863347  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.16930619571046118  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.10079320425913857  - accuracy: 0.875\n",
      "At: 1783 [==========>] Loss 0.1203960325695341  - accuracy: 0.875\n",
      "At: 1784 [==========>] Loss 0.058354750759051804  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.10653603128716746  - accuracy: 0.8125\n",
      "At: 1786 [==========>] Loss 0.11105608689835726  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.10726317461788107  - accuracy: 0.90625\n",
      "At: 1788 [==========>] Loss 0.12625657769596338  - accuracy: 0.8125\n",
      "At: 1789 [==========>] Loss 0.08823172482650887  - accuracy: 0.8125\n",
      "At: 1790 [==========>] Loss 0.16484980390979714  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.06957350591034914  - accuracy: 0.96875\n",
      "At: 1792 [==========>] Loss 0.1286960957887564  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.1031986801371256  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.1899801579064053  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.08280008547826179  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.1323842828153854  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.1344111584278322  - accuracy: 0.84375\n",
      "At: 1798 [==========>] Loss 0.14548262115679866  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.07220676255698  - accuracy: 0.9375\n",
      "At: 1800 [==========>] Loss 0.12199278144424226  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.1760478664050124  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.12143670359643267  - accuracy: 0.84375\n",
      "At: 1803 [==========>] Loss 0.15738309360799804  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.11977103242328327  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.04322531184639338  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.15688117274274527  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.1737233945584755  - accuracy: 0.75\n",
      "At: 1808 [==========>] Loss 0.15695904562675866  - accuracy: 0.75\n",
      "At: 1809 [==========>] Loss 0.10726636343005364  - accuracy: 0.84375\n",
      "At: 1810 [==========>] Loss 0.14310732619539218  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.14134045742379056  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.08244000758771859  - accuracy: 0.9375\n",
      "At: 1813 [==========>] Loss 0.14723756385801742  - accuracy: 0.75\n",
      "At: 1814 [==========>] Loss 0.11532097158589946  - accuracy: 0.8125\n",
      "At: 1815 [==========>] Loss 0.15042531517912333  - accuracy: 0.71875\n",
      "At: 1816 [==========>] Loss 0.053625001595253005  - accuracy: 0.9375\n",
      "At: 1817 [==========>] Loss 0.1324361144468916  - accuracy: 0.8125\n",
      "At: 1818 [==========>] Loss 0.12208950966599252  - accuracy: 0.75\n",
      "At: 1819 [==========>] Loss 0.18680848984059714  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.10818303383181813  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.09704911821707374  - accuracy: 0.90625\n",
      "At: 1822 [==========>] Loss 0.1459532497657285  - accuracy: 0.78125\n",
      "At: 1823 [==========>] Loss 0.19739132190602543  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.16908197758911997  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.12264317698980906  - accuracy: 0.84375\n",
      "At: 1826 [==========>] Loss 0.050015577615984984  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.10572276038660916  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.1513192043399282  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.131189118097392  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.13624856568912694  - accuracy: 0.8125\n",
      "At: 1831 [==========>] Loss 0.1288865084572518  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.11892937599775871  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.12064182191595915  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.08908500988639706  - accuracy: 0.8125\n",
      "At: 1835 [==========>] Loss 0.14506548738741967  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.11844678927568343  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.03642117750407699  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.08779762106320654  - accuracy: 0.90625\n",
      "At: 1839 [==========>] Loss 0.08614276987412724  - accuracy: 0.90625\n",
      "At: 1840 [==========>] Loss 0.12251600769593902  - accuracy: 0.8125\n",
      "At: 1841 [==========>] Loss 0.12385825181271315  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.145599420853227  - accuracy: 0.8125\n",
      "At: 1843 [==========>] Loss 0.11762036278893662  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.10566740128531515  - accuracy: 0.84375\n",
      "At: 1845 [==========>] Loss 0.1546692357187781  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.12364219854597497  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.059974041100806806  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.05846027023997041  - accuracy: 0.9375\n",
      "At: 1849 [==========>] Loss 0.1918995217743174  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.034680000017316547  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.16884037213321934  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.07973422163814266  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.1216404745887003  - accuracy: 0.84375\n",
      "At: 1854 [==========>] Loss 0.1263873621887269  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.1463672861288529  - accuracy: 0.78125\n",
      "At: 1856 [==========>] Loss 0.129988471213998  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.14328111636963342  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.10631746304668668  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.13594488850144296  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.12769336820785268  - accuracy: 0.75\n",
      "At: 1861 [==========>] Loss 0.10707437719549766  - accuracy: 0.8125\n",
      "At: 1862 [==========>] Loss 0.1723874382203388  - accuracy: 0.78125\n",
      "At: 1863 [==========>] Loss 0.1437384790327217  - accuracy: 0.75\n",
      "At: 1864 [==========>] Loss 0.13289620263816254  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.11021884676381487  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.21982040057671903  - accuracy: 0.71875\n",
      "At: 1867 [==========>] Loss 0.0943128572079552  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.16681748153547926  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.17257659895750582  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.12628598581674857  - accuracy: 0.78125\n",
      "At: 1871 [==========>] Loss 0.12529107646602186  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.10247420085523809  - accuracy: 0.84375\n",
      "At: 1873 [==========>] Loss 0.08091580478640949  - accuracy: 0.9375\n",
      "At: 1874 [==========>] Loss 0.1563200633512416  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.09871056486429355  - accuracy: 0.84375\n",
      "At: 1876 [==========>] Loss 0.1991544599853538  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.08793111907915704  - accuracy: 0.90625\n",
      "At: 1878 [==========>] Loss 0.10490826909807224  - accuracy: 0.875\n",
      "At: 1879 [==========>] Loss 0.11778493573708645  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.09842671561527556  - accuracy: 0.84375\n",
      "At: 1881 [==========>] Loss 0.1080085005065024  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.10123748056743156  - accuracy: 0.9375\n",
      "At: 1883 [==========>] Loss 0.1227441161746674  - accuracy: 0.84375\n",
      "At: 1884 [==========>] Loss 0.1102851463508995  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.10524217880221495  - accuracy: 0.875\n",
      "At: 1886 [==========>] Loss 0.1284636787461694  - accuracy: 0.8125\n",
      "At: 1887 [==========>] Loss 0.08723360424023724  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.14778716172291106  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.11065768325684225  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.14821363457033548  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.059804013958208876  - accuracy: 0.96875\n",
      "At: 1892 [==========>] Loss 0.06092271050226283  - accuracy: 0.9375\n",
      "At: 1893 [==========>] Loss 0.08057128274389075  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.09375155570309116  - accuracy: 0.9375\n",
      "At: 1895 [==========>] Loss 0.07685764122682834  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.1295487461114857  - accuracy: 0.78125\n",
      "At: 1897 [==========>] Loss 0.0668794480720816  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.10423708958033706  - accuracy: 0.8125\n",
      "At: 1899 [==========>] Loss 0.10318137473987152  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.10889861762244551  - accuracy: 0.90625\n",
      "At: 1901 [==========>] Loss 0.0955129596857957  - accuracy: 0.90625\n",
      "At: 1902 [==========>] Loss 0.15012423584910992  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.1066328839107551  - accuracy: 0.84375\n",
      "At: 1904 [==========>] Loss 0.05561111087696162  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.1403255596692504  - accuracy: 0.8125\n",
      "At: 1906 [==========>] Loss 0.10813042276150811  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.08934917898912481  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.09713316039574657  - accuracy: 0.8125\n",
      "At: 1909 [==========>] Loss 0.10560233791259772  - accuracy: 0.90625\n",
      "At: 1910 [==========>] Loss 0.05444277509090255  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.09381211783757488  - accuracy: 0.90625\n",
      "At: 1912 [==========>] Loss 0.11835650641568754  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.18638989706746312  - accuracy: 0.71875\n",
      "At: 1914 [==========>] Loss 0.07338529202823638  - accuracy: 0.875\n",
      "At: 1915 [==========>] Loss 0.1189088381399014  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.13890013910611249  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.17420821698030875  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.14041988250586832  - accuracy: 0.75\n",
      "At: 1919 [==========>] Loss 0.10656557199820836  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.10895006578756981  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.1391174420713502  - accuracy: 0.78125\n",
      "At: 1922 [==========>] Loss 0.13330021814326892  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.2094259200944824  - accuracy: 0.6875\n",
      "At: 1924 [==========>] Loss 0.14318074616332377  - accuracy: 0.75\n",
      "At: 1925 [==========>] Loss 0.18393726913037736  - accuracy: 0.78125\n",
      "At: 1926 [==========>] Loss 0.1087465893054522  - accuracy: 0.78125\n",
      "At: 1927 [==========>] Loss 0.13474933499066094  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.11392886198084128  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.1830205018962343  - accuracy: 0.6875\n",
      "At: 1930 [==========>] Loss 0.1605182152796891  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.10470502503179245  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.14825940674002297  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.06308598352655449  - accuracy: 0.9375\n",
      "At: 1934 [==========>] Loss 0.1426067565068248  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.12312939175683221  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.09826729787823377  - accuracy: 0.875\n",
      "At: 1937 [==========>] Loss 0.12797109480144717  - accuracy: 0.8125\n",
      "At: 1938 [==========>] Loss 0.2097069872270049  - accuracy: 0.6875\n",
      "At: 1939 [==========>] Loss 0.11651637680051936  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.13072227366812608  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.14843529096780167  - accuracy: 0.8125\n",
      "At: 1942 [==========>] Loss 0.17172771636383843  - accuracy: 0.75\n",
      "At: 1943 [==========>] Loss 0.14281744200818153  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.1285334751738451  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.15985911194762187  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.09345629250873985  - accuracy: 0.875\n",
      "At: 1947 [==========>] Loss 0.1265627452771024  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.10531443819368722  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.06561486431122403  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.14743478312743785  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.13971451367166737  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.08283290515665592  - accuracy: 0.84375\n",
      "At: 1953 [==========>] Loss 0.08364070050082931  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.18119296672584484  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.07125681266122467  - accuracy: 0.9375\n",
      "At: 1956 [==========>] Loss 0.12307007069560474  - accuracy: 0.90625\n",
      "At: 1957 [==========>] Loss 0.06464750403650081  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.09223282929630941  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.1269847918970007  - accuracy: 0.78125\n",
      "At: 1960 [==========>] Loss 0.055104090603054254  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.16420979428329685  - accuracy: 0.78125\n",
      "At: 1962 [==========>] Loss 0.17325271628547362  - accuracy: 0.78125\n",
      "At: 1963 [==========>] Loss 0.06315623590100264  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.15907883027298564  - accuracy: 0.78125\n",
      "At: 1965 [==========>] Loss 0.14746386298161426  - accuracy: 0.84375\n",
      "At: 1966 [==========>] Loss 0.11748108994380052  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.12927908544230782  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.21289949066279415  - accuracy: 0.65625\n",
      "At: 1969 [==========>] Loss 0.1735473094657063  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.09664962648676889  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.2212815682115024  - accuracy: 0.6875\n",
      "At: 1972 [==========>] Loss 0.10719619472997854  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.13065093529860577  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.13413073505720302  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.1615435371927365  - accuracy: 0.8125\n",
      "At: 1976 [==========>] Loss 0.07564667602215286  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.10160852953803023  - accuracy: 0.8125\n",
      "At: 1978 [==========>] Loss 0.13144854793823385  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.12911554061060726  - accuracy: 0.8125\n",
      "At: 1980 [==========>] Loss 0.12187022488447438  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.17297520651874843  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.05705125007074466  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.12823174098989554  - accuracy: 0.875\n",
      "At: 1984 [==========>] Loss 0.10571712061242018  - accuracy: 0.84375\n",
      "At: 1985 [==========>] Loss 0.13559060184878013  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.196374559771255  - accuracy: 0.65625\n",
      "At: 1987 [==========>] Loss 0.11463941872039177  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.10620788097967329  - accuracy: 0.84375\n",
      "At: 1989 [==========>] Loss 0.0855561950485014  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.1096403435976912  - accuracy: 0.875\n",
      "At: 1991 [==========>] Loss 0.1426640107345022  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.12958688829659784  - accuracy: 0.8125\n",
      "At: 1993 [==========>] Loss 0.14182170672028707  - accuracy: 0.8125\n",
      "At: 1994 [==========>] Loss 0.1237476601735796  - accuracy: 0.84375\n",
      "At: 1995 [==========>] Loss 0.2048303737076026  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.09649871963373324  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.19121843699905663  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.1726646334564096  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.08628633840498967  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.11358615113921719  - accuracy: 0.84375\n",
      "At: 2001 [==========>] Loss 0.07733896923753852  - accuracy: 0.84375\n",
      "At: 2002 [==========>] Loss 0.06612917555716247  - accuracy: 0.90625\n",
      "At: 2003 [==========>] Loss 0.13630983482198356  - accuracy: 0.78125\n",
      "At: 2004 [==========>] Loss 0.1931278633158099  - accuracy: 0.6875\n",
      "At: 2005 [==========>] Loss 0.1203447034579075  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.11961863309232142  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.07785332917528853  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.110403858066832  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.1377581822283308  - accuracy: 0.84375\n",
      "At: 2010 [==========>] Loss 0.1102317702738041  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.1142482334784796  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.0891341203566971  - accuracy: 0.875\n",
      "At: 2013 [==========>] Loss 0.1116349492149111  - accuracy: 0.84375\n",
      "At: 2014 [==========>] Loss 0.20034714705377876  - accuracy: 0.6875\n",
      "At: 2015 [==========>] Loss 0.08246540603187964  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.12277129576709919  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.0968524600599979  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.09022177387759867  - accuracy: 0.90625\n",
      "At: 2019 [==========>] Loss 0.14440073367791628  - accuracy: 0.78125\n",
      "At: 2020 [==========>] Loss 0.07799677655635615  - accuracy: 0.875\n",
      "At: 2021 [==========>] Loss 0.11044687378894276  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.14899421886595143  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.07860594013421844  - accuracy: 0.9375\n",
      "At: 2024 [==========>] Loss 0.09257044191940557  - accuracy: 0.875\n",
      "At: 2025 [==========>] Loss 0.19595086202998574  - accuracy: 0.71875\n",
      "At: 2026 [==========>] Loss 0.0878612609109279  - accuracy: 0.9375\n",
      "At: 2027 [==========>] Loss 0.14927209989613288  - accuracy: 0.75\n",
      "At: 2028 [==========>] Loss 0.10984440395741338  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.12111494964355071  - accuracy: 0.84375\n",
      "At: 2030 [==========>] Loss 0.17384974618453874  - accuracy: 0.75\n",
      "At: 2031 [==========>] Loss 0.1377503560123009  - accuracy: 0.8125\n",
      "At: 2032 [==========>] Loss 0.13216686649492462  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.14945927758006836  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.22143400249024364  - accuracy: 0.65625\n",
      "At: 2035 [==========>] Loss 0.09782595258233957  - accuracy: 0.90625\n",
      "At: 2036 [==========>] Loss 0.10657054839896443  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.10305676051968238  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.08819656512400909  - accuracy: 0.84375\n",
      "At: 2039 [==========>] Loss 0.08926276524301066  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.11125732088872463  - accuracy: 0.875\n",
      "At: 2041 [==========>] Loss 0.0630276040151394  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.13066133794900026  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.11691367576467612  - accuracy: 0.8125\n",
      "At: 2044 [==========>] Loss 0.0863669858060237  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.22458096344436224  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.06584085913712912  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.06899674155665615  - accuracy: 0.9375\n",
      "At: 2048 [==========>] Loss 0.11504356870388135  - accuracy: 0.84375\n",
      "At: 2049 [==========>] Loss 0.14963114970540542  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.1546748869124111  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.18958813074333564  - accuracy: 0.75\n",
      "At: 2052 [==========>] Loss 0.05664819608994556  - accuracy: 0.96875\n",
      "At: 2053 [==========>] Loss 0.10301887449098411  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.10896105822824496  - accuracy: 0.84375\n",
      "At: 2055 [==========>] Loss 0.07010594683112589  - accuracy: 0.875\n",
      "At: 2056 [==========>] Loss 0.1216943015099963  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.15481513987607182  - accuracy: 0.75\n",
      "At: 2058 [==========>] Loss 0.13261814642212905  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.2047598487587297  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.12481020553162475  - accuracy: 0.78125\n",
      "At: 2061 [==========>] Loss 0.1458580942548314  - accuracy: 0.84375\n",
      "At: 2062 [==========>] Loss 0.13397843868559683  - accuracy: 0.84375\n",
      "At: 2063 [==========>] Loss 0.09006512333054056  - accuracy: 0.9375\n",
      "At: 2064 [==========>] Loss 0.17290604145459285  - accuracy: 0.65625\n",
      "At: 2065 [==========>] Loss 0.034803352101121676  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.13014048337558026  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.10236285198918561  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.09438110270705748  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.11050586686711435  - accuracy: 0.78125\n",
      "At: 2070 [==========>] Loss 0.15374186069665594  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.11590752446891381  - accuracy: 0.84375\n",
      "At: 2072 [==========>] Loss 0.0531960831101618  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.11797872947208705  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.09649782961190714  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.11832561855079052  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.14307187950056044  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.12697966172780978  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.118933251449686  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07190810353823486  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.11360357011136411  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.1416007150552337  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.12817801562175077  - accuracy: 0.90625\n",
      "At: 2083 [==========>] Loss 0.17226406240734882  - accuracy: 0.75\n",
      "At: 2084 [==========>] Loss 0.10300484565063109  - accuracy: 0.84375\n",
      "At: 2085 [==========>] Loss 0.09951646319605542  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.08653766639065076  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.1579071979189555  - accuracy: 0.8125\n",
      "At: 2088 [==========>] Loss 0.09625451641506634  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.1121287134240177  - accuracy: 0.90625\n",
      "At: 2090 [==========>] Loss 0.09069189292039061  - accuracy: 0.875\n",
      "At: 2091 [==========>] Loss 0.10804160925427032  - accuracy: 0.8125\n",
      "At: 2092 [==========>] Loss 0.10620391600866746  - accuracy: 0.8125\n",
      "At: 2093 [==========>] Loss 0.15433403865646378  - accuracy: 0.78125\n",
      "At: 2094 [==========>] Loss 0.11782281123314726  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.11194315999916404  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.1408856346409929  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.10944020392798452  - accuracy: 0.875\n",
      "At: 2098 [==========>] Loss 0.132007936021769  - accuracy: 0.78125\n",
      "At: 2099 [==========>] Loss 0.10069505901867139  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.05811569479728869  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.1668580913726005  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.07131327762006637  - accuracy: 0.90625\n",
      "At: 2103 [==========>] Loss 0.16890625159394274  - accuracy: 0.75\n",
      "At: 2104 [==========>] Loss 0.11034073714575782  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.17665586067290567  - accuracy: 0.6875\n",
      "At: 2106 [==========>] Loss 0.14352593062858113  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.10267814943752536  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.12500689269423063  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.11919135870561737  - accuracy: 0.875\n",
      "At: 2110 [==========>] Loss 0.0768640541288181  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.1289642725241555  - accuracy: 0.78125\n",
      "At: 2112 [==========>] Loss 0.11599933724949796  - accuracy: 0.84375\n",
      "At: 2113 [==========>] Loss 0.10361467847483671  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.129691848915664  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.11332434923594553  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.10057533515321532  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.14805621345171383  - accuracy: 0.75\n",
      "At: 2118 [==========>] Loss 0.13763718863893934  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.07232271531104476  - accuracy: 0.90625\n",
      "At: 2120 [==========>] Loss 0.15853840823807414  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.13550194876430333  - accuracy: 0.875\n",
      "At: 2122 [==========>] Loss 0.1445066614157607  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.20090916500719636  - accuracy: 0.71875\n",
      "At: 2124 [==========>] Loss 0.13706425581103174  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.07683786446043175  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.05343240027953712  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.09358034047147043  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.1066241779574015  - accuracy: 0.875\n",
      "At: 2129 [==========>] Loss 0.14672062641647565  - accuracy: 0.875\n",
      "At: 2130 [==========>] Loss 0.06555205756362917  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.10338386460502166  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.21155300380093328  - accuracy: 0.78125\n",
      "At: 2133 [==========>] Loss 0.15360192757684543  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.11515095783037473  - accuracy: 0.78125\n",
      "At: 2135 [==========>] Loss 0.097515383114447  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.12203620535733004  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.11064231623102652  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.08731546045225275  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.17069200020143618  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.10734056570448938  - accuracy: 0.8125\n",
      "At: 2141 [==========>] Loss 0.10847957394345588  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.10690173660705697  - accuracy: 0.875\n",
      "At: 2143 [==========>] Loss 0.0991984804002384  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.08925943377016402  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.12118944536776288  - accuracy: 0.78125\n",
      "At: 2146 [==========>] Loss 0.1882012668573209  - accuracy: 0.6875\n",
      "At: 2147 [==========>] Loss 0.11285040342780464  - accuracy: 0.875\n",
      "At: 2148 [==========>] Loss 0.2050556129792499  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.13799497307576944  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.115166252439547  - accuracy: 0.875\n",
      "At: 2151 [==========>] Loss 0.0946779438019524  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.23683399327420077  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.16889695056666823  - accuracy: 0.78125\n",
      "At: 2154 [==========>] Loss 0.1862492345481152  - accuracy: 0.71875\n",
      "At: 2155 [==========>] Loss 0.13429571144817795  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.10254003904429954  - accuracy: 0.90625\n",
      "At: 2157 [==========>] Loss 0.08606591812057848  - accuracy: 0.875\n",
      "At: 2158 [==========>] Loss 0.14354784283661154  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.07577571310531087  - accuracy: 0.875\n",
      "At: 2160 [==========>] Loss 0.13204928689961615  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.12944444998117915  - accuracy: 0.8125\n",
      "At: 2162 [==========>] Loss 0.08782637600560907  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.14217047896808402  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.16810819560497925  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.09764322007733423  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.12883514724828365  - accuracy: 0.8125\n",
      "At: 2167 [==========>] Loss 0.08688214573690833  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.08439463720105464  - accuracy: 0.90625\n",
      "At: 2169 [==========>] Loss 0.13620236041684536  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11104865808036254  - accuracy: 0.90625\n",
      "At: 2171 [==========>] Loss 0.13206978157189547  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.1031548278786881  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.1405627820317037  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.14576027660984653  - accuracy: 0.8125\n",
      "At: 2175 [==========>] Loss 0.12003075997549716  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.13457793524163614  - accuracy: 0.78125\n",
      "At: 2177 [==========>] Loss 0.14764770602481925  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.07862443444874209  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.1264848497970854  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.09662110131207727  - accuracy: 0.90625\n",
      "At: 2181 [==========>] Loss 0.15757539010487648  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.11829846213798952  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.2352199625234328  - accuracy: 0.65625\n",
      "At: 2184 [==========>] Loss 0.0894622745081963  - accuracy: 0.84375\n",
      "At: 2185 [==========>] Loss 0.11153696323077344  - accuracy: 0.84375\n",
      "At: 2186 [==========>] Loss 0.13830481996861782  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.1969995955329406  - accuracy: 0.6875\n",
      "At: 2188 [==========>] Loss 0.07638144903540225  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.07072538465732661  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.09347608175753395  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.09987638191124933  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.11278253946568072  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.1697812421869031  - accuracy: 0.8125\n",
      "At: 2194 [==========>] Loss 0.12027641976380868  - accuracy: 0.875\n",
      "At: 2195 [==========>] Loss 0.1892514935659678  - accuracy: 0.6875\n",
      "At: 2196 [==========>] Loss 0.16934925730648157  - accuracy: 0.75\n",
      "At: 2197 [==========>] Loss 0.09850778585600638  - accuracy: 0.84375\n",
      "At: 2198 [==========>] Loss 0.07816076848474562  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.035348132921609134  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.04966602404030526  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.1037806708316394  - accuracy: 0.90625\n",
      "At: 2202 [==========>] Loss 0.08889615680402527  - accuracy: 0.875\n",
      "At: 2203 [==========>] Loss 0.08267536740854604  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.1301285498328779  - accuracy: 0.875\n",
      "At: 2205 [==========>] Loss 0.11809872382926903  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.08225788156742249  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.11055376239140106  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.1712095443454199  - accuracy: 0.75\n",
      "At: 2209 [==========>] Loss 0.18761077507246193  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.1489838092671219  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.15854504275307751  - accuracy: 0.75\n",
      "At: 2212 [==========>] Loss 0.10239522904545813  - accuracy: 0.90625\n",
      "At: 2213 [==========>] Loss 0.1423158914985973  - accuracy: 0.78125\n",
      "At: 2214 [==========>] Loss 0.13451938035821126  - accuracy: 0.75\n",
      "At: 2215 [==========>] Loss 0.1308161967102905  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.11867260697647256  - accuracy: 0.875\n",
      "At: 2217 [==========>] Loss 0.12473914355081353  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.1695851567172738  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.07705990350379976  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.1365964391907963  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.17921560681473558  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.10829590584205356  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.14481183217022287  - accuracy: 0.8125\n",
      "At: 2224 [==========>] Loss 0.15331744870544875  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.14362928985271733  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.09902445025764697  - accuracy: 0.875\n",
      "At: 2227 [==========>] Loss 0.18843876950595384  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.0893187364791392  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.14040307422498502  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.11245634207511812  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.15865419087917643  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.16866393480771263  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.14598142218411594  - accuracy: 0.84375\n",
      "At: 2234 [==========>] Loss 0.16422292952687254  - accuracy: 0.6875\n",
      "At: 2235 [==========>] Loss 0.10230011939355398  - accuracy: 0.875\n",
      "At: 2236 [==========>] Loss 0.08351362667094223  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.11960012649294534  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.13378626560826307  - accuracy: 0.78125\n",
      "At: 2239 [==========>] Loss 0.17866717269955373  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.16879489264013842  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.12026037004629256  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.16120695815988537  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.08022481350360627  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.08791569478656236  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.08214961257262363  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.15488079243594777  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.09977422133570125  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.1712126616105819  - accuracy: 0.75\n",
      "At: 2249 [==========>] Loss 0.12205922996273329  - accuracy: 0.8125\n",
      "At: 2250 [==========>] Loss 0.08234422656016671  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.08338736342730498  - accuracy: 0.84375\n",
      "At: 2252 [==========>] Loss 0.11128968368232302  - accuracy: 0.875\n",
      "At: 2253 [==========>] Loss 0.13649084872639566  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.13188601401919864  - accuracy: 0.78125\n",
      "At: 2255 [==========>] Loss 0.14009335460608707  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.15292728640669678  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.10127265653387918  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.12308203077328038  - accuracy: 0.8125\n",
      "At: 2259 [==========>] Loss 0.10281475934547088  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.16908279361464496  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.07460001227059941  - accuracy: 0.9375\n",
      "At: 2262 [==========>] Loss 0.1731020677899668  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.17010737338116544  - accuracy: 0.75\n",
      "At: 2264 [==========>] Loss 0.09569421957219751  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.08099588209002792  - accuracy: 0.90625\n",
      "At: 2266 [==========>] Loss 0.14376883451418684  - accuracy: 0.75\n",
      "At: 2267 [==========>] Loss 0.06428720939318176  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.09068049153342342  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.046215599697090395  - accuracy: 0.9375\n",
      "At: 2270 [==========>] Loss 0.09689394156142146  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.13042783041930817  - accuracy: 0.8125\n",
      "At: 2272 [==========>] Loss 0.08730338001111249  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.10848160381068236  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.08640637839991806  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.07238204275331869  - accuracy: 0.9375\n",
      "At: 2276 [==========>] Loss 0.08880874932304564  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.13076937398676958  - accuracy: 0.84375\n",
      "At: 2278 [==========>] Loss 0.08520505008615738  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.1419890604191706  - accuracy: 0.75\n",
      "At: 2280 [==========>] Loss 0.1479082382011015  - accuracy: 0.75\n",
      "At: 2281 [==========>] Loss 0.10507510711655826  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.08326340015940278  - accuracy: 0.90625\n",
      "At: 2283 [==========>] Loss 0.17004252313409848  - accuracy: 0.6875\n",
      "At: 2284 [==========>] Loss 0.1361695694455233  - accuracy: 0.71875\n",
      "At: 2285 [==========>] Loss 0.11474658082865158  - accuracy: 0.8125\n",
      "At: 2286 [==========>] Loss 0.09949220201848824  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.1435762466499384  - accuracy: 0.78125\n",
      "At: 2288 [==========>] Loss 0.09032118621008545  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.1306048673947946  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.06501149314692911  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.14603836859045932  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.08470956619844594  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.06230881363881462  - accuracy: 0.90625\n",
      "At: 2294 [==========>] Loss 0.061568236474413035  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.13622508965961827  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.15244695748955778  - accuracy: 0.8125\n",
      "At: 2297 [==========>] Loss 0.06544993360074502  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.12305434030527188  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.11490463568537176  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.11580406509475932  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.16786233605461753  - accuracy: 0.75\n",
      "At: 2302 [==========>] Loss 0.1691563490348903  - accuracy: 0.78125\n",
      "At: 2303 [==========>] Loss 0.06590963110486223  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.08909438245784174  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.1196653433139144  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.12098368348269853  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.16209413455471178  - accuracy: 0.71875\n",
      "At: 2308 [==========>] Loss 0.18178366108436603  - accuracy: 0.71875\n",
      "At: 2309 [==========>] Loss 0.11680428281102542  - accuracy: 0.78125\n",
      "At: 2310 [==========>] Loss 0.1599727247625187  - accuracy: 0.78125\n",
      "At: 2311 [==========>] Loss 0.16573609240245574  - accuracy: 0.75\n",
      "At: 2312 [==========>] Loss 0.10185445779006586  - accuracy: 0.84375\n",
      "At: 2313 [==========>] Loss 0.09103805297817186  - accuracy: 0.875\n",
      "At: 2314 [==========>] Loss 0.12722077612084812  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.12453679367085878  - accuracy: 0.75\n",
      "At: 2316 [==========>] Loss 0.1328815900504441  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.17225976769722628  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.15651422984363844  - accuracy: 0.75\n",
      "At: 2319 [==========>] Loss 0.10078691048422393  - accuracy: 0.875\n",
      "At: 2320 [==========>] Loss 0.1135944078885184  - accuracy: 0.84375\n",
      "At: 2321 [==========>] Loss 0.13580284769924594  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.1842249677863566  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.15345492401782135  - accuracy: 0.75\n",
      "At: 2324 [==========>] Loss 0.15871413541223653  - accuracy: 0.8125\n",
      "At: 2325 [==========>] Loss 0.12367362956438821  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.0834147957527296  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.05770834658474233  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.15269978282166066  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.10512054894424178  - accuracy: 0.90625\n",
      "At: 2330 [==========>] Loss 0.1205854163233914  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.1246490952646246  - accuracy: 0.84375\n",
      "At: 2332 [==========>] Loss 0.11230986137316085  - accuracy: 0.84375\n",
      "At: 2333 [==========>] Loss 0.09351253294895912  - accuracy: 0.90625\n",
      "At: 2334 [==========>] Loss 0.15363217412538338  - accuracy: 0.78125\n",
      "At: 2335 [==========>] Loss 0.10973830119069719  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.10008320301404981  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.15176399720879497  - accuracy: 0.75\n",
      "At: 2338 [==========>] Loss 0.11328403587352004  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.09281079668778222  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.15635999215712615  - accuracy: 0.78125\n",
      "At: 2341 [==========>] Loss 0.13796118070635638  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.1760128640236607  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.08618667521384701  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.2053367035786488  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.13182531205601644  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.08378965690455603  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.12456804366834172  - accuracy: 0.90625\n",
      "At: 2348 [==========>] Loss 0.06956382093721258  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.11504711500557743  - accuracy: 0.78125\n",
      "At: 2350 [==========>] Loss 0.14988792053114589  - accuracy: 0.78125\n",
      "At: 2351 [==========>] Loss 0.08566374409933405  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.1130764421123585  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.10909619174146894  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.18179426497888868  - accuracy: 0.6875\n",
      "At: 2355 [==========>] Loss 0.04105305960862344  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.11873179206623619  - accuracy: 0.875\n",
      "At: 2357 [==========>] Loss 0.15461815584349647  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.17004347181868695  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.2040900296888077  - accuracy: 0.625\n",
      "At: 2360 [==========>] Loss 0.13109562023378102  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.1805434144719328  - accuracy: 0.75\n",
      "At: 2362 [==========>] Loss 0.07938668362232046  - accuracy: 0.875\n",
      "At: 2363 [==========>] Loss 0.17080834339696715  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.08960773560507244  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.07823111165680913  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.1530790468576177  - accuracy: 0.75\n",
      "At: 2367 [==========>] Loss 0.10780493751416467  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.11724110840020394  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.09611230574585092  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.07024884721748362  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.08048369029265412  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.1074791090906106  - accuracy: 0.875\n",
      "At: 2373 [==========>] Loss 0.10662937603090364  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.08947045952624598  - accuracy: 0.875\n",
      "At: 2375 [==========>] Loss 0.06331749308436013  - accuracy: 0.875\n",
      "At: 2376 [==========>] Loss 0.10686208112203566  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.08561461101894277  - accuracy: 0.875\n",
      "At: 2378 [==========>] Loss 0.11138281468066383  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.1829095514010306  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.06295344509931507  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.09754340968431158  - accuracy: 0.84375\n",
      "At: 2382 [==========>] Loss 0.09132586326081887  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.14285381988940643  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.12013039132526555  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.14081769096365468  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.07656833723622711  - accuracy: 0.90625\n",
      "At: 2387 [==========>] Loss 0.07846090537987399  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.11961754286369283  - accuracy: 0.8125\n",
      "At: 2389 [==========>] Loss 0.0542857734382947  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.07517303372174432  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.190270959889185  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.14479392941560726  - accuracy: 0.84375\n",
      "At: 2393 [==========>] Loss 0.0946728061555153  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.06595828004699764  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.08029530582431413  - accuracy: 0.9375\n",
      "At: 2396 [==========>] Loss 0.07402627988448461  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.07845462670645847  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.1258002598528606  - accuracy: 0.84375\n",
      "At: 2399 [==========>] Loss 0.14247940069094747  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.09066855966239062  - accuracy: 0.90625\n",
      "At: 2401 [==========>] Loss 0.09178284756291191  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.0960663671999492  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.1551920409992107  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.12275333356646151  - accuracy: 0.875\n",
      "At: 2405 [==========>] Loss 0.07602532653821936  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.11094256342494327  - accuracy: 0.875\n",
      "At: 2407 [==========>] Loss 0.11198684155029859  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.07335755577713748  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.16455498167857693  - accuracy: 0.84375\n",
      "At: 2410 [==========>] Loss 0.15550765863789698  - accuracy: 0.75\n",
      "At: 2411 [==========>] Loss 0.1032091435733955  - accuracy: 0.84375\n",
      "At: 2412 [==========>] Loss 0.08266759938518864  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.09164911720771576  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.07008303233865938  - accuracy: 0.9375\n",
      "At: 2415 [==========>] Loss 0.0972731199871425  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.09231809056477563  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.18891922010250242  - accuracy: 0.71875\n",
      "At: 2418 [==========>] Loss 0.11812542250281591  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.12217248775756887  - accuracy: 0.875\n",
      "At: 2420 [==========>] Loss 0.1367805006013404  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.10112650451085545  - accuracy: 0.90625\n",
      "At: 2422 [==========>] Loss 0.14919942545315373  - accuracy: 0.8125\n",
      "At: 2423 [==========>] Loss 0.1495598250343166  - accuracy: 0.75\n",
      "At: 2424 [==========>] Loss 0.1043682699480945  - accuracy: 0.90625\n",
      "At: 2425 [==========>] Loss 0.10341788324857926  - accuracy: 0.84375\n",
      "At: 2426 [==========>] Loss 0.19351721410658013  - accuracy: 0.6875\n",
      "At: 2427 [==========>] Loss 0.13902814699674226  - accuracy: 0.75\n",
      "At: 2428 [==========>] Loss 0.07056214911118498  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.10306401639502469  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.13834621079178244  - accuracy: 0.84375\n",
      "At: 2431 [==========>] Loss 0.1527678415364475  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.06531831109756131  - accuracy: 0.96875\n",
      "At: 2433 [==========>] Loss 0.05305359286747181  - accuracy: 0.96875\n",
      "At: 2434 [==========>] Loss 0.05593850244548016  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.14926344456378723  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.11140786530220598  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.17558265907672133  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.09054055841983485  - accuracy: 0.90625\n",
      "At: 2439 [==========>] Loss 0.1383207687992808  - accuracy: 0.8125\n",
      "At: 2440 [==========>] Loss 0.11594368510351696  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.1181554663425989  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.12654572467314176  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.1296723981166479  - accuracy: 0.75\n",
      "At: 2444 [==========>] Loss 0.08897581847870958  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.062272224226630146  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.1757951510167906  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.17325683987951807  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.0869322741669639  - accuracy: 0.875\n",
      "At: 2449 [==========>] Loss 0.0700111492535936  - accuracy: 0.9375\n",
      "At: 2450 [==========>] Loss 0.0822906956195553  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.060397223745794995  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.14164755213133584  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.13351997150602107  - accuracy: 0.78125\n",
      "At: 2454 [==========>] Loss 0.13563155803903226  - accuracy: 0.84375\n",
      "At: 2455 [==========>] Loss 0.13214460034280712  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.13884106745431565  - accuracy: 0.78125\n",
      "At: 2457 [==========>] Loss 0.1862199328297972  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.07816619168299829  - accuracy: 0.9375\n",
      "At: 2459 [==========>] Loss 0.1317252314203482  - accuracy: 0.84375\n",
      "At: 2460 [==========>] Loss 0.0830277985972537  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.05102608655111972  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.1648883907181937  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.09519687653000958  - accuracy: 0.9375\n",
      "At: 2464 [==========>] Loss 0.1821112676170867  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.11085507474360412  - accuracy: 0.875\n",
      "At: 2466 [==========>] Loss 0.09659233483816468  - accuracy: 0.875\n",
      "At: 2467 [==========>] Loss 0.12604255451874447  - accuracy: 0.8125\n",
      "At: 2468 [==========>] Loss 0.08150915217222901  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.14282076934143192  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.12782680687162593  - accuracy: 0.875\n",
      "At: 2471 [==========>] Loss 0.10936119447951691  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.09729986668205622  - accuracy: 0.90625\n",
      "At: 2473 [==========>] Loss 0.11276887563220928  - accuracy: 0.875\n",
      "At: 2474 [==========>] Loss 0.06717735264661936  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.1054369993544605  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.06040280733278497  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.1361765941968081  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.13122734630754565  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.06990827166879557  - accuracy: 0.9375\n",
      "At: 2480 [==========>] Loss 0.1102518299164163  - accuracy: 0.90625\n",
      "At: 2481 [==========>] Loss 0.08933527861751103  - accuracy: 0.90625\n",
      "At: 2482 [==========>] Loss 0.17625131688923296  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.10803222066990928  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.08950205963786667  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.13584545928921815  - accuracy: 0.8125\n",
      "At: 2486 [==========>] Loss 0.11646954065853128  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.13669349286464405  - accuracy: 0.84375\n",
      "At: 2488 [==========>] Loss 0.16301769826914378  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.19407493934003417  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.1175265783515411  - accuracy: 0.84375\n",
      "At: 2491 [==========>] Loss 0.11945579638060487  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.11594963416305959  - accuracy: 0.78125\n",
      "At: 2493 [==========>] Loss 0.08890648762142858  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.09779291803480994  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.09878643503658771  - accuracy: 0.84375\n",
      "At: 2496 [==========>] Loss 0.06634370192602347  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.22457006150423964  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.10781620786350953  - accuracy: 0.90625\n",
      "At: 2499 [==========>] Loss 0.05191907389568434  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.19563561606052693  - accuracy: 0.78125\n",
      "At: 2501 [==========>] Loss 0.1690559427011169  - accuracy: 0.8125\n",
      "At: 2502 [==========>] Loss 0.09710419811698104  - accuracy: 0.90625\n",
      "At: 2503 [==========>] Loss 0.12390210118761485  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.15747414456559927  - accuracy: 0.75\n",
      "At: 2505 [==========>] Loss 0.10800108413943996  - accuracy: 0.875\n",
      "At: 2506 [==========>] Loss 0.10434635772188904  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.14084150912466936  - accuracy: 0.8125\n",
      "At: 2508 [==========>] Loss 0.11898984761341003  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.16067238759597055  - accuracy: 0.75\n",
      "At: 2510 [==========>] Loss 0.10473931999357053  - accuracy: 0.875\n",
      "At: 2511 [==========>] Loss 0.14556228823964085  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.0988504494976212  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.14208381292198902  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.14347120125444732  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.20432472343876085  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.17195131112780046  - accuracy: 0.78125\n",
      "At: 2517 [==========>] Loss 0.12252639545451306  - accuracy: 0.75\n",
      "At: 2518 [==========>] Loss 0.10904745088026374  - accuracy: 0.90625\n",
      "At: 2519 [==========>] Loss 0.15027917418096629  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.15349869571670485  - accuracy: 0.78125\n",
      "At: 2521 [==========>] Loss 0.11804546415639422  - accuracy: 0.8125\n",
      "At: 2522 [==========>] Loss 0.2072047844647319  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.12265419484467366  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.15612880728200043  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.06983420733997069  - accuracy: 0.9375\n",
      "At: 2526 [==========>] Loss 0.11396901858413433  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.13853416237364863  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.07310686469149202  - accuracy: 0.90625\n",
      "At: 2529 [==========>] Loss 0.14869115818838513  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.14768723608028433  - accuracy: 0.78125\n",
      "At: 2531 [==========>] Loss 0.035678891028751596  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.10639225784916015  - accuracy: 0.875\n",
      "At: 2533 [==========>] Loss 0.08603082252940215  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.06977756759361081  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.08125954978479202  - accuracy: 0.875\n",
      "At: 2536 [==========>] Loss 0.17228550874454165  - accuracy: 0.78125\n",
      "At: 2537 [==========>] Loss 0.09253018925753251  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.16587803360740044  - accuracy: 0.6875\n",
      "At: 2539 [==========>] Loss 0.07762441160093053  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.13078267056017456  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.06340337479713605  - accuracy: 0.9375\n",
      "At: 2542 [==========>] Loss 0.07248191964129613  - accuracy: 0.875\n",
      "At: 2543 [==========>] Loss 0.15781901146554037  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.12105430515957535  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.051072838821123664  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.12358544752898835  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.09709641253444784  - accuracy: 0.90625\n",
      "At: 2548 [==========>] Loss 0.08475781435568944  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.08467713141406338  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.13525896818577246  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.13291003654585765  - accuracy: 0.78125\n",
      "At: 2552 [==========>] Loss 0.10931740136094893  - accuracy: 0.84375\n",
      "At: 2553 [==========>] Loss 0.08774880414872088  - accuracy: 0.84375\n",
      "At: 2554 [==========>] Loss 0.13737648732381677  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.16883248158236636  - accuracy: 0.8125\n",
      "At: 2556 [==========>] Loss 0.09253103315089985  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.13347167173339938  - accuracy: 0.78125\n",
      "At: 2558 [==========>] Loss 0.07851749160627387  - accuracy: 0.875\n",
      "At: 2559 [==========>] Loss 0.09235632421419664  - accuracy: 0.8125\n",
      "At: 2560 [==========>] Loss 0.18939293844767968  - accuracy: 0.65625\n",
      "At: 2561 [==========>] Loss 0.09328763589301299  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.1055719595013286  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.0898086077850283  - accuracy: 0.84375\n",
      "At: 2564 [==========>] Loss 0.07774031737788856  - accuracy: 0.90625\n",
      "At: 2565 [==========>] Loss 0.1246388186400683  - accuracy: 0.875\n",
      "At: 2566 [==========>] Loss 0.19576105613443734  - accuracy: 0.78125\n",
      "At: 2567 [==========>] Loss 0.12251532093616074  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.11386588506394887  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.061099598773540105  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.1499288215685904  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.09555545485210598  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.18534761628842644  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.19138743230433036  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.15116831084413088  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.06720814853762543  - accuracy: 0.90625\n",
      "At: 2576 [==========>] Loss 0.10030824882278612  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.21381720089273754  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.1740134310205033  - accuracy: 0.75\n",
      "At: 2579 [==========>] Loss 0.18290521208787255  - accuracy: 0.6875\n",
      "At: 2580 [==========>] Loss 0.1671962756081523  - accuracy: 0.75\n",
      "At: 2581 [==========>] Loss 0.07504769179156814  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.21835570188754289  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.07755490937318545  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.1893782889434083  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.09139554202954403  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.08502737975802632  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.16865584128532202  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.13378849918170804  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.10974498718049314  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.18845437002081222  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.13909218442256205  - accuracy: 0.8125\n",
      "At: 2592 [==========>] Loss 0.08997908874346564  - accuracy: 0.90625\n",
      "At: 2593 [==========>] Loss 0.0695488307167049  - accuracy: 0.90625\n",
      "At: 2594 [==========>] Loss 0.09325645091311546  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.07430517278552455  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.10658289380474148  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.07159526890838555  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.1257485692565031  - accuracy: 0.78125\n",
      "At: 2599 [==========>] Loss 0.11897674483576616  - accuracy: 0.84375\n",
      "At: 2600 [==========>] Loss 0.14505498947542422  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.1680985905022401  - accuracy: 0.78125\n",
      "At: 2602 [==========>] Loss 0.06920843780677713  - accuracy: 0.9375\n",
      "At: 2603 [==========>] Loss 0.15563471006904855  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.10203734870789843  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.133536318127706  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.1315103842036071  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.13667059126200978  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.0862918877069061  - accuracy: 0.90625\n",
      "At: 2609 [==========>] Loss 0.09668279834286463  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.13681001113692629  - accuracy: 0.75\n",
      "At: 2611 [==========>] Loss 0.12935814867790643  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.10630929990091692  - accuracy: 0.8125\n",
      "At: 2613 [==========>] Loss 0.09204082243958125  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.11718720813959954  - accuracy: 0.8125\n",
      "At: 2615 [==========>] Loss 0.08312530776547657  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06237891298864272  - accuracy: 0.875\n",
      "At: 2617 [==========>] Loss 0.0721766691974481  - accuracy: 0.90625\n",
      "At: 2618 [==========>] Loss 0.06741717729161774  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.11150003795662046  - accuracy: 0.84375\n",
      "At: 2620 [==========>] Loss 0.1255494237273015  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.12512730102545266  - accuracy: 0.875\n",
      "At: 2622 [==========>] Loss 0.11780029463886812  - accuracy: 0.78125\n",
      "At: 2623 [==========>] Loss 0.08426449378968395  - accuracy: 0.9375\n",
      "At: 2624 [==========>] Loss 0.15024857859064275  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.059173753116835766  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.05879442499940661  - accuracy: 0.90625\n",
      "At: 2627 [==========>] Loss 0.13309634410948296  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.07003383845212179  - accuracy: 0.96875\n",
      "At: 2629 [==========>] Loss 0.09217517133920625  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.0897211168353883  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.12507072478488981  - accuracy: 0.8125\n",
      "At: 2632 [==========>] Loss 0.13925965475207835  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.09378145936355155  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.06239733557256959  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.20270277673734663  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.11905165779874599  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.12704406035848193  - accuracy: 0.84375\n",
      "At: 2638 [==========>] Loss 0.09558825680690893  - accuracy: 0.875\n",
      "At: 2639 [==========>] Loss 0.07275233003056042  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.11837405245782849  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.13352242672490755  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.16788778482275454  - accuracy: 0.65625\n",
      "At: 2643 [==========>] Loss 0.07313338016766252  - accuracy: 0.90625\n",
      "At: 2644 [==========>] Loss 0.14255366980121947  - accuracy: 0.8125\n",
      "At: 2645 [==========>] Loss 0.08501389967310885  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.14896288237734562  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.142584811113854  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.15654283938732347  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.126070017408206  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.10758559148740712  - accuracy: 0.90625\n",
      "At: 2651 [==========>] Loss 0.18488471536260243  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.10162694586642881  - accuracy: 0.8125\n",
      "At: 2653 [==========>] Loss 0.10748783050709625  - accuracy: 0.875\n",
      "At: 2654 [==========>] Loss 0.10452662752133028  - accuracy: 0.84375\n",
      "At: 2655 [==========>] Loss 0.24351624235282124  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.01632222566443147  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.10146979860269081  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.08877917950549274  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.06298260911614903  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.0805575083552735  - accuracy: 0.90625\n",
      "At: 2661 [==========>] Loss 0.0864738388570543  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.06227722020251332  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.08885836279462075  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.08363113062253227  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.09858670732424493  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.11663852468100079  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.15041147610316985  - accuracy: 0.8125\n",
      "At: 2668 [==========>] Loss 0.1682562603734824  - accuracy: 0.71875\n",
      "At: 2669 [==========>] Loss 0.1567435657676569  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.09499352476460067  - accuracy: 0.90625\n",
      "At: 2671 [==========>] Loss 0.07444510782661364  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.11751328577398235  - accuracy: 0.84375\n",
      "At: 2673 [==========>] Loss 0.12492052294365705  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.10685980061845418  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.14541123886815438  - accuracy: 0.75\n",
      "At: 2676 [==========>] Loss 0.11632079005746689  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.109570926367707  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.03869561944206701  - accuracy: 1.0\n",
      "At: 2679 [==========>] Loss 0.06754462935729166  - accuracy: 0.9375\n",
      "At: 2680 [==========>] Loss 0.07788715761764364  - accuracy: 0.90625\n",
      "At: 2681 [==========>] Loss 0.08758677471338489  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.14159302975794652  - accuracy: 0.8125\n",
      "At: 2683 [==========>] Loss 0.1753883515137406  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09192638304841813  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.11821368850557219  - accuracy: 0.84375\n",
      "At: 2686 [==========>] Loss 0.06744773708237915  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.141124550643688  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.16815420691603544  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.11980881583580293  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.14750989285113003  - accuracy: 0.78125\n",
      "Epochs  4 / 10\n",
      "At: 1 [==========>] Loss 0.15714831194866827  - accuracy: 0.78125\n",
      "At: 2 [==========>] Loss 0.16720715606628667  - accuracy: 0.78125\n",
      "At: 3 [==========>] Loss 0.14682298311374703  - accuracy: 0.8125\n",
      "At: 4 [==========>] Loss 0.20782080987103685  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.11326352391869131  - accuracy: 0.84375\n",
      "At: 6 [==========>] Loss 0.139911755257078  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.2037175695406746  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.2342398144256414  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.31875737614690336  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.2541889010335072  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.21858966657781237  - accuracy: 0.71875\n",
      "At: 12 [==========>] Loss 0.21406663773450363  - accuracy: 0.75\n",
      "At: 13 [==========>] Loss 0.19615822898073615  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.10095457825481309  - accuracy: 0.9375\n",
      "At: 15 [==========>] Loss 0.18897811141290582  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.16015530261229088  - accuracy: 0.8125\n",
      "At: 17 [==========>] Loss 0.1831960330409403  - accuracy: 0.8125\n",
      "At: 18 [==========>] Loss 0.20348031502789984  - accuracy: 0.75\n",
      "At: 19 [==========>] Loss 0.1943370858592644  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.13842530610206633  - accuracy: 0.84375\n",
      "At: 21 [==========>] Loss 0.25786782243843476  - accuracy: 0.65625\n",
      "At: 22 [==========>] Loss 0.18456395448009  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.08592595847046394  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.25076036015676845  - accuracy: 0.65625\n",
      "At: 25 [==========>] Loss 0.23469366028840027  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.2214269348686983  - accuracy: 0.71875\n",
      "At: 27 [==========>] Loss 0.278158479175309  - accuracy: 0.59375\n",
      "At: 28 [==========>] Loss 0.20935461024821658  - accuracy: 0.71875\n",
      "At: 29 [==========>] Loss 0.1335100306907892  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.20797953261860688  - accuracy: 0.75\n",
      "At: 31 [==========>] Loss 0.25990258668727173  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.1881028041620193  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.11870917774700637  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.16286465135019082  - accuracy: 0.8125\n",
      "At: 35 [==========>] Loss 0.17623181017484169  - accuracy: 0.8125\n",
      "At: 36 [==========>] Loss 0.2061823824359212  - accuracy: 0.78125\n",
      "At: 37 [==========>] Loss 0.26389370849659377  - accuracy: 0.6875\n",
      "At: 38 [==========>] Loss 0.2551297835805396  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.20376276856349743  - accuracy: 0.71875\n",
      "At: 40 [==========>] Loss 0.28320634884282003  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.11006434187510179  - accuracy: 0.875\n",
      "At: 42 [==========>] Loss 0.18852722606425648  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.18701702899127548  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.18310199304195923  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.10636664511754855  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.2144567293994863  - accuracy: 0.75\n",
      "At: 47 [==========>] Loss 0.2507637377773795  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.17427866664255776  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.1385773697409053  - accuracy: 0.8125\n",
      "At: 50 [==========>] Loss 0.2166394959630359  - accuracy: 0.75\n",
      "At: 51 [==========>] Loss 0.20446997350050944  - accuracy: 0.71875\n",
      "At: 52 [==========>] Loss 0.2815477190841832  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.17381009984214182  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.18159538868302877  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.26391328217760524  - accuracy: 0.6875\n",
      "At: 56 [==========>] Loss 0.14150556616899396  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.16052434979544603  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.16649265350709988  - accuracy: 0.8125\n",
      "At: 59 [==========>] Loss 0.23262504208403428  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.22527101415250939  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.2774348276581101  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.2144306660622208  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.1853145534797136  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.20765912996416136  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.2441784668197293  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.2229674656789738  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.2690232661869824  - accuracy: 0.65625\n",
      "At: 68 [==========>] Loss 0.13223402653997976  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.14317471427335524  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.2275473956154631  - accuracy: 0.71875\n",
      "At: 71 [==========>] Loss 0.17875777834982653  - accuracy: 0.8125\n",
      "At: 72 [==========>] Loss 0.1770968183682184  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.17599748490427758  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.24556234670643368  - accuracy: 0.75\n",
      "At: 75 [==========>] Loss 0.22479818330563678  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.3381506446440574  - accuracy: 0.59375\n",
      "At: 77 [==========>] Loss 0.2535948893535493  - accuracy: 0.6875\n",
      "At: 78 [==========>] Loss 0.14336056236751965  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.167837723701912  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.2503749259618093  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.20167283274165343  - accuracy: 0.75\n",
      "At: 82 [==========>] Loss 0.21084431332698766  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.1960794552153559  - accuracy: 0.78125\n",
      "At: 84 [==========>] Loss 0.1874574323183763  - accuracy: 0.78125\n",
      "At: 85 [==========>] Loss 0.20763146406895405  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.16560890913970866  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.14887842593485773  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.3249687225180091  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.2261360876761791  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.24507749214886987  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.1866248293055403  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.09734277458654073  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.16077099745102907  - accuracy: 0.78125\n",
      "At: 94 [==========>] Loss 0.15228444244584466  - accuracy: 0.84375\n",
      "At: 95 [==========>] Loss 0.16893470946253084  - accuracy: 0.8125\n",
      "At: 96 [==========>] Loss 0.146559257094107  - accuracy: 0.84375\n",
      "At: 97 [==========>] Loss 0.06662673747756753  - accuracy: 0.9375\n",
      "At: 98 [==========>] Loss 0.31172516089693825  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.1761525807283232  - accuracy: 0.8125\n",
      "At: 100 [==========>] Loss 0.11377965760512636  - accuracy: 0.84375\n",
      "At: 101 [==========>] Loss 0.15885584744205966  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.21846900088461343  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.17807314206559952  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.1615926435585563  - accuracy: 0.78125\n",
      "At: 105 [==========>] Loss 0.195755700863372  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.24809283053925885  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.2368454433373085  - accuracy: 0.6875\n",
      "At: 108 [==========>] Loss 0.28679453029583235  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.07091415671424664  - accuracy: 0.9375\n",
      "At: 110 [==========>] Loss 0.22599192823268655  - accuracy: 0.71875\n",
      "At: 111 [==========>] Loss 0.08386798415894994  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.18592021775300477  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.2002472901035507  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.16660884271676973  - accuracy: 0.84375\n",
      "At: 115 [==========>] Loss 0.18544880980131911  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.1913827686288493  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.17159275907254215  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.3137348989311254  - accuracy: 0.625\n",
      "At: 119 [==========>] Loss 0.14282925200598862  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.1930589642315077  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.1344231018985565  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.19606525439484285  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.24645069956267385  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.25879770247165174  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.18552090546740158  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.27742694997345063  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.2076466790658819  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.2882210137889458  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.15431261440659438  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.21575992537394334  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.22443423196463183  - accuracy: 0.71875\n",
      "At: 132 [==========>] Loss 0.2309751879189702  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.22303010817961186  - accuracy: 0.6875\n",
      "At: 134 [==========>] Loss 0.174724057628669  - accuracy: 0.78125\n",
      "At: 135 [==========>] Loss 0.21307494964936624  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.18832849213350877  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.0988307681586803  - accuracy: 0.875\n",
      "At: 138 [==========>] Loss 0.1950452845234868  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.13669632168696955  - accuracy: 0.8125\n",
      "At: 140 [==========>] Loss 0.1260290642996728  - accuracy: 0.875\n",
      "At: 141 [==========>] Loss 0.31067301548815796  - accuracy: 0.5625\n",
      "At: 142 [==========>] Loss 0.19428807002425413  - accuracy: 0.8125\n",
      "At: 143 [==========>] Loss 0.17710275696983163  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.11570486001328378  - accuracy: 0.84375\n",
      "At: 145 [==========>] Loss 0.12069711102735944  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.16808980424024672  - accuracy: 0.75\n",
      "At: 147 [==========>] Loss 0.2061495322249792  - accuracy: 0.71875\n",
      "At: 148 [==========>] Loss 0.16930864628105366  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.18673929214949625  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.16922034517899007  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.18387590018114286  - accuracy: 0.75\n",
      "At: 152 [==========>] Loss 0.2006528040444535  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.2128986961464034  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.17750257352822843  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.18618557233741534  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.14719929380715918  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.24052922175519986  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.14973474693387823  - accuracy: 0.84375\n",
      "At: 159 [==========>] Loss 0.12118050513977954  - accuracy: 0.875\n",
      "At: 160 [==========>] Loss 0.1566233185385436  - accuracy: 0.78125\n",
      "At: 161 [==========>] Loss 0.1069489997541348  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.22399271836394685  - accuracy: 0.75\n",
      "At: 163 [==========>] Loss 0.18349142082919226  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.17851878216405098  - accuracy: 0.8125\n",
      "At: 165 [==========>] Loss 0.2097069796249661  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.18194877787791305  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.17337523881621608  - accuracy: 0.8125\n",
      "At: 168 [==========>] Loss 0.19949110356176555  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.14875143394239845  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.15443528298650527  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.21444132868394197  - accuracy: 0.71875\n",
      "At: 172 [==========>] Loss 0.15974803166286144  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.24074461436314015  - accuracy: 0.625\n",
      "At: 174 [==========>] Loss 0.2022280356027937  - accuracy: 0.71875\n",
      "At: 175 [==========>] Loss 0.17580569415008485  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.17333291738368817  - accuracy: 0.8125\n",
      "At: 177 [==========>] Loss 0.12946156577192502  - accuracy: 0.84375\n",
      "At: 178 [==========>] Loss 0.18921091267109522  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.13598859230492671  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.16815474261253716  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.04268114930907098  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.1976207354345242  - accuracy: 0.6875\n",
      "At: 183 [==========>] Loss 0.14956474274045584  - accuracy: 0.84375\n",
      "At: 184 [==========>] Loss 0.17039655787149843  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.08627366123546931  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.18011977620844435  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.17007439101162736  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.22421741996109792  - accuracy: 0.65625\n",
      "At: 189 [==========>] Loss 0.2198247003269156  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.15994459896667004  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.31963022458112916  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.12802283456350425  - accuracy: 0.8125\n",
      "At: 193 [==========>] Loss 0.21456763113426364  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.17548611028635186  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.1689722781119779  - accuracy: 0.75\n",
      "At: 196 [==========>] Loss 0.16796724049604506  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.1535899699836545  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.13727092845306593  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.13353049450790427  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.1996473772473885  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.12673884071965147  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.08541556071985257  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.17109022587625117  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.2399377471026201  - accuracy: 0.71875\n",
      "At: 205 [==========>] Loss 0.1478318465069981  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.1042922802787128  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.10223381305295651  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.2253877200972327  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.14717408887826744  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.12381798308127354  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.16523122099914855  - accuracy: 0.78125\n",
      "At: 212 [==========>] Loss 0.2044541267301347  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.1837159741304825  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.20350189211465153  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.1464116184385924  - accuracy: 0.875\n",
      "At: 216 [==========>] Loss 0.21577107428176845  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.2658637456531313  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.17277815366089824  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.21102054758508768  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.19538324630273085  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.16884758526927623  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.11647972799153275  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.270881673641505  - accuracy: 0.59375\n",
      "At: 224 [==========>] Loss 0.17861672169652582  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.09502142406484401  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.20106343079639283  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.15309444378500617  - accuracy: 0.78125\n",
      "At: 228 [==========>] Loss 0.17174559089028316  - accuracy: 0.71875\n",
      "At: 229 [==========>] Loss 0.19459671618862315  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.18480420516650736  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.20306707366408705  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.2745363252379661  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.19630415757319827  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.1263736563006189  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.19253878252122225  - accuracy: 0.75\n",
      "At: 236 [==========>] Loss 0.198202247707999  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.14541775141325444  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.13543931334565004  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.1409563704395915  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.22467875497898918  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.09442502226499386  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.16291826917954644  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.11737457265576128  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.16161648187177774  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.16810158369221379  - accuracy: 0.75\n",
      "At: 246 [==========>] Loss 0.12883658971975798  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.2025778626149491  - accuracy: 0.75\n",
      "At: 248 [==========>] Loss 0.09595090525682215  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.11733587071239383  - accuracy: 0.875\n",
      "At: 250 [==========>] Loss 0.21060033193691496  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.19692176928610478  - accuracy: 0.6875\n",
      "At: 252 [==========>] Loss 0.12212778900847802  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.17817970170566286  - accuracy: 0.8125\n",
      "At: 254 [==========>] Loss 0.11381266001186194  - accuracy: 0.875\n",
      "At: 255 [==========>] Loss 0.1610134931243234  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.21705326329704652  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.135374383158467  - accuracy: 0.8125\n",
      "At: 258 [==========>] Loss 0.1979961685937304  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.15086778014286753  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.17805722892956038  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.10512348962069075  - accuracy: 0.875\n",
      "At: 262 [==========>] Loss 0.18157006328329423  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.10132523033430335  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.13110592979380178  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.21611715436034767  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.29035637251741786  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1443571674336162  - accuracy: 0.78125\n",
      "At: 268 [==========>] Loss 0.21302100633753512  - accuracy: 0.65625\n",
      "At: 269 [==========>] Loss 0.13489108624669716  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.27524916841521624  - accuracy: 0.5625\n",
      "At: 271 [==========>] Loss 0.17373581087587597  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.09844107674723626  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.1652546115031701  - accuracy: 0.71875\n",
      "At: 274 [==========>] Loss 0.18026783453695702  - accuracy: 0.75\n",
      "At: 275 [==========>] Loss 0.10002624060208604  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.24094093550958728  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.14350436544178363  - accuracy: 0.8125\n",
      "At: 278 [==========>] Loss 0.12444619804922549  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.17129247280010548  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.1800927194422148  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.15166890040281758  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.23394769582987557  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.1490481623763768  - accuracy: 0.78125\n",
      "At: 284 [==========>] Loss 0.11113736039500356  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.15617853872478582  - accuracy: 0.78125\n",
      "At: 286 [==========>] Loss 0.12408208945853835  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.17256644165344576  - accuracy: 0.78125\n",
      "At: 288 [==========>] Loss 0.11574607336863704  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.13226786144932506  - accuracy: 0.8125\n",
      "At: 290 [==========>] Loss 0.12731107810405942  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.14438081019952131  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.16930907644960225  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.19935354771435726  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.1563739566686974  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.21649686243732283  - accuracy: 0.75\n",
      "At: 296 [==========>] Loss 0.14007988617210207  - accuracy: 0.84375\n",
      "At: 297 [==========>] Loss 0.17352446757501322  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.14118128034821992  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.17645867034653767  - accuracy: 0.78125\n",
      "At: 300 [==========>] Loss 0.1688206447779904  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.18532538495680329  - accuracy: 0.75\n",
      "At: 302 [==========>] Loss 0.11102106655530383  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.1274540586533038  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.19645282434438482  - accuracy: 0.78125\n",
      "At: 305 [==========>] Loss 0.26510919876184574  - accuracy: 0.65625\n",
      "At: 306 [==========>] Loss 0.1381950305253286  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.2726547779885776  - accuracy: 0.625\n",
      "At: 308 [==========>] Loss 0.18119564758261525  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.10112106518117425  - accuracy: 0.875\n",
      "At: 310 [==========>] Loss 0.2304061723653377  - accuracy: 0.6875\n",
      "At: 311 [==========>] Loss 0.09861003147547338  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.14939074153167953  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.1260363628980942  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.20947860823893055  - accuracy: 0.75\n",
      "At: 315 [==========>] Loss 0.1368303841781915  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.24064204882446244  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.2893154611428217  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.14920118620722833  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.10318524526834554  - accuracy: 0.90625\n",
      "At: 320 [==========>] Loss 0.20499599156441817  - accuracy: 0.75\n",
      "At: 321 [==========>] Loss 0.20902449706616172  - accuracy: 0.71875\n",
      "At: 322 [==========>] Loss 0.11628579036771639  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.10907341872690604  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.15569332072757175  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.11749087890657504  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.1951847400394079  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.10634195747117348  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.12541148961820892  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.14909283835738757  - accuracy: 0.8125\n",
      "At: 330 [==========>] Loss 0.17133575781438182  - accuracy: 0.75\n",
      "At: 331 [==========>] Loss 0.20341903665262828  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.29393293515761765  - accuracy: 0.5625\n",
      "At: 333 [==========>] Loss 0.16545591761324147  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.12736907706041178  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.13610820524402753  - accuracy: 0.78125\n",
      "At: 336 [==========>] Loss 0.12997416443675444  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.22125692664760083  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.12627726373643047  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.1755102450365592  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.14475509970216804  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.14059024571447598  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.17185027674145362  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.24193307189548227  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.21550074881007886  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.17359000191731028  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.1322709558086762  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.11454597469515564  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.13224721800947675  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.17111254931000178  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.10365113199056532  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.2907702987273811  - accuracy: 0.59375\n",
      "At: 352 [==========>] Loss 0.1390924121856957  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.16025423588506663  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.17435086337666206  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.10811737141892566  - accuracy: 0.90625\n",
      "At: 356 [==========>] Loss 0.18762856288487306  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.13202050201173776  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.13960399157814501  - accuracy: 0.875\n",
      "At: 359 [==========>] Loss 0.07800313923006977  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.12394388607809963  - accuracy: 0.8125\n",
      "At: 361 [==========>] Loss 0.09214700119836003  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.17695591922619563  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.1130518509715486  - accuracy: 0.90625\n",
      "At: 364 [==========>] Loss 0.21287414416471928  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.13504412042637243  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.2141684145102669  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.1875875154000115  - accuracy: 0.6875\n",
      "At: 368 [==========>] Loss 0.20851945265671581  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.16512752723798235  - accuracy: 0.78125\n",
      "At: 370 [==========>] Loss 0.1979198730974515  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.0883175839090153  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.1341474329216924  - accuracy: 0.8125\n",
      "At: 373 [==========>] Loss 0.20671644487804952  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.0645703323958462  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1457302474706541  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.08313021370583278  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.21088689041203315  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.14108680357693337  - accuracy: 0.78125\n",
      "At: 379 [==========>] Loss 0.13783806844646745  - accuracy: 0.78125\n",
      "At: 380 [==========>] Loss 0.15979010037571256  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.18116623263414275  - accuracy: 0.78125\n",
      "At: 382 [==========>] Loss 0.10238844736113742  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.23924727872064477  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.18669462973012432  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.16725680566929635  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.22348066763041236  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.0934340702188077  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.18868869534552024  - accuracy: 0.6875\n",
      "At: 389 [==========>] Loss 0.21762808418999163  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.1251261845853064  - accuracy: 0.8125\n",
      "At: 391 [==========>] Loss 0.11496285153668188  - accuracy: 0.78125\n",
      "At: 392 [==========>] Loss 0.14485767435730332  - accuracy: 0.84375\n",
      "At: 393 [==========>] Loss 0.2407084326073728  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.08307696282026966  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.213192591221261  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.1725631792820671  - accuracy: 0.75\n",
      "At: 397 [==========>] Loss 0.16539214400940266  - accuracy: 0.78125\n",
      "At: 398 [==========>] Loss 0.17319662366359437  - accuracy: 0.75\n",
      "At: 399 [==========>] Loss 0.21874043241071878  - accuracy: 0.65625\n",
      "At: 400 [==========>] Loss 0.1753704266340646  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.12803359382087948  - accuracy: 0.8125\n",
      "At: 402 [==========>] Loss 0.1214051172296089  - accuracy: 0.78125\n",
      "At: 403 [==========>] Loss 0.050217699389562664  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.10511936367699765  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.16727920581458053  - accuracy: 0.78125\n",
      "At: 406 [==========>] Loss 0.14538431128147322  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.19038849851090875  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.20347071780931897  - accuracy: 0.75\n",
      "At: 409 [==========>] Loss 0.2265520621906143  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.12879347809588554  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10931864468177367  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.13151570390184047  - accuracy: 0.78125\n",
      "At: 413 [==========>] Loss 0.1330904180534951  - accuracy: 0.78125\n",
      "At: 414 [==========>] Loss 0.17088255452860773  - accuracy: 0.78125\n",
      "At: 415 [==========>] Loss 0.13922214214821374  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.17756614529690934  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.19564592637149963  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.15581674308411192  - accuracy: 0.75\n",
      "At: 419 [==========>] Loss 0.13771211012669093  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.14287067785867738  - accuracy: 0.8125\n",
      "At: 421 [==========>] Loss 0.12473831359468332  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.13647700020034584  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.144871617979307  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.2271481680753646  - accuracy: 0.6875\n",
      "At: 425 [==========>] Loss 0.20816990976651123  - accuracy: 0.6875\n",
      "At: 426 [==========>] Loss 0.1467842437717166  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.15129864970737328  - accuracy: 0.8125\n",
      "At: 428 [==========>] Loss 0.23237001559075662  - accuracy: 0.71875\n",
      "At: 429 [==========>] Loss 0.1744046397247741  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.1310597126560442  - accuracy: 0.8125\n",
      "At: 431 [==========>] Loss 0.1261829256594443  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.14656508381026573  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.10321690780194735  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.12630542050751675  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.19030095425530702  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.15367712969247221  - accuracy: 0.8125\n",
      "At: 437 [==========>] Loss 0.11348417406380301  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.13487113290894526  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.09967557498533122  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.10147498017396676  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.19349574045468312  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.23511954803634916  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.17607742442901547  - accuracy: 0.75\n",
      "At: 444 [==========>] Loss 0.13833514481703585  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.13776210288253324  - accuracy: 0.8125\n",
      "At: 446 [==========>] Loss 0.22119669435321299  - accuracy: 0.6875\n",
      "At: 447 [==========>] Loss 0.15063711057557322  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.13733937152114745  - accuracy: 0.8125\n",
      "At: 449 [==========>] Loss 0.08092375537722657  - accuracy: 0.875\n",
      "At: 450 [==========>] Loss 0.16156871605925077  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.1589775219930984  - accuracy: 0.8125\n",
      "At: 452 [==========>] Loss 0.14323471603007773  - accuracy: 0.75\n",
      "At: 453 [==========>] Loss 0.1391242002893327  - accuracy: 0.84375\n",
      "At: 454 [==========>] Loss 0.20524134731864363  - accuracy: 0.65625\n",
      "At: 455 [==========>] Loss 0.17818482456992413  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.15556753047691102  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.14874571852602034  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.1240122415986463  - accuracy: 0.875\n",
      "At: 459 [==========>] Loss 0.23069156312581504  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.10567337870613724  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.22350695367556914  - accuracy: 0.65625\n",
      "At: 462 [==========>] Loss 0.19050789339794194  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.1960480178009223  - accuracy: 0.75\n",
      "At: 464 [==========>] Loss 0.17722025013962173  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.18976122476351154  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.0862101192597636  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.1881783719363192  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.1071112837686305  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.13661067216670986  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.11311751467593253  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.16324379445975323  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.15315981303322815  - accuracy: 0.75\n",
      "At: 473 [==========>] Loss 0.14989950411743574  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.12893574604307195  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.13153909144425457  - accuracy: 0.84375\n",
      "At: 476 [==========>] Loss 0.18028451069199333  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.13043397152185338  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.13901215680326262  - accuracy: 0.78125\n",
      "At: 479 [==========>] Loss 0.17948036399261263  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.18409462442450925  - accuracy: 0.6875\n",
      "At: 481 [==========>] Loss 0.121380428353175  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.07987979106273158  - accuracy: 0.9375\n",
      "At: 483 [==========>] Loss 0.08552683561896576  - accuracy: 0.90625\n",
      "At: 484 [==========>] Loss 0.09659355675159866  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.10484349804496965  - accuracy: 0.90625\n",
      "At: 486 [==========>] Loss 0.19566400212158253  - accuracy: 0.6875\n",
      "At: 487 [==========>] Loss 0.17187866570617538  - accuracy: 0.78125\n",
      "At: 488 [==========>] Loss 0.1225253206239903  - accuracy: 0.84375\n",
      "At: 489 [==========>] Loss 0.1709191148298173  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.15943949954614806  - accuracy: 0.75\n",
      "At: 491 [==========>] Loss 0.17390899176728591  - accuracy: 0.78125\n",
      "At: 492 [==========>] Loss 0.21272207405223992  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.15200553496248018  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.18209527234645556  - accuracy: 0.71875\n",
      "At: 495 [==========>] Loss 0.12238131977557723  - accuracy: 0.875\n",
      "At: 496 [==========>] Loss 0.16960850814424644  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.16143850806385096  - accuracy: 0.8125\n",
      "At: 498 [==========>] Loss 0.08116287379696424  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.18807667116670929  - accuracy: 0.75\n",
      "At: 500 [==========>] Loss 0.13141298443778496  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.17337962916742472  - accuracy: 0.71875\n",
      "At: 502 [==========>] Loss 0.1374475973593974  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.12459516751231947  - accuracy: 0.8125\n",
      "At: 504 [==========>] Loss 0.11501129268138752  - accuracy: 0.875\n",
      "At: 505 [==========>] Loss 0.18630855159943932  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.2189580232587174  - accuracy: 0.625\n",
      "At: 507 [==========>] Loss 0.12137319057940846  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.09488720139755466  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.16202731934585146  - accuracy: 0.75\n",
      "At: 510 [==========>] Loss 0.19028287173895297  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.11781812914716148  - accuracy: 0.875\n",
      "At: 512 [==========>] Loss 0.18911431711230808  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.2535639926335239  - accuracy: 0.59375\n",
      "At: 514 [==========>] Loss 0.12451452349972832  - accuracy: 0.84375\n",
      "At: 515 [==========>] Loss 0.13647132361022815  - accuracy: 0.8125\n",
      "At: 516 [==========>] Loss 0.18028910339678142  - accuracy: 0.6875\n",
      "At: 517 [==========>] Loss 0.13283577987530515  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.178080599803156  - accuracy: 0.71875\n",
      "At: 519 [==========>] Loss 0.12266913798174259  - accuracy: 0.875\n",
      "At: 520 [==========>] Loss 0.10693321188841495  - accuracy: 0.84375\n",
      "At: 521 [==========>] Loss 0.15483681895762363  - accuracy: 0.78125\n",
      "At: 522 [==========>] Loss 0.1508986755534961  - accuracy: 0.78125\n",
      "At: 523 [==========>] Loss 0.15115261702919439  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.08743958586987946  - accuracy: 0.90625\n",
      "At: 525 [==========>] Loss 0.1563680178990385  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.1539214053740347  - accuracy: 0.78125\n",
      "At: 527 [==========>] Loss 0.2506025280599392  - accuracy: 0.59375\n",
      "At: 528 [==========>] Loss 0.2182779296279614  - accuracy: 0.65625\n",
      "At: 529 [==========>] Loss 0.11834341753479083  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.1861924758982605  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.15381858237909957  - accuracy: 0.8125\n",
      "At: 532 [==========>] Loss 0.13520380022875428  - accuracy: 0.78125\n",
      "At: 533 [==========>] Loss 0.09024291314212307  - accuracy: 0.90625\n",
      "At: 534 [==========>] Loss 0.13357293789030944  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.12152140305656292  - accuracy: 0.78125\n",
      "At: 536 [==========>] Loss 0.1641761186960945  - accuracy: 0.8125\n",
      "At: 537 [==========>] Loss 0.10302244652444484  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.16172185797095973  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.12272583538404072  - accuracy: 0.84375\n",
      "At: 540 [==========>] Loss 0.23653575371959806  - accuracy: 0.625\n",
      "At: 541 [==========>] Loss 0.17956643882805964  - accuracy: 0.75\n",
      "At: 542 [==========>] Loss 0.15082379155835463  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.16698844017104034  - accuracy: 0.71875\n",
      "At: 544 [==========>] Loss 0.22101137899479537  - accuracy: 0.71875\n",
      "At: 545 [==========>] Loss 0.08914246967452591  - accuracy: 0.90625\n",
      "At: 546 [==========>] Loss 0.19117426383714325  - accuracy: 0.71875\n",
      "At: 547 [==========>] Loss 0.13196836285392616  - accuracy: 0.8125\n",
      "At: 548 [==========>] Loss 0.1313343360668377  - accuracy: 0.78125\n",
      "At: 549 [==========>] Loss 0.1489499512884096  - accuracy: 0.75\n",
      "At: 550 [==========>] Loss 0.11026464715210009  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.11362162333372917  - accuracy: 0.78125\n",
      "At: 552 [==========>] Loss 0.13961792117036514  - accuracy: 0.78125\n",
      "At: 553 [==========>] Loss 0.1333358187800586  - accuracy: 0.78125\n",
      "At: 554 [==========>] Loss 0.08804380777434812  - accuracy: 0.90625\n",
      "At: 555 [==========>] Loss 0.13496553089100843  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.14507080068524952  - accuracy: 0.75\n",
      "At: 557 [==========>] Loss 0.12476982867350693  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.18156708876603253  - accuracy: 0.75\n",
      "At: 559 [==========>] Loss 0.188122096465961  - accuracy: 0.71875\n",
      "At: 560 [==========>] Loss 0.1318326390907893  - accuracy: 0.8125\n",
      "At: 561 [==========>] Loss 0.12220241958573981  - accuracy: 0.875\n",
      "At: 562 [==========>] Loss 0.0614801971437786  - accuracy: 0.96875\n",
      "At: 563 [==========>] Loss 0.12179280985252008  - accuracy: 0.8125\n",
      "At: 564 [==========>] Loss 0.17411964887673015  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.11634587387324033  - accuracy: 0.8125\n",
      "At: 566 [==========>] Loss 0.1594869692321179  - accuracy: 0.8125\n",
      "At: 567 [==========>] Loss 0.16698556473497297  - accuracy: 0.78125\n",
      "At: 568 [==========>] Loss 0.23172419075814626  - accuracy: 0.59375\n",
      "At: 569 [==========>] Loss 0.16743157186123986  - accuracy: 0.6875\n",
      "At: 570 [==========>] Loss 0.10689899394221096  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.14872609925324717  - accuracy: 0.78125\n",
      "At: 572 [==========>] Loss 0.13884676205196012  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.07943162731715411  - accuracy: 0.90625\n",
      "At: 574 [==========>] Loss 0.1589877108300411  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.13253862729964788  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.10099577587059488  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.18395744923966711  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.180019953354922  - accuracy: 0.75\n",
      "At: 579 [==========>] Loss 0.09447178636506211  - accuracy: 0.84375\n",
      "At: 580 [==========>] Loss 0.1252450845897237  - accuracy: 0.875\n",
      "At: 581 [==========>] Loss 0.16218925277118162  - accuracy: 0.6875\n",
      "At: 582 [==========>] Loss 0.16595526417265785  - accuracy: 0.75\n",
      "At: 583 [==========>] Loss 0.15766935821954317  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.0926308658877514  - accuracy: 0.90625\n",
      "At: 585 [==========>] Loss 0.12999605082009824  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.05285296254642589  - accuracy: 0.9375\n",
      "At: 587 [==========>] Loss 0.14075696126493767  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.14897545292899536  - accuracy: 0.8125\n",
      "At: 589 [==========>] Loss 0.15297382802204595  - accuracy: 0.8125\n",
      "At: 590 [==========>] Loss 0.0805037332978359  - accuracy: 0.9375\n",
      "At: 591 [==========>] Loss 0.14280314699732696  - accuracy: 0.8125\n",
      "At: 592 [==========>] Loss 0.11690284896040135  - accuracy: 0.875\n",
      "At: 593 [==========>] Loss 0.19525027768989914  - accuracy: 0.71875\n",
      "At: 594 [==========>] Loss 0.10410850030645027  - accuracy: 0.875\n",
      "At: 595 [==========>] Loss 0.15155560633663653  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.1285915988356829  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.1871564547802737  - accuracy: 0.78125\n",
      "At: 598 [==========>] Loss 0.1344140105875149  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.15857337423089635  - accuracy: 0.6875\n",
      "At: 600 [==========>] Loss 0.07929225178631039  - accuracy: 0.9375\n",
      "At: 601 [==========>] Loss 0.12661751530029053  - accuracy: 0.75\n",
      "At: 602 [==========>] Loss 0.16067416816440386  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.1505600423053664  - accuracy: 0.75\n",
      "At: 604 [==========>] Loss 0.20489993012462357  - accuracy: 0.75\n",
      "At: 605 [==========>] Loss 0.1230267907235143  - accuracy: 0.84375\n",
      "At: 606 [==========>] Loss 0.1512712794244198  - accuracy: 0.78125\n",
      "At: 607 [==========>] Loss 0.17197733981318655  - accuracy: 0.75\n",
      "At: 608 [==========>] Loss 0.13446527288542998  - accuracy: 0.84375\n",
      "At: 609 [==========>] Loss 0.12687375873565554  - accuracy: 0.875\n",
      "At: 610 [==========>] Loss 0.1552223807633843  - accuracy: 0.8125\n",
      "At: 611 [==========>] Loss 0.1175392583370607  - accuracy: 0.8125\n",
      "At: 612 [==========>] Loss 0.12532914057725855  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.16580196597379943  - accuracy: 0.75\n",
      "At: 614 [==========>] Loss 0.12665612415188482  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.15001449565632094  - accuracy: 0.78125\n",
      "At: 616 [==========>] Loss 0.2142100419356431  - accuracy: 0.6875\n",
      "At: 617 [==========>] Loss 0.13832934209767922  - accuracy: 0.75\n",
      "At: 618 [==========>] Loss 0.2106656600950022  - accuracy: 0.6875\n",
      "At: 619 [==========>] Loss 0.15815207795486697  - accuracy: 0.8125\n",
      "At: 620 [==========>] Loss 0.1361207926396345  - accuracy: 0.8125\n",
      "At: 621 [==========>] Loss 0.07673282179943278  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.21147794918825571  - accuracy: 0.6875\n",
      "At: 623 [==========>] Loss 0.12974024385288785  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.10694925106402264  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.14226192004665225  - accuracy: 0.78125\n",
      "At: 626 [==========>] Loss 0.12825318322332324  - accuracy: 0.78125\n",
      "At: 627 [==========>] Loss 0.1327592193647641  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.0999329098312321  - accuracy: 0.875\n",
      "At: 629 [==========>] Loss 0.23361461422548987  - accuracy: 0.625\n",
      "At: 630 [==========>] Loss 0.19270879805390848  - accuracy: 0.625\n",
      "At: 631 [==========>] Loss 0.18399734694197117  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.13876018269507612  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.1460914720480292  - accuracy: 0.78125\n",
      "At: 634 [==========>] Loss 0.10351611996064762  - accuracy: 0.90625\n",
      "At: 635 [==========>] Loss 0.1293482926716101  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.12948072613541123  - accuracy: 0.84375\n",
      "At: 637 [==========>] Loss 0.13686093955311077  - accuracy: 0.78125\n",
      "At: 638 [==========>] Loss 0.13404168038894676  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.13024882612891447  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.2366573798591831  - accuracy: 0.65625\n",
      "At: 641 [==========>] Loss 0.1325830358690256  - accuracy: 0.84375\n",
      "At: 642 [==========>] Loss 0.17039568213694828  - accuracy: 0.78125\n",
      "At: 643 [==========>] Loss 0.1416198311865944  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.07946608896879162  - accuracy: 0.9375\n",
      "At: 645 [==========>] Loss 0.14082541888334324  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.11707816214493647  - accuracy: 0.875\n",
      "At: 647 [==========>] Loss 0.18276810322210102  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.19600807318207628  - accuracy: 0.71875\n",
      "At: 649 [==========>] Loss 0.17395080169604366  - accuracy: 0.71875\n",
      "At: 650 [==========>] Loss 0.09874864727232793  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.15703249265295122  - accuracy: 0.8125\n",
      "At: 652 [==========>] Loss 0.1005228790308946  - accuracy: 0.90625\n",
      "At: 653 [==========>] Loss 0.1141002063915664  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.09670782021621421  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.11773341774921342  - accuracy: 0.84375\n",
      "At: 656 [==========>] Loss 0.12754690551734374  - accuracy: 0.875\n",
      "At: 657 [==========>] Loss 0.12739163366379494  - accuracy: 0.875\n",
      "At: 658 [==========>] Loss 0.14585028660572852  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.11748118203093258  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.12910770689273537  - accuracy: 0.75\n",
      "At: 661 [==========>] Loss 0.16352251026112086  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.08798316215479242  - accuracy: 0.875\n",
      "At: 663 [==========>] Loss 0.09384187638845079  - accuracy: 0.90625\n",
      "At: 664 [==========>] Loss 0.15422682030282683  - accuracy: 0.78125\n",
      "At: 665 [==========>] Loss 0.19693897320727544  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.196514799879347  - accuracy: 0.75\n",
      "At: 667 [==========>] Loss 0.12939911982256477  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.1550989272770183  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.17725848802636474  - accuracy: 0.78125\n",
      "At: 670 [==========>] Loss 0.18174503673090045  - accuracy: 0.75\n",
      "At: 671 [==========>] Loss 0.10671778004649458  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.15704812174387278  - accuracy: 0.78125\n",
      "At: 673 [==========>] Loss 0.08074403666774951  - accuracy: 0.875\n",
      "At: 674 [==========>] Loss 0.11876501922574766  - accuracy: 0.78125\n",
      "At: 675 [==========>] Loss 0.11014739265501378  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.15569623741255317  - accuracy: 0.84375\n",
      "At: 677 [==========>] Loss 0.13904213846756014  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.10350932589231854  - accuracy: 0.9375\n",
      "At: 679 [==========>] Loss 0.09955649304642519  - accuracy: 0.8125\n",
      "At: 680 [==========>] Loss 0.11734087149567397  - accuracy: 0.875\n",
      "At: 681 [==========>] Loss 0.13946780532620787  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.13280752072635432  - accuracy: 0.84375\n",
      "At: 683 [==========>] Loss 0.16077864637882394  - accuracy: 0.71875\n",
      "At: 684 [==========>] Loss 0.09927844931249741  - accuracy: 0.90625\n",
      "At: 685 [==========>] Loss 0.15276116120900485  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.09330518577755878  - accuracy: 0.9375\n",
      "At: 687 [==========>] Loss 0.07841685086367184  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.09964411646443261  - accuracy: 0.84375\n",
      "At: 689 [==========>] Loss 0.15959689870794116  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.15154255712585268  - accuracy: 0.75\n",
      "At: 691 [==========>] Loss 0.09813096029652951  - accuracy: 0.875\n",
      "At: 692 [==========>] Loss 0.10516328792891025  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.15069549977905122  - accuracy: 0.75\n",
      "At: 694 [==========>] Loss 0.1905603361636641  - accuracy: 0.6875\n",
      "At: 695 [==========>] Loss 0.16309215893484408  - accuracy: 0.75\n",
      "At: 696 [==========>] Loss 0.13137825043830037  - accuracy: 0.875\n",
      "At: 697 [==========>] Loss 0.18616951905015172  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.12257136453059592  - accuracy: 0.90625\n",
      "At: 699 [==========>] Loss 0.13281606047343114  - accuracy: 0.8125\n",
      "At: 700 [==========>] Loss 0.09775446030424946  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.1447718124681914  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.06908761770837996  - accuracy: 0.9375\n",
      "At: 703 [==========>] Loss 0.16762921402886533  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.17032632288005062  - accuracy: 0.78125\n",
      "At: 705 [==========>] Loss 0.19485689703196862  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.13762081963606682  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.11406104554720428  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.11053289890134246  - accuracy: 0.875\n",
      "At: 709 [==========>] Loss 0.16984113445253543  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.17852276833412073  - accuracy: 0.6875\n",
      "At: 711 [==========>] Loss 0.2124856156785257  - accuracy: 0.6875\n",
      "At: 712 [==========>] Loss 0.17917753297752165  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.1866484332172838  - accuracy: 0.78125\n",
      "At: 714 [==========>] Loss 0.21476928096446057  - accuracy: 0.59375\n",
      "At: 715 [==========>] Loss 0.07918608589383425  - accuracy: 0.90625\n",
      "At: 716 [==========>] Loss 0.1246014346904003  - accuracy: 0.8125\n",
      "At: 717 [==========>] Loss 0.0957753757007426  - accuracy: 0.875\n",
      "At: 718 [==========>] Loss 0.19221639679988917  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.0921272281954402  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.1310014592477696  - accuracy: 0.875\n",
      "At: 721 [==========>] Loss 0.10273557114155263  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.14299834687333834  - accuracy: 0.8125\n",
      "At: 723 [==========>] Loss 0.13927907684459945  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.08495738104532474  - accuracy: 0.84375\n",
      "At: 725 [==========>] Loss 0.15702143361961207  - accuracy: 0.6875\n",
      "At: 726 [==========>] Loss 0.178865346345287  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.12995287687115287  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.18571069745145333  - accuracy: 0.6875\n",
      "At: 729 [==========>] Loss 0.20314375213338437  - accuracy: 0.71875\n",
      "At: 730 [==========>] Loss 0.13561523940261716  - accuracy: 0.8125\n",
      "At: 731 [==========>] Loss 0.15260417460720316  - accuracy: 0.78125\n",
      "At: 732 [==========>] Loss 0.1738969766814205  - accuracy: 0.78125\n",
      "At: 733 [==========>] Loss 0.12547668062708495  - accuracy: 0.84375\n",
      "At: 734 [==========>] Loss 0.1924964406181735  - accuracy: 0.6875\n",
      "At: 735 [==========>] Loss 0.1484420584159288  - accuracy: 0.78125\n",
      "At: 736 [==========>] Loss 0.11273838196184706  - accuracy: 0.8125\n",
      "At: 737 [==========>] Loss 0.12994135271989207  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.11388664063523454  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.08555012046598304  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.14102396049833366  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.11022921457082345  - accuracy: 0.875\n",
      "At: 742 [==========>] Loss 0.1304631078912922  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.16976629430957949  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.14783795311279968  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.19254762540658127  - accuracy: 0.71875\n",
      "At: 746 [==========>] Loss 0.1155340338521377  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.12403264159732255  - accuracy: 0.8125\n",
      "At: 748 [==========>] Loss 0.14109422642992017  - accuracy: 0.8125\n",
      "At: 749 [==========>] Loss 0.21445469347027551  - accuracy: 0.625\n",
      "At: 750 [==========>] Loss 0.15219799757162336  - accuracy: 0.8125\n",
      "At: 751 [==========>] Loss 0.17983998969835152  - accuracy: 0.75\n",
      "At: 752 [==========>] Loss 0.06704808639587213  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.19616596849859044  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.15946176687437902  - accuracy: 0.71875\n",
      "At: 755 [==========>] Loss 0.08192689930573069  - accuracy: 0.9375\n",
      "At: 756 [==========>] Loss 0.20261108237476042  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.09038795155301634  - accuracy: 0.84375\n",
      "At: 758 [==========>] Loss 0.13704896127677801  - accuracy: 0.8125\n",
      "At: 759 [==========>] Loss 0.08273301253367454  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.19876418187070805  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.1238558168514454  - accuracy: 0.875\n",
      "At: 762 [==========>] Loss 0.13134335985261553  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.18680892617204814  - accuracy: 0.65625\n",
      "At: 764 [==========>] Loss 0.1320013892117063  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.18897842042383897  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.13607808450539005  - accuracy: 0.78125\n",
      "At: 767 [==========>] Loss 0.15375564093487312  - accuracy: 0.75\n",
      "At: 768 [==========>] Loss 0.14853409862903014  - accuracy: 0.75\n",
      "At: 769 [==========>] Loss 0.12184053654523827  - accuracy: 0.875\n",
      "At: 770 [==========>] Loss 0.12818465334192056  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.17831356660193565  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.13137311677874258  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.07807526353698441  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.14734586609339362  - accuracy: 0.875\n",
      "At: 775 [==========>] Loss 0.21228197415380232  - accuracy: 0.65625\n",
      "At: 776 [==========>] Loss 0.18151038007838344  - accuracy: 0.71875\n",
      "At: 777 [==========>] Loss 0.08846542227753626  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.16854461266200227  - accuracy: 0.8125\n",
      "At: 779 [==========>] Loss 0.11219648098617878  - accuracy: 0.84375\n",
      "At: 780 [==========>] Loss 0.08445640281904038  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.14335970753657096  - accuracy: 0.84375\n",
      "At: 782 [==========>] Loss 0.163432494196035  - accuracy: 0.8125\n",
      "At: 783 [==========>] Loss 0.1709546745444961  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.14521869486913946  - accuracy: 0.78125\n",
      "At: 785 [==========>] Loss 0.21549732901309404  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.13821837872243486  - accuracy: 0.78125\n",
      "At: 787 [==========>] Loss 0.18521865332179505  - accuracy: 0.75\n",
      "At: 788 [==========>] Loss 0.08774518466029632  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.13531567411386225  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.13551496676477048  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.18209353001955764  - accuracy: 0.65625\n",
      "At: 792 [==========>] Loss 0.16855219417663725  - accuracy: 0.75\n",
      "At: 793 [==========>] Loss 0.12627562765574044  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.12829840100326328  - accuracy: 0.78125\n",
      "At: 795 [==========>] Loss 0.11556877049255661  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.11148584409999926  - accuracy: 0.8125\n",
      "At: 797 [==========>] Loss 0.17852951042025766  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.16446538626540746  - accuracy: 0.78125\n",
      "At: 799 [==========>] Loss 0.05873981569607151  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.1660626912679855  - accuracy: 0.6875\n",
      "At: 801 [==========>] Loss 0.09138737771002392  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.1918621707075785  - accuracy: 0.6875\n",
      "At: 803 [==========>] Loss 0.167005118938382  - accuracy: 0.78125\n",
      "At: 804 [==========>] Loss 0.11061555547719552  - accuracy: 0.84375\n",
      "At: 805 [==========>] Loss 0.17278793778589663  - accuracy: 0.78125\n",
      "At: 806 [==========>] Loss 0.10173111627611506  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.10930789101033986  - accuracy: 0.8125\n",
      "At: 808 [==========>] Loss 0.11537859497630601  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.09911638657032855  - accuracy: 0.90625\n",
      "At: 810 [==========>] Loss 0.1626633185699981  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.11826796073946558  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.134788178295975  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.204641748266925  - accuracy: 0.71875\n",
      "At: 814 [==========>] Loss 0.19581896150418548  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.13409078628364823  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.14811740467393741  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.05536584252581614  - accuracy: 0.9375\n",
      "At: 818 [==========>] Loss 0.10986463448048664  - accuracy: 0.90625\n",
      "At: 819 [==========>] Loss 0.11128146640145599  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.13410553509707313  - accuracy: 0.78125\n",
      "At: 821 [==========>] Loss 0.14741698004180503  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.16979965506818007  - accuracy: 0.71875\n",
      "At: 823 [==========>] Loss 0.1505257190978707  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.13703740663728006  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.18079293859657936  - accuracy: 0.75\n",
      "At: 826 [==========>] Loss 0.13363513288074244  - accuracy: 0.84375\n",
      "At: 827 [==========>] Loss 0.074563408680554  - accuracy: 0.9375\n",
      "At: 828 [==========>] Loss 0.05079417094255395  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.08445293942263427  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.07262298648760124  - accuracy: 0.875\n",
      "At: 831 [==========>] Loss 0.07047005044758273  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.12045963288192299  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.2031073981313642  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.10858562803006207  - accuracy: 0.875\n",
      "At: 835 [==========>] Loss 0.0819896225645852  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.11752725912481929  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.10207963396897618  - accuracy: 0.84375\n",
      "At: 838 [==========>] Loss 0.10876148253383949  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.11585372743588902  - accuracy: 0.875\n",
      "At: 840 [==========>] Loss 0.11074113719288513  - accuracy: 0.875\n",
      "At: 841 [==========>] Loss 0.04708054303655362  - accuracy: 0.96875\n",
      "At: 842 [==========>] Loss 0.08312358349889537  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.1440850334611582  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.1275086588746567  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.1513266098854602  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.12758823581868367  - accuracy: 0.78125\n",
      "At: 847 [==========>] Loss 0.10749991361636647  - accuracy: 0.8125\n",
      "At: 848 [==========>] Loss 0.16487108567928155  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.16177445748928818  - accuracy: 0.75\n",
      "At: 850 [==========>] Loss 0.08436390724625317  - accuracy: 0.90625\n",
      "At: 851 [==========>] Loss 0.08617419263149415  - accuracy: 0.90625\n",
      "At: 852 [==========>] Loss 0.1414092155015845  - accuracy: 0.78125\n",
      "At: 853 [==========>] Loss 0.18412012231606298  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.22595242143291103  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.07637867566324807  - accuracy: 0.90625\n",
      "At: 856 [==========>] Loss 0.08605142080855675  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.06346419962041086  - accuracy: 0.96875\n",
      "At: 858 [==========>] Loss 0.27519873848175314  - accuracy: 0.625\n",
      "At: 859 [==========>] Loss 0.11826270581118352  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.12988119686020627  - accuracy: 0.8125\n",
      "At: 861 [==========>] Loss 0.10031070311445335  - accuracy: 0.90625\n",
      "At: 862 [==========>] Loss 0.09843419918449571  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.174055770589926  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.17997397343750654  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.21409894003624796  - accuracy: 0.71875\n",
      "At: 866 [==========>] Loss 0.1485792998903336  - accuracy: 0.8125\n",
      "At: 867 [==========>] Loss 0.08427799877200427  - accuracy: 0.84375\n",
      "At: 868 [==========>] Loss 0.15339292449804753  - accuracy: 0.78125\n",
      "At: 869 [==========>] Loss 0.15430071983590643  - accuracy: 0.78125\n",
      "At: 870 [==========>] Loss 0.12895120859342962  - accuracy: 0.875\n",
      "At: 871 [==========>] Loss 0.06176947247287998  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.06721216618052443  - accuracy: 0.9375\n",
      "At: 873 [==========>] Loss 0.17847764071166472  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.11371289814011731  - accuracy: 0.875\n",
      "At: 875 [==========>] Loss 0.13347295953648303  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.10356353824267782  - accuracy: 0.875\n",
      "At: 877 [==========>] Loss 0.1443561974317872  - accuracy: 0.84375\n",
      "At: 878 [==========>] Loss 0.08163336120009754  - accuracy: 0.875\n",
      "At: 879 [==========>] Loss 0.131540046311016  - accuracy: 0.84375\n",
      "At: 880 [==========>] Loss 0.10645800444268944  - accuracy: 0.875\n",
      "At: 881 [==========>] Loss 0.13076340706998169  - accuracy: 0.875\n",
      "At: 882 [==========>] Loss 0.11332538913898402  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.14178590318401046  - accuracy: 0.78125\n",
      "At: 884 [==========>] Loss 0.13808639981401688  - accuracy: 0.75\n",
      "At: 885 [==========>] Loss 0.13227339626940748  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.10640289620577514  - accuracy: 0.8125\n",
      "At: 887 [==========>] Loss 0.14791717512736924  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.1512037884298331  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.11680534072286473  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.1526967657999989  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.09424650659145828  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.12185926760079566  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.16099678801859557  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.13949207495021504  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.13135416348589035  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.11135217688727415  - accuracy: 0.8125\n",
      "At: 897 [==========>] Loss 0.12738911507088405  - accuracy: 0.84375\n",
      "At: 898 [==========>] Loss 0.17558797345398178  - accuracy: 0.71875\n",
      "At: 899 [==========>] Loss 0.13392625818883047  - accuracy: 0.78125\n",
      "At: 900 [==========>] Loss 0.1600784806912046  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.17704841321710196  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.09675803945722979  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.13683338934975603  - accuracy: 0.84375\n",
      "At: 904 [==========>] Loss 0.09692949464182893  - accuracy: 0.90625\n",
      "At: 905 [==========>] Loss 0.09363210655311552  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.08493556757361607  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.16406651324865928  - accuracy: 0.84375\n",
      "At: 908 [==========>] Loss 0.11519508272359268  - accuracy: 0.875\n",
      "At: 909 [==========>] Loss 0.13339888476059417  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.117845783511807  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.15425554723977894  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.1788238699768886  - accuracy: 0.75\n",
      "At: 913 [==========>] Loss 0.09967043593272215  - accuracy: 0.875\n",
      "At: 914 [==========>] Loss 0.15199038536556136  - accuracy: 0.78125\n",
      "At: 915 [==========>] Loss 0.12407832028392916  - accuracy: 0.84375\n",
      "At: 916 [==========>] Loss 0.18177463260340004  - accuracy: 0.75\n",
      "At: 917 [==========>] Loss 0.1795368038872373  - accuracy: 0.71875\n",
      "At: 918 [==========>] Loss 0.14802752421384663  - accuracy: 0.75\n",
      "At: 919 [==========>] Loss 0.11234761084779964  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.11609914971232088  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.1462565147255041  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.12246198274714098  - accuracy: 0.84375\n",
      "At: 923 [==========>] Loss 0.12035873657436538  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.19551260605695414  - accuracy: 0.78125\n",
      "At: 925 [==========>] Loss 0.13864273417695394  - accuracy: 0.84375\n",
      "At: 926 [==========>] Loss 0.11303407675032195  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.11578189797716149  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.134367026950571  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.134982445203991  - accuracy: 0.84375\n",
      "At: 930 [==========>] Loss 0.10482723700294208  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.16482195598258828  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.0978222339653775  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.10192928840970399  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.12499982385673346  - accuracy: 0.875\n",
      "At: 935 [==========>] Loss 0.05103322086715941  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.15307640227105823  - accuracy: 0.8125\n",
      "At: 937 [==========>] Loss 0.15380731910057382  - accuracy: 0.875\n",
      "At: 938 [==========>] Loss 0.12165683159411395  - accuracy: 0.875\n",
      "At: 939 [==========>] Loss 0.08870580231203765  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.20326607683727263  - accuracy: 0.75\n",
      "At: 941 [==========>] Loss 0.11571110275986662  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.17524120363043721  - accuracy: 0.78125\n",
      "At: 943 [==========>] Loss 0.12282324141348783  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.13968175788049986  - accuracy: 0.78125\n",
      "At: 945 [==========>] Loss 0.10631994380199823  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.10701013865800466  - accuracy: 0.875\n",
      "At: 947 [==========>] Loss 0.12193480859069353  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.1507205262517587  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.06667073464907233  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.12568297237958403  - accuracy: 0.84375\n",
      "At: 951 [==========>] Loss 0.09947810599394191  - accuracy: 0.875\n",
      "At: 952 [==========>] Loss 0.08561274763067142  - accuracy: 0.9375\n",
      "At: 953 [==========>] Loss 0.041590072817745655  - accuracy: 0.96875\n",
      "At: 954 [==========>] Loss 0.09899162963543376  - accuracy: 0.875\n",
      "At: 955 [==========>] Loss 0.14109272645858395  - accuracy: 0.8125\n",
      "At: 956 [==========>] Loss 0.05337989345059927  - accuracy: 0.96875\n",
      "At: 957 [==========>] Loss 0.09868673324743316  - accuracy: 0.9375\n",
      "At: 958 [==========>] Loss 0.0712413674258645  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.15755172132965095  - accuracy: 0.78125\n",
      "At: 960 [==========>] Loss 0.10765367135811826  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.12472415594979804  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.1002567433213511  - accuracy: 0.90625\n",
      "At: 963 [==========>] Loss 0.12040450688408366  - accuracy: 0.8125\n",
      "At: 964 [==========>] Loss 0.1426308698636415  - accuracy: 0.8125\n",
      "At: 965 [==========>] Loss 0.139316771714273  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.12940675795440346  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.12971615749295984  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.14230526275556815  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.13232951454363523  - accuracy: 0.75\n",
      "At: 970 [==========>] Loss 0.09967984675568053  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.121557136174957  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.06683028351626547  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.1300891027157709  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.08841437388761822  - accuracy: 0.875\n",
      "At: 975 [==========>] Loss 0.11289831393079393  - accuracy: 0.90625\n",
      "At: 976 [==========>] Loss 0.10314140540437272  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.10693287288600578  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.1601776752691187  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.09461042218290548  - accuracy: 0.90625\n",
      "At: 980 [==========>] Loss 0.14970359827070606  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.19804982192125387  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.06322802719322668  - accuracy: 0.96875\n",
      "At: 983 [==========>] Loss 0.10999174365037823  - accuracy: 0.875\n",
      "At: 984 [==========>] Loss 0.08786634623331308  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.14838940147571578  - accuracy: 0.8125\n",
      "At: 986 [==========>] Loss 0.1471489010740976  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.13065510851992004  - accuracy: 0.75\n",
      "At: 988 [==========>] Loss 0.11420024633399024  - accuracy: 0.8125\n",
      "At: 989 [==========>] Loss 0.1372781988381953  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.1451331022345237  - accuracy: 0.8125\n",
      "At: 991 [==========>] Loss 0.164185082389542  - accuracy: 0.75\n",
      "At: 992 [==========>] Loss 0.20754745031915417  - accuracy: 0.75\n",
      "At: 993 [==========>] Loss 0.14356659104906697  - accuracy: 0.875\n",
      "At: 994 [==========>] Loss 0.1403275937606707  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.1621222646469503  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.06510355530819387  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.14510766022611393  - accuracy: 0.75\n",
      "At: 998 [==========>] Loss 0.11217872028569152  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.16353716792978806  - accuracy: 0.71875\n",
      "At: 1000 [==========>] Loss 0.23320399019300825  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.1253636090935706  - accuracy: 0.8125\n",
      "At: 1002 [==========>] Loss 0.1925292324897933  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.10559046059165916  - accuracy: 0.84375\n",
      "At: 1004 [==========>] Loss 0.11180436130949223  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.09145706940430413  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.10159424647118014  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.14500213552620284  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.20214365237865883  - accuracy: 0.6875\n",
      "At: 1009 [==========>] Loss 0.1354492351918692  - accuracy: 0.8125\n",
      "At: 1010 [==========>] Loss 0.11545823794818041  - accuracy: 0.84375\n",
      "At: 1011 [==========>] Loss 0.13352619541018476  - accuracy: 0.84375\n",
      "At: 1012 [==========>] Loss 0.08611314112869498  - accuracy: 0.90625\n",
      "At: 1013 [==========>] Loss 0.07498171238594015  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.06791120725543695  - accuracy: 0.96875\n",
      "At: 1015 [==========>] Loss 0.19740606211685557  - accuracy: 0.71875\n",
      "At: 1016 [==========>] Loss 0.1329533950169503  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.1681108497597214  - accuracy: 0.71875\n",
      "At: 1018 [==========>] Loss 0.13714572343188447  - accuracy: 0.8125\n",
      "At: 1019 [==========>] Loss 0.19167002152490692  - accuracy: 0.78125\n",
      "At: 1020 [==========>] Loss 0.10777348049258659  - accuracy: 0.875\n",
      "At: 1021 [==========>] Loss 0.12434449842065873  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.10008064145900994  - accuracy: 0.875\n",
      "At: 1023 [==========>] Loss 0.1495950348473575  - accuracy: 0.75\n",
      "At: 1024 [==========>] Loss 0.18358542450659268  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.17644659348373337  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.12341780903145723  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.10471976070458247  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.1851896064743686  - accuracy: 0.75\n",
      "At: 1029 [==========>] Loss 0.09925917265811417  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.10296826438380813  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.17435900148336436  - accuracy: 0.71875\n",
      "At: 1032 [==========>] Loss 0.1485785097621319  - accuracy: 0.78125\n",
      "At: 1033 [==========>] Loss 0.1530468182346444  - accuracy: 0.78125\n",
      "At: 1034 [==========>] Loss 0.07394748597979724  - accuracy: 0.96875\n",
      "At: 1035 [==========>] Loss 0.09752407582825721  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.15237735095892274  - accuracy: 0.78125\n",
      "At: 1037 [==========>] Loss 0.12325827066177537  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.10719735194955998  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.10686208833681285  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.1278068732218907  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.16863810044969255  - accuracy: 0.71875\n",
      "At: 1042 [==========>] Loss 0.12381520670653054  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.17861434762646572  - accuracy: 0.75\n",
      "At: 1044 [==========>] Loss 0.11355482322844333  - accuracy: 0.84375\n",
      "At: 1045 [==========>] Loss 0.13033614657551523  - accuracy: 0.84375\n",
      "At: 1046 [==========>] Loss 0.16406186265210676  - accuracy: 0.78125\n",
      "At: 1047 [==========>] Loss 0.1313251699320468  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.1555532659142047  - accuracy: 0.8125\n",
      "At: 1049 [==========>] Loss 0.1496424356782613  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.14351289628638914  - accuracy: 0.84375\n",
      "At: 1051 [==========>] Loss 0.09661373497416462  - accuracy: 0.875\n",
      "At: 1052 [==========>] Loss 0.12009253388551205  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.09600943667906509  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.11677055611202719  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.1913986983068675  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.12325683570479025  - accuracy: 0.84375\n",
      "At: 1057 [==========>] Loss 0.12284023474911379  - accuracy: 0.84375\n",
      "At: 1058 [==========>] Loss 0.0514037397261542  - accuracy: 0.9375\n",
      "At: 1059 [==========>] Loss 0.07628295229337928  - accuracy: 0.90625\n",
      "At: 1060 [==========>] Loss 0.07539460268230945  - accuracy: 0.90625\n",
      "At: 1061 [==========>] Loss 0.09069677541089753  - accuracy: 0.84375\n",
      "At: 1062 [==========>] Loss 0.1754462340761025  - accuracy: 0.78125\n",
      "At: 1063 [==========>] Loss 0.12686103439516122  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.13784313648719493  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.09740709621549415  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.09878574297129994  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.11561582899844441  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.11475379939024047  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.1238079943004779  - accuracy: 0.84375\n",
      "At: 1070 [==========>] Loss 0.14168860828996452  - accuracy: 0.84375\n",
      "At: 1071 [==========>] Loss 0.08599289463735035  - accuracy: 0.9375\n",
      "At: 1072 [==========>] Loss 0.12653638965260597  - accuracy: 0.8125\n",
      "At: 1073 [==========>] Loss 0.14684302970992422  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.1652366539211138  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.12286053182815092  - accuracy: 0.84375\n",
      "At: 1076 [==========>] Loss 0.11455736932726827  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.06492124864749663  - accuracy: 0.9375\n",
      "At: 1078 [==========>] Loss 0.0805296367112548  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.1466425530365369  - accuracy: 0.78125\n",
      "At: 1080 [==========>] Loss 0.13713103071672672  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.12201104978703528  - accuracy: 0.8125\n",
      "At: 1082 [==========>] Loss 0.09904445018297543  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.10688312856455247  - accuracy: 0.875\n",
      "At: 1084 [==========>] Loss 0.0948194308283037  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.11524458012626067  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.13722746475330827  - accuracy: 0.78125\n",
      "At: 1087 [==========>] Loss 0.12238835750619646  - accuracy: 0.90625\n",
      "At: 1088 [==========>] Loss 0.16219992673672057  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.08714223492678697  - accuracy: 0.9375\n",
      "At: 1090 [==========>] Loss 0.06886391272811139  - accuracy: 0.9375\n",
      "At: 1091 [==========>] Loss 0.20616711350467604  - accuracy: 0.71875\n",
      "At: 1092 [==========>] Loss 0.1135242493502745  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.15489308264734142  - accuracy: 0.84375\n",
      "At: 1094 [==========>] Loss 0.1648220504511061  - accuracy: 0.78125\n",
      "At: 1095 [==========>] Loss 0.14233537163365928  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.09258375111444618  - accuracy: 0.84375\n",
      "At: 1097 [==========>] Loss 0.08559751437448265  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.13440495055041887  - accuracy: 0.84375\n",
      "At: 1099 [==========>] Loss 0.14870882197172985  - accuracy: 0.78125\n",
      "At: 1100 [==========>] Loss 0.09421421678464112  - accuracy: 0.875\n",
      "At: 1101 [==========>] Loss 0.1073081824604179  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.10761297599868982  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.11192451009664572  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.08435376169864423  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.0831285225587593  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.08474959497958572  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.1902528323281324  - accuracy: 0.71875\n",
      "At: 1108 [==========>] Loss 0.0829954783875989  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.07281847484856022  - accuracy: 0.90625\n",
      "At: 1110 [==========>] Loss 0.11969301825721954  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.17170742101321834  - accuracy: 0.78125\n",
      "At: 1112 [==========>] Loss 0.13549603041018474  - accuracy: 0.90625\n",
      "At: 1113 [==========>] Loss 0.14081723517268396  - accuracy: 0.75\n",
      "At: 1114 [==========>] Loss 0.0965294503219389  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.12125037874157879  - accuracy: 0.84375\n",
      "At: 1116 [==========>] Loss 0.11654324292565098  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.06094850807853823  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.09000667310225181  - accuracy: 0.90625\n",
      "At: 1119 [==========>] Loss 0.13668844775353758  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.05961939835557131  - accuracy: 0.96875\n",
      "At: 1121 [==========>] Loss 0.1014569526406176  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.08379453497407024  - accuracy: 0.84375\n",
      "At: 1123 [==========>] Loss 0.1074096908441012  - accuracy: 0.875\n",
      "At: 1124 [==========>] Loss 0.12175355156506595  - accuracy: 0.84375\n",
      "At: 1125 [==========>] Loss 0.15056175373093422  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.11055541941449415  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.12643953658406315  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.0741330818401724  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.15017838770181535  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.11088150176467615  - accuracy: 0.875\n",
      "At: 1131 [==========>] Loss 0.07797764124834246  - accuracy: 0.9375\n",
      "At: 1132 [==========>] Loss 0.09679834442277066  - accuracy: 0.8125\n",
      "At: 1133 [==========>] Loss 0.12607721400491884  - accuracy: 0.84375\n",
      "At: 1134 [==========>] Loss 0.07460227025256888  - accuracy: 0.90625\n",
      "At: 1135 [==========>] Loss 0.11860114545340163  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.15160938599565832  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.09970682914584314  - accuracy: 0.84375\n",
      "At: 1138 [==========>] Loss 0.08399608604992626  - accuracy: 0.90625\n",
      "At: 1139 [==========>] Loss 0.0934875617582106  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.16693007136314736  - accuracy: 0.75\n",
      "At: 1141 [==========>] Loss 0.1582220423801693  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.1262847322688822  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.08191277409727546  - accuracy: 0.875\n",
      "At: 1144 [==========>] Loss 0.12098568213823485  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.13340203467922623  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.11245140032211573  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.2385842955936181  - accuracy: 0.65625\n",
      "At: 1148 [==========>] Loss 0.07473279192503307  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.09421726277756315  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.14259633813319647  - accuracy: 0.71875\n",
      "At: 1151 [==========>] Loss 0.17740669843629536  - accuracy: 0.71875\n",
      "At: 1152 [==========>] Loss 0.09313484313234255  - accuracy: 0.9375\n",
      "At: 1153 [==========>] Loss 0.18337646287795478  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.11695928365157485  - accuracy: 0.8125\n",
      "At: 1155 [==========>] Loss 0.08906886142594556  - accuracy: 0.90625\n",
      "At: 1156 [==========>] Loss 0.15907048713684263  - accuracy: 0.78125\n",
      "At: 1157 [==========>] Loss 0.11525099087588997  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.17036079968705725  - accuracy: 0.75\n",
      "At: 1159 [==========>] Loss 0.07907069915480297  - accuracy: 0.9375\n",
      "At: 1160 [==========>] Loss 0.09166438763878249  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.07279399989832966  - accuracy: 0.90625\n",
      "At: 1162 [==========>] Loss 0.1234617285538043  - accuracy: 0.84375\n",
      "At: 1163 [==========>] Loss 0.1875064035170243  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.06559450760282151  - accuracy: 0.9375\n",
      "At: 1165 [==========>] Loss 0.1585161198844534  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.0955228404576705  - accuracy: 0.84375\n",
      "At: 1167 [==========>] Loss 0.12996150636804268  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.1136580842080768  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.09946285029626026  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.18756206150752003  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.07550795372401196  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.14314452843544861  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.1118884307900124  - accuracy: 0.875\n",
      "At: 1174 [==========>] Loss 0.15349692110065763  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.13450064026240768  - accuracy: 0.84375\n",
      "At: 1176 [==========>] Loss 0.09195921257473645  - accuracy: 0.90625\n",
      "At: 1177 [==========>] Loss 0.06787338262671547  - accuracy: 0.90625\n",
      "At: 1178 [==========>] Loss 0.1323727939447262  - accuracy: 0.8125\n",
      "At: 1179 [==========>] Loss 0.14153329972393636  - accuracy: 0.78125\n",
      "At: 1180 [==========>] Loss 0.1845731219296302  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.07511420776108763  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.08830046662489177  - accuracy: 0.84375\n",
      "At: 1183 [==========>] Loss 0.1591436910596422  - accuracy: 0.84375\n",
      "At: 1184 [==========>] Loss 0.11965807602959781  - accuracy: 0.84375\n",
      "At: 1185 [==========>] Loss 0.10493787724853254  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.14123668677022766  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.10687831073906617  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.06452109117741667  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.16841450770287697  - accuracy: 0.75\n",
      "At: 1190 [==========>] Loss 0.09812310479656829  - accuracy: 0.875\n",
      "At: 1191 [==========>] Loss 0.17536978411883647  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.09736880323433764  - accuracy: 0.84375\n",
      "At: 1193 [==========>] Loss 0.1418798418371533  - accuracy: 0.84375\n",
      "At: 1194 [==========>] Loss 0.11709381595721473  - accuracy: 0.875\n",
      "At: 1195 [==========>] Loss 0.13622536978229008  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.10175108246313658  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.10258076716295325  - accuracy: 0.8125\n",
      "At: 1198 [==========>] Loss 0.08041610977651834  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.1689550499588312  - accuracy: 0.75\n",
      "At: 1200 [==========>] Loss 0.05204526195494368  - accuracy: 0.96875\n",
      "At: 1201 [==========>] Loss 0.11144377004926609  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.15125563762243333  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.12990843388641649  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.12689038735578084  - accuracy: 0.84375\n",
      "At: 1205 [==========>] Loss 0.08102553706705265  - accuracy: 0.84375\n",
      "At: 1206 [==========>] Loss 0.10053479318021202  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.16654496055883977  - accuracy: 0.71875\n",
      "At: 1208 [==========>] Loss 0.09802701907167864  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.07377108719474207  - accuracy: 0.90625\n",
      "At: 1210 [==========>] Loss 0.1560400814389842  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.15969243433438834  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.11528867609745487  - accuracy: 0.78125\n",
      "At: 1213 [==========>] Loss 0.18770219232321467  - accuracy: 0.6875\n",
      "At: 1214 [==========>] Loss 0.15254423468025072  - accuracy: 0.8125\n",
      "At: 1215 [==========>] Loss 0.16247453216093016  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.09388146806475622  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.08005910420792853  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.10754199930261779  - accuracy: 0.84375\n",
      "At: 1219 [==========>] Loss 0.14700751806756002  - accuracy: 0.8125\n",
      "At: 1220 [==========>] Loss 0.10494808283085585  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.06290053259783535  - accuracy: 0.9375\n",
      "At: 1222 [==========>] Loss 0.17068456570511462  - accuracy: 0.78125\n",
      "At: 1223 [==========>] Loss 0.10423390018779874  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08078330524927015  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.08363972664763192  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.08214994477994646  - accuracy: 0.90625\n",
      "At: 1227 [==========>] Loss 0.1620121879341227  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.13436299231558485  - accuracy: 0.84375\n",
      "At: 1229 [==========>] Loss 0.14145070092653944  - accuracy: 0.78125\n",
      "At: 1230 [==========>] Loss 0.16118531570410122  - accuracy: 0.78125\n",
      "At: 1231 [==========>] Loss 0.15342540827009912  - accuracy: 0.78125\n",
      "At: 1232 [==========>] Loss 0.07629208891055478  - accuracy: 0.96875\n",
      "At: 1233 [==========>] Loss 0.11456094120381077  - accuracy: 0.875\n",
      "At: 1234 [==========>] Loss 0.15431162920121144  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.08320976639188196  - accuracy: 0.9375\n",
      "At: 1236 [==========>] Loss 0.15602648600842717  - accuracy: 0.75\n",
      "At: 1237 [==========>] Loss 0.06935868643430869  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.08782819380092885  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.15313294321779952  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.11221992808435263  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.11469628848866278  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.17974601670792528  - accuracy: 0.78125\n",
      "At: 1243 [==========>] Loss 0.10701767075905247  - accuracy: 0.875\n",
      "At: 1244 [==========>] Loss 0.16253053022548436  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.10179423256122114  - accuracy: 0.90625\n",
      "At: 1246 [==========>] Loss 0.0874870899198884  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.15955206198671928  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.1062394639831571  - accuracy: 0.78125\n",
      "At: 1249 [==========>] Loss 0.12315866767602603  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.12921609920923383  - accuracy: 0.78125\n",
      "At: 1251 [==========>] Loss 0.11188990674727992  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.08294687197958318  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.12001104574310477  - accuracy: 0.84375\n",
      "At: 1254 [==========>] Loss 0.16447479824544667  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.09694815728573059  - accuracy: 0.9375\n",
      "At: 1256 [==========>] Loss 0.10886963132784871  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.15481534063144464  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.07302327994818684  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.1303587740114041  - accuracy: 0.8125\n",
      "At: 1260 [==========>] Loss 0.09037995369482837  - accuracy: 0.90625\n",
      "At: 1261 [==========>] Loss 0.10809828027430393  - accuracy: 0.8125\n",
      "At: 1262 [==========>] Loss 0.1393034712162339  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.115471650005947  - accuracy: 0.84375\n",
      "At: 1264 [==========>] Loss 0.0754718109044036  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.12691881377978717  - accuracy: 0.78125\n",
      "At: 1266 [==========>] Loss 0.10498615312075879  - accuracy: 0.84375\n",
      "At: 1267 [==========>] Loss 0.09481711061508463  - accuracy: 0.90625\n",
      "At: 1268 [==========>] Loss 0.1467455634637486  - accuracy: 0.84375\n",
      "At: 1269 [==========>] Loss 0.11525298268935807  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.11601987618123921  - accuracy: 0.875\n",
      "At: 1271 [==========>] Loss 0.16491405489064115  - accuracy: 0.75\n",
      "At: 1272 [==========>] Loss 0.06846727136991287  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.18987105328982468  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.11918657349534376  - accuracy: 0.875\n",
      "At: 1275 [==========>] Loss 0.09418211647429492  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.11950412569268119  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.0829441376810126  - accuracy: 0.90625\n",
      "At: 1278 [==========>] Loss 0.14306368713838796  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.08962627438981689  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.09926345171497195  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.15041668433440827  - accuracy: 0.75\n",
      "At: 1282 [==========>] Loss 0.11067946677879267  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.12288964695953873  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.15250573054553954  - accuracy: 0.78125\n",
      "At: 1285 [==========>] Loss 0.07075128821158172  - accuracy: 0.9375\n",
      "At: 1286 [==========>] Loss 0.14579132039973822  - accuracy: 0.8125\n",
      "At: 1287 [==========>] Loss 0.10707871799632757  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.15230518364378243  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.14727221334787555  - accuracy: 0.75\n",
      "At: 1290 [==========>] Loss 0.15207898878378262  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.16412051798503424  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.1154823210102468  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.15280441822012888  - accuracy: 0.8125\n",
      "At: 1294 [==========>] Loss 0.16658478818299682  - accuracy: 0.75\n",
      "At: 1295 [==========>] Loss 0.15835340819719274  - accuracy: 0.8125\n",
      "At: 1296 [==========>] Loss 0.1777820999952007  - accuracy: 0.71875\n",
      "At: 1297 [==========>] Loss 0.14521667916179104  - accuracy: 0.8125\n",
      "At: 1298 [==========>] Loss 0.09290659782160962  - accuracy: 0.90625\n",
      "At: 1299 [==========>] Loss 0.12302697857516381  - accuracy: 0.8125\n",
      "At: 1300 [==========>] Loss 0.14546655850786483  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.12094300250195468  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.05959776900745132  - accuracy: 0.96875\n",
      "At: 1303 [==========>] Loss 0.10209529616926921  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.12391672918363111  - accuracy: 0.875\n",
      "At: 1305 [==========>] Loss 0.11623904291648907  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.0753526328753722  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.15733979137056178  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.05046695913637639  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.17650707989715708  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.15419162747679074  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.13991179181770802  - accuracy: 0.78125\n",
      "At: 1312 [==========>] Loss 0.056324128492457645  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.17021720290262995  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.07917352089671502  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.12481330254317805  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.12695079504603005  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.09635066473314308  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.12236052694785475  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.12589737402229212  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.1393803255771224  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.10902175736938577  - accuracy: 0.84375\n",
      "At: 1322 [==========>] Loss 0.12427757781357729  - accuracy: 0.84375\n",
      "At: 1323 [==========>] Loss 0.0949578699290261  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.1486849472022473  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.09408333799790891  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.07283038924905849  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.15706118944712144  - accuracy: 0.78125\n",
      "At: 1328 [==========>] Loss 0.07615705016762417  - accuracy: 0.9375\n",
      "At: 1329 [==========>] Loss 0.0787995803905651  - accuracy: 0.84375\n",
      "At: 1330 [==========>] Loss 0.11647109696413709  - accuracy: 0.8125\n",
      "At: 1331 [==========>] Loss 0.19006333644489803  - accuracy: 0.65625\n",
      "At: 1332 [==========>] Loss 0.13077078632573275  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.14276763551722002  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.08316944015925276  - accuracy: 0.90625\n",
      "At: 1335 [==========>] Loss 0.08794380509253347  - accuracy: 0.875\n",
      "At: 1336 [==========>] Loss 0.11550448375971117  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.14202496226432135  - accuracy: 0.875\n",
      "At: 1338 [==========>] Loss 0.13945753419003654  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.10842633893260693  - accuracy: 0.875\n",
      "At: 1340 [==========>] Loss 0.1436574130389939  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.09157619112684699  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.11074493740421623  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.19425933748825497  - accuracy: 0.75\n",
      "At: 1344 [==========>] Loss 0.20480300624849895  - accuracy: 0.65625\n",
      "At: 1345 [==========>] Loss 0.10927496635858741  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.07577388078361445  - accuracy: 0.9375\n",
      "At: 1347 [==========>] Loss 0.10118879904487632  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09896268114072856  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.15301408901877472  - accuracy: 0.75\n",
      "At: 1350 [==========>] Loss 0.11718418660334119  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.09778980208899812  - accuracy: 0.875\n",
      "At: 1352 [==========>] Loss 0.10451978871309987  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.1632942576384725  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.15480895722218696  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.08080890626202367  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.11648558060275005  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.12355040600563626  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.12044216328091219  - accuracy: 0.875\n",
      "At: 1359 [==========>] Loss 0.05575385049645913  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.15650788571546206  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.06765627361959733  - accuracy: 0.96875\n",
      "At: 1362 [==========>] Loss 0.12997238223997215  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.11571709915708325  - accuracy: 0.8125\n",
      "At: 1364 [==========>] Loss 0.15302715839767322  - accuracy: 0.75\n",
      "At: 1365 [==========>] Loss 0.13121251645651866  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.1417402323836006  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.08951754305818312  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.17529739665854105  - accuracy: 0.71875\n",
      "At: 1369 [==========>] Loss 0.08987403001131641  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.12233775774169259  - accuracy: 0.84375\n",
      "At: 1371 [==========>] Loss 0.17709558020172528  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.12055340863069755  - accuracy: 0.78125\n",
      "At: 1373 [==========>] Loss 0.11841322109631233  - accuracy: 0.84375\n",
      "At: 1374 [==========>] Loss 0.13207651484354244  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.1255642790078466  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.09851439530784487  - accuracy: 0.875\n",
      "At: 1377 [==========>] Loss 0.15313048263709464  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.10569668127306427  - accuracy: 0.875\n",
      "At: 1379 [==========>] Loss 0.14610758216015146  - accuracy: 0.8125\n",
      "At: 1380 [==========>] Loss 0.1529424264069041  - accuracy: 0.75\n",
      "At: 1381 [==========>] Loss 0.09339283181886975  - accuracy: 0.84375\n",
      "At: 1382 [==========>] Loss 0.12531230760195763  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.1114899456400747  - accuracy: 0.90625\n",
      "At: 1384 [==========>] Loss 0.10153039409832972  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.13955812262190231  - accuracy: 0.8125\n",
      "At: 1386 [==========>] Loss 0.15316040152084076  - accuracy: 0.8125\n",
      "At: 1387 [==========>] Loss 0.05697581907711211  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.15113908625619352  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.13783809273292252  - accuracy: 0.78125\n",
      "At: 1390 [==========>] Loss 0.1234421915692733  - accuracy: 0.84375\n",
      "At: 1391 [==========>] Loss 0.13107107445687133  - accuracy: 0.78125\n",
      "At: 1392 [==========>] Loss 0.07867807221886346  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.15658970916109657  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.0953354524530924  - accuracy: 0.90625\n",
      "At: 1395 [==========>] Loss 0.22763120492928476  - accuracy: 0.65625\n",
      "At: 1396 [==========>] Loss 0.048334757696434104  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.13289851241359027  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.10209660198214096  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.10647136046003822  - accuracy: 0.875\n",
      "At: 1400 [==========>] Loss 0.14285451885717865  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.07212589896078178  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.14757190294074468  - accuracy: 0.8125\n",
      "At: 1403 [==========>] Loss 0.1614413926797844  - accuracy: 0.6875\n",
      "At: 1404 [==========>] Loss 0.09534710135548821  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.10172664650762803  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.1587974857674245  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.09805720177513663  - accuracy: 0.90625\n",
      "At: 1408 [==========>] Loss 0.12591588634681644  - accuracy: 0.84375\n",
      "At: 1409 [==========>] Loss 0.031421249557378475  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.1523168896785309  - accuracy: 0.75\n",
      "At: 1411 [==========>] Loss 0.15569506449087067  - accuracy: 0.78125\n",
      "At: 1412 [==========>] Loss 0.1341277564307947  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.0845433014499267  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.16630528293855484  - accuracy: 0.8125\n",
      "At: 1415 [==========>] Loss 0.08216144717142015  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.14406140189416022  - accuracy: 0.75\n",
      "At: 1417 [==========>] Loss 0.1273111123460489  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.11309153382064095  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.11099383073223999  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.1060971928262758  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.10717712640382285  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.1173589056479617  - accuracy: 0.875\n",
      "At: 1423 [==========>] Loss 0.13815788642537097  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.12347348139198369  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.08138487204203719  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.10988708565573571  - accuracy: 0.90625\n",
      "At: 1427 [==========>] Loss 0.12199761727095919  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.11676830047238021  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.16901692034187393  - accuracy: 0.75\n",
      "At: 1430 [==========>] Loss 0.07897428942940468  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.11548216680583395  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.10270964950035999  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.1011875516997899  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.16526905741271716  - accuracy: 0.84375\n",
      "At: 1435 [==========>] Loss 0.13175531437401714  - accuracy: 0.875\n",
      "At: 1436 [==========>] Loss 0.06629760326470774  - accuracy: 0.9375\n",
      "At: 1437 [==========>] Loss 0.12170330913949129  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.15500870730499935  - accuracy: 0.78125\n",
      "At: 1439 [==========>] Loss 0.09549195799318987  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.10485801899837352  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.07189506758175357  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.10890904788363068  - accuracy: 0.875\n",
      "At: 1443 [==========>] Loss 0.14236774965080892  - accuracy: 0.78125\n",
      "At: 1444 [==========>] Loss 0.11866852930155238  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.17310285616999438  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.16958709367067415  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.14768282985826314  - accuracy: 0.8125\n",
      "At: 1448 [==========>] Loss 0.08218505114149934  - accuracy: 0.875\n",
      "At: 1449 [==========>] Loss 0.13791145814676503  - accuracy: 0.84375\n",
      "At: 1450 [==========>] Loss 0.13691358648315596  - accuracy: 0.78125\n",
      "At: 1451 [==========>] Loss 0.11257088578829941  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.0772275483219792  - accuracy: 0.9375\n",
      "At: 1453 [==========>] Loss 0.07082336734255304  - accuracy: 0.875\n",
      "At: 1454 [==========>] Loss 0.1552169691192085  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.10600123793383624  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.09656336543829334  - accuracy: 0.8125\n",
      "At: 1457 [==========>] Loss 0.07322589556101952  - accuracy: 0.90625\n",
      "At: 1458 [==========>] Loss 0.139096293343392  - accuracy: 0.84375\n",
      "At: 1459 [==========>] Loss 0.12239946652705722  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.16482588155944145  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.1153935688035234  - accuracy: 0.8125\n",
      "At: 1462 [==========>] Loss 0.15908041037048293  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.08345467215389178  - accuracy: 0.90625\n",
      "At: 1464 [==========>] Loss 0.17587838147464618  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.12035513667149278  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.08096142483664436  - accuracy: 0.9375\n",
      "At: 1467 [==========>] Loss 0.22557621329865926  - accuracy: 0.6875\n",
      "At: 1468 [==========>] Loss 0.18296806849752523  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.17320515600997116  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.14586549769326163  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.16499800776511306  - accuracy: 0.71875\n",
      "At: 1472 [==========>] Loss 0.08797727433420675  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.1204949945072768  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.20258194764832582  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.14961896817038417  - accuracy: 0.75\n",
      "At: 1476 [==========>] Loss 0.1108541298889393  - accuracy: 0.84375\n",
      "At: 1477 [==========>] Loss 0.08492001009802166  - accuracy: 0.90625\n",
      "At: 1478 [==========>] Loss 0.0915888379844965  - accuracy: 0.875\n",
      "At: 1479 [==========>] Loss 0.13673658677062833  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.10292259409373819  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.13674132394442137  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.1179499729777053  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.2186818455871241  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.15066972296861297  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.1538089814766583  - accuracy: 0.78125\n",
      "At: 1486 [==========>] Loss 0.11242934910363919  - accuracy: 0.84375\n",
      "At: 1487 [==========>] Loss 0.05436615826351919  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.12200778326187134  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.22247677311689312  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.11124439874965028  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.15657513821215407  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.11781419678070557  - accuracy: 0.84375\n",
      "At: 1493 [==========>] Loss 0.18008346753612897  - accuracy: 0.71875\n",
      "At: 1494 [==========>] Loss 0.15560581040898724  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.12832436410036557  - accuracy: 0.84375\n",
      "At: 1496 [==========>] Loss 0.10515346780625176  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.16836440375572867  - accuracy: 0.71875\n",
      "At: 1498 [==========>] Loss 0.13913847535661572  - accuracy: 0.84375\n",
      "At: 1499 [==========>] Loss 0.08678739752854957  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.09604650901764568  - accuracy: 0.90625\n",
      "At: 1501 [==========>] Loss 0.07984482216584643  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.11874204709706651  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.13901818488936432  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.13210568875763595  - accuracy: 0.8125\n",
      "At: 1505 [==========>] Loss 0.12385574546725846  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.1645521895233203  - accuracy: 0.8125\n",
      "At: 1507 [==========>] Loss 0.1280501055626868  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.21948599020956022  - accuracy: 0.6875\n",
      "At: 1509 [==========>] Loss 0.10565758302460349  - accuracy: 0.875\n",
      "At: 1510 [==========>] Loss 0.09755033477438305  - accuracy: 0.875\n",
      "At: 1511 [==========>] Loss 0.11987877816829831  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.09715656528214407  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.11441688416557719  - accuracy: 0.90625\n",
      "At: 1514 [==========>] Loss 0.12777798736603121  - accuracy: 0.84375\n",
      "At: 1515 [==========>] Loss 0.11867131156289337  - accuracy: 0.875\n",
      "At: 1516 [==========>] Loss 0.1223940934888192  - accuracy: 0.84375\n",
      "At: 1517 [==========>] Loss 0.14555753197043247  - accuracy: 0.84375\n",
      "At: 1518 [==========>] Loss 0.12524908362716922  - accuracy: 0.78125\n",
      "At: 1519 [==========>] Loss 0.16485724761135517  - accuracy: 0.78125\n",
      "At: 1520 [==========>] Loss 0.10015895900982597  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.07032241701791508  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.16229725045145194  - accuracy: 0.78125\n",
      "At: 1523 [==========>] Loss 0.09846285632580719  - accuracy: 0.90625\n",
      "At: 1524 [==========>] Loss 0.1697319846535703  - accuracy: 0.71875\n",
      "At: 1525 [==========>] Loss 0.13133586420861218  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.07369308095557814  - accuracy: 0.9375\n",
      "At: 1527 [==========>] Loss 0.13332428370587313  - accuracy: 0.84375\n",
      "At: 1528 [==========>] Loss 0.12256250151324927  - accuracy: 0.84375\n",
      "At: 1529 [==========>] Loss 0.06084898754059582  - accuracy: 0.9375\n",
      "At: 1530 [==========>] Loss 0.061530015798428504  - accuracy: 0.90625\n",
      "At: 1531 [==========>] Loss 0.097778400201245  - accuracy: 0.90625\n",
      "At: 1532 [==========>] Loss 0.16810665573492933  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.1510638861755182  - accuracy: 0.8125\n",
      "At: 1534 [==========>] Loss 0.1113478482776807  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.14119526237063912  - accuracy: 0.8125\n",
      "At: 1536 [==========>] Loss 0.14993700371487784  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10290754594214402  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.11870333551206715  - accuracy: 0.875\n",
      "At: 1539 [==========>] Loss 0.07416928524030869  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.13496365403573715  - accuracy: 0.78125\n",
      "At: 1541 [==========>] Loss 0.11795753457238972  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.06911226093021611  - accuracy: 0.9375\n",
      "At: 1543 [==========>] Loss 0.14487555973440458  - accuracy: 0.78125\n",
      "At: 1544 [==========>] Loss 0.14112663809906717  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.2202726031310201  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.12840811098179755  - accuracy: 0.78125\n",
      "At: 1547 [==========>] Loss 0.15935967121316086  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.1360441680628933  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.1338215769938883  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.06868228447376148  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.16014614126731802  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.08301252860113817  - accuracy: 0.875\n",
      "At: 1553 [==========>] Loss 0.08165013839423793  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.1381644835407268  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.1438096723135906  - accuracy: 0.8125\n",
      "At: 1556 [==========>] Loss 0.16602975860348823  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.0904103861964712  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.1282550008793972  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.08846958241288716  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.12292047166462683  - accuracy: 0.78125\n",
      "At: 1561 [==========>] Loss 0.1624770394775143  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.10875833230972591  - accuracy: 0.84375\n",
      "At: 1563 [==========>] Loss 0.09070759623962289  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.10839360678352752  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.11480647228277538  - accuracy: 0.875\n",
      "At: 1566 [==========>] Loss 0.12516726992173205  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.174091016085355  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.08313451195480184  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.10460286107682523  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.07395796711885215  - accuracy: 0.875\n",
      "At: 1571 [==========>] Loss 0.16030806047364585  - accuracy: 0.78125\n",
      "At: 1572 [==========>] Loss 0.1403634706206827  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.04138386210546509  - accuracy: 0.96875\n",
      "At: 1574 [==========>] Loss 0.10975687854100957  - accuracy: 0.875\n",
      "At: 1575 [==========>] Loss 0.11128883477673353  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.1158518938171042  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.07182323377567251  - accuracy: 0.9375\n",
      "At: 1578 [==========>] Loss 0.08087363210027637  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.08166463550996268  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.10822264636559983  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.10723180817849644  - accuracy: 0.875\n",
      "At: 1582 [==========>] Loss 0.17326652672558052  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.08259327889790634  - accuracy: 0.90625\n",
      "At: 1584 [==========>] Loss 0.10775147434807844  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.130914140978298  - accuracy: 0.78125\n",
      "At: 1586 [==========>] Loss 0.1378668614925094  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.07904616184334583  - accuracy: 0.90625\n",
      "At: 1588 [==========>] Loss 0.10555084543806527  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.10927923976505838  - accuracy: 0.8125\n",
      "At: 1590 [==========>] Loss 0.15254845584321258  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.09070339986407427  - accuracy: 0.90625\n",
      "At: 1592 [==========>] Loss 0.08428510908838882  - accuracy: 0.90625\n",
      "At: 1593 [==========>] Loss 0.1721299244685278  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.0972507598439263  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.11456812372924904  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.1811124835739444  - accuracy: 0.71875\n",
      "At: 1597 [==========>] Loss 0.150280287039354  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.16415327107806776  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.217893380191306  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.14325080084669994  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.06428954249932284  - accuracy: 0.96875\n",
      "At: 1602 [==========>] Loss 0.12556463124751238  - accuracy: 0.8125\n",
      "At: 1603 [==========>] Loss 0.1796547389785904  - accuracy: 0.78125\n",
      "At: 1604 [==========>] Loss 0.24359994747245434  - accuracy: 0.6875\n",
      "At: 1605 [==========>] Loss 0.08105581576672785  - accuracy: 0.90625\n",
      "At: 1606 [==========>] Loss 0.13471387558001607  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.17967549016970957  - accuracy: 0.75\n",
      "At: 1608 [==========>] Loss 0.1396389132316598  - accuracy: 0.8125\n",
      "At: 1609 [==========>] Loss 0.15618890738906338  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.17313366307779685  - accuracy: 0.84375\n",
      "At: 1611 [==========>] Loss 0.07298176847586313  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.06881208840853258  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.1317542971210724  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.13334970925069242  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.0769055253467905  - accuracy: 0.9375\n",
      "At: 1616 [==========>] Loss 0.12205752959202344  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.08737265584092188  - accuracy: 0.9375\n",
      "At: 1618 [==========>] Loss 0.11951319787637307  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.1806493703205173  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.09152205314206935  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.11259674291520108  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.15280811469513875  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.08203732347359413  - accuracy: 0.875\n",
      "At: 1624 [==========>] Loss 0.17178727600516985  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.15142458482658822  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.09069700538735748  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.0922288740160497  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.1741249921612182  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.14417503311101618  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.08460138669758885  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.09759082433587035  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.08095045130065989  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.07749380560875993  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.11310503049875206  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.23045084916534714  - accuracy: 0.6875\n",
      "At: 1636 [==========>] Loss 0.10248099180125322  - accuracy: 0.84375\n",
      "At: 1637 [==========>] Loss 0.07921255579756636  - accuracy: 0.84375\n",
      "At: 1638 [==========>] Loss 0.15208910717295193  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.1402433740224896  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.09543053494221931  - accuracy: 0.84375\n",
      "At: 1641 [==========>] Loss 0.08666940958644812  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.11929537806791747  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.09764037559602542  - accuracy: 0.90625\n",
      "At: 1644 [==========>] Loss 0.11593382287188396  - accuracy: 0.8125\n",
      "At: 1645 [==========>] Loss 0.05338191178491896  - accuracy: 0.9375\n",
      "At: 1646 [==========>] Loss 0.12919840360254353  - accuracy: 0.78125\n",
      "At: 1647 [==========>] Loss 0.1509944723155711  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1314527203130467  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.0805784059137973  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.11307880894666583  - accuracy: 0.8125\n",
      "At: 1651 [==========>] Loss 0.1259186070367368  - accuracy: 0.8125\n",
      "At: 1652 [==========>] Loss 0.09017354954664099  - accuracy: 0.84375\n",
      "At: 1653 [==========>] Loss 0.10363312935980085  - accuracy: 0.84375\n",
      "At: 1654 [==========>] Loss 0.045942731232199274  - accuracy: 0.96875\n",
      "At: 1655 [==========>] Loss 0.17141571668725783  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.11174341812271599  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.13255961872594618  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.1872963543372562  - accuracy: 0.71875\n",
      "At: 1659 [==========>] Loss 0.0923125092000488  - accuracy: 0.9375\n",
      "At: 1660 [==========>] Loss 0.033880070102897696  - accuracy: 1.0\n",
      "At: 1661 [==========>] Loss 0.0765108010528831  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.117083568852656  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16581067882950884  - accuracy: 0.78125\n",
      "At: 1664 [==========>] Loss 0.10772447410587739  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.08973998169432432  - accuracy: 0.9375\n",
      "At: 1666 [==========>] Loss 0.1132285357840245  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.16698147914504619  - accuracy: 0.71875\n",
      "At: 1668 [==========>] Loss 0.1474822388800232  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.15087222388043578  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.06757416419546212  - accuracy: 0.9375\n",
      "At: 1671 [==========>] Loss 0.11155729079206703  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.11029133553429729  - accuracy: 0.8125\n",
      "At: 1673 [==========>] Loss 0.07613305762949572  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.13976373036922374  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.205020336092184  - accuracy: 0.6875\n",
      "At: 1676 [==========>] Loss 0.21213784028814872  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.09391485941696089  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.15894335291250816  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.10028811874038829  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.143484692365829  - accuracy: 0.84375\n",
      "At: 1681 [==========>] Loss 0.13940572773632187  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.09096965579960331  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.15946274905150915  - accuracy: 0.84375\n",
      "At: 1684 [==========>] Loss 0.12515768893840837  - accuracy: 0.84375\n",
      "At: 1685 [==========>] Loss 0.0974974606851993  - accuracy: 0.875\n",
      "At: 1686 [==========>] Loss 0.11258255207079057  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.18492623133561276  - accuracy: 0.6875\n",
      "At: 1688 [==========>] Loss 0.060494969236877826  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.12413813541186854  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.11154970141401846  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.10411674454661671  - accuracy: 0.8125\n",
      "At: 1692 [==========>] Loss 0.16289629901619018  - accuracy: 0.71875\n",
      "At: 1693 [==========>] Loss 0.08966098372162057  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.10569770260169653  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.10753032858791928  - accuracy: 0.84375\n",
      "At: 1696 [==========>] Loss 0.18331630855871522  - accuracy: 0.6875\n",
      "At: 1697 [==========>] Loss 0.09783329265912391  - accuracy: 0.90625\n",
      "At: 1698 [==========>] Loss 0.08340689490811283  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.09229630769561419  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.12310388589752352  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.09097915893964986  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.11796636043413956  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.1872758099864968  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.08859364810528148  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.11568740153904529  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.16862696833882684  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.17690040854586353  - accuracy: 0.8125\n",
      "At: 1708 [==========>] Loss 0.07730360837865241  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.20338968758231832  - accuracy: 0.6875\n",
      "At: 1710 [==========>] Loss 0.1415067988684513  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.08184785180736648  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.12859544343402243  - accuracy: 0.8125\n",
      "At: 1713 [==========>] Loss 0.08457024363947463  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.13904699914526494  - accuracy: 0.84375\n",
      "At: 1715 [==========>] Loss 0.11191564446326725  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.07201012023642658  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.08475973641046484  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.13455522054769806  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.12335569897150941  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.05376537173967942  - accuracy: 0.96875\n",
      "At: 1721 [==========>] Loss 0.13151454534252274  - accuracy: 0.8125\n",
      "At: 1722 [==========>] Loss 0.06356706156969452  - accuracy: 0.96875\n",
      "At: 1723 [==========>] Loss 0.17502441681437414  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.07601232811143074  - accuracy: 0.9375\n",
      "At: 1725 [==========>] Loss 0.14229585502512815  - accuracy: 0.8125\n",
      "At: 1726 [==========>] Loss 0.11356292030419007  - accuracy: 0.84375\n",
      "At: 1727 [==========>] Loss 0.13061735805822247  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.11962057462305352  - accuracy: 0.78125\n",
      "At: 1729 [==========>] Loss 0.17808754606751664  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.12479472772450534  - accuracy: 0.84375\n",
      "At: 1731 [==========>] Loss 0.09028254207686202  - accuracy: 0.90625\n",
      "At: 1732 [==========>] Loss 0.08064177684907946  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.1580506329669074  - accuracy: 0.78125\n",
      "At: 1734 [==========>] Loss 0.09886537076207258  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.1656844336710619  - accuracy: 0.75\n",
      "At: 1736 [==========>] Loss 0.09463638408955936  - accuracy: 0.90625\n",
      "At: 1737 [==========>] Loss 0.13714736231228597  - accuracy: 0.84375\n",
      "At: 1738 [==========>] Loss 0.10369237135529907  - accuracy: 0.875\n",
      "At: 1739 [==========>] Loss 0.11549496612509573  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.1499174688532762  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.13288405930310437  - accuracy: 0.8125\n",
      "At: 1742 [==========>] Loss 0.05093975123243254  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.12498565328378547  - accuracy: 0.84375\n",
      "At: 1744 [==========>] Loss 0.08694797164044209  - accuracy: 0.84375\n",
      "At: 1745 [==========>] Loss 0.11679421536561373  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.16203673221032952  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.10811183569690651  - accuracy: 0.875\n",
      "At: 1748 [==========>] Loss 0.12333769837179834  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.090735066996355  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.09489607576037752  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.14596035593229323  - accuracy: 0.84375\n",
      "At: 1752 [==========>] Loss 0.09791777794722259  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.0745363051906151  - accuracy: 0.9375\n",
      "At: 1754 [==========>] Loss 0.12130094170295738  - accuracy: 0.78125\n",
      "At: 1755 [==========>] Loss 0.05851260530265024  - accuracy: 0.96875\n",
      "At: 1756 [==========>] Loss 0.15728359464116726  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.160173806141894  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.06812115407924696  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.11375619366848641  - accuracy: 0.8125\n",
      "At: 1760 [==========>] Loss 0.08459315501101014  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.10013085640617278  - accuracy: 0.875\n",
      "At: 1762 [==========>] Loss 0.15089699007903884  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.11789931613065376  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.13510853460008732  - accuracy: 0.84375\n",
      "At: 1765 [==========>] Loss 0.15445135718002467  - accuracy: 0.78125\n",
      "At: 1766 [==========>] Loss 0.08162737826403568  - accuracy: 0.84375\n",
      "At: 1767 [==========>] Loss 0.07162983607361041  - accuracy: 0.96875\n",
      "At: 1768 [==========>] Loss 0.08132390053588114  - accuracy: 0.9375\n",
      "At: 1769 [==========>] Loss 0.07596604385543917  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.07605806626630002  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.15375129389403877  - accuracy: 0.75\n",
      "At: 1772 [==========>] Loss 0.16114039246631148  - accuracy: 0.75\n",
      "At: 1773 [==========>] Loss 0.10189481276507667  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.15818316000270374  - accuracy: 0.75\n",
      "At: 1775 [==========>] Loss 0.0953119288493825  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.1266013907118131  - accuracy: 0.84375\n",
      "At: 1777 [==========>] Loss 0.11428771130707728  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.09228635449651594  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.11715239609241312  - accuracy: 0.875\n",
      "At: 1780 [==========>] Loss 0.11398451384809162  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.1846206768132331  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.10465029942883805  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.14278302379712782  - accuracy: 0.75\n",
      "At: 1784 [==========>] Loss 0.062360985183118944  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.11369995907311511  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.12619231277040163  - accuracy: 0.875\n",
      "At: 1787 [==========>] Loss 0.12949665215095296  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.11046939425785779  - accuracy: 0.8125\n",
      "At: 1789 [==========>] Loss 0.09559050356701805  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.17064549332917364  - accuracy: 0.78125\n",
      "At: 1791 [==========>] Loss 0.0742077029600175  - accuracy: 0.875\n",
      "At: 1792 [==========>] Loss 0.1165627258034704  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.10352697719674889  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.17959259545176276  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.07383328843700379  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.12941139886613617  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.09498341734638585  - accuracy: 0.90625\n",
      "At: 1798 [==========>] Loss 0.13791971288136362  - accuracy: 0.8125\n",
      "At: 1799 [==========>] Loss 0.0836436897903973  - accuracy: 0.875\n",
      "At: 1800 [==========>] Loss 0.11436395226210991  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.17587253778197912  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.11413996313457146  - accuracy: 0.84375\n",
      "At: 1803 [==========>] Loss 0.1413937015334999  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.13836402498586334  - accuracy: 0.78125\n",
      "At: 1805 [==========>] Loss 0.040599077461500224  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.16082147822345588  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.17383333956481511  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.1591509712332661  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.08697490762196716  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.14111476753166835  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.12494871903560917  - accuracy: 0.875\n",
      "At: 1812 [==========>] Loss 0.10640493353287765  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.13426253702707938  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.13659467589294777  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.15552939039688984  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.04280300707468081  - accuracy: 0.9375\n",
      "At: 1817 [==========>] Loss 0.14799008290970045  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.1028377368250149  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.16423662057351812  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.09258873357715114  - accuracy: 0.875\n",
      "At: 1821 [==========>] Loss 0.07751630941191825  - accuracy: 0.9375\n",
      "At: 1822 [==========>] Loss 0.16229490395768806  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.16397131717770078  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.14651348326130836  - accuracy: 0.71875\n",
      "At: 1825 [==========>] Loss 0.10599498495794771  - accuracy: 0.875\n",
      "At: 1826 [==========>] Loss 0.08112444722005135  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.12260665742976543  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.1484263222077898  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.13456876739581825  - accuracy: 0.84375\n",
      "At: 1830 [==========>] Loss 0.14072618458731012  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.146970517944898  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.12177732840248742  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.1190109786202732  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.0690878027392394  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.14059252231991498  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.08339481620222383  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.0381995030391478  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.08558270482925431  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.09015713861744312  - accuracy: 0.875\n",
      "At: 1840 [==========>] Loss 0.1242557692429935  - accuracy: 0.78125\n",
      "At: 1841 [==========>] Loss 0.10073742224403004  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.14937534480294953  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.11517196560287299  - accuracy: 0.875\n",
      "At: 1844 [==========>] Loss 0.0891968261915628  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.17155136757754333  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.12374737780687231  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.06480827663959399  - accuracy: 0.90625\n",
      "At: 1848 [==========>] Loss 0.06356802256242138  - accuracy: 0.90625\n",
      "At: 1849 [==========>] Loss 0.17055842902920948  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.03197501725409917  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.17385805827208822  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.08781285118898574  - accuracy: 0.875\n",
      "At: 1853 [==========>] Loss 0.10095183901136875  - accuracy: 0.90625\n",
      "At: 1854 [==========>] Loss 0.13667604344297773  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.1392036059289205  - accuracy: 0.84375\n",
      "At: 1856 [==========>] Loss 0.11218098847035046  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.13727376287324922  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.12577720319518843  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.13984336167503417  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.1483503837626911  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.10613546212079661  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.18739973613280275  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.117168898642376  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.14607688264103375  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.09606636120130752  - accuracy: 0.84375\n",
      "At: 1866 [==========>] Loss 0.2105458970130958  - accuracy: 0.75\n",
      "At: 1867 [==========>] Loss 0.09721300627699722  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.16395922214734548  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.17715650492922713  - accuracy: 0.75\n",
      "At: 1870 [==========>] Loss 0.12039976288301708  - accuracy: 0.8125\n",
      "At: 1871 [==========>] Loss 0.15424666271024112  - accuracy: 0.8125\n",
      "At: 1872 [==========>] Loss 0.11859849459433505  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.08707843350192618  - accuracy: 0.875\n",
      "At: 1874 [==========>] Loss 0.15415532088483247  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.07792126820342875  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.18935372487596824  - accuracy: 0.6875\n",
      "At: 1877 [==========>] Loss 0.07768438572397707  - accuracy: 0.90625\n",
      "At: 1878 [==========>] Loss 0.1197465442563598  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.12472685159241491  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.09288124045005368  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.08309251514046122  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.10545783703180191  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.1636064977982496  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.0957674650291235  - accuracy: 0.90625\n",
      "At: 1885 [==========>] Loss 0.0987165769056814  - accuracy: 0.875\n",
      "At: 1886 [==========>] Loss 0.137857768240994  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.07663043469615277  - accuracy: 0.9375\n",
      "At: 1888 [==========>] Loss 0.1434632022536308  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.12509642230633633  - accuracy: 0.78125\n",
      "At: 1890 [==========>] Loss 0.16206478790524706  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.05034974392581202  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.06704092709540885  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.07036585570840986  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.09326840017380023  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.07267301870492668  - accuracy: 0.90625\n",
      "At: 1896 [==========>] Loss 0.10810216482505929  - accuracy: 0.90625\n",
      "At: 1897 [==========>] Loss 0.06685612898225407  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.1067922222593553  - accuracy: 0.84375\n",
      "At: 1899 [==========>] Loss 0.09896821883162027  - accuracy: 0.8125\n",
      "At: 1900 [==========>] Loss 0.10711488206197098  - accuracy: 0.875\n",
      "At: 1901 [==========>] Loss 0.11410736839777302  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.14804757610134278  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.11402542535884487  - accuracy: 0.84375\n",
      "At: 1904 [==========>] Loss 0.044097254415326144  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.15964989857317946  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.08729916344646697  - accuracy: 0.875\n",
      "At: 1907 [==========>] Loss 0.08482520425421194  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.09847390170632969  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.10871223725095878  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.05761195713346585  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.12660823618488148  - accuracy: 0.78125\n",
      "At: 1912 [==========>] Loss 0.12582107881275  - accuracy: 0.8125\n",
      "At: 1913 [==========>] Loss 0.14973227896896876  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.059919590264370666  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.11479053804885214  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.13508631711367558  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.1664785210340125  - accuracy: 0.71875\n",
      "At: 1918 [==========>] Loss 0.13465709936910975  - accuracy: 0.78125\n",
      "At: 1919 [==========>] Loss 0.08662572437093063  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.09375718973668556  - accuracy: 0.875\n",
      "At: 1921 [==========>] Loss 0.14705404517337078  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.13551987956687972  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.1809191022262292  - accuracy: 0.75\n",
      "At: 1924 [==========>] Loss 0.14317699564130104  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.18227051001229633  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.08761034150565823  - accuracy: 0.90625\n",
      "At: 1927 [==========>] Loss 0.09144097452127264  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12423843648220928  - accuracy: 0.8125\n",
      "At: 1929 [==========>] Loss 0.17688877654056967  - accuracy: 0.8125\n",
      "At: 1930 [==========>] Loss 0.17431723740596122  - accuracy: 0.6875\n",
      "At: 1931 [==========>] Loss 0.10364536657271994  - accuracy: 0.8125\n",
      "At: 1932 [==========>] Loss 0.16279508148223892  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.08995574053519649  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.13681978482917165  - accuracy: 0.84375\n",
      "At: 1935 [==========>] Loss 0.14246752857717415  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.1187346630748188  - accuracy: 0.8125\n",
      "At: 1937 [==========>] Loss 0.1396540550625317  - accuracy: 0.75\n",
      "At: 1938 [==========>] Loss 0.1993921752324161  - accuracy: 0.6875\n",
      "At: 1939 [==========>] Loss 0.09948206230809871  - accuracy: 0.90625\n",
      "At: 1940 [==========>] Loss 0.11751455268768207  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.12803316710618215  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.1488586140346604  - accuracy: 0.8125\n",
      "At: 1943 [==========>] Loss 0.13726839474147004  - accuracy: 0.78125\n",
      "At: 1944 [==========>] Loss 0.10701647100488651  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.15384425393154108  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.0990697922453844  - accuracy: 0.875\n",
      "At: 1947 [==========>] Loss 0.12088880822250633  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.12062459450176563  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.07145513438616419  - accuracy: 0.875\n",
      "At: 1950 [==========>] Loss 0.12854250147190593  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.11930103355970134  - accuracy: 0.84375\n",
      "At: 1952 [==========>] Loss 0.0701437435095405  - accuracy: 0.90625\n",
      "At: 1953 [==========>] Loss 0.07682532349591757  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.18280037058854656  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.05729742598827502  - accuracy: 0.96875\n",
      "At: 1956 [==========>] Loss 0.11240383169370051  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.09007274938231521  - accuracy: 0.875\n",
      "At: 1958 [==========>] Loss 0.08544332960641982  - accuracy: 0.90625\n",
      "At: 1959 [==========>] Loss 0.14025907817606698  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.059078458172444365  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.17039007949669574  - accuracy: 0.78125\n",
      "At: 1962 [==========>] Loss 0.18336891969013397  - accuracy: 0.71875\n",
      "At: 1963 [==========>] Loss 0.06657088004887507  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.18281792935964322  - accuracy: 0.71875\n",
      "At: 1965 [==========>] Loss 0.15712256547842163  - accuracy: 0.75\n",
      "At: 1966 [==========>] Loss 0.11796375985990956  - accuracy: 0.75\n",
      "At: 1967 [==========>] Loss 0.13463458112188126  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.1965876162396325  - accuracy: 0.6875\n",
      "At: 1969 [==========>] Loss 0.1766124150764271  - accuracy: 0.75\n",
      "At: 1970 [==========>] Loss 0.11813936605338124  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.21407463224789347  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.07660927869350392  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.13832648554176696  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.12376319670328356  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.15757946096458464  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.06549660426515867  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.1006819165733604  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.14972570433118393  - accuracy: 0.78125\n",
      "At: 1979 [==========>] Loss 0.11875341628915055  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.12222421249381413  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.1541927772618407  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.04906234188496017  - accuracy: 0.96875\n",
      "At: 1983 [==========>] Loss 0.14594100738035876  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.09699931025751998  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.1379171894153649  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.2018928240576287  - accuracy: 0.625\n",
      "At: 1987 [==========>] Loss 0.10164646834259572  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.09180307720585387  - accuracy: 0.90625\n",
      "At: 1989 [==========>] Loss 0.09155951424355897  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.08983684295904851  - accuracy: 0.9375\n",
      "At: 1991 [==========>] Loss 0.14582213888566822  - accuracy: 0.78125\n",
      "At: 1992 [==========>] Loss 0.11491129498872561  - accuracy: 0.8125\n",
      "At: 1993 [==========>] Loss 0.13918760100055844  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.10150520752062055  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.19116519213878205  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.10190604291143637  - accuracy: 0.8125\n",
      "At: 1997 [==========>] Loss 0.2044428283147701  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.15851226408359104  - accuracy: 0.78125\n",
      "At: 1999 [==========>] Loss 0.08236020787131933  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.13672163232210977  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.06861878781760669  - accuracy: 0.90625\n",
      "At: 2002 [==========>] Loss 0.0769228663816375  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.09595043045220267  - accuracy: 0.875\n",
      "At: 2004 [==========>] Loss 0.15771379425430168  - accuracy: 0.75\n",
      "At: 2005 [==========>] Loss 0.09759656235978761  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.12343706984602831  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.1257269415186748  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.12840006533984205  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.11915810805365393  - accuracy: 0.90625\n",
      "At: 2010 [==========>] Loss 0.11637572793479113  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.10577302805490663  - accuracy: 0.90625\n",
      "At: 2012 [==========>] Loss 0.1199799326474865  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09443524417608606  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.22646856947581345  - accuracy: 0.59375\n",
      "At: 2015 [==========>] Loss 0.07782354331821678  - accuracy: 0.84375\n",
      "At: 2016 [==========>] Loss 0.12186814587951823  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.08020544600529098  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.08545145185762135  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.15101626266498142  - accuracy: 0.78125\n",
      "At: 2020 [==========>] Loss 0.08363698600319835  - accuracy: 0.84375\n",
      "At: 2021 [==========>] Loss 0.10360746681116902  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.12008655632385985  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.08987194079890512  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.07194633085811065  - accuracy: 0.9375\n",
      "At: 2025 [==========>] Loss 0.172789799042794  - accuracy: 0.75\n",
      "At: 2026 [==========>] Loss 0.1049371720790577  - accuracy: 0.84375\n",
      "At: 2027 [==========>] Loss 0.13003351881574762  - accuracy: 0.84375\n",
      "At: 2028 [==========>] Loss 0.10009006224966632  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.10433213988923222  - accuracy: 0.875\n",
      "At: 2030 [==========>] Loss 0.13529562722385532  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.15555320446655202  - accuracy: 0.8125\n",
      "At: 2032 [==========>] Loss 0.13239516710384264  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.15813297940023696  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.21287709247811204  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.10809520205741174  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09271829503067108  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.09304976761133366  - accuracy: 0.875\n",
      "At: 2038 [==========>] Loss 0.10453172749484493  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.08483974040454612  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.09748403384743304  - accuracy: 0.875\n",
      "At: 2041 [==========>] Loss 0.06307568170146298  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.11309618247455883  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.10119957701151364  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.08740330358045022  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.23150450140941636  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.07738045923083106  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.06387637555843653  - accuracy: 0.9375\n",
      "At: 2048 [==========>] Loss 0.12183272192271702  - accuracy: 0.84375\n",
      "At: 2049 [==========>] Loss 0.13253815740441058  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.18051784329336878  - accuracy: 0.71875\n",
      "At: 2051 [==========>] Loss 0.168264724441665  - accuracy: 0.8125\n",
      "At: 2052 [==========>] Loss 0.05158282465122177  - accuracy: 0.96875\n",
      "At: 2053 [==========>] Loss 0.11116717166887083  - accuracy: 0.875\n",
      "At: 2054 [==========>] Loss 0.13156984966152657  - accuracy: 0.71875\n",
      "At: 2055 [==========>] Loss 0.06237205039872011  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.12355820766128742  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.13626744720734804  - accuracy: 0.84375\n",
      "At: 2058 [==========>] Loss 0.11815888698696765  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.1746384592155722  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.13606765770388746  - accuracy: 0.8125\n",
      "At: 2061 [==========>] Loss 0.11864491464375508  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.13781809901295156  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.10426958144418169  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.1767690791604724  - accuracy: 0.6875\n",
      "At: 2065 [==========>] Loss 0.05040324299880042  - accuracy: 0.9375\n",
      "At: 2066 [==========>] Loss 0.155621585333641  - accuracy: 0.78125\n",
      "At: 2067 [==========>] Loss 0.06502066368212746  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.08167044801865331  - accuracy: 0.90625\n",
      "At: 2069 [==========>] Loss 0.10731294836990654  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.13689892927917302  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.14375436781651613  - accuracy: 0.75\n",
      "At: 2072 [==========>] Loss 0.0489547313926008  - accuracy: 0.96875\n",
      "At: 2073 [==========>] Loss 0.09952582025038173  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.0957340091287808  - accuracy: 0.875\n",
      "At: 2075 [==========>] Loss 0.14712928617867133  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.13670473045601667  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.12863104394811772  - accuracy: 0.78125\n",
      "At: 2078 [==========>] Loss 0.09946428556217077  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07228507007869567  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.10675443595006234  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.15329145410995404  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.1263457802033081  - accuracy: 0.8125\n",
      "At: 2083 [==========>] Loss 0.20537512391804813  - accuracy: 0.6875\n",
      "At: 2084 [==========>] Loss 0.0944475309567574  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.08820962670247101  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.10227508043790726  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.1549746236732918  - accuracy: 0.84375\n",
      "At: 2088 [==========>] Loss 0.09092109748765631  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.08827731346525158  - accuracy: 0.90625\n",
      "At: 2090 [==========>] Loss 0.08070057616292453  - accuracy: 0.90625\n",
      "At: 2091 [==========>] Loss 0.13374217900920093  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.10581737978777467  - accuracy: 0.84375\n",
      "At: 2093 [==========>] Loss 0.1600939591325108  - accuracy: 0.8125\n",
      "At: 2094 [==========>] Loss 0.11822596579651162  - accuracy: 0.875\n",
      "At: 2095 [==========>] Loss 0.1117409704730042  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.16060198751354687  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.08705123475404657  - accuracy: 0.90625\n",
      "At: 2098 [==========>] Loss 0.13298131736915195  - accuracy: 0.75\n",
      "At: 2099 [==========>] Loss 0.09907775410887523  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.052296915067303024  - accuracy: 0.96875\n",
      "At: 2101 [==========>] Loss 0.1474400850102772  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.09432787226214431  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.16199160834555776  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.11173058845519214  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.16753514653516977  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.17804117446170029  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.09687241825178988  - accuracy: 0.84375\n",
      "At: 2108 [==========>] Loss 0.16133216997584682  - accuracy: 0.78125\n",
      "At: 2109 [==========>] Loss 0.12377530641312741  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.06917371237582642  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.1197744010556473  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.12462108094397903  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.09880485015313939  - accuracy: 0.8125\n",
      "At: 2114 [==========>] Loss 0.12461143916251947  - accuracy: 0.90625\n",
      "At: 2115 [==========>] Loss 0.11891069021980237  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.09816897257155509  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.15371241214924858  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.13488111435808692  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.0910278389113423  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.1527005364737679  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.140543670368665  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.15469581074117364  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.1689866396858861  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.14093201147969547  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.08928163866790145  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.058048851040378886  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.11867294992789398  - accuracy: 0.8125\n",
      "At: 2128 [==========>] Loss 0.08693786174079844  - accuracy: 0.875\n",
      "At: 2129 [==========>] Loss 0.14590038344952677  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.06649020932050233  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.0975648069056126  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.20271827703247147  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.17177709717039966  - accuracy: 0.6875\n",
      "At: 2134 [==========>] Loss 0.1068531284364321  - accuracy: 0.84375\n",
      "At: 2135 [==========>] Loss 0.09436413445246267  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.12506977833884478  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.11497540070875159  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.10479274045650072  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.15026844026591246  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.09273446668629687  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.10929159290622736  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.1336988949881676  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.08576193597685658  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.07501379336857286  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.10856581891068243  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.17916952370915018  - accuracy: 0.75\n",
      "At: 2147 [==========>] Loss 0.12203797516892728  - accuracy: 0.78125\n",
      "At: 2148 [==========>] Loss 0.20074007285966028  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.12822522763270622  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.13249465310444264  - accuracy: 0.84375\n",
      "At: 2151 [==========>] Loss 0.09736088595800846  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.26608499387071616  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.18000656712200702  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.14890163391425523  - accuracy: 0.84375\n",
      "At: 2155 [==========>] Loss 0.12884869129337442  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.11741221808389113  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.09997485380699231  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.15983787658610688  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.07764051348905224  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.1422980238519712  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.10632622526112787  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.10145629296571937  - accuracy: 0.84375\n",
      "At: 2163 [==========>] Loss 0.13156047187874703  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.17395078519110008  - accuracy: 0.71875\n",
      "At: 2165 [==========>] Loss 0.11132817513888492  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.1238694194154616  - accuracy: 0.8125\n",
      "At: 2167 [==========>] Loss 0.0839928416214332  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.08410884952236089  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.11125172057455124  - accuracy: 0.875\n",
      "At: 2170 [==========>] Loss 0.12587718818997884  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.13628229665260297  - accuracy: 0.8125\n",
      "At: 2172 [==========>] Loss 0.09963906986472826  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.13387509674703213  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.13121080790333345  - accuracy: 0.8125\n",
      "At: 2175 [==========>] Loss 0.11318753160860698  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.12843627640410876  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.13415727078277973  - accuracy: 0.84375\n",
      "At: 2178 [==========>] Loss 0.08027740339812184  - accuracy: 0.9375\n",
      "At: 2179 [==========>] Loss 0.1062529498410828  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.1083442196766572  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.16698933442159228  - accuracy: 0.75\n",
      "At: 2182 [==========>] Loss 0.13006866504763956  - accuracy: 0.78125\n",
      "At: 2183 [==========>] Loss 0.23949814549665888  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.0738460596726137  - accuracy: 0.96875\n",
      "At: 2185 [==========>] Loss 0.1358393529546184  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.15401354634825715  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.1812179309827357  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.06867788692904506  - accuracy: 0.9375\n",
      "At: 2189 [==========>] Loss 0.06670598297441596  - accuracy: 0.9375\n",
      "At: 2190 [==========>] Loss 0.12431631955340658  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.09547843277059698  - accuracy: 0.84375\n",
      "At: 2192 [==========>] Loss 0.12129920777014466  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.17001021840359856  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.0867790063019287  - accuracy: 0.875\n",
      "At: 2195 [==========>] Loss 0.1503424831720021  - accuracy: 0.78125\n",
      "At: 2196 [==========>] Loss 0.1451427458680692  - accuracy: 0.8125\n",
      "At: 2197 [==========>] Loss 0.08264105633343757  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.08919534899251699  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.05339552491942557  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.04472576242245664  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.10752621932893582  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.08588616287145495  - accuracy: 0.8125\n",
      "At: 2203 [==========>] Loss 0.0913531072718778  - accuracy: 0.84375\n",
      "At: 2204 [==========>] Loss 0.11786833724657464  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.11809737128062742  - accuracy: 0.8125\n",
      "At: 2206 [==========>] Loss 0.08304616558366043  - accuracy: 0.84375\n",
      "At: 2207 [==========>] Loss 0.10197881339662074  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.15182900666630228  - accuracy: 0.8125\n",
      "At: 2209 [==========>] Loss 0.20328074066487972  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.13967061604899134  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.12703138071833864  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.10595695173144037  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.13102140106495097  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.13071957970501225  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.12878353127795727  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.1058741434407752  - accuracy: 0.875\n",
      "At: 2217 [==========>] Loss 0.12396914124572275  - accuracy: 0.78125\n",
      "At: 2218 [==========>] Loss 0.16445188654002751  - accuracy: 0.84375\n",
      "At: 2219 [==========>] Loss 0.07678371862623631  - accuracy: 0.875\n",
      "At: 2220 [==========>] Loss 0.11138401465627533  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.16421434856035777  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.12195129808631511  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.15751386925619687  - accuracy: 0.6875\n",
      "At: 2224 [==========>] Loss 0.1251659920634282  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.11817264706082331  - accuracy: 0.90625\n",
      "At: 2226 [==========>] Loss 0.12496147723460865  - accuracy: 0.8125\n",
      "At: 2227 [==========>] Loss 0.1855807392295496  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.09350497245778773  - accuracy: 0.84375\n",
      "At: 2229 [==========>] Loss 0.15683207069719018  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.1224215412949248  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.1515778303844672  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.15765193519480225  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.16400121889786054  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.15066375859107087  - accuracy: 0.71875\n",
      "At: 2235 [==========>] Loss 0.10528433669303253  - accuracy: 0.875\n",
      "At: 2236 [==========>] Loss 0.06985567266182668  - accuracy: 0.9375\n",
      "At: 2237 [==========>] Loss 0.10466792584589925  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.16079960262750848  - accuracy: 0.71875\n",
      "At: 2239 [==========>] Loss 0.19089987382837104  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.13667058319374656  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.1380907604903159  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.16399965537209865  - accuracy: 0.78125\n",
      "At: 2243 [==========>] Loss 0.07450383642443992  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.09665250116085156  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.09479337564790807  - accuracy: 0.84375\n",
      "At: 2246 [==========>] Loss 0.1262042622311811  - accuracy: 0.84375\n",
      "At: 2247 [==========>] Loss 0.12460841115228241  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.14633996698606372  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.11546913742708956  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.09280964047299614  - accuracy: 0.875\n",
      "At: 2251 [==========>] Loss 0.07700520593015336  - accuracy: 0.9375\n",
      "At: 2252 [==========>] Loss 0.11525624932081041  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.13703949157184708  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.1472685760541298  - accuracy: 0.75\n",
      "At: 2255 [==========>] Loss 0.14444868829893484  - accuracy: 0.78125\n",
      "At: 2256 [==========>] Loss 0.15483893179574584  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.11368635005523792  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.1159258181865818  - accuracy: 0.8125\n",
      "At: 2259 [==========>] Loss 0.1213928086345305  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.1518469572701025  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.08955541439965227  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.1668369947681854  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.16200410442484992  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.09088291338238544  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.09881687420077573  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.10205931727246205  - accuracy: 0.875\n",
      "At: 2267 [==========>] Loss 0.07255497545780702  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.09356108161524966  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.05091009758860636  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.09357191478576915  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.1305781823541666  - accuracy: 0.8125\n",
      "At: 2272 [==========>] Loss 0.0813521540384049  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.09394764165641825  - accuracy: 0.875\n",
      "At: 2274 [==========>] Loss 0.11009106963438  - accuracy: 0.8125\n",
      "At: 2275 [==========>] Loss 0.09808276272402451  - accuracy: 0.90625\n",
      "At: 2276 [==========>] Loss 0.09378801484357005  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.14161617774780078  - accuracy: 0.78125\n",
      "At: 2278 [==========>] Loss 0.07899514727172802  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.12588675490869333  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.1351823113856646  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.12275802093288875  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.0741290286996577  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.15367496770344952  - accuracy: 0.75\n",
      "At: 2284 [==========>] Loss 0.11405003995757268  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.13578558269130384  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.13233501891121463  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.14953566102848936  - accuracy: 0.75\n",
      "At: 2288 [==========>] Loss 0.07845035113367095  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.1358772062344084  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.06989732059999923  - accuracy: 0.90625\n",
      "At: 2291 [==========>] Loss 0.11495585532826483  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.09216858739964315  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.050483142063801396  - accuracy: 0.9375\n",
      "At: 2294 [==========>] Loss 0.06357050695994527  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.11945386015970479  - accuracy: 0.8125\n",
      "At: 2296 [==========>] Loss 0.13114638793501884  - accuracy: 0.84375\n",
      "At: 2297 [==========>] Loss 0.06189873897898941  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.09738228679800866  - accuracy: 0.90625\n",
      "At: 2299 [==========>] Loss 0.12513226102698477  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.15054665804112027  - accuracy: 0.71875\n",
      "At: 2301 [==========>] Loss 0.20378747438273398  - accuracy: 0.6875\n",
      "At: 2302 [==========>] Loss 0.17877306319667097  - accuracy: 0.8125\n",
      "At: 2303 [==========>] Loss 0.08375093786848936  - accuracy: 0.875\n",
      "At: 2304 [==========>] Loss 0.08142895743675788  - accuracy: 0.96875\n",
      "At: 2305 [==========>] Loss 0.11463576318451446  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.11880069961792725  - accuracy: 0.75\n",
      "At: 2307 [==========>] Loss 0.16771961415381864  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.17127160661904098  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.12649288763478392  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.15417662640172589  - accuracy: 0.75\n",
      "At: 2311 [==========>] Loss 0.1495762961268765  - accuracy: 0.75\n",
      "At: 2312 [==========>] Loss 0.10548510898136351  - accuracy: 0.90625\n",
      "At: 2313 [==========>] Loss 0.09371456439557983  - accuracy: 0.875\n",
      "At: 2314 [==========>] Loss 0.11373344137630186  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.09305740369109206  - accuracy: 0.84375\n",
      "At: 2316 [==========>] Loss 0.1256517471994039  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.17061471544408383  - accuracy: 0.71875\n",
      "At: 2318 [==========>] Loss 0.17371396621870464  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.11085647064690096  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.10140547199914754  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.12505712630107177  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.20794623345270863  - accuracy: 0.6875\n",
      "At: 2323 [==========>] Loss 0.1216056645499344  - accuracy: 0.875\n",
      "At: 2324 [==========>] Loss 0.1422411041795684  - accuracy: 0.78125\n",
      "At: 2325 [==========>] Loss 0.12834687754127636  - accuracy: 0.78125\n",
      "At: 2326 [==========>] Loss 0.06831107739156717  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.06965569576423854  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12298575582086099  - accuracy: 0.8125\n",
      "At: 2329 [==========>] Loss 0.11302152397938489  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.1404018575368432  - accuracy: 0.78125\n",
      "At: 2331 [==========>] Loss 0.09987617270362409  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.13089284193237574  - accuracy: 0.75\n",
      "At: 2333 [==========>] Loss 0.11937696717047222  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.17108542263818668  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.10075175810137564  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.10285889374127832  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.1373988609923939  - accuracy: 0.78125\n",
      "At: 2338 [==========>] Loss 0.12349088283463738  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.10460199806946767  - accuracy: 0.8125\n",
      "At: 2340 [==========>] Loss 0.13598857690127863  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.14579341376571658  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.1741298754125672  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.0828974724181636  - accuracy: 0.90625\n",
      "At: 2344 [==========>] Loss 0.21196671900554284  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.15052143827568215  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.1098583890948915  - accuracy: 0.84375\n",
      "At: 2347 [==========>] Loss 0.1293072284934264  - accuracy: 0.875\n",
      "At: 2348 [==========>] Loss 0.05023653664308046  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.06260557980954945  - accuracy: 0.90625\n",
      "At: 2350 [==========>] Loss 0.11868446286689391  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.10254661751322533  - accuracy: 0.78125\n",
      "At: 2352 [==========>] Loss 0.10935821860682812  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.09533268818171411  - accuracy: 0.78125\n",
      "At: 2354 [==========>] Loss 0.16162238988892386  - accuracy: 0.75\n",
      "At: 2355 [==========>] Loss 0.05660562258541327  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.11942031707358215  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.150686011591735  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.1586338058614606  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.17163756459732965  - accuracy: 0.6875\n",
      "At: 2360 [==========>] Loss 0.14798622213624957  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.16825093911782818  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.07147737256704356  - accuracy: 0.84375\n",
      "At: 2363 [==========>] Loss 0.1510304245993166  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.08055548986219588  - accuracy: 0.9375\n",
      "At: 2365 [==========>] Loss 0.09607833741408983  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.17727160878800363  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.09417133651011654  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.10629476120895781  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.09807867922155108  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.0832165585288179  - accuracy: 0.875\n",
      "At: 2371 [==========>] Loss 0.08173460013018498  - accuracy: 0.90625\n",
      "At: 2372 [==========>] Loss 0.13166727354131041  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.13570924858839564  - accuracy: 0.78125\n",
      "At: 2374 [==========>] Loss 0.1001253571407476  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.0527784978372146  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.10794377171421846  - accuracy: 0.875\n",
      "At: 2377 [==========>] Loss 0.0972064642965502  - accuracy: 0.875\n",
      "At: 2378 [==========>] Loss 0.10756255823976027  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.16512572203006165  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.06697254773695632  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.0790232406620163  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.11982070778310835  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1503242431389008  - accuracy: 0.78125\n",
      "At: 2384 [==========>] Loss 0.10672664233859974  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.11738024954416282  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.07526452667455111  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.07661543983190258  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.11610012029830408  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.06054395162897813  - accuracy: 0.90625\n",
      "At: 2390 [==========>] Loss 0.06074504000029603  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.176215602929782  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.15678812201313708  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.11643089528471709  - accuracy: 0.78125\n",
      "At: 2394 [==========>] Loss 0.05654880915659527  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.0883957219063003  - accuracy: 0.875\n",
      "At: 2396 [==========>] Loss 0.06393602895075758  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.09904344706607013  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.11380920837697986  - accuracy: 0.84375\n",
      "At: 2399 [==========>] Loss 0.1467257875085356  - accuracy: 0.8125\n",
      "At: 2400 [==========>] Loss 0.09509872464386823  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.10958707245460902  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.0783425852299518  - accuracy: 0.9375\n",
      "At: 2403 [==========>] Loss 0.16577384693738273  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.1294350696304895  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.059246830497720344  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.12499793603080384  - accuracy: 0.875\n",
      "At: 2407 [==========>] Loss 0.11149140993006448  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.07461614554434168  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.13341286237780847  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.1361789608931795  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.08177822083054659  - accuracy: 0.9375\n",
      "At: 2412 [==========>] Loss 0.09660860655695105  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.08647648569567246  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.06044603330556871  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.10092960296347507  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.08395694208286228  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.142352189095807  - accuracy: 0.8125\n",
      "At: 2418 [==========>] Loss 0.13225877722976231  - accuracy: 0.84375\n",
      "At: 2419 [==========>] Loss 0.12116510673562927  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.13599831369609808  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.08271509438930576  - accuracy: 0.9375\n",
      "At: 2422 [==========>] Loss 0.1372043128128274  - accuracy: 0.8125\n",
      "At: 2423 [==========>] Loss 0.1577871223366948  - accuracy: 0.75\n",
      "At: 2424 [==========>] Loss 0.111533675582399  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08083882240508675  - accuracy: 0.9375\n",
      "At: 2426 [==========>] Loss 0.1800383718770905  - accuracy: 0.71875\n",
      "At: 2427 [==========>] Loss 0.13020107901215955  - accuracy: 0.84375\n",
      "At: 2428 [==========>] Loss 0.0784579399756554  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.09380911751967021  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.15168339744175444  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.12318303420204163  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.05161632839769094  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.057517405741104054  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.036287442847722194  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.1559432006234326  - accuracy: 0.8125\n",
      "At: 2436 [==========>] Loss 0.10309237283485677  - accuracy: 0.8125\n",
      "At: 2437 [==========>] Loss 0.17093464113307508  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.10655727385448215  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.15755272343150578  - accuracy: 0.75\n",
      "At: 2440 [==========>] Loss 0.10426474211759709  - accuracy: 0.8125\n",
      "At: 2441 [==========>] Loss 0.10762994846493903  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.13236853232921086  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.1418838959393754  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.0849257102261369  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.06875469702447934  - accuracy: 0.9375\n",
      "At: 2446 [==========>] Loss 0.17274981028938047  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.18298769621440392  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.09161348057401192  - accuracy: 0.875\n",
      "At: 2449 [==========>] Loss 0.07490298273441968  - accuracy: 0.9375\n",
      "At: 2450 [==========>] Loss 0.07489789184514452  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.06912018290095005  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.12652794209129722  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.1518980401053636  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.16907348350209256  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.12900084746563598  - accuracy: 0.78125\n",
      "At: 2456 [==========>] Loss 0.11097817311447442  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.1713827459242067  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.07373420022850466  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.15162020405075882  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.06894555353809653  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.05800622421843697  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.16702256185087824  - accuracy: 0.71875\n",
      "At: 2463 [==========>] Loss 0.07588704916932724  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.16360241547278925  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.1303724553010548  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.10324691364707841  - accuracy: 0.8125\n",
      "At: 2467 [==========>] Loss 0.09515720312626108  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.06707564382614278  - accuracy: 0.9375\n",
      "At: 2469 [==========>] Loss 0.1300432323958545  - accuracy: 0.8125\n",
      "At: 2470 [==========>] Loss 0.1372880307949406  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.11726756502901642  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.0823310605866056  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.10484400762090673  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.07957026015995575  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.09736374855274711  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.07117275321165059  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.1420870850892615  - accuracy: 0.84375\n",
      "At: 2478 [==========>] Loss 0.1131714423531252  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.08308468584592063  - accuracy: 0.90625\n",
      "At: 2480 [==========>] Loss 0.11943406538899409  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.06506940020739536  - accuracy: 0.96875\n",
      "At: 2482 [==========>] Loss 0.1718608208659548  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.13825199497809393  - accuracy: 0.78125\n",
      "At: 2484 [==========>] Loss 0.08964165327644806  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.10799079790070332  - accuracy: 0.875\n",
      "At: 2486 [==========>] Loss 0.11787472679679574  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.1258123838404655  - accuracy: 0.84375\n",
      "At: 2488 [==========>] Loss 0.16408566520791043  - accuracy: 0.75\n",
      "At: 2489 [==========>] Loss 0.1886709661358544  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.09682521295738755  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.15480345784421107  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.1345082318375098  - accuracy: 0.8125\n",
      "At: 2493 [==========>] Loss 0.08162175235066796  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.0901285718994849  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.09074319907227199  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.06094962685869153  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.222984891357084  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.1032326107638606  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.06825269141823231  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.17723710146922603  - accuracy: 0.71875\n",
      "At: 2501 [==========>] Loss 0.1647412633313653  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.13219798686927403  - accuracy: 0.8125\n",
      "At: 2503 [==========>] Loss 0.10772239544502087  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.14682206066688303  - accuracy: 0.8125\n",
      "At: 2505 [==========>] Loss 0.1057792705927343  - accuracy: 0.875\n",
      "At: 2506 [==========>] Loss 0.09750978275848132  - accuracy: 0.9375\n",
      "At: 2507 [==========>] Loss 0.1313043986841841  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.11337146270597316  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.1471731541096082  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.11731663363856443  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14817741212163088  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.09497058515706094  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.139144180532771  - accuracy: 0.8125\n",
      "At: 2514 [==========>] Loss 0.13147657464523638  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.18568551708277237  - accuracy: 0.78125\n",
      "At: 2516 [==========>] Loss 0.198692123809166  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.11667227839530403  - accuracy: 0.78125\n",
      "At: 2518 [==========>] Loss 0.11706459963814167  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.1432495466274693  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.12981768356899193  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.10040047435301538  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.21203212373586944  - accuracy: 0.625\n",
      "At: 2523 [==========>] Loss 0.13881087712032034  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.1747859864172461  - accuracy: 0.75\n",
      "At: 2525 [==========>] Loss 0.06380970371378615  - accuracy: 0.875\n",
      "At: 2526 [==========>] Loss 0.10568559431672933  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.11853886080513285  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.08186723863816975  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.13433492677687445  - accuracy: 0.8125\n",
      "At: 2530 [==========>] Loss 0.10830939079432876  - accuracy: 0.875\n",
      "At: 2531 [==========>] Loss 0.04424774079927377  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.09837551008810111  - accuracy: 0.90625\n",
      "At: 2533 [==========>] Loss 0.10051774378576366  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.06128219082056595  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.09852193740674475  - accuracy: 0.8125\n",
      "At: 2536 [==========>] Loss 0.15496176513999374  - accuracy: 0.8125\n",
      "At: 2537 [==========>] Loss 0.10945352919926439  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.15083101770701052  - accuracy: 0.75\n",
      "At: 2539 [==========>] Loss 0.08555557361167146  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.1272492544871171  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.06856704897149503  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.06093553006554592  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16828077636186298  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.12395240688790973  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.0646530089568408  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.13272894153174325  - accuracy: 0.875\n",
      "At: 2547 [==========>] Loss 0.08928675588038645  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.07713501712678536  - accuracy: 0.9375\n",
      "At: 2549 [==========>] Loss 0.07182778721087615  - accuracy: 0.9375\n",
      "At: 2550 [==========>] Loss 0.1251265975463644  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.11082803847384032  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.11014932241035522  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.07926378517766135  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.14139276233430514  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.18362739489704555  - accuracy: 0.78125\n",
      "At: 2556 [==========>] Loss 0.11673266990729858  - accuracy: 0.84375\n",
      "At: 2557 [==========>] Loss 0.09277324998477485  - accuracy: 0.875\n",
      "At: 2558 [==========>] Loss 0.09807965387485253  - accuracy: 0.8125\n",
      "At: 2559 [==========>] Loss 0.10748546440239416  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.20336728322669057  - accuracy: 0.71875\n",
      "At: 2561 [==========>] Loss 0.09305827444108966  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.1121254036541628  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.09749930940241229  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.07638394299025106  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.11515973697909113  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.1768653719736172  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.13145320550253592  - accuracy: 0.78125\n",
      "At: 2568 [==========>] Loss 0.10056075295959466  - accuracy: 0.90625\n",
      "At: 2569 [==========>] Loss 0.06381375432206898  - accuracy: 0.96875\n",
      "At: 2570 [==========>] Loss 0.17423720869910808  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.07875285988050043  - accuracy: 0.9375\n",
      "At: 2572 [==========>] Loss 0.17896790804434273  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.19893418654706077  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.11891735789079896  - accuracy: 0.84375\n",
      "At: 2575 [==========>] Loss 0.05297091179393454  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.10663154127862451  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.2146627406064568  - accuracy: 0.75\n",
      "At: 2578 [==========>] Loss 0.18159935954696688  - accuracy: 0.6875\n",
      "At: 2579 [==========>] Loss 0.18328884213434862  - accuracy: 0.78125\n",
      "At: 2580 [==========>] Loss 0.16681946430462744  - accuracy: 0.8125\n",
      "At: 2581 [==========>] Loss 0.07135863677361191  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.20959328733523608  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.06914642223851926  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.17500224157358124  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.09290035110639858  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.10164798918223343  - accuracy: 0.8125\n",
      "At: 2587 [==========>] Loss 0.16988082200358018  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.13958225912739358  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.10161144610256378  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.22001954897674175  - accuracy: 0.71875\n",
      "At: 2591 [==========>] Loss 0.14341205685353497  - accuracy: 0.75\n",
      "At: 2592 [==========>] Loss 0.10730747475766313  - accuracy: 0.78125\n",
      "At: 2593 [==========>] Loss 0.07399658184743368  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.09164526290149919  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.0594364489208526  - accuracy: 0.9375\n",
      "At: 2596 [==========>] Loss 0.10750851148176087  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.08274621851931474  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.1178765047632224  - accuracy: 0.8125\n",
      "At: 2599 [==========>] Loss 0.11886770373561414  - accuracy: 0.84375\n",
      "At: 2600 [==========>] Loss 0.13673886090448067  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.12970819442176088  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.09223364353612093  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.15476910850532438  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.09762156377077244  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.1295004308158396  - accuracy: 0.75\n",
      "At: 2606 [==========>] Loss 0.12884818518100954  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.1321326666470659  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.06855384926398812  - accuracy: 0.90625\n",
      "At: 2609 [==========>] Loss 0.08311288751476639  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.12493018549027088  - accuracy: 0.8125\n",
      "At: 2611 [==========>] Loss 0.15006827602657613  - accuracy: 0.75\n",
      "At: 2612 [==========>] Loss 0.10889007190386596  - accuracy: 0.8125\n",
      "At: 2613 [==========>] Loss 0.08735809940793798  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.12493711957481393  - accuracy: 0.8125\n",
      "At: 2615 [==========>] Loss 0.09485405178987663  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.05736987818989878  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.08770791215912108  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.05936538157711572  - accuracy: 0.9375\n",
      "At: 2619 [==========>] Loss 0.10166544518352585  - accuracy: 0.90625\n",
      "At: 2620 [==========>] Loss 0.11046184963144731  - accuracy: 0.875\n",
      "At: 2621 [==========>] Loss 0.11748636686380012  - accuracy: 0.8125\n",
      "At: 2622 [==========>] Loss 0.11772092116015309  - accuracy: 0.84375\n",
      "At: 2623 [==========>] Loss 0.07274183181408389  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.11437074028252399  - accuracy: 0.84375\n",
      "At: 2625 [==========>] Loss 0.0631969079340773  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.05265918584578465  - accuracy: 0.90625\n",
      "At: 2627 [==========>] Loss 0.12089566167481391  - accuracy: 0.75\n",
      "At: 2628 [==========>] Loss 0.059470925805297434  - accuracy: 0.9375\n",
      "At: 2629 [==========>] Loss 0.10431958929427264  - accuracy: 0.84375\n",
      "At: 2630 [==========>] Loss 0.09783268204895101  - accuracy: 0.84375\n",
      "At: 2631 [==========>] Loss 0.12222260132471678  - accuracy: 0.8125\n",
      "At: 2632 [==========>] Loss 0.12212237071445933  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.10563769354978883  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.06298110848175756  - accuracy: 0.9375\n",
      "At: 2635 [==========>] Loss 0.18955254581796901  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.11975953317889985  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.12123168657216418  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.07646076598661411  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.06934795444002628  - accuracy: 0.9375\n",
      "At: 2640 [==========>] Loss 0.10181621228100832  - accuracy: 0.875\n",
      "At: 2641 [==========>] Loss 0.12284437624617989  - accuracy: 0.875\n",
      "At: 2642 [==========>] Loss 0.1612778662387992  - accuracy: 0.6875\n",
      "At: 2643 [==========>] Loss 0.07711503023326696  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.14769597241570162  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.07663044431878271  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.15860037127858162  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.12460805917015302  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.1428707903964427  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.14020235467404707  - accuracy: 0.78125\n",
      "At: 2650 [==========>] Loss 0.10618604350708603  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.16737824785015445  - accuracy: 0.78125\n",
      "At: 2652 [==========>] Loss 0.10602962943967513  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.11607530644464821  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.101908533592944  - accuracy: 0.90625\n",
      "At: 2655 [==========>] Loss 0.2407394367278997  - accuracy: 0.59375\n",
      "At: 2656 [==========>] Loss 0.01936685072100761  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.11319445496402672  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.08647566966503574  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.07303571187237946  - accuracy: 0.90625\n",
      "At: 2660 [==========>] Loss 0.09369639881342876  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.08615935520493756  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.06384499154667575  - accuracy: 0.90625\n",
      "At: 2663 [==========>] Loss 0.08895330096167739  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.09573043211326138  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.09427702296674267  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.14436407830532336  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.1348671688556532  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.15133342181047393  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.13258617937716996  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.10104668156576269  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.08311742707445706  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.08588443313224507  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.1220018760707042  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.10639850118759739  - accuracy: 0.875\n",
      "At: 2675 [==========>] Loss 0.11855978376776877  - accuracy: 0.8125\n",
      "At: 2676 [==========>] Loss 0.11582418101636124  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.11208764289004737  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.06159400013350226  - accuracy: 0.90625\n",
      "At: 2679 [==========>] Loss 0.07009467807080484  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.07527209853672417  - accuracy: 0.90625\n",
      "At: 2681 [==========>] Loss 0.09493021316518824  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.1553122318061561  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.17714028818959524  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09445164694681049  - accuracy: 0.84375\n",
      "At: 2685 [==========>] Loss 0.0875002291031691  - accuracy: 0.9375\n",
      "At: 2686 [==========>] Loss 0.07922503995573453  - accuracy: 0.90625\n",
      "At: 2687 [==========>] Loss 0.13091329710791239  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.1590992836637336  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.11744605645282265  - accuracy: 0.8125\n",
      "At: 2690 [==========>] Loss 0.12760500908034142  - accuracy: 0.8125\n",
      "Epochs  5 / 10\n",
      "At: 1 [==========>] Loss 0.19781817044517735  - accuracy: 0.71875\n",
      "At: 2 [==========>] Loss 0.16828334726837535  - accuracy: 0.84375\n",
      "At: 3 [==========>] Loss 0.134412824581727  - accuracy: 0.75\n",
      "At: 4 [==========>] Loss 0.1900354380062073  - accuracy: 0.75\n",
      "At: 5 [==========>] Loss 0.08654780061565896  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.13028304870061483  - accuracy: 0.875\n",
      "At: 7 [==========>] Loss 0.16086105603911793  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.23594065360814367  - accuracy: 0.65625\n",
      "At: 9 [==========>] Loss 0.30101987194846364  - accuracy: 0.625\n",
      "At: 10 [==========>] Loss 0.19689210904373827  - accuracy: 0.71875\n",
      "At: 11 [==========>] Loss 0.1616857842970032  - accuracy: 0.75\n",
      "At: 12 [==========>] Loss 0.13630094207669824  - accuracy: 0.78125\n",
      "At: 13 [==========>] Loss 0.13249642986441162  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.1052969439583774  - accuracy: 0.9375\n",
      "At: 15 [==========>] Loss 0.16929152262686947  - accuracy: 0.75\n",
      "At: 16 [==========>] Loss 0.17858187776480067  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.12935078221574664  - accuracy: 0.8125\n",
      "At: 18 [==========>] Loss 0.24137612255539015  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.18776858216948497  - accuracy: 0.78125\n",
      "At: 20 [==========>] Loss 0.13828807832522977  - accuracy: 0.71875\n",
      "At: 21 [==========>] Loss 0.21492448680162513  - accuracy: 0.71875\n",
      "At: 22 [==========>] Loss 0.14377922327462866  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.10423157987247585  - accuracy: 0.84375\n",
      "At: 24 [==========>] Loss 0.2276063091078801  - accuracy: 0.65625\n",
      "At: 25 [==========>] Loss 0.20274427682229632  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.22182475193837525  - accuracy: 0.71875\n",
      "At: 27 [==========>] Loss 0.27009938540874945  - accuracy: 0.65625\n",
      "At: 28 [==========>] Loss 0.18707797872232992  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.1670601489392543  - accuracy: 0.78125\n",
      "At: 30 [==========>] Loss 0.23841054313037127  - accuracy: 0.59375\n",
      "At: 31 [==========>] Loss 0.274920830336983  - accuracy: 0.59375\n",
      "At: 32 [==========>] Loss 0.2031570032827471  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.1641227614789854  - accuracy: 0.78125\n",
      "At: 34 [==========>] Loss 0.12382120947382994  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.15168396902794756  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.17679651493159443  - accuracy: 0.71875\n",
      "At: 37 [==========>] Loss 0.20111387295576122  - accuracy: 0.65625\n",
      "At: 38 [==========>] Loss 0.2153962048056509  - accuracy: 0.71875\n",
      "At: 39 [==========>] Loss 0.18263835134276485  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.23714276099668027  - accuracy: 0.71875\n",
      "At: 41 [==========>] Loss 0.04967885016686486  - accuracy: 0.96875\n",
      "At: 42 [==========>] Loss 0.144362618455878  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.1721623025795761  - accuracy: 0.78125\n",
      "At: 44 [==========>] Loss 0.160482909876026  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.06896199058042052  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.15557727746109618  - accuracy: 0.8125\n",
      "At: 47 [==========>] Loss 0.16267971336146234  - accuracy: 0.78125\n",
      "At: 48 [==========>] Loss 0.15717901363391634  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.11221664665462927  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.1888571166442788  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.1717892349433646  - accuracy: 0.78125\n",
      "At: 52 [==========>] Loss 0.19039592957856122  - accuracy: 0.71875\n",
      "At: 53 [==========>] Loss 0.15480269956129108  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.14825055420278552  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.24306256769128495  - accuracy: 0.6875\n",
      "At: 56 [==========>] Loss 0.13230410731711945  - accuracy: 0.84375\n",
      "At: 57 [==========>] Loss 0.16292752008304734  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.18244791693884166  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.1937476961795419  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.1711832623884496  - accuracy: 0.78125\n",
      "At: 61 [==========>] Loss 0.19351096844866975  - accuracy: 0.6875\n",
      "At: 62 [==========>] Loss 0.1735148870269177  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.18344476066558577  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.1455909871394259  - accuracy: 0.78125\n",
      "At: 65 [==========>] Loss 0.17777189594482284  - accuracy: 0.6875\n",
      "At: 66 [==========>] Loss 0.16786532017834946  - accuracy: 0.78125\n",
      "At: 67 [==========>] Loss 0.1909324952076683  - accuracy: 0.78125\n",
      "At: 68 [==========>] Loss 0.10470265292046381  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.1292049648206701  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.16246633604169858  - accuracy: 0.78125\n",
      "At: 71 [==========>] Loss 0.116905673643669  - accuracy: 0.84375\n",
      "At: 72 [==========>] Loss 0.1176292000794369  - accuracy: 0.84375\n",
      "At: 73 [==========>] Loss 0.1383718295564078  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.21409828364170852  - accuracy: 0.6875\n",
      "At: 75 [==========>] Loss 0.20387774821272234  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.27872113152216144  - accuracy: 0.59375\n",
      "At: 77 [==========>] Loss 0.20214506368491375  - accuracy: 0.78125\n",
      "At: 78 [==========>] Loss 0.10640822527688035  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.15611202837635044  - accuracy: 0.78125\n",
      "At: 80 [==========>] Loss 0.19358286579397657  - accuracy: 0.78125\n",
      "At: 81 [==========>] Loss 0.15248303801099122  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.19849384371246112  - accuracy: 0.71875\n",
      "At: 83 [==========>] Loss 0.1588904536066562  - accuracy: 0.75\n",
      "At: 84 [==========>] Loss 0.19461221959713426  - accuracy: 0.71875\n",
      "At: 85 [==========>] Loss 0.18411886682056117  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.20794255450771978  - accuracy: 0.75\n",
      "At: 87 [==========>] Loss 0.13476047611006742  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.28205881426437596  - accuracy: 0.65625\n",
      "At: 89 [==========>] Loss 0.19391810639148058  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.2026576272205743  - accuracy: 0.78125\n",
      "At: 91 [==========>] Loss 0.1853457097157053  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.1095441703945888  - accuracy: 0.84375\n",
      "At: 93 [==========>] Loss 0.12788029752161742  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.15619671905210547  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.0983442604569412  - accuracy: 0.875\n",
      "At: 96 [==========>] Loss 0.1496995158916735  - accuracy: 0.75\n",
      "At: 97 [==========>] Loss 0.09081864849673259  - accuracy: 0.84375\n",
      "At: 98 [==========>] Loss 0.25609417918824456  - accuracy: 0.625\n",
      "At: 99 [==========>] Loss 0.14824955900328332  - accuracy: 0.78125\n",
      "At: 100 [==========>] Loss 0.09292279484209279  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.1583513384713026  - accuracy: 0.78125\n",
      "At: 102 [==========>] Loss 0.18795372983275588  - accuracy: 0.75\n",
      "At: 103 [==========>] Loss 0.15571166530303254  - accuracy: 0.78125\n",
      "At: 104 [==========>] Loss 0.134820000442458  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.16466030290965428  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.16977651524661597  - accuracy: 0.78125\n",
      "At: 107 [==========>] Loss 0.13483782173990283  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.20069752637825994  - accuracy: 0.65625\n",
      "At: 109 [==========>] Loss 0.11780051203778602  - accuracy: 0.8125\n",
      "At: 110 [==========>] Loss 0.19029302986039454  - accuracy: 0.75\n",
      "At: 111 [==========>] Loss 0.08463067256434037  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.1759335170228568  - accuracy: 0.75\n",
      "At: 113 [==========>] Loss 0.1547141883488274  - accuracy: 0.8125\n",
      "At: 114 [==========>] Loss 0.16091198123782152  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.11063203744440328  - accuracy: 0.84375\n",
      "At: 116 [==========>] Loss 0.1989086507163128  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.1864642524476243  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.27264458162916855  - accuracy: 0.59375\n",
      "At: 119 [==========>] Loss 0.14674003117415124  - accuracy: 0.75\n",
      "At: 120 [==========>] Loss 0.16112702785364275  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.12104550584213425  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.1598221416500506  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.1871430773644139  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.2158800723007766  - accuracy: 0.71875\n",
      "At: 125 [==========>] Loss 0.14148423253301387  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.2261063402111136  - accuracy: 0.625\n",
      "At: 127 [==========>] Loss 0.1564355623440788  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.24279480526203717  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.10988061669612098  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.21435503272337963  - accuracy: 0.6875\n",
      "At: 131 [==========>] Loss 0.16220970648849758  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.2081051173767132  - accuracy: 0.65625\n",
      "At: 133 [==========>] Loss 0.19224022132008117  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.16036028685231657  - accuracy: 0.78125\n",
      "At: 135 [==========>] Loss 0.18911947058906448  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.13624204939300644  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.09117689292019909  - accuracy: 0.875\n",
      "At: 138 [==========>] Loss 0.15602054279689845  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.15898043806072698  - accuracy: 0.78125\n",
      "At: 140 [==========>] Loss 0.14022621767450277  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.25335284005004416  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.17270154652477443  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.16692110971678686  - accuracy: 0.75\n",
      "At: 144 [==========>] Loss 0.12363935172820469  - accuracy: 0.84375\n",
      "At: 145 [==========>] Loss 0.14386346750619414  - accuracy: 0.8125\n",
      "At: 146 [==========>] Loss 0.1619567030595834  - accuracy: 0.71875\n",
      "At: 147 [==========>] Loss 0.18155844779751357  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.14324173614811814  - accuracy: 0.78125\n",
      "At: 149 [==========>] Loss 0.1767479506954402  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.1504118338077206  - accuracy: 0.78125\n",
      "At: 151 [==========>] Loss 0.15918791121212222  - accuracy: 0.8125\n",
      "At: 152 [==========>] Loss 0.13076193804195252  - accuracy: 0.84375\n",
      "At: 153 [==========>] Loss 0.17153605062464905  - accuracy: 0.78125\n",
      "At: 154 [==========>] Loss 0.14103043318494668  - accuracy: 0.75\n",
      "At: 155 [==========>] Loss 0.1893103132401803  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.11679535019491347  - accuracy: 0.78125\n",
      "At: 157 [==========>] Loss 0.19725344838764883  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.19375708204712885  - accuracy: 0.71875\n",
      "At: 159 [==========>] Loss 0.13496835832370901  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.13671422435760872  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.1080671682687318  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.16925954888140346  - accuracy: 0.8125\n",
      "At: 163 [==========>] Loss 0.14222865451348582  - accuracy: 0.8125\n",
      "At: 164 [==========>] Loss 0.16714692026529104  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.20710776192245234  - accuracy: 0.6875\n",
      "At: 166 [==========>] Loss 0.14680775359613105  - accuracy: 0.8125\n",
      "At: 167 [==========>] Loss 0.0977917824420155  - accuracy: 0.875\n",
      "At: 168 [==========>] Loss 0.2155625457544928  - accuracy: 0.6875\n",
      "At: 169 [==========>] Loss 0.17300790272660446  - accuracy: 0.71875\n",
      "At: 170 [==========>] Loss 0.146841866008064  - accuracy: 0.78125\n",
      "At: 171 [==========>] Loss 0.20459890278354487  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.140800995878554  - accuracy: 0.78125\n",
      "At: 173 [==========>] Loss 0.20052760281311233  - accuracy: 0.71875\n",
      "At: 174 [==========>] Loss 0.10714038094284195  - accuracy: 0.8125\n",
      "At: 175 [==========>] Loss 0.16677656298242316  - accuracy: 0.75\n",
      "At: 176 [==========>] Loss 0.21940021925434677  - accuracy: 0.625\n",
      "At: 177 [==========>] Loss 0.09094198256940775  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.15159933346628887  - accuracy: 0.8125\n",
      "At: 179 [==========>] Loss 0.13614575306915866  - accuracy: 0.84375\n",
      "At: 180 [==========>] Loss 0.125347185308978  - accuracy: 0.84375\n",
      "At: 181 [==========>] Loss 0.09185406742193933  - accuracy: 0.84375\n",
      "At: 182 [==========>] Loss 0.20359930408529978  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.1278375226710966  - accuracy: 0.84375\n",
      "At: 184 [==========>] Loss 0.14866658325692234  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.12425233571529867  - accuracy: 0.8125\n",
      "At: 186 [==========>] Loss 0.14382819372710293  - accuracy: 0.75\n",
      "At: 187 [==========>] Loss 0.2079719098743943  - accuracy: 0.65625\n",
      "At: 188 [==========>] Loss 0.16115338758255943  - accuracy: 0.75\n",
      "At: 189 [==========>] Loss 0.18096104857381523  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.11981786247079619  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.31388813594896225  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.13916029809825223  - accuracy: 0.78125\n",
      "At: 193 [==========>] Loss 0.20102572741192706  - accuracy: 0.71875\n",
      "At: 194 [==========>] Loss 0.18074701661788367  - accuracy: 0.8125\n",
      "At: 195 [==========>] Loss 0.17909136193263817  - accuracy: 0.75\n",
      "At: 196 [==========>] Loss 0.15330378246979864  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.12875150038949212  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.14288018706343472  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.09362581645076566  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.15705228485225609  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.11408986262562121  - accuracy: 0.8125\n",
      "At: 202 [==========>] Loss 0.09822477434510979  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.1594496810645381  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.1952400341325059  - accuracy: 0.78125\n",
      "At: 205 [==========>] Loss 0.13121049154397962  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.0955824536195056  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.08339241983426836  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.18249411522939946  - accuracy: 0.6875\n",
      "At: 209 [==========>] Loss 0.20694733125439824  - accuracy: 0.6875\n",
      "At: 210 [==========>] Loss 0.09941486994080803  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.15474733710750613  - accuracy: 0.75\n",
      "At: 212 [==========>] Loss 0.19715238785831457  - accuracy: 0.71875\n",
      "At: 213 [==========>] Loss 0.18815333083979763  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.20339293479974613  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.1215657042701988  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.16865770383615952  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.18133271999071904  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.08726944493669556  - accuracy: 0.90625\n",
      "At: 219 [==========>] Loss 0.20904443543529133  - accuracy: 0.6875\n",
      "At: 220 [==========>] Loss 0.16245646807865682  - accuracy: 0.78125\n",
      "At: 221 [==========>] Loss 0.15228089474963344  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.09712212365550088  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.23215724710156166  - accuracy: 0.75\n",
      "At: 224 [==========>] Loss 0.14665586665042085  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.14603504415964483  - accuracy: 0.78125\n",
      "At: 226 [==========>] Loss 0.18463739781878968  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.09916640272186503  - accuracy: 0.8125\n",
      "At: 228 [==========>] Loss 0.1780967408894429  - accuracy: 0.71875\n",
      "At: 229 [==========>] Loss 0.20796195260117764  - accuracy: 0.65625\n",
      "At: 230 [==========>] Loss 0.14841509066066327  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.17122680425529674  - accuracy: 0.75\n",
      "At: 232 [==========>] Loss 0.1981402780511641  - accuracy: 0.71875\n",
      "At: 233 [==========>] Loss 0.15269259020827097  - accuracy: 0.78125\n",
      "At: 234 [==========>] Loss 0.14682126781705152  - accuracy: 0.84375\n",
      "At: 235 [==========>] Loss 0.19329291516053354  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.18217588252867356  - accuracy: 0.78125\n",
      "At: 237 [==========>] Loss 0.12089409695697464  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.09452625318194283  - accuracy: 0.90625\n",
      "At: 239 [==========>] Loss 0.13625751263066715  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.17937028903531338  - accuracy: 0.71875\n",
      "At: 241 [==========>] Loss 0.10900524442813742  - accuracy: 0.875\n",
      "At: 242 [==========>] Loss 0.14386058313551287  - accuracy: 0.71875\n",
      "At: 243 [==========>] Loss 0.1384542293361765  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.1300751699426935  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.10310414514430195  - accuracy: 0.875\n",
      "At: 246 [==========>] Loss 0.14657603861868657  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.12520112168984693  - accuracy: 0.8125\n",
      "At: 248 [==========>] Loss 0.07303236415540379  - accuracy: 0.90625\n",
      "At: 249 [==========>] Loss 0.1507508022646183  - accuracy: 0.75\n",
      "At: 250 [==========>] Loss 0.2065254463622386  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.16713497904996583  - accuracy: 0.75\n",
      "At: 252 [==========>] Loss 0.10098348409459035  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.15802247656618557  - accuracy: 0.75\n",
      "At: 254 [==========>] Loss 0.09499054754350508  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.16647656199905575  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.15395676742598657  - accuracy: 0.8125\n",
      "At: 257 [==========>] Loss 0.0918964670889415  - accuracy: 0.875\n",
      "At: 258 [==========>] Loss 0.17659107690186393  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.12043103344246317  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.1257063095778117  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.08642494281723939  - accuracy: 0.9375\n",
      "At: 262 [==========>] Loss 0.14517987102432628  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.06749066678349755  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.10511514347262925  - accuracy: 0.875\n",
      "At: 265 [==========>] Loss 0.17211179903646717  - accuracy: 0.71875\n",
      "At: 266 [==========>] Loss 0.2344258567752341  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.12798423110595764  - accuracy: 0.875\n",
      "At: 268 [==========>] Loss 0.1826411241180908  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.11585672131122896  - accuracy: 0.8125\n",
      "At: 270 [==========>] Loss 0.22723706951456096  - accuracy: 0.71875\n",
      "At: 271 [==========>] Loss 0.1517560115394303  - accuracy: 0.6875\n",
      "At: 272 [==========>] Loss 0.0861257505777135  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.13068283367751077  - accuracy: 0.78125\n",
      "At: 274 [==========>] Loss 0.14703626602470937  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.07723298215554511  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.17847128695867964  - accuracy: 0.6875\n",
      "At: 277 [==========>] Loss 0.15506367249130454  - accuracy: 0.78125\n",
      "At: 278 [==========>] Loss 0.1247113972051499  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.14836213752489036  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.13625326274816993  - accuracy: 0.84375\n",
      "At: 281 [==========>] Loss 0.16898234095605005  - accuracy: 0.71875\n",
      "At: 282 [==========>] Loss 0.1866093089574033  - accuracy: 0.75\n",
      "At: 283 [==========>] Loss 0.13070367823226195  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.09121820684669488  - accuracy: 0.9375\n",
      "At: 285 [==========>] Loss 0.14984848561515293  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.05061440476021454  - accuracy: 0.96875\n",
      "At: 287 [==========>] Loss 0.16564561914172526  - accuracy: 0.78125\n",
      "At: 288 [==========>] Loss 0.10063070717746789  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.0879232666780852  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.1327410743370563  - accuracy: 0.78125\n",
      "At: 291 [==========>] Loss 0.15520699863321352  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.13786115783732455  - accuracy: 0.75\n",
      "At: 293 [==========>] Loss 0.20787681952941345  - accuracy: 0.75\n",
      "At: 294 [==========>] Loss 0.14516792613702165  - accuracy: 0.8125\n",
      "At: 295 [==========>] Loss 0.13196116495159757  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.07789646570692815  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.10563330425770008  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.133576796607994  - accuracy: 0.78125\n",
      "At: 299 [==========>] Loss 0.1889054044339722  - accuracy: 0.65625\n",
      "At: 300 [==========>] Loss 0.15968917931698184  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.16670993387816116  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.12874888262168407  - accuracy: 0.84375\n",
      "At: 303 [==========>] Loss 0.0917865303776296  - accuracy: 0.90625\n",
      "At: 304 [==========>] Loss 0.15072505246522871  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.22339589534380744  - accuracy: 0.71875\n",
      "At: 306 [==========>] Loss 0.10381764714378577  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.2748059582644632  - accuracy: 0.625\n",
      "At: 308 [==========>] Loss 0.1483937032078721  - accuracy: 0.8125\n",
      "At: 309 [==========>] Loss 0.11220262984197191  - accuracy: 0.8125\n",
      "At: 310 [==========>] Loss 0.20060973591128944  - accuracy: 0.65625\n",
      "At: 311 [==========>] Loss 0.08155819738795947  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.13900552219707285  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.0978037353062574  - accuracy: 0.875\n",
      "At: 314 [==========>] Loss 0.1723373060791774  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.12012773791884769  - accuracy: 0.8125\n",
      "At: 316 [==========>] Loss 0.1639943601948271  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.26511105608568086  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.12852930307743193  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.10056413729469464  - accuracy: 0.8125\n",
      "At: 320 [==========>] Loss 0.16999699379599084  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.1588305840926338  - accuracy: 0.8125\n",
      "At: 322 [==========>] Loss 0.07117665784227901  - accuracy: 0.9375\n",
      "At: 323 [==========>] Loss 0.1202059771738267  - accuracy: 0.84375\n",
      "At: 324 [==========>] Loss 0.11743846635636043  - accuracy: 0.875\n",
      "At: 325 [==========>] Loss 0.10276601206610919  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.18235465496582498  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.11428215454601583  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.11544243630686424  - accuracy: 0.90625\n",
      "At: 329 [==========>] Loss 0.15673372602588265  - accuracy: 0.8125\n",
      "At: 330 [==========>] Loss 0.12842184612693605  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.16739452863476062  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.2641775452932012  - accuracy: 0.65625\n",
      "At: 333 [==========>] Loss 0.15646068789440848  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.10074345390543703  - accuracy: 0.84375\n",
      "At: 335 [==========>] Loss 0.15189964305276205  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.14603538168400815  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.1391744625734746  - accuracy: 0.84375\n",
      "At: 338 [==========>] Loss 0.10883322510444843  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.1539399596680346  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.13106644005620255  - accuracy: 0.78125\n",
      "At: 341 [==========>] Loss 0.1517283010485912  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.10617967137627514  - accuracy: 0.84375\n",
      "At: 343 [==========>] Loss 0.2129979984124882  - accuracy: 0.6875\n",
      "At: 344 [==========>] Loss 0.19454653638499797  - accuracy: 0.65625\n",
      "At: 345 [==========>] Loss 0.13894775508780344  - accuracy: 0.8125\n",
      "At: 346 [==========>] Loss 0.12710005096561175  - accuracy: 0.875\n",
      "At: 347 [==========>] Loss 0.10973940428437545  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.09824133701630619  - accuracy: 0.90625\n",
      "At: 349 [==========>] Loss 0.14776900136830579  - accuracy: 0.78125\n",
      "At: 350 [==========>] Loss 0.12571215363868007  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.1876879731499621  - accuracy: 0.75\n",
      "At: 352 [==========>] Loss 0.11761173132541076  - accuracy: 0.875\n",
      "At: 353 [==========>] Loss 0.11015866520509505  - accuracy: 0.875\n",
      "At: 354 [==========>] Loss 0.18354907155988653  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.08912799675950149  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.16187658712123437  - accuracy: 0.78125\n",
      "At: 357 [==========>] Loss 0.1328890474079166  - accuracy: 0.78125\n",
      "At: 358 [==========>] Loss 0.1446775254030231  - accuracy: 0.78125\n",
      "At: 359 [==========>] Loss 0.07565214346718685  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.10215656913260554  - accuracy: 0.875\n",
      "At: 361 [==========>] Loss 0.09825890966695242  - accuracy: 0.9375\n",
      "At: 362 [==========>] Loss 0.17190036728421734  - accuracy: 0.71875\n",
      "At: 363 [==========>] Loss 0.16055339555580994  - accuracy: 0.75\n",
      "At: 364 [==========>] Loss 0.19777328147971895  - accuracy: 0.6875\n",
      "At: 365 [==========>] Loss 0.10575402150157429  - accuracy: 0.90625\n",
      "At: 366 [==========>] Loss 0.16959862608198834  - accuracy: 0.8125\n",
      "At: 367 [==========>] Loss 0.13997506329972628  - accuracy: 0.84375\n",
      "At: 368 [==========>] Loss 0.18365506498634573  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.13980198610598388  - accuracy: 0.84375\n",
      "At: 370 [==========>] Loss 0.17325645099587622  - accuracy: 0.78125\n",
      "At: 371 [==========>] Loss 0.0970765683926824  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.1400979653401865  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.22212359848027646  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.0715464820007554  - accuracy: 0.90625\n",
      "At: 375 [==========>] Loss 0.08681487079678277  - accuracy: 0.90625\n",
      "At: 376 [==========>] Loss 0.06572844318879181  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.16555852130345516  - accuracy: 0.78125\n",
      "At: 378 [==========>] Loss 0.14457144554706539  - accuracy: 0.8125\n",
      "At: 379 [==========>] Loss 0.14986973695705982  - accuracy: 0.78125\n",
      "At: 380 [==========>] Loss 0.14890788820117107  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.19908245649734166  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.10856873748518137  - accuracy: 0.8125\n",
      "At: 383 [==========>] Loss 0.18412399712835417  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.16863167785804578  - accuracy: 0.8125\n",
      "At: 385 [==========>] Loss 0.14700779435521436  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.17075365822748753  - accuracy: 0.78125\n",
      "At: 387 [==========>] Loss 0.09030805236423298  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.19348906179382072  - accuracy: 0.6875\n",
      "At: 389 [==========>] Loss 0.15979214637480807  - accuracy: 0.78125\n",
      "At: 390 [==========>] Loss 0.10572790899635043  - accuracy: 0.90625\n",
      "At: 391 [==========>] Loss 0.08757379685382231  - accuracy: 0.875\n",
      "At: 392 [==========>] Loss 0.14726076068295071  - accuracy: 0.75\n",
      "At: 393 [==========>] Loss 0.26103803602728115  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.09116556686868742  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.17833691630830145  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.13455334145104297  - accuracy: 0.84375\n",
      "At: 397 [==========>] Loss 0.11094446270765815  - accuracy: 0.84375\n",
      "At: 398 [==========>] Loss 0.18771555057833772  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.14505977780325605  - accuracy: 0.78125\n",
      "At: 400 [==========>] Loss 0.1955623111329214  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.09816123405029145  - accuracy: 0.875\n",
      "At: 402 [==========>] Loss 0.1404385402265265  - accuracy: 0.78125\n",
      "At: 403 [==========>] Loss 0.04814300691472541  - accuracy: 0.9375\n",
      "At: 404 [==========>] Loss 0.07829706341643841  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.1609408295695645  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.154575859284571  - accuracy: 0.75\n",
      "At: 407 [==========>] Loss 0.15789442707899007  - accuracy: 0.71875\n",
      "At: 408 [==========>] Loss 0.18458946273786997  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.16987403876267865  - accuracy: 0.75\n",
      "At: 410 [==========>] Loss 0.11346547927286763  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10283960633702754  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.1577000533387522  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.11780885631323151  - accuracy: 0.8125\n",
      "At: 414 [==========>] Loss 0.13281216430736437  - accuracy: 0.84375\n",
      "At: 415 [==========>] Loss 0.15654832027086213  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.1606391878383265  - accuracy: 0.78125\n",
      "At: 417 [==========>] Loss 0.18377217832306048  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.12651663943083113  - accuracy: 0.875\n",
      "At: 419 [==========>] Loss 0.13472477813966757  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.1335528364788524  - accuracy: 0.8125\n",
      "At: 421 [==========>] Loss 0.10008304579875255  - accuracy: 0.90625\n",
      "At: 422 [==========>] Loss 0.11544501154358636  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.1259771821711953  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.17139948170074504  - accuracy: 0.75\n",
      "At: 425 [==========>] Loss 0.1730833843937502  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.15257173217128292  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.16842682700181943  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.22789633607262438  - accuracy: 0.6875\n",
      "At: 429 [==========>] Loss 0.16918922408300893  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.13607423028628118  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.11991654829146527  - accuracy: 0.8125\n",
      "At: 432 [==========>] Loss 0.11918333725400095  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.10481817424117346  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.11452909759646383  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.15870524439259953  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.16774404070947121  - accuracy: 0.71875\n",
      "At: 437 [==========>] Loss 0.1261014943791397  - accuracy: 0.75\n",
      "At: 438 [==========>] Loss 0.12014864134780014  - accuracy: 0.8125\n",
      "At: 439 [==========>] Loss 0.1345164872609117  - accuracy: 0.8125\n",
      "At: 440 [==========>] Loss 0.08214915970945529  - accuracy: 0.9375\n",
      "At: 441 [==========>] Loss 0.16615980626385735  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.19985999684508282  - accuracy: 0.75\n",
      "At: 443 [==========>] Loss 0.15092040950540705  - accuracy: 0.75\n",
      "At: 444 [==========>] Loss 0.1635420832795268  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.14789536847346935  - accuracy: 0.8125\n",
      "At: 446 [==========>] Loss 0.20242751121222505  - accuracy: 0.78125\n",
      "At: 447 [==========>] Loss 0.13302671484884665  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.10876126278224389  - accuracy: 0.8125\n",
      "At: 449 [==========>] Loss 0.0752468515701899  - accuracy: 0.90625\n",
      "At: 450 [==========>] Loss 0.11728305957813122  - accuracy: 0.84375\n",
      "At: 451 [==========>] Loss 0.10180036666219014  - accuracy: 0.90625\n",
      "At: 452 [==========>] Loss 0.11213863672556787  - accuracy: 0.90625\n",
      "At: 453 [==========>] Loss 0.13185933996795893  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.2072302102297936  - accuracy: 0.75\n",
      "At: 455 [==========>] Loss 0.15689925413894712  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.128458473636047  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.1274338758732874  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.09916657134268897  - accuracy: 0.90625\n",
      "At: 459 [==========>] Loss 0.18000605813380793  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.11652534993051505  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.15330121155654394  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.1773772883519192  - accuracy: 0.71875\n",
      "At: 463 [==========>] Loss 0.11891862021263029  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.15861784340994656  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.18938607204443902  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.09320723881609182  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.157937264438997  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.08395714692696692  - accuracy: 0.90625\n",
      "At: 469 [==========>] Loss 0.12473134402407098  - accuracy: 0.84375\n",
      "At: 470 [==========>] Loss 0.09449370449469158  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.16819376854546972  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.12388404609889755  - accuracy: 0.84375\n",
      "At: 473 [==========>] Loss 0.15371664910772936  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.13055719441700908  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.13817905208498443  - accuracy: 0.84375\n",
      "At: 476 [==========>] Loss 0.161051280171422  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.1433028128498022  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.12453035025756555  - accuracy: 0.8125\n",
      "At: 479 [==========>] Loss 0.13361635607482975  - accuracy: 0.875\n",
      "At: 480 [==========>] Loss 0.17506747394789426  - accuracy: 0.71875\n",
      "At: 481 [==========>] Loss 0.1196887968017678  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.09100438120963372  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.1071942261701022  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.09164578267771156  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.12829854891073467  - accuracy: 0.78125\n",
      "At: 486 [==========>] Loss 0.20680404822659515  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.13909155680098662  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.09538737653344286  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.11748939575407508  - accuracy: 0.8125\n",
      "At: 490 [==========>] Loss 0.12999889067436818  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.12984488607498218  - accuracy: 0.78125\n",
      "At: 492 [==========>] Loss 0.19613421216571247  - accuracy: 0.78125\n",
      "At: 493 [==========>] Loss 0.11155456526066901  - accuracy: 0.875\n",
      "At: 494 [==========>] Loss 0.13077115442345721  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.12753290652421112  - accuracy: 0.875\n",
      "At: 496 [==========>] Loss 0.16627885979453355  - accuracy: 0.75\n",
      "At: 497 [==========>] Loss 0.19264846617834122  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.06569160606339068  - accuracy: 0.96875\n",
      "At: 499 [==========>] Loss 0.14960610895841808  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.14946394792798984  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.17607658726670833  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.109003948091254  - accuracy: 0.875\n",
      "At: 503 [==========>] Loss 0.11307943716310341  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.10733726800640442  - accuracy: 0.875\n",
      "At: 505 [==========>] Loss 0.14794141142614245  - accuracy: 0.78125\n",
      "At: 506 [==========>] Loss 0.20211974845911757  - accuracy: 0.6875\n",
      "At: 507 [==========>] Loss 0.10757241355179897  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.09119062974422787  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.14222022242819335  - accuracy: 0.78125\n",
      "At: 510 [==========>] Loss 0.17222894612430478  - accuracy: 0.78125\n",
      "At: 511 [==========>] Loss 0.12366952959161936  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.16753609376520603  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.16131191718438942  - accuracy: 0.78125\n",
      "At: 514 [==========>] Loss 0.16430684903178255  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.11341712014488844  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.17666438444988264  - accuracy: 0.78125\n",
      "At: 517 [==========>] Loss 0.12725170762154997  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.12872824857609572  - accuracy: 0.84375\n",
      "At: 519 [==========>] Loss 0.09271531052425962  - accuracy: 0.90625\n",
      "At: 520 [==========>] Loss 0.126903809027506  - accuracy: 0.84375\n",
      "At: 521 [==========>] Loss 0.14201973282246852  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.10998473292838155  - accuracy: 0.84375\n",
      "At: 523 [==========>] Loss 0.15542755968635022  - accuracy: 0.71875\n",
      "At: 524 [==========>] Loss 0.10866778326964452  - accuracy: 0.8125\n",
      "At: 525 [==========>] Loss 0.12776577431966674  - accuracy: 0.875\n",
      "At: 526 [==========>] Loss 0.16265440219566515  - accuracy: 0.8125\n",
      "At: 527 [==========>] Loss 0.23910211854820962  - accuracy: 0.65625\n",
      "At: 528 [==========>] Loss 0.2038179413332835  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.11646361872981295  - accuracy: 0.875\n",
      "At: 530 [==========>] Loss 0.1456079912343769  - accuracy: 0.78125\n",
      "At: 531 [==========>] Loss 0.16242666003429979  - accuracy: 0.75\n",
      "At: 532 [==========>] Loss 0.11704715739232852  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.06688840534019945  - accuracy: 0.9375\n",
      "At: 534 [==========>] Loss 0.15069428069453938  - accuracy: 0.71875\n",
      "At: 535 [==========>] Loss 0.11349842704702723  - accuracy: 0.90625\n",
      "At: 536 [==========>] Loss 0.11186351963468112  - accuracy: 0.8125\n",
      "At: 537 [==========>] Loss 0.10373997009367225  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.15263672828325042  - accuracy: 0.8125\n",
      "At: 539 [==========>] Loss 0.10489326974084459  - accuracy: 0.875\n",
      "At: 540 [==========>] Loss 0.23497317135773582  - accuracy: 0.65625\n",
      "At: 541 [==========>] Loss 0.14904596710026052  - accuracy: 0.78125\n",
      "At: 542 [==========>] Loss 0.1354921428658677  - accuracy: 0.875\n",
      "At: 543 [==========>] Loss 0.12195726950866195  - accuracy: 0.78125\n",
      "At: 544 [==========>] Loss 0.2246468188110436  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.1106101435368905  - accuracy: 0.84375\n",
      "At: 546 [==========>] Loss 0.18753805924623132  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.12352662887365794  - accuracy: 0.875\n",
      "At: 548 [==========>] Loss 0.12387559247688842  - accuracy: 0.84375\n",
      "At: 549 [==========>] Loss 0.12225816981154225  - accuracy: 0.84375\n",
      "At: 550 [==========>] Loss 0.08353183660587292  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.11036099853686945  - accuracy: 0.875\n",
      "At: 552 [==========>] Loss 0.1348953864990091  - accuracy: 0.84375\n",
      "At: 553 [==========>] Loss 0.1194248144903115  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.06622817903736425  - accuracy: 0.9375\n",
      "At: 555 [==========>] Loss 0.15204705903364987  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.17038042955028765  - accuracy: 0.75\n",
      "At: 557 [==========>] Loss 0.11442640459740376  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.16776180237539828  - accuracy: 0.84375\n",
      "At: 559 [==========>] Loss 0.19286291939892103  - accuracy: 0.6875\n",
      "At: 560 [==========>] Loss 0.1373733815729706  - accuracy: 0.75\n",
      "At: 561 [==========>] Loss 0.12657142759871168  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.044177716953348115  - accuracy: 1.0\n",
      "At: 563 [==========>] Loss 0.12898291261946404  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.12806515250780695  - accuracy: 0.8125\n",
      "At: 565 [==========>] Loss 0.10136806502041124  - accuracy: 0.90625\n",
      "At: 566 [==========>] Loss 0.14137228052488066  - accuracy: 0.8125\n",
      "At: 567 [==========>] Loss 0.1676330287531811  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.2109938500619957  - accuracy: 0.71875\n",
      "At: 569 [==========>] Loss 0.13501173967535526  - accuracy: 0.78125\n",
      "At: 570 [==========>] Loss 0.12220988962907911  - accuracy: 0.78125\n",
      "At: 571 [==========>] Loss 0.0968523139022629  - accuracy: 0.90625\n",
      "At: 572 [==========>] Loss 0.1367927598086363  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.10013112623302833  - accuracy: 0.9375\n",
      "At: 574 [==========>] Loss 0.157468751447321  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.12693065899039274  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.10863296076841009  - accuracy: 0.90625\n",
      "At: 577 [==========>] Loss 0.18114452593768737  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.1977354246935798  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.07934094719686309  - accuracy: 0.90625\n",
      "At: 580 [==========>] Loss 0.12280565216416474  - accuracy: 0.84375\n",
      "At: 581 [==========>] Loss 0.15423411844108947  - accuracy: 0.78125\n",
      "At: 582 [==========>] Loss 0.1322258071249164  - accuracy: 0.8125\n",
      "At: 583 [==========>] Loss 0.15191299135790606  - accuracy: 0.78125\n",
      "At: 584 [==========>] Loss 0.06589333959713416  - accuracy: 0.9375\n",
      "At: 585 [==========>] Loss 0.14385886880353152  - accuracy: 0.75\n",
      "At: 586 [==========>] Loss 0.05399217099066957  - accuracy: 0.90625\n",
      "At: 587 [==========>] Loss 0.1617837781066063  - accuracy: 0.8125\n",
      "At: 588 [==========>] Loss 0.12090795578775494  - accuracy: 0.84375\n",
      "At: 589 [==========>] Loss 0.1323997235717903  - accuracy: 0.8125\n",
      "At: 590 [==========>] Loss 0.0839474368844956  - accuracy: 0.875\n",
      "At: 591 [==========>] Loss 0.1589416987187055  - accuracy: 0.75\n",
      "At: 592 [==========>] Loss 0.08085321946262064  - accuracy: 0.875\n",
      "At: 593 [==========>] Loss 0.18846960533533635  - accuracy: 0.75\n",
      "At: 594 [==========>] Loss 0.12598081436277384  - accuracy: 0.84375\n",
      "At: 595 [==========>] Loss 0.1544643331301478  - accuracy: 0.75\n",
      "At: 596 [==========>] Loss 0.13734125155365984  - accuracy: 0.875\n",
      "At: 597 [==========>] Loss 0.1996738200787988  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.12785681854883907  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.13910355995827373  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.1104120696326835  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.1233213232488388  - accuracy: 0.75\n",
      "At: 602 [==========>] Loss 0.14976036512726448  - accuracy: 0.78125\n",
      "At: 603 [==========>] Loss 0.1774896411520882  - accuracy: 0.75\n",
      "At: 604 [==========>] Loss 0.14947142147374853  - accuracy: 0.875\n",
      "At: 605 [==========>] Loss 0.09735969197480798  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.1159464191458276  - accuracy: 0.84375\n",
      "At: 607 [==========>] Loss 0.1241459327839736  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.11826542904635183  - accuracy: 0.8125\n",
      "At: 609 [==========>] Loss 0.10037476062073801  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.14167197049364574  - accuracy: 0.78125\n",
      "At: 611 [==========>] Loss 0.1260074515285738  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.13711941167922487  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.15880738006505582  - accuracy: 0.75\n",
      "At: 614 [==========>] Loss 0.1273606673891146  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.1426454874788499  - accuracy: 0.8125\n",
      "At: 616 [==========>] Loss 0.1621505921385736  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.12377668346977183  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.15291789464382893  - accuracy: 0.78125\n",
      "At: 619 [==========>] Loss 0.14858188586294846  - accuracy: 0.71875\n",
      "At: 620 [==========>] Loss 0.13271172044976778  - accuracy: 0.875\n",
      "At: 621 [==========>] Loss 0.07451066326866405  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.2222474346556957  - accuracy: 0.6875\n",
      "At: 623 [==========>] Loss 0.1413221806477451  - accuracy: 0.78125\n",
      "At: 624 [==========>] Loss 0.08769340270154101  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.10702338919120136  - accuracy: 0.875\n",
      "At: 626 [==========>] Loss 0.11280205195550333  - accuracy: 0.84375\n",
      "At: 627 [==========>] Loss 0.13890805949667087  - accuracy: 0.75\n",
      "At: 628 [==========>] Loss 0.1298923041033121  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.20350991997120796  - accuracy: 0.65625\n",
      "At: 630 [==========>] Loss 0.19593704937368359  - accuracy: 0.6875\n",
      "At: 631 [==========>] Loss 0.19644993988089937  - accuracy: 0.6875\n",
      "At: 632 [==========>] Loss 0.1275032667774485  - accuracy: 0.84375\n",
      "At: 633 [==========>] Loss 0.15801060194428174  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.1232945031237655  - accuracy: 0.84375\n",
      "At: 635 [==========>] Loss 0.14648572959126818  - accuracy: 0.78125\n",
      "At: 636 [==========>] Loss 0.1560017048641915  - accuracy: 0.78125\n",
      "At: 637 [==========>] Loss 0.12796050114249816  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.11865418312855863  - accuracy: 0.90625\n",
      "At: 639 [==========>] Loss 0.1621753356299782  - accuracy: 0.71875\n",
      "At: 640 [==========>] Loss 0.19358240745121308  - accuracy: 0.75\n",
      "At: 641 [==========>] Loss 0.09433821798318903  - accuracy: 0.84375\n",
      "At: 642 [==========>] Loss 0.20204475811486422  - accuracy: 0.6875\n",
      "At: 643 [==========>] Loss 0.11290473113931382  - accuracy: 0.8125\n",
      "At: 644 [==========>] Loss 0.06369050383030155  - accuracy: 0.96875\n",
      "At: 645 [==========>] Loss 0.14019400546445943  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.11336483472971774  - accuracy: 0.8125\n",
      "At: 647 [==========>] Loss 0.16425392365196334  - accuracy: 0.8125\n",
      "At: 648 [==========>] Loss 0.1461157097160273  - accuracy: 0.78125\n",
      "At: 649 [==========>] Loss 0.16426065935544681  - accuracy: 0.71875\n",
      "At: 650 [==========>] Loss 0.08662581381414142  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.16758517389827937  - accuracy: 0.65625\n",
      "At: 652 [==========>] Loss 0.12305705240583029  - accuracy: 0.84375\n",
      "At: 653 [==========>] Loss 0.11080001342769304  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.10913642277189316  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.17931720966037584  - accuracy: 0.71875\n",
      "At: 656 [==========>] Loss 0.14326243546592635  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.11989983939417057  - accuracy: 0.84375\n",
      "At: 658 [==========>] Loss 0.10654178583889581  - accuracy: 0.90625\n",
      "At: 659 [==========>] Loss 0.10944278403529512  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.09687053708618032  - accuracy: 0.875\n",
      "At: 661 [==========>] Loss 0.133570190844627  - accuracy: 0.78125\n",
      "At: 662 [==========>] Loss 0.08122155195207642  - accuracy: 0.875\n",
      "At: 663 [==========>] Loss 0.11959397101738475  - accuracy: 0.84375\n",
      "At: 664 [==========>] Loss 0.09448768539384159  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.21944149026296159  - accuracy: 0.625\n",
      "At: 666 [==========>] Loss 0.1758859995735889  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.11871260228599251  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.13223480121542694  - accuracy: 0.78125\n",
      "At: 669 [==========>] Loss 0.12003301301046768  - accuracy: 0.875\n",
      "At: 670 [==========>] Loss 0.18931483944266217  - accuracy: 0.75\n",
      "At: 671 [==========>] Loss 0.08542047497792854  - accuracy: 0.84375\n",
      "At: 672 [==========>] Loss 0.15117127536755706  - accuracy: 0.78125\n",
      "At: 673 [==========>] Loss 0.06941307389429785  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.12010235847860076  - accuracy: 0.84375\n",
      "At: 675 [==========>] Loss 0.10165082689837707  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.15463043371297497  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.1647774586824709  - accuracy: 0.8125\n",
      "At: 678 [==========>] Loss 0.11833872949475564  - accuracy: 0.84375\n",
      "At: 679 [==========>] Loss 0.08798917347196059  - accuracy: 0.875\n",
      "At: 680 [==========>] Loss 0.10866489839150312  - accuracy: 0.875\n",
      "At: 681 [==========>] Loss 0.12860095338019517  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.1352667764282584  - accuracy: 0.8125\n",
      "At: 683 [==========>] Loss 0.16743590286911675  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.11845832795069694  - accuracy: 0.8125\n",
      "At: 685 [==========>] Loss 0.10810326615008475  - accuracy: 0.875\n",
      "At: 686 [==========>] Loss 0.10045059424181117  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.06037908427879616  - accuracy: 0.96875\n",
      "At: 688 [==========>] Loss 0.09649962184109899  - accuracy: 0.875\n",
      "At: 689 [==========>] Loss 0.16212414716338824  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.13260057239911535  - accuracy: 0.78125\n",
      "At: 691 [==========>] Loss 0.07397706815005753  - accuracy: 0.875\n",
      "At: 692 [==========>] Loss 0.10623350780611723  - accuracy: 0.84375\n",
      "At: 693 [==========>] Loss 0.1510747109323549  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.20161356648282314  - accuracy: 0.6875\n",
      "At: 695 [==========>] Loss 0.17045841012537033  - accuracy: 0.8125\n",
      "At: 696 [==========>] Loss 0.14480722793080553  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.18003899669285564  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.11653226822104042  - accuracy: 0.84375\n",
      "At: 699 [==========>] Loss 0.10245345584441926  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.08091114392094466  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.12600912829022742  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.07437069460586906  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.13674044814259523  - accuracy: 0.8125\n",
      "At: 704 [==========>] Loss 0.15385387271049472  - accuracy: 0.8125\n",
      "At: 705 [==========>] Loss 0.22683273245549312  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.15406116934151265  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.11732364608017798  - accuracy: 0.8125\n",
      "At: 708 [==========>] Loss 0.16918858338776308  - accuracy: 0.6875\n",
      "At: 709 [==========>] Loss 0.18428003853441924  - accuracy: 0.75\n",
      "At: 710 [==========>] Loss 0.13035043985042227  - accuracy: 0.84375\n",
      "At: 711 [==========>] Loss 0.17953606863671617  - accuracy: 0.75\n",
      "At: 712 [==========>] Loss 0.14410958475829277  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.17132955257064425  - accuracy: 0.8125\n",
      "At: 714 [==========>] Loss 0.2052000498245407  - accuracy: 0.75\n",
      "At: 715 [==========>] Loss 0.09614643546788618  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.09569792170466221  - accuracy: 0.90625\n",
      "At: 717 [==========>] Loss 0.08440926533588561  - accuracy: 0.96875\n",
      "At: 718 [==========>] Loss 0.1759002616954677  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.11087142428793254  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.11909717275170911  - accuracy: 0.875\n",
      "At: 721 [==========>] Loss 0.08649239995176589  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.15306480492965244  - accuracy: 0.8125\n",
      "At: 723 [==========>] Loss 0.12844325441055343  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.07566826113604957  - accuracy: 0.90625\n",
      "At: 725 [==========>] Loss 0.15355809371831772  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.15649031047647624  - accuracy: 0.8125\n",
      "At: 727 [==========>] Loss 0.12046698988380936  - accuracy: 0.84375\n",
      "At: 728 [==========>] Loss 0.17183913342586893  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.18203240212406213  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.1304229018188523  - accuracy: 0.8125\n",
      "At: 731 [==========>] Loss 0.16901411463032973  - accuracy: 0.75\n",
      "At: 732 [==========>] Loss 0.19235058388316173  - accuracy: 0.6875\n",
      "At: 733 [==========>] Loss 0.10286047463596479  - accuracy: 0.90625\n",
      "At: 734 [==========>] Loss 0.18880577089467865  - accuracy: 0.6875\n",
      "At: 735 [==========>] Loss 0.10514975425661643  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.11451314440726565  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.12810448819011827  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.10822773703449776  - accuracy: 0.90625\n",
      "At: 739 [==========>] Loss 0.07446072092729014  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.14098256895729186  - accuracy: 0.78125\n",
      "At: 741 [==========>] Loss 0.12456334111872557  - accuracy: 0.78125\n",
      "At: 742 [==========>] Loss 0.09891927663902074  - accuracy: 0.875\n",
      "At: 743 [==========>] Loss 0.15817125778449959  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.14190610954507277  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.18475863022881922  - accuracy: 0.71875\n",
      "At: 746 [==========>] Loss 0.12498868431982432  - accuracy: 0.84375\n",
      "At: 747 [==========>] Loss 0.1283437198258827  - accuracy: 0.8125\n",
      "At: 748 [==========>] Loss 0.13960582872651248  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.13630451794763807  - accuracy: 0.8125\n",
      "At: 750 [==========>] Loss 0.09231803906624024  - accuracy: 0.90625\n",
      "At: 751 [==========>] Loss 0.13201255816979757  - accuracy: 0.8125\n",
      "At: 752 [==========>] Loss 0.0651286022221744  - accuracy: 0.9375\n",
      "At: 753 [==========>] Loss 0.18276309065913843  - accuracy: 0.71875\n",
      "At: 754 [==========>] Loss 0.1312799601974651  - accuracy: 0.8125\n",
      "At: 755 [==========>] Loss 0.08372593209519207  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.19694258949427051  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.11571556091413845  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.12235863054959958  - accuracy: 0.78125\n",
      "At: 759 [==========>] Loss 0.07833671018907726  - accuracy: 0.9375\n",
      "At: 760 [==========>] Loss 0.2012261114965605  - accuracy: 0.6875\n",
      "At: 761 [==========>] Loss 0.1598595847522679  - accuracy: 0.71875\n",
      "At: 762 [==========>] Loss 0.13064072419170156  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.1328700303133504  - accuracy: 0.8125\n",
      "At: 764 [==========>] Loss 0.12519308340661395  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.11318357627258696  - accuracy: 0.875\n",
      "At: 766 [==========>] Loss 0.11986153137125444  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.1237278197022247  - accuracy: 0.8125\n",
      "At: 768 [==========>] Loss 0.16526050861989783  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.11970655803495675  - accuracy: 0.84375\n",
      "At: 770 [==========>] Loss 0.13607536648423366  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.16969797970878803  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.10113668559986433  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.07674601078494359  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.14754227969646466  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.18969379664270375  - accuracy: 0.71875\n",
      "At: 776 [==========>] Loss 0.13293253726971258  - accuracy: 0.8125\n",
      "At: 777 [==========>] Loss 0.07437883418037655  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.15602650149875522  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.08501468081308175  - accuracy: 0.90625\n",
      "At: 780 [==========>] Loss 0.080315150820636  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.13086241839546975  - accuracy: 0.90625\n",
      "At: 782 [==========>] Loss 0.18597585084798035  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.13883635096657893  - accuracy: 0.84375\n",
      "At: 784 [==========>] Loss 0.16550134193060806  - accuracy: 0.8125\n",
      "At: 785 [==========>] Loss 0.20376770007471612  - accuracy: 0.75\n",
      "At: 786 [==========>] Loss 0.12732130777996564  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.14872319067224388  - accuracy: 0.875\n",
      "At: 788 [==========>] Loss 0.07411826140007749  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.14673985325780414  - accuracy: 0.75\n",
      "At: 790 [==========>] Loss 0.11074183909475263  - accuracy: 0.84375\n",
      "At: 791 [==========>] Loss 0.17362353900423838  - accuracy: 0.71875\n",
      "At: 792 [==========>] Loss 0.17461957248552223  - accuracy: 0.75\n",
      "At: 793 [==========>] Loss 0.10753680925140516  - accuracy: 0.90625\n",
      "At: 794 [==========>] Loss 0.13530368182854535  - accuracy: 0.8125\n",
      "At: 795 [==========>] Loss 0.12173849680523698  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.11438713955064439  - accuracy: 0.84375\n",
      "At: 797 [==========>] Loss 0.1851694664690362  - accuracy: 0.75\n",
      "At: 798 [==========>] Loss 0.15420743428783334  - accuracy: 0.75\n",
      "At: 799 [==========>] Loss 0.05865987004965249  - accuracy: 0.9375\n",
      "At: 800 [==========>] Loss 0.17554767996485476  - accuracy: 0.6875\n",
      "At: 801 [==========>] Loss 0.07977466282960283  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.18610628279469332  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.16866531861233638  - accuracy: 0.78125\n",
      "At: 804 [==========>] Loss 0.14502258343845748  - accuracy: 0.8125\n",
      "At: 805 [==========>] Loss 0.13438304069991935  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.07320277349310146  - accuracy: 0.90625\n",
      "At: 807 [==========>] Loss 0.10017562081738746  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.12961593936762095  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.09737326358444627  - accuracy: 0.9375\n",
      "At: 810 [==========>] Loss 0.13397223206616027  - accuracy: 0.8125\n",
      "At: 811 [==========>] Loss 0.08515217259052366  - accuracy: 0.875\n",
      "At: 812 [==========>] Loss 0.11237374160951802  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.17475027674495006  - accuracy: 0.75\n",
      "At: 814 [==========>] Loss 0.17541225997146345  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.15014764661936447  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.14875963317842078  - accuracy: 0.75\n",
      "At: 817 [==========>] Loss 0.06523137312177264  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.14032552888015548  - accuracy: 0.8125\n",
      "At: 819 [==========>] Loss 0.11795853247212726  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.15151333106355477  - accuracy: 0.75\n",
      "At: 821 [==========>] Loss 0.12238565727507159  - accuracy: 0.84375\n",
      "At: 822 [==========>] Loss 0.1397699271038711  - accuracy: 0.8125\n",
      "At: 823 [==========>] Loss 0.14251214691882003  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.12062323358936622  - accuracy: 0.8125\n",
      "At: 825 [==========>] Loss 0.1457614546178266  - accuracy: 0.78125\n",
      "At: 826 [==========>] Loss 0.11406640030750383  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.0899943078076941  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.04862096827360038  - accuracy: 0.96875\n",
      "At: 829 [==========>] Loss 0.08528341754025093  - accuracy: 0.9375\n",
      "At: 830 [==========>] Loss 0.08609416459148445  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.07887289571202785  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.1277813493999026  - accuracy: 0.78125\n",
      "At: 833 [==========>] Loss 0.1705960066319832  - accuracy: 0.6875\n",
      "At: 834 [==========>] Loss 0.06968796159094912  - accuracy: 0.90625\n",
      "At: 835 [==========>] Loss 0.08300443177000748  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.11793480024367936  - accuracy: 0.875\n",
      "At: 837 [==========>] Loss 0.10625061462082333  - accuracy: 0.875\n",
      "At: 838 [==========>] Loss 0.12344832620635754  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.1237796573806699  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.1424533385091371  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.04866087428394748  - accuracy: 0.96875\n",
      "At: 842 [==========>] Loss 0.07970303490090123  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.10072659500274117  - accuracy: 0.90625\n",
      "At: 844 [==========>] Loss 0.11114273171735674  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.13062148584488334  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.10940152547231047  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.08056716780015424  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.1359777993082613  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.16377302717087786  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.11225769660836299  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.08984467743028596  - accuracy: 0.90625\n",
      "At: 852 [==========>] Loss 0.11854798215978232  - accuracy: 0.84375\n",
      "At: 853 [==========>] Loss 0.15517816501839263  - accuracy: 0.84375\n",
      "At: 854 [==========>] Loss 0.20903280283797265  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.05915313136811688  - accuracy: 0.9375\n",
      "At: 856 [==========>] Loss 0.06741216066458114  - accuracy: 0.9375\n",
      "At: 857 [==========>] Loss 0.07656837400743521  - accuracy: 0.90625\n",
      "At: 858 [==========>] Loss 0.26279261953375477  - accuracy: 0.6875\n",
      "At: 859 [==========>] Loss 0.10196688439903825  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.10817265416759524  - accuracy: 0.84375\n",
      "At: 861 [==========>] Loss 0.08683316170689867  - accuracy: 0.90625\n",
      "At: 862 [==========>] Loss 0.07471906711487197  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.13147714919211217  - accuracy: 0.84375\n",
      "At: 864 [==========>] Loss 0.16351090956548686  - accuracy: 0.75\n",
      "At: 865 [==========>] Loss 0.17103743719501896  - accuracy: 0.78125\n",
      "At: 866 [==========>] Loss 0.1384360955637959  - accuracy: 0.8125\n",
      "At: 867 [==========>] Loss 0.09084374414134112  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.1465190586258345  - accuracy: 0.8125\n",
      "At: 869 [==========>] Loss 0.16658610852331784  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.13149612917740874  - accuracy: 0.84375\n",
      "At: 871 [==========>] Loss 0.05945401985271939  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.10865080412858696  - accuracy: 0.84375\n",
      "At: 873 [==========>] Loss 0.17419168057029258  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.12398838350158899  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.11827010523231948  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.10534288772971281  - accuracy: 0.90625\n",
      "At: 877 [==========>] Loss 0.17618096200339717  - accuracy: 0.75\n",
      "At: 878 [==========>] Loss 0.06448190616398004  - accuracy: 0.90625\n",
      "At: 879 [==========>] Loss 0.15375645632046406  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.12859921167131555  - accuracy: 0.75\n",
      "At: 881 [==========>] Loss 0.10187934031160592  - accuracy: 0.84375\n",
      "At: 882 [==========>] Loss 0.1144599040281873  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.10822218660692437  - accuracy: 0.875\n",
      "At: 884 [==========>] Loss 0.12506772901180316  - accuracy: 0.84375\n",
      "At: 885 [==========>] Loss 0.10513614888512968  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.09457985124990745  - accuracy: 0.84375\n",
      "At: 887 [==========>] Loss 0.15282070200977182  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.10416690631022613  - accuracy: 0.8125\n",
      "At: 889 [==========>] Loss 0.09993001723336312  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.14088246429865098  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.08382293667854851  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.14798159093554156  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.16761782107688367  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.1339689145738059  - accuracy: 0.84375\n",
      "At: 895 [==========>] Loss 0.10270389328560203  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.10962739553562581  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.13301664130679897  - accuracy: 0.84375\n",
      "At: 898 [==========>] Loss 0.13342990311140882  - accuracy: 0.84375\n",
      "At: 899 [==========>] Loss 0.07489522219580572  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.13522612973611778  - accuracy: 0.84375\n",
      "At: 901 [==========>] Loss 0.17047514996229862  - accuracy: 0.75\n",
      "At: 902 [==========>] Loss 0.09410857339156667  - accuracy: 0.875\n",
      "At: 903 [==========>] Loss 0.11505468856751451  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.12928341515092462  - accuracy: 0.75\n",
      "At: 905 [==========>] Loss 0.07683405960555922  - accuracy: 0.9375\n",
      "At: 906 [==========>] Loss 0.06780482841854157  - accuracy: 0.9375\n",
      "At: 907 [==========>] Loss 0.17740567559322235  - accuracy: 0.84375\n",
      "At: 908 [==========>] Loss 0.1025964985574844  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.11858368980987721  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.13034506382430358  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.1199876641862636  - accuracy: 0.84375\n",
      "At: 912 [==========>] Loss 0.12674282255452698  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.12708207794827472  - accuracy: 0.875\n",
      "At: 914 [==========>] Loss 0.1381207889964221  - accuracy: 0.78125\n",
      "At: 915 [==========>] Loss 0.09909401239371544  - accuracy: 0.84375\n",
      "At: 916 [==========>] Loss 0.18056770666841315  - accuracy: 0.6875\n",
      "At: 917 [==========>] Loss 0.17362982112901743  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.15882830610299184  - accuracy: 0.84375\n",
      "At: 919 [==========>] Loss 0.10880669619766281  - accuracy: 0.84375\n",
      "At: 920 [==========>] Loss 0.12989701239820323  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.16430933534452646  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.13080827907965742  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.11869688830497344  - accuracy: 0.78125\n",
      "At: 924 [==========>] Loss 0.2014737444137379  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.1273395170904824  - accuracy: 0.875\n",
      "At: 926 [==========>] Loss 0.13714338428576028  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.14611733746427297  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.1363621044579406  - accuracy: 0.8125\n",
      "At: 929 [==========>] Loss 0.1535437955408341  - accuracy: 0.78125\n",
      "At: 930 [==========>] Loss 0.1069328992311242  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.160866830436159  - accuracy: 0.84375\n",
      "At: 932 [==========>] Loss 0.09329657172510543  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.05598589463349112  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.13133070605684155  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.04267416152059153  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.1793884573225648  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.15800299634967296  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.12754896998487653  - accuracy: 0.84375\n",
      "At: 939 [==========>] Loss 0.08171955703542713  - accuracy: 0.90625\n",
      "At: 940 [==========>] Loss 0.21244489079090598  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.11446449943711018  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.11738006371714449  - accuracy: 0.875\n",
      "At: 943 [==========>] Loss 0.11434903769111868  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.15691009054386423  - accuracy: 0.71875\n",
      "At: 945 [==========>] Loss 0.0934792661264047  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.10093989147797267  - accuracy: 0.90625\n",
      "At: 947 [==========>] Loss 0.1323301253988195  - accuracy: 0.84375\n",
      "At: 948 [==========>] Loss 0.16678577620685422  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.05700931482945215  - accuracy: 0.9375\n",
      "At: 950 [==========>] Loss 0.09222178399300043  - accuracy: 0.9375\n",
      "At: 951 [==========>] Loss 0.08943164681093149  - accuracy: 0.96875\n",
      "At: 952 [==========>] Loss 0.058723509469915156  - accuracy: 0.9375\n",
      "At: 953 [==========>] Loss 0.052863538882784354  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.088591929944973  - accuracy: 0.9375\n",
      "At: 955 [==========>] Loss 0.11540936891360239  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.09548507371155206  - accuracy: 0.8125\n",
      "At: 957 [==========>] Loss 0.12679642639187377  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.0857891024824786  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.13643701288391608  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.10515542489006025  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.10923631705655812  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.0923965773345151  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.11258558555426851  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.13120467324256893  - accuracy: 0.84375\n",
      "At: 965 [==========>] Loss 0.12181952728973043  - accuracy: 0.875\n",
      "At: 966 [==========>] Loss 0.15708981297859945  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.10508494153277265  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.13972907710690693  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.1273811460824447  - accuracy: 0.8125\n",
      "At: 970 [==========>] Loss 0.0827153282598719  - accuracy: 0.875\n",
      "At: 971 [==========>] Loss 0.08679208128532272  - accuracy: 0.90625\n",
      "At: 972 [==========>] Loss 0.05286422671292758  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.12439893499327469  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.0730797812708449  - accuracy: 0.9375\n",
      "At: 975 [==========>] Loss 0.10993989697993042  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.09715378382693729  - accuracy: 0.875\n",
      "At: 977 [==========>] Loss 0.10320622640440014  - accuracy: 0.90625\n",
      "At: 978 [==========>] Loss 0.1451327607777984  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.09425338149750044  - accuracy: 0.9375\n",
      "At: 980 [==========>] Loss 0.12763082610733062  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.17804529139050834  - accuracy: 0.6875\n",
      "At: 982 [==========>] Loss 0.06954876252750067  - accuracy: 0.96875\n",
      "At: 983 [==========>] Loss 0.09391725964148008  - accuracy: 0.90625\n",
      "At: 984 [==========>] Loss 0.09198481232882517  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.1708095270373674  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.10690893789795816  - accuracy: 0.875\n",
      "At: 987 [==========>] Loss 0.13239432028696568  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.08531091397020849  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.11558130511024657  - accuracy: 0.84375\n",
      "At: 990 [==========>] Loss 0.13841334713347805  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.12470384119729748  - accuracy: 0.78125\n",
      "At: 992 [==========>] Loss 0.20998314704489868  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.15242945379039485  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.1284439182790183  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.16072593578706204  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.06659276085463478  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.12454886257542912  - accuracy: 0.78125\n",
      "At: 998 [==========>] Loss 0.09876999909928252  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.15382994647233714  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.22025226150896526  - accuracy: 0.6875\n",
      "At: 1001 [==========>] Loss 0.15513200354792966  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.17415888390155937  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.1193161572740446  - accuracy: 0.84375\n",
      "At: 1004 [==========>] Loss 0.12565373183460724  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.0851611137876507  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.09967912561585923  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.1430629253347488  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.16871543515455673  - accuracy: 0.75\n",
      "At: 1009 [==========>] Loss 0.10698394163178268  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.15451668647042638  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.14813314497790941  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.08338855013514551  - accuracy: 0.90625\n",
      "At: 1013 [==========>] Loss 0.06682804957319294  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.08376321419137021  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.18899476491556916  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.14041806345620955  - accuracy: 0.8125\n",
      "At: 1017 [==========>] Loss 0.13060920448850877  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.15164868231435202  - accuracy: 0.75\n",
      "At: 1019 [==========>] Loss 0.19562802233987375  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.11759351556459992  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.10346757647683182  - accuracy: 0.84375\n",
      "At: 1022 [==========>] Loss 0.13280843185125782  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.18926030112637043  - accuracy: 0.625\n",
      "At: 1024 [==========>] Loss 0.18565843451896855  - accuracy: 0.65625\n",
      "At: 1025 [==========>] Loss 0.18638241791672208  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.11200548717953762  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.13088051022571615  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.22791493301944588  - accuracy: 0.59375\n",
      "At: 1029 [==========>] Loss 0.10218475460840507  - accuracy: 0.875\n",
      "At: 1030 [==========>] Loss 0.10045491737763612  - accuracy: 0.90625\n",
      "At: 1031 [==========>] Loss 0.15760882724274017  - accuracy: 0.84375\n",
      "At: 1032 [==========>] Loss 0.13604642223530852  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.13316796793450225  - accuracy: 0.75\n",
      "At: 1034 [==========>] Loss 0.06566429045539858  - accuracy: 0.875\n",
      "At: 1035 [==========>] Loss 0.0926531851071993  - accuracy: 0.84375\n",
      "At: 1036 [==========>] Loss 0.16342522303880522  - accuracy: 0.8125\n",
      "At: 1037 [==========>] Loss 0.14788728067026205  - accuracy: 0.8125\n",
      "At: 1038 [==========>] Loss 0.10972247467189716  - accuracy: 0.8125\n",
      "At: 1039 [==========>] Loss 0.0949347830734714  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.10503061002873623  - accuracy: 0.875\n",
      "At: 1041 [==========>] Loss 0.1299871858544709  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.10003726776190908  - accuracy: 0.875\n",
      "At: 1043 [==========>] Loss 0.188110961719754  - accuracy: 0.71875\n",
      "At: 1044 [==========>] Loss 0.13813597271619557  - accuracy: 0.78125\n",
      "At: 1045 [==========>] Loss 0.12934840872456965  - accuracy: 0.84375\n",
      "At: 1046 [==========>] Loss 0.13115824569816165  - accuracy: 0.8125\n",
      "At: 1047 [==========>] Loss 0.13727360316393358  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.15482549198574672  - accuracy: 0.8125\n",
      "At: 1049 [==========>] Loss 0.13083076932793783  - accuracy: 0.8125\n",
      "At: 1050 [==========>] Loss 0.1299858869083576  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.07157502168470009  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.1089214004683293  - accuracy: 0.90625\n",
      "At: 1053 [==========>] Loss 0.12515350023977026  - accuracy: 0.75\n",
      "At: 1054 [==========>] Loss 0.11164305593615176  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.17854538632089112  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.10591586811832973  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.15576520591968596  - accuracy: 0.75\n",
      "At: 1058 [==========>] Loss 0.05088337879688669  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.0779147740329464  - accuracy: 0.9375\n",
      "At: 1060 [==========>] Loss 0.10194809166355516  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.07074990600857242  - accuracy: 0.9375\n",
      "At: 1062 [==========>] Loss 0.17386156871414282  - accuracy: 0.78125\n",
      "At: 1063 [==========>] Loss 0.11411029198860223  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.13985401683620277  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.08332001601341037  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.0980754391577174  - accuracy: 0.875\n",
      "At: 1067 [==========>] Loss 0.13062899657811353  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.08517183769027385  - accuracy: 0.875\n",
      "At: 1069 [==========>] Loss 0.13527024234806595  - accuracy: 0.84375\n",
      "At: 1070 [==========>] Loss 0.10388514887058825  - accuracy: 0.90625\n",
      "At: 1071 [==========>] Loss 0.08414741751107187  - accuracy: 0.90625\n",
      "At: 1072 [==========>] Loss 0.11012550146498905  - accuracy: 0.90625\n",
      "At: 1073 [==========>] Loss 0.16311609920758607  - accuracy: 0.78125\n",
      "At: 1074 [==========>] Loss 0.13528029935506128  - accuracy: 0.8125\n",
      "At: 1075 [==========>] Loss 0.10319271599820656  - accuracy: 0.90625\n",
      "At: 1076 [==========>] Loss 0.13212955415326647  - accuracy: 0.84375\n",
      "At: 1077 [==========>] Loss 0.07872478554935153  - accuracy: 0.9375\n",
      "At: 1078 [==========>] Loss 0.06774718960105727  - accuracy: 0.90625\n",
      "At: 1079 [==========>] Loss 0.11841115404280633  - accuracy: 0.875\n",
      "At: 1080 [==========>] Loss 0.11384808511722547  - accuracy: 0.875\n",
      "At: 1081 [==========>] Loss 0.0926335287950875  - accuracy: 0.875\n",
      "At: 1082 [==========>] Loss 0.12504377390846466  - accuracy: 0.84375\n",
      "At: 1083 [==========>] Loss 0.08972767563240285  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.07757715541637526  - accuracy: 0.9375\n",
      "At: 1085 [==========>] Loss 0.12042309544053231  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.07003926231782054  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.12477285737086392  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.16042664666924183  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.08789487809444221  - accuracy: 0.90625\n",
      "At: 1090 [==========>] Loss 0.088677752371228  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.17355380750813648  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.09069226631924986  - accuracy: 0.90625\n",
      "At: 1093 [==========>] Loss 0.14378980850117773  - accuracy: 0.875\n",
      "At: 1094 [==========>] Loss 0.12823756885617948  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.1445662011168632  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.07849120701087273  - accuracy: 0.9375\n",
      "At: 1097 [==========>] Loss 0.06279219355420854  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.15645816814126834  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.16289743721916572  - accuracy: 0.75\n",
      "At: 1100 [==========>] Loss 0.09151266618340212  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.10825775353364608  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.10204381184623498  - accuracy: 0.875\n",
      "At: 1103 [==========>] Loss 0.07118658644549802  - accuracy: 0.90625\n",
      "At: 1104 [==========>] Loss 0.0740026079904223  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.09536077307801283  - accuracy: 0.875\n",
      "At: 1106 [==========>] Loss 0.05977089814532632  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.22042245175738967  - accuracy: 0.65625\n",
      "At: 1108 [==========>] Loss 0.09871924852676721  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.08064448556121859  - accuracy: 0.875\n",
      "At: 1110 [==========>] Loss 0.10196703144358762  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.21891114619035507  - accuracy: 0.71875\n",
      "At: 1112 [==========>] Loss 0.13365515042917347  - accuracy: 0.84375\n",
      "At: 1113 [==========>] Loss 0.11045861287561273  - accuracy: 0.84375\n",
      "At: 1114 [==========>] Loss 0.07376205763221762  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.12892965128483044  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.09517207175736808  - accuracy: 0.90625\n",
      "At: 1117 [==========>] Loss 0.05849779516493363  - accuracy: 0.9375\n",
      "At: 1118 [==========>] Loss 0.10626610034520412  - accuracy: 0.875\n",
      "At: 1119 [==========>] Loss 0.1256338197433818  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.08969015304453501  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.11422250243189054  - accuracy: 0.84375\n",
      "At: 1122 [==========>] Loss 0.09640263584019745  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.11522736011556772  - accuracy: 0.8125\n",
      "At: 1124 [==========>] Loss 0.1401564394356194  - accuracy: 0.78125\n",
      "At: 1125 [==========>] Loss 0.12690010298778048  - accuracy: 0.84375\n",
      "At: 1126 [==========>] Loss 0.10908348854093394  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.14875821004145967  - accuracy: 0.71875\n",
      "At: 1128 [==========>] Loss 0.0819344624716889  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.1657868569626484  - accuracy: 0.71875\n",
      "At: 1130 [==========>] Loss 0.1208533081862237  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.06761808297102627  - accuracy: 0.96875\n",
      "At: 1132 [==========>] Loss 0.09813542348054816  - accuracy: 0.90625\n",
      "At: 1133 [==========>] Loss 0.1338150373558004  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.07353506785875989  - accuracy: 0.90625\n",
      "At: 1135 [==========>] Loss 0.11504728353805796  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.13963210138720786  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.08395835398343848  - accuracy: 0.84375\n",
      "At: 1138 [==========>] Loss 0.08595337908956235  - accuracy: 0.9375\n",
      "At: 1139 [==========>] Loss 0.08318354477042826  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.1631194567231719  - accuracy: 0.71875\n",
      "At: 1141 [==========>] Loss 0.09199003107222753  - accuracy: 0.84375\n",
      "At: 1142 [==========>] Loss 0.14197005240028757  - accuracy: 0.75\n",
      "At: 1143 [==========>] Loss 0.07091558039527845  - accuracy: 0.90625\n",
      "At: 1144 [==========>] Loss 0.09882544472410634  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.16725574867005633  - accuracy: 0.75\n",
      "At: 1146 [==========>] Loss 0.11514538639273665  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.25023171904394426  - accuracy: 0.625\n",
      "At: 1148 [==========>] Loss 0.07252440105633912  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.11182856371704344  - accuracy: 0.84375\n",
      "At: 1150 [==========>] Loss 0.12701011447153115  - accuracy: 0.8125\n",
      "At: 1151 [==========>] Loss 0.15988191058054424  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.15313182272780465  - accuracy: 0.75\n",
      "At: 1153 [==========>] Loss 0.15056736096183337  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.08335912668388533  - accuracy: 0.9375\n",
      "At: 1155 [==========>] Loss 0.09532481920059042  - accuracy: 0.90625\n",
      "At: 1156 [==========>] Loss 0.12926353676231606  - accuracy: 0.75\n",
      "At: 1157 [==========>] Loss 0.11128205023988291  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.1889588828289338  - accuracy: 0.71875\n",
      "At: 1159 [==========>] Loss 0.10563716536211104  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.07078088525294259  - accuracy: 0.9375\n",
      "At: 1161 [==========>] Loss 0.06814887283315502  - accuracy: 0.9375\n",
      "At: 1162 [==========>] Loss 0.13516628084693533  - accuracy: 0.84375\n",
      "At: 1163 [==========>] Loss 0.20004506331765323  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.06138388097889297  - accuracy: 0.96875\n",
      "At: 1165 [==========>] Loss 0.1372072058202263  - accuracy: 0.8125\n",
      "At: 1166 [==========>] Loss 0.05271952697673468  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.12546687516726068  - accuracy: 0.875\n",
      "At: 1168 [==========>] Loss 0.0968572635422282  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.08843985716258423  - accuracy: 0.8125\n",
      "At: 1170 [==========>] Loss 0.15708132727403767  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.07412499444378168  - accuracy: 0.84375\n",
      "At: 1172 [==========>] Loss 0.12491246090997313  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.12247463118418402  - accuracy: 0.84375\n",
      "At: 1174 [==========>] Loss 0.17331154358974193  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.15261287312050792  - accuracy: 0.78125\n",
      "At: 1176 [==========>] Loss 0.13387166212163315  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.07158250008593393  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.1318498983923326  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.1335097388127982  - accuracy: 0.84375\n",
      "At: 1180 [==========>] Loss 0.17702945991124922  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.08357304020901557  - accuracy: 0.875\n",
      "At: 1182 [==========>] Loss 0.11026222870657088  - accuracy: 0.8125\n",
      "At: 1183 [==========>] Loss 0.15397320838313935  - accuracy: 0.8125\n",
      "At: 1184 [==========>] Loss 0.13016116663808602  - accuracy: 0.84375\n",
      "At: 1185 [==========>] Loss 0.08886433899449338  - accuracy: 0.90625\n",
      "At: 1186 [==========>] Loss 0.14138535128525614  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.09634157921187314  - accuracy: 0.875\n",
      "At: 1188 [==========>] Loss 0.07003136851746881  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.15078468572149606  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.08686001804461801  - accuracy: 0.90625\n",
      "At: 1191 [==========>] Loss 0.1662127292363962  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.08021061928569664  - accuracy: 0.84375\n",
      "At: 1193 [==========>] Loss 0.12626926115833934  - accuracy: 0.8125\n",
      "At: 1194 [==========>] Loss 0.12164242457418802  - accuracy: 0.875\n",
      "At: 1195 [==========>] Loss 0.1286660423478979  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.12225919077306131  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.14048416719164458  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.0850564983593472  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.15287508427534635  - accuracy: 0.78125\n",
      "At: 1200 [==========>] Loss 0.07389889842942267  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.11502793245791192  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.1571488721949063  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.12993255591998537  - accuracy: 0.84375\n",
      "At: 1204 [==========>] Loss 0.11943410561853743  - accuracy: 0.8125\n",
      "At: 1205 [==========>] Loss 0.04775101914579623  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.10229484922750712  - accuracy: 0.875\n",
      "At: 1207 [==========>] Loss 0.162067983619139  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.10775003909464552  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.09190219351820732  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.13339165362626298  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.1385040427937342  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.09801980120674104  - accuracy: 0.875\n",
      "At: 1213 [==========>] Loss 0.17397061426565297  - accuracy: 0.75\n",
      "At: 1214 [==========>] Loss 0.1292655171641769  - accuracy: 0.84375\n",
      "At: 1215 [==========>] Loss 0.14399850957835897  - accuracy: 0.8125\n",
      "At: 1216 [==========>] Loss 0.07594226543274857  - accuracy: 0.96875\n",
      "At: 1217 [==========>] Loss 0.08449811012283104  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.08236633595586741  - accuracy: 0.875\n",
      "At: 1219 [==========>] Loss 0.12390372134370373  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.12206707119737466  - accuracy: 0.8125\n",
      "At: 1221 [==========>] Loss 0.09460221618148083  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.1589239101848154  - accuracy: 0.78125\n",
      "At: 1223 [==========>] Loss 0.07873618752200415  - accuracy: 0.9375\n",
      "At: 1224 [==========>] Loss 0.07177980954120997  - accuracy: 0.9375\n",
      "At: 1225 [==========>] Loss 0.0808158284312098  - accuracy: 0.90625\n",
      "At: 1226 [==========>] Loss 0.09522914922739269  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.14916034304771503  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.14679948592757025  - accuracy: 0.71875\n",
      "At: 1229 [==========>] Loss 0.11461697236397345  - accuracy: 0.875\n",
      "At: 1230 [==========>] Loss 0.1547810968606681  - accuracy: 0.8125\n",
      "At: 1231 [==========>] Loss 0.15214219045869892  - accuracy: 0.78125\n",
      "At: 1232 [==========>] Loss 0.08209306596705354  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.12005779745619966  - accuracy: 0.84375\n",
      "At: 1234 [==========>] Loss 0.16456215651052525  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.07670157878602708  - accuracy: 0.9375\n",
      "At: 1236 [==========>] Loss 0.17556343434326332  - accuracy: 0.71875\n",
      "At: 1237 [==========>] Loss 0.061994507565839144  - accuracy: 0.9375\n",
      "At: 1238 [==========>] Loss 0.10334731102835058  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.16854848438422837  - accuracy: 0.78125\n",
      "At: 1240 [==========>] Loss 0.09846436169194026  - accuracy: 0.8125\n",
      "At: 1241 [==========>] Loss 0.12013120667006857  - accuracy: 0.75\n",
      "At: 1242 [==========>] Loss 0.14885515541326672  - accuracy: 0.84375\n",
      "At: 1243 [==========>] Loss 0.12446524420443839  - accuracy: 0.84375\n",
      "At: 1244 [==========>] Loss 0.13114327177762347  - accuracy: 0.8125\n",
      "At: 1245 [==========>] Loss 0.0974240220274098  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.06334045434512095  - accuracy: 0.9375\n",
      "At: 1247 [==========>] Loss 0.14869197526547395  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.09674761763555509  - accuracy: 0.875\n",
      "At: 1249 [==========>] Loss 0.12044287806787649  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.11590882299682392  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.11266088434503331  - accuracy: 0.875\n",
      "At: 1252 [==========>] Loss 0.08490344174100074  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.1000938388111118  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.17003705886683915  - accuracy: 0.78125\n",
      "At: 1255 [==========>] Loss 0.10456869530700189  - accuracy: 0.875\n",
      "At: 1256 [==========>] Loss 0.14198943926742724  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.14195362301241504  - accuracy: 0.78125\n",
      "At: 1258 [==========>] Loss 0.06916110936663351  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.12379917911726915  - accuracy: 0.8125\n",
      "At: 1260 [==========>] Loss 0.09533022665875024  - accuracy: 0.84375\n",
      "At: 1261 [==========>] Loss 0.11337193481933397  - accuracy: 0.8125\n",
      "At: 1262 [==========>] Loss 0.13194412686237897  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.11948629266272998  - accuracy: 0.84375\n",
      "At: 1264 [==========>] Loss 0.08309422712359034  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.10458093847612468  - accuracy: 0.90625\n",
      "At: 1266 [==========>] Loss 0.11090280185997202  - accuracy: 0.84375\n",
      "At: 1267 [==========>] Loss 0.11448971564747933  - accuracy: 0.90625\n",
      "At: 1268 [==========>] Loss 0.12283567970608167  - accuracy: 0.8125\n",
      "At: 1269 [==========>] Loss 0.10118556027501202  - accuracy: 0.875\n",
      "At: 1270 [==========>] Loss 0.13155104894824884  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.16252522063884617  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.09281237571147731  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.17390335798176593  - accuracy: 0.78125\n",
      "At: 1274 [==========>] Loss 0.15354398874890138  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.09926935952574636  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.10860825346021531  - accuracy: 0.90625\n",
      "At: 1277 [==========>] Loss 0.10047188593387406  - accuracy: 0.84375\n",
      "At: 1278 [==========>] Loss 0.12230253501832664  - accuracy: 0.84375\n",
      "At: 1279 [==========>] Loss 0.09338976316794467  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.10703534167927015  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.14356458131667604  - accuracy: 0.78125\n",
      "At: 1282 [==========>] Loss 0.11223139235371558  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.10795038228713749  - accuracy: 0.875\n",
      "At: 1284 [==========>] Loss 0.14817796456402488  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.07908877340679975  - accuracy: 0.875\n",
      "At: 1286 [==========>] Loss 0.1354040736015242  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.11259340385743731  - accuracy: 0.90625\n",
      "At: 1288 [==========>] Loss 0.14004031191676386  - accuracy: 0.8125\n",
      "At: 1289 [==========>] Loss 0.11051176126017465  - accuracy: 0.8125\n",
      "At: 1290 [==========>] Loss 0.16332691856710152  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.1652041766193273  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.11312828788785644  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.13204199303916098  - accuracy: 0.84375\n",
      "At: 1294 [==========>] Loss 0.11748379526261911  - accuracy: 0.84375\n",
      "At: 1295 [==========>] Loss 0.13355973937031776  - accuracy: 0.8125\n",
      "At: 1296 [==========>] Loss 0.12909718885982346  - accuracy: 0.8125\n",
      "At: 1297 [==========>] Loss 0.1376494793961382  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.1172718927573645  - accuracy: 0.8125\n",
      "At: 1299 [==========>] Loss 0.12155490550310175  - accuracy: 0.8125\n",
      "At: 1300 [==========>] Loss 0.1120290688434172  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.11975523021673196  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.08219678550882059  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.08745475780488715  - accuracy: 0.84375\n",
      "At: 1304 [==========>] Loss 0.1263735733330829  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.1329542800483846  - accuracy: 0.78125\n",
      "At: 1306 [==========>] Loss 0.06505967075172056  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.16596050100565674  - accuracy: 0.78125\n",
      "At: 1308 [==========>] Loss 0.064286092044993  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.14954850174775736  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.14254580852158227  - accuracy: 0.8125\n",
      "At: 1311 [==========>] Loss 0.12667832180949606  - accuracy: 0.8125\n",
      "At: 1312 [==========>] Loss 0.07837198385748025  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.18377168166262012  - accuracy: 0.75\n",
      "At: 1314 [==========>] Loss 0.056733961298584754  - accuracy: 0.96875\n",
      "At: 1315 [==========>] Loss 0.1171197178467517  - accuracy: 0.84375\n",
      "At: 1316 [==========>] Loss 0.13229390856185455  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.10901767182687601  - accuracy: 0.84375\n",
      "At: 1318 [==========>] Loss 0.10525799548874701  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.10956430958287217  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.12527209527692085  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.08447594695111757  - accuracy: 0.875\n",
      "At: 1322 [==========>] Loss 0.14029693991609293  - accuracy: 0.84375\n",
      "At: 1323 [==========>] Loss 0.08212124027347356  - accuracy: 0.9375\n",
      "At: 1324 [==========>] Loss 0.1491320968863029  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.08731198179926905  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.081038984239259  - accuracy: 0.9375\n",
      "At: 1327 [==========>] Loss 0.1270537204763727  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.10142040392349069  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.09028726758441186  - accuracy: 0.8125\n",
      "At: 1330 [==========>] Loss 0.11408074283266492  - accuracy: 0.875\n",
      "At: 1331 [==========>] Loss 0.1994297918819076  - accuracy: 0.75\n",
      "At: 1332 [==========>] Loss 0.13121769598585264  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.13725089372478771  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.0914943019286707  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.10595227127317697  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.09129878478127869  - accuracy: 0.875\n",
      "At: 1337 [==========>] Loss 0.17113499953271713  - accuracy: 0.78125\n",
      "At: 1338 [==========>] Loss 0.13010539640839291  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.1469044210847461  - accuracy: 0.78125\n",
      "At: 1340 [==========>] Loss 0.11600171986732827  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.07044163554670521  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.11461424090983585  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.16500861670453862  - accuracy: 0.8125\n",
      "At: 1344 [==========>] Loss 0.17787181156526832  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.09206962553666809  - accuracy: 0.90625\n",
      "At: 1346 [==========>] Loss 0.08782865908539553  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.0991805787948595  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09300640270353655  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.15272873158422406  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.0952587928119471  - accuracy: 0.90625\n",
      "At: 1351 [==========>] Loss 0.0960016317987022  - accuracy: 0.875\n",
      "At: 1352 [==========>] Loss 0.09231407587591495  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.17422906199766497  - accuracy: 0.75\n",
      "At: 1354 [==========>] Loss 0.14467482121364839  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.08699080355661473  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.09305323608824263  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.12174927513839562  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.11537106768911062  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.07067860911004653  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.1706369830168832  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.07818550390717134  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.11824248173327488  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.11120708752666844  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.153099253570948  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.13447441795632076  - accuracy: 0.84375\n",
      "At: 1366 [==========>] Loss 0.11171113257739049  - accuracy: 0.875\n",
      "At: 1367 [==========>] Loss 0.12041334346257898  - accuracy: 0.84375\n",
      "At: 1368 [==========>] Loss 0.1676781766895378  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.07200717327393939  - accuracy: 0.96875\n",
      "At: 1370 [==========>] Loss 0.12423243865298611  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.17114449635962137  - accuracy: 0.78125\n",
      "At: 1372 [==========>] Loss 0.13117952199811986  - accuracy: 0.78125\n",
      "At: 1373 [==========>] Loss 0.11062400956287544  - accuracy: 0.84375\n",
      "At: 1374 [==========>] Loss 0.12680691333048077  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.09676203459547632  - accuracy: 0.90625\n",
      "At: 1376 [==========>] Loss 0.0966679728973  - accuracy: 0.875\n",
      "At: 1377 [==========>] Loss 0.16023074691779693  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.10608815387487551  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.11710487673551306  - accuracy: 0.8125\n",
      "At: 1380 [==========>] Loss 0.14666921490778112  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.0945556000998696  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.1254263776700707  - accuracy: 0.84375\n",
      "At: 1383 [==========>] Loss 0.1014565486793194  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.11000963583229653  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.1696103502175461  - accuracy: 0.8125\n",
      "At: 1386 [==========>] Loss 0.12274756465659828  - accuracy: 0.84375\n",
      "At: 1387 [==========>] Loss 0.08681576046840848  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.14787653971368112  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.09814566378718334  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.13637017633487028  - accuracy: 0.78125\n",
      "At: 1391 [==========>] Loss 0.09811161613202696  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.10544062064681133  - accuracy: 0.8125\n",
      "At: 1393 [==========>] Loss 0.16411202782204687  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.0838966805206327  - accuracy: 0.9375\n",
      "At: 1395 [==========>] Loss 0.18993921602869235  - accuracy: 0.6875\n",
      "At: 1396 [==========>] Loss 0.057738082823683014  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.1225571092308846  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.099841087369641  - accuracy: 0.90625\n",
      "At: 1399 [==========>] Loss 0.10878675636283489  - accuracy: 0.84375\n",
      "At: 1400 [==========>] Loss 0.13978996364907373  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.05914084386487798  - accuracy: 0.90625\n",
      "At: 1402 [==========>] Loss 0.16314777071394293  - accuracy: 0.71875\n",
      "At: 1403 [==========>] Loss 0.13449506594835178  - accuracy: 0.78125\n",
      "At: 1404 [==========>] Loss 0.11949239650277074  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.10372821342327389  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.1855829859935921  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.09509534242815  - accuracy: 0.90625\n",
      "At: 1408 [==========>] Loss 0.12287637626052142  - accuracy: 0.875\n",
      "At: 1409 [==========>] Loss 0.03364435044922136  - accuracy: 0.96875\n",
      "At: 1410 [==========>] Loss 0.13439525612625858  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.12566195479759112  - accuracy: 0.84375\n",
      "At: 1412 [==========>] Loss 0.1314099581170729  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.1050034736716332  - accuracy: 0.8125\n",
      "At: 1414 [==========>] Loss 0.19549033190949516  - accuracy: 0.75\n",
      "At: 1415 [==========>] Loss 0.08181201864006665  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.14266737745744956  - accuracy: 0.78125\n",
      "At: 1417 [==========>] Loss 0.10916892576870774  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.14918275733079933  - accuracy: 0.84375\n",
      "At: 1419 [==========>] Loss 0.12811112897019286  - accuracy: 0.78125\n",
      "At: 1420 [==========>] Loss 0.12097960015039796  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.10506848818771025  - accuracy: 0.84375\n",
      "At: 1422 [==========>] Loss 0.13148626899416638  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.11304496204472236  - accuracy: 0.84375\n",
      "At: 1424 [==========>] Loss 0.11867208906091362  - accuracy: 0.875\n",
      "At: 1425 [==========>] Loss 0.08250713957119132  - accuracy: 0.875\n",
      "At: 1426 [==========>] Loss 0.14358398895505628  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.08619446551420865  - accuracy: 0.90625\n",
      "At: 1428 [==========>] Loss 0.10634395079792758  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.12929251006162978  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.09036004763889677  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.13940765448266296  - accuracy: 0.78125\n",
      "At: 1432 [==========>] Loss 0.11991224508134243  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.11656542361125029  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.1458422763828278  - accuracy: 0.84375\n",
      "At: 1435 [==========>] Loss 0.11469982988624736  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.08011781675148219  - accuracy: 0.84375\n",
      "At: 1437 [==========>] Loss 0.1294363082264903  - accuracy: 0.78125\n",
      "At: 1438 [==========>] Loss 0.1404289764178885  - accuracy: 0.78125\n",
      "At: 1439 [==========>] Loss 0.12615894159831623  - accuracy: 0.8125\n",
      "At: 1440 [==========>] Loss 0.10688543507262849  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.0655203303437347  - accuracy: 0.9375\n",
      "At: 1442 [==========>] Loss 0.11168420084232218  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.13912321995324334  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.10836878538516985  - accuracy: 0.90625\n",
      "At: 1445 [==========>] Loss 0.15861013515489308  - accuracy: 0.8125\n",
      "At: 1446 [==========>] Loss 0.16904211902396368  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.15775718058713661  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.06379340469768405  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.14980824726873976  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.12715036394375712  - accuracy: 0.8125\n",
      "At: 1451 [==========>] Loss 0.10566991178909677  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.11581151255537234  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.029974893537553123  - accuracy: 1.0\n",
      "At: 1454 [==========>] Loss 0.16009136281692568  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.10816113197985733  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.07463835470230197  - accuracy: 0.90625\n",
      "At: 1457 [==========>] Loss 0.0866722063179107  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.14842405965311395  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.11776084808856283  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.16044225474569385  - accuracy: 0.78125\n",
      "At: 1461 [==========>] Loss 0.12552400899288862  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.1302221080461017  - accuracy: 0.84375\n",
      "At: 1463 [==========>] Loss 0.10393961658118209  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.16907071681608565  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.10474524236599864  - accuracy: 0.875\n",
      "At: 1466 [==========>] Loss 0.07375477385682114  - accuracy: 0.9375\n",
      "At: 1467 [==========>] Loss 0.1973346164997896  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.16674286778995692  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.16003111633809877  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.11809121702571332  - accuracy: 0.84375\n",
      "At: 1471 [==========>] Loss 0.1205615912928249  - accuracy: 0.84375\n",
      "At: 1472 [==========>] Loss 0.06518221083559181  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.10824054888272973  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.22262537754306266  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.14808663217371265  - accuracy: 0.75\n",
      "At: 1476 [==========>] Loss 0.09967611456316136  - accuracy: 0.84375\n",
      "At: 1477 [==========>] Loss 0.08526360831623221  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.08695534548683809  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.1311813206375755  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.08511663521356741  - accuracy: 0.875\n",
      "At: 1481 [==========>] Loss 0.12609142424126343  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.10037709672464207  - accuracy: 0.90625\n",
      "At: 1483 [==========>] Loss 0.20522437870167456  - accuracy: 0.6875\n",
      "At: 1484 [==========>] Loss 0.13709431901425095  - accuracy: 0.84375\n",
      "At: 1485 [==========>] Loss 0.12693247753205697  - accuracy: 0.8125\n",
      "At: 1486 [==========>] Loss 0.1055107986719651  - accuracy: 0.84375\n",
      "At: 1487 [==========>] Loss 0.04561088473774223  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.12503342018442073  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.19112269990196956  - accuracy: 0.75\n",
      "At: 1490 [==========>] Loss 0.09022736969794032  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.1884185849708892  - accuracy: 0.71875\n",
      "At: 1492 [==========>] Loss 0.13096574542596562  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.19840628716821346  - accuracy: 0.71875\n",
      "At: 1494 [==========>] Loss 0.157610865856824  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.13823572839149806  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.10006358461017674  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.18850165940123792  - accuracy: 0.71875\n",
      "At: 1498 [==========>] Loss 0.13185445795447737  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.08976392677408943  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.0921715850963687  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.10171555242085423  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.14055870549276692  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.12912795123271098  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.12327030386587931  - accuracy: 0.84375\n",
      "At: 1505 [==========>] Loss 0.13458323692215707  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.1477483434268156  - accuracy: 0.8125\n",
      "At: 1507 [==========>] Loss 0.13305780526130442  - accuracy: 0.78125\n",
      "At: 1508 [==========>] Loss 0.1833467772147721  - accuracy: 0.75\n",
      "At: 1509 [==========>] Loss 0.11061193425363172  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.12376726154236341  - accuracy: 0.78125\n",
      "At: 1511 [==========>] Loss 0.12221630464107333  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.09371920471478262  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.14914229117502353  - accuracy: 0.78125\n",
      "At: 1514 [==========>] Loss 0.1344837389877048  - accuracy: 0.8125\n",
      "At: 1515 [==========>] Loss 0.1435863605148246  - accuracy: 0.75\n",
      "At: 1516 [==========>] Loss 0.10337617393145124  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.12879543942765415  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.11687178039741081  - accuracy: 0.78125\n",
      "At: 1519 [==========>] Loss 0.14705424125687658  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.12261905658725016  - accuracy: 0.8125\n",
      "At: 1521 [==========>] Loss 0.07843461335306982  - accuracy: 0.90625\n",
      "At: 1522 [==========>] Loss 0.17875280405193059  - accuracy: 0.71875\n",
      "At: 1523 [==========>] Loss 0.09850167614474611  - accuracy: 0.90625\n",
      "At: 1524 [==========>] Loss 0.1468962818074957  - accuracy: 0.875\n",
      "At: 1525 [==========>] Loss 0.12281165782717397  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.07096641675598994  - accuracy: 0.96875\n",
      "At: 1527 [==========>] Loss 0.13960762371893998  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.1220256443524494  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.06405467288648856  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.06444122677959244  - accuracy: 0.90625\n",
      "At: 1531 [==========>] Loss 0.12117783107363245  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.17868798565546887  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.13351530617030577  - accuracy: 0.84375\n",
      "At: 1534 [==========>] Loss 0.11183034232507752  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.13803200000341448  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.13515588542120296  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.09240134981421742  - accuracy: 0.90625\n",
      "At: 1538 [==========>] Loss 0.1194774906595668  - accuracy: 0.84375\n",
      "At: 1539 [==========>] Loss 0.09371374789284995  - accuracy: 0.875\n",
      "At: 1540 [==========>] Loss 0.14708715714062617  - accuracy: 0.8125\n",
      "At: 1541 [==========>] Loss 0.11059845089572201  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.06977853230348852  - accuracy: 0.9375\n",
      "At: 1543 [==========>] Loss 0.134268425439906  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.14224398217311435  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.1844804181706546  - accuracy: 0.75\n",
      "At: 1546 [==========>] Loss 0.09926375791781934  - accuracy: 0.875\n",
      "At: 1547 [==========>] Loss 0.14719523972920712  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.12073542087148866  - accuracy: 0.875\n",
      "At: 1549 [==========>] Loss 0.11832664453193423  - accuracy: 0.84375\n",
      "At: 1550 [==========>] Loss 0.06143586437606332  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.14079209620107735  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.08785210922697483  - accuracy: 0.875\n",
      "At: 1553 [==========>] Loss 0.06591831138187969  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.13648202303404394  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.10513512930425684  - accuracy: 0.8125\n",
      "At: 1556 [==========>] Loss 0.16231913451626176  - accuracy: 0.8125\n",
      "At: 1557 [==========>] Loss 0.0766305865595995  - accuracy: 0.9375\n",
      "At: 1558 [==========>] Loss 0.15024364644554586  - accuracy: 0.78125\n",
      "At: 1559 [==========>] Loss 0.07740774985334808  - accuracy: 0.9375\n",
      "At: 1560 [==========>] Loss 0.11032581058750696  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.16506592881110482  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.09432948947118662  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.09782281016457312  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.10682646503435675  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.11925567779415383  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.12921971857366948  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.1320976289555145  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.0754932299883041  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.09407602197686171  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.07612172165486397  - accuracy: 0.9375\n",
      "At: 1571 [==========>] Loss 0.12873801966553958  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.12901822189926845  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.05233624831352249  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.12899348069226052  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.12320858840120968  - accuracy: 0.78125\n",
      "At: 1576 [==========>] Loss 0.12431429400059965  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.05105812667583814  - accuracy: 1.0\n",
      "At: 1578 [==========>] Loss 0.08778951036778934  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.10483529657906658  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.12401580530662164  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.08770009651194527  - accuracy: 0.90625\n",
      "At: 1582 [==========>] Loss 0.15606260878650857  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.07012651040002613  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.11161020681368995  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.09663426728989914  - accuracy: 0.90625\n",
      "At: 1586 [==========>] Loss 0.1451069415744502  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.07281674061623392  - accuracy: 0.9375\n",
      "At: 1588 [==========>] Loss 0.11709731198351382  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.12527288899319294  - accuracy: 0.84375\n",
      "At: 1590 [==========>] Loss 0.13608803629220795  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.11904653914857344  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.05632035741626383  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.1536066784754322  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.07444767315179776  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.13740119626209377  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.17131034896031935  - accuracy: 0.78125\n",
      "At: 1597 [==========>] Loss 0.16142798447140816  - accuracy: 0.78125\n",
      "At: 1598 [==========>] Loss 0.15628409630077889  - accuracy: 0.8125\n",
      "At: 1599 [==========>] Loss 0.20400684387047618  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.15747512362896338  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.06336838620279844  - accuracy: 0.96875\n",
      "At: 1602 [==========>] Loss 0.09006244043587958  - accuracy: 0.875\n",
      "At: 1603 [==========>] Loss 0.15760472778951495  - accuracy: 0.78125\n",
      "At: 1604 [==========>] Loss 0.21751872724628546  - accuracy: 0.71875\n",
      "At: 1605 [==========>] Loss 0.0717205274681982  - accuracy: 0.9375\n",
      "At: 1606 [==========>] Loss 0.0957913888472167  - accuracy: 0.90625\n",
      "At: 1607 [==========>] Loss 0.17804615121795403  - accuracy: 0.65625\n",
      "At: 1608 [==========>] Loss 0.13150004670978682  - accuracy: 0.8125\n",
      "At: 1609 [==========>] Loss 0.14775830916715685  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.13917581484612787  - accuracy: 0.84375\n",
      "At: 1611 [==========>] Loss 0.07472585803217546  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.04829449114991144  - accuracy: 0.96875\n",
      "At: 1613 [==========>] Loss 0.1321319128128921  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.14694495710165667  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.11390626283651754  - accuracy: 0.875\n",
      "At: 1616 [==========>] Loss 0.10726306336676039  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.09721056394871351  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.12192294379451593  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.17840432024201497  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.10999079225297473  - accuracy: 0.8125\n",
      "At: 1621 [==========>] Loss 0.11737119460278356  - accuracy: 0.875\n",
      "At: 1622 [==========>] Loss 0.17703201596790213  - accuracy: 0.71875\n",
      "At: 1623 [==========>] Loss 0.09985444852090031  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.1641177728946032  - accuracy: 0.78125\n",
      "At: 1625 [==========>] Loss 0.11971346079693669  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.09005280388657648  - accuracy: 0.8125\n",
      "At: 1627 [==========>] Loss 0.09385071306468651  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.21425165158126155  - accuracy: 0.65625\n",
      "At: 1629 [==========>] Loss 0.11487257500385493  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.106089374900463  - accuracy: 0.84375\n",
      "At: 1631 [==========>] Loss 0.13691362341317034  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.08237445915599942  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.06646307274861274  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.11076746802321094  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.23214258031858429  - accuracy: 0.6875\n",
      "At: 1636 [==========>] Loss 0.09529147685894462  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.09916297864663895  - accuracy: 0.875\n",
      "At: 1638 [==========>] Loss 0.15924221259999283  - accuracy: 0.75\n",
      "At: 1639 [==========>] Loss 0.15699955236233643  - accuracy: 0.78125\n",
      "At: 1640 [==========>] Loss 0.13344601364512965  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.07160936944003063  - accuracy: 0.90625\n",
      "At: 1642 [==========>] Loss 0.09443682389607647  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.09887581793531655  - accuracy: 0.90625\n",
      "At: 1644 [==========>] Loss 0.12087289465579701  - accuracy: 0.875\n",
      "At: 1645 [==========>] Loss 0.06603587797496818  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.12338680137638958  - accuracy: 0.84375\n",
      "At: 1647 [==========>] Loss 0.1503758750768257  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1222808379378892  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.08172290545496624  - accuracy: 0.90625\n",
      "At: 1650 [==========>] Loss 0.09862807022839148  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.11989625687630799  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.08356479995061464  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.08883016259713605  - accuracy: 0.84375\n",
      "At: 1654 [==========>] Loss 0.05135103851505969  - accuracy: 1.0\n",
      "At: 1655 [==========>] Loss 0.18130877881795715  - accuracy: 0.71875\n",
      "At: 1656 [==========>] Loss 0.12409962164995322  - accuracy: 0.8125\n",
      "At: 1657 [==========>] Loss 0.12358349633677232  - accuracy: 0.84375\n",
      "At: 1658 [==========>] Loss 0.21591980414971595  - accuracy: 0.65625\n",
      "At: 1659 [==========>] Loss 0.08638657112820286  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.04376608591639767  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.10418419173569274  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.08874448888526253  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.15546933269753316  - accuracy: 0.78125\n",
      "At: 1664 [==========>] Loss 0.1070096847991932  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.08637586269311809  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.09463295145878622  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.16512512113276961  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.13718598625562448  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.11323804517545677  - accuracy: 0.8125\n",
      "At: 1670 [==========>] Loss 0.073846882126281  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.12297457368977248  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.1369217933561652  - accuracy: 0.75\n",
      "At: 1673 [==========>] Loss 0.07244718308305269  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.17448206174706066  - accuracy: 0.75\n",
      "At: 1675 [==========>] Loss 0.16841734224854651  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.20425890722772633  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.08807195113624973  - accuracy: 0.90625\n",
      "At: 1678 [==========>] Loss 0.16718052461491684  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.11237028604928836  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.13927196208162812  - accuracy: 0.71875\n",
      "At: 1681 [==========>] Loss 0.1494505748293664  - accuracy: 0.8125\n",
      "At: 1682 [==========>] Loss 0.094356513656007  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.15645875674247936  - accuracy: 0.84375\n",
      "At: 1684 [==========>] Loss 0.11948875767655284  - accuracy: 0.84375\n",
      "At: 1685 [==========>] Loss 0.0908031403766308  - accuracy: 0.875\n",
      "At: 1686 [==========>] Loss 0.13388153720458412  - accuracy: 0.875\n",
      "At: 1687 [==========>] Loss 0.16056758575141591  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.054275974453609946  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.12848639075111304  - accuracy: 0.8125\n",
      "At: 1690 [==========>] Loss 0.13986203986840978  - accuracy: 0.75\n",
      "At: 1691 [==========>] Loss 0.08659731862143978  - accuracy: 0.90625\n",
      "At: 1692 [==========>] Loss 0.1693362841772832  - accuracy: 0.78125\n",
      "At: 1693 [==========>] Loss 0.08239011159888499  - accuracy: 0.9375\n",
      "At: 1694 [==========>] Loss 0.10176824417902838  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.10763339157985968  - accuracy: 0.875\n",
      "At: 1696 [==========>] Loss 0.1742864095241684  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.09465665611150231  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.08850856429561224  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.08497663410991096  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.11410963534379831  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.07783523588610244  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.08862488301225854  - accuracy: 0.90625\n",
      "At: 1703 [==========>] Loss 0.18217496556605717  - accuracy: 0.78125\n",
      "At: 1704 [==========>] Loss 0.06550231033175252  - accuracy: 0.9375\n",
      "At: 1705 [==========>] Loss 0.11491756662318806  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.13338533756777077  - accuracy: 0.8125\n",
      "At: 1707 [==========>] Loss 0.17221225583146416  - accuracy: 0.78125\n",
      "At: 1708 [==========>] Loss 0.09249766653005935  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.18361957104918014  - accuracy: 0.71875\n",
      "At: 1710 [==========>] Loss 0.17985817795252163  - accuracy: 0.78125\n",
      "At: 1711 [==========>] Loss 0.08784356786179287  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.09457702753266559  - accuracy: 0.90625\n",
      "At: 1713 [==========>] Loss 0.07968374873250386  - accuracy: 0.90625\n",
      "At: 1714 [==========>] Loss 0.15966396052789256  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.09867480355801703  - accuracy: 0.875\n",
      "At: 1716 [==========>] Loss 0.07013393459096406  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.08018796195420977  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.13720846270179057  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.11991615885473143  - accuracy: 0.8125\n",
      "At: 1720 [==========>] Loss 0.0693331083405265  - accuracy: 0.875\n",
      "At: 1721 [==========>] Loss 0.1567229646481026  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.06632260972421025  - accuracy: 0.875\n",
      "At: 1723 [==========>] Loss 0.15498120437201618  - accuracy: 0.75\n",
      "At: 1724 [==========>] Loss 0.082660697190979  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.15394523054909154  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.10410105320603663  - accuracy: 0.84375\n",
      "At: 1727 [==========>] Loss 0.1306062583687576  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.09193128764071987  - accuracy: 0.90625\n",
      "At: 1729 [==========>] Loss 0.20268180019900936  - accuracy: 0.6875\n",
      "At: 1730 [==========>] Loss 0.14179628691534152  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.11361058891702785  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.07660471465360169  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.1498463734728353  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.12825144295121893  - accuracy: 0.8125\n",
      "At: 1735 [==========>] Loss 0.15008501273441355  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.09786971839489009  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.1419824528383209  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.14027448669390474  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.10736213400042179  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.13918630496586237  - accuracy: 0.78125\n",
      "At: 1741 [==========>] Loss 0.12804240637729103  - accuracy: 0.84375\n",
      "At: 1742 [==========>] Loss 0.029081643848115755  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.12809505816107858  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.08842542811095751  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.10050013553262366  - accuracy: 0.8125\n",
      "At: 1746 [==========>] Loss 0.17370830451396677  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.10786710474735904  - accuracy: 0.90625\n",
      "At: 1748 [==========>] Loss 0.1140805523452586  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.10201231244658396  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.1075444639641199  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.16629317073813316  - accuracy: 0.8125\n",
      "At: 1752 [==========>] Loss 0.10820796733183596  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.0868090115268523  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.1010358972578011  - accuracy: 0.90625\n",
      "At: 1755 [==========>] Loss 0.05720581737393382  - accuracy: 0.96875\n",
      "At: 1756 [==========>] Loss 0.16556502517439045  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.11693732639507955  - accuracy: 0.8125\n",
      "At: 1758 [==========>] Loss 0.06681656507611392  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.10054539492855163  - accuracy: 0.8125\n",
      "At: 1760 [==========>] Loss 0.09013812893839622  - accuracy: 0.875\n",
      "At: 1761 [==========>] Loss 0.0882850020303429  - accuracy: 0.875\n",
      "At: 1762 [==========>] Loss 0.14707405100973986  - accuracy: 0.84375\n",
      "At: 1763 [==========>] Loss 0.09313789366316202  - accuracy: 0.875\n",
      "At: 1764 [==========>] Loss 0.12454090510740184  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.14301970208669976  - accuracy: 0.8125\n",
      "At: 1766 [==========>] Loss 0.07246423648667395  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.07787948299887455  - accuracy: 0.9375\n",
      "At: 1768 [==========>] Loss 0.07028139101488556  - accuracy: 0.90625\n",
      "At: 1769 [==========>] Loss 0.07679380810851663  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.06990497734252084  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.15066001597454765  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.13792760883638314  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.09698098247897902  - accuracy: 0.90625\n",
      "At: 1774 [==========>] Loss 0.1485687013607832  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.09255765716213993  - accuracy: 0.9375\n",
      "At: 1776 [==========>] Loss 0.12216030796669444  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11975762323135936  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.11316251913070613  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.09758505480028354  - accuracy: 0.84375\n",
      "At: 1780 [==========>] Loss 0.10180728200771658  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.1971390064604815  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.08704149645108855  - accuracy: 0.90625\n",
      "At: 1783 [==========>] Loss 0.13167261699651578  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.07187872239268384  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.07626865496319586  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.13277018619803194  - accuracy: 0.78125\n",
      "At: 1787 [==========>] Loss 0.12817566800339997  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.08609457357294584  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.10186608664568494  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.15691216595992205  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.06614077142490636  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.10187092660107876  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.11059459960036036  - accuracy: 0.8125\n",
      "At: 1794 [==========>] Loss 0.17020712804244256  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.08313957293956972  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.12782263554288298  - accuracy: 0.84375\n",
      "At: 1797 [==========>] Loss 0.12203723581665507  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.13165124211715729  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.07634110083156567  - accuracy: 0.9375\n",
      "At: 1800 [==========>] Loss 0.11546393298244717  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.14770809866542373  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.1316163704448231  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.1768013542748446  - accuracy: 0.71875\n",
      "At: 1804 [==========>] Loss 0.13002559184920282  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.04695997823328043  - accuracy: 0.9375\n",
      "At: 1806 [==========>] Loss 0.16860939797742897  - accuracy: 0.75\n",
      "At: 1807 [==========>] Loss 0.14168105162146022  - accuracy: 0.84375\n",
      "At: 1808 [==========>] Loss 0.17159883655476715  - accuracy: 0.78125\n",
      "At: 1809 [==========>] Loss 0.08321167549561306  - accuracy: 0.875\n",
      "At: 1810 [==========>] Loss 0.1448002544873129  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.1294062090014348  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.11164761663664165  - accuracy: 0.8125\n",
      "At: 1813 [==========>] Loss 0.11386353827613736  - accuracy: 0.875\n",
      "At: 1814 [==========>] Loss 0.09385795466620896  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.16892132634100843  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.04710033031410468  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.1263512751542748  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.1368852854068377  - accuracy: 0.78125\n",
      "At: 1819 [==========>] Loss 0.1628409927509974  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.10586011749462912  - accuracy: 0.875\n",
      "At: 1821 [==========>] Loss 0.09224306065928446  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.16068741138497838  - accuracy: 0.78125\n",
      "At: 1823 [==========>] Loss 0.15458530726734124  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.16759218469324128  - accuracy: 0.75\n",
      "At: 1825 [==========>] Loss 0.0951836382350793  - accuracy: 0.90625\n",
      "At: 1826 [==========>] Loss 0.05831521001561143  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11215256794735973  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.1394847399564228  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.14438604943067998  - accuracy: 0.84375\n",
      "At: 1830 [==========>] Loss 0.1202750973715933  - accuracy: 0.875\n",
      "At: 1831 [==========>] Loss 0.12266178190376302  - accuracy: 0.875\n",
      "At: 1832 [==========>] Loss 0.12841016237384764  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.10533256929940302  - accuracy: 0.90625\n",
      "At: 1834 [==========>] Loss 0.07434088612326241  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.12522688917439395  - accuracy: 0.84375\n",
      "At: 1836 [==========>] Loss 0.11204537655821764  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.04578782935982854  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.09919907859284031  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.08531259927575462  - accuracy: 0.84375\n",
      "At: 1840 [==========>] Loss 0.1259465597622759  - accuracy: 0.8125\n",
      "At: 1841 [==========>] Loss 0.10255101138386116  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.13010543688503576  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.09113928796267437  - accuracy: 0.90625\n",
      "At: 1844 [==========>] Loss 0.10917416278238865  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.14900654088552462  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.1465049578118291  - accuracy: 0.8125\n",
      "At: 1847 [==========>] Loss 0.07042809801423772  - accuracy: 0.90625\n",
      "At: 1848 [==========>] Loss 0.055424416169126245  - accuracy: 0.9375\n",
      "At: 1849 [==========>] Loss 0.17627471969971092  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.02969730255718965  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.1660140216870095  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.0621072692051066  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.10914229615995948  - accuracy: 0.84375\n",
      "At: 1854 [==========>] Loss 0.11241571611599818  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.17767380001121189  - accuracy: 0.8125\n",
      "At: 1856 [==========>] Loss 0.1351157646272429  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.16127971551285133  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.1308147002251438  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.15546725825300942  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.1287325964556909  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.08221289861483345  - accuracy: 0.9375\n",
      "At: 1862 [==========>] Loss 0.20110720387447717  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.13369462988458877  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.13901038357227685  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.08104196876990458  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.17389070765199824  - accuracy: 0.75\n",
      "At: 1867 [==========>] Loss 0.09190729652247986  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.15951492949650387  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.17455667947096484  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.13591847156883807  - accuracy: 0.78125\n",
      "At: 1871 [==========>] Loss 0.1426608485698989  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.14831355750784536  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.0997783252020025  - accuracy: 0.84375\n",
      "At: 1874 [==========>] Loss 0.12588106493307205  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.10491377779640901  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.19608780177860152  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.05930623420022711  - accuracy: 0.9375\n",
      "At: 1878 [==========>] Loss 0.13754174569024813  - accuracy: 0.78125\n",
      "At: 1879 [==========>] Loss 0.11682362821003263  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.07526414666609946  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.08658636536660572  - accuracy: 0.9375\n",
      "At: 1882 [==========>] Loss 0.09667592505326565  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.14246564811793316  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.09037550844762925  - accuracy: 0.90625\n",
      "At: 1885 [==========>] Loss 0.1125227260792194  - accuracy: 0.84375\n",
      "At: 1886 [==========>] Loss 0.12483824041042577  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.06993023865239792  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.12036121735784142  - accuracy: 0.84375\n",
      "At: 1889 [==========>] Loss 0.08976324581047995  - accuracy: 0.875\n",
      "At: 1890 [==========>] Loss 0.18569114427523345  - accuracy: 0.71875\n",
      "At: 1891 [==========>] Loss 0.06047150431945779  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.07785980450636067  - accuracy: 0.9375\n",
      "At: 1893 [==========>] Loss 0.058844549131425734  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.09463494234205119  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.06881641964790143  - accuracy: 0.90625\n",
      "At: 1896 [==========>] Loss 0.10984639896724924  - accuracy: 0.90625\n",
      "At: 1897 [==========>] Loss 0.06743836549517354  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.11152671144686181  - accuracy: 0.8125\n",
      "At: 1899 [==========>] Loss 0.0854664627154629  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.0873224484504421  - accuracy: 0.90625\n",
      "At: 1901 [==========>] Loss 0.11672943576228487  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.13460819641716618  - accuracy: 0.8125\n",
      "At: 1903 [==========>] Loss 0.10608539296018935  - accuracy: 0.875\n",
      "At: 1904 [==========>] Loss 0.05933290517450713  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.1647802939394771  - accuracy: 0.6875\n",
      "At: 1906 [==========>] Loss 0.10722739569087575  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.07094529436449676  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.09736588317881417  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.11629788438753551  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.03954174531922138  - accuracy: 0.96875\n",
      "At: 1911 [==========>] Loss 0.13271746867015188  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.10991730909120959  - accuracy: 0.875\n",
      "At: 1913 [==========>] Loss 0.16600816496238097  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.0780496066644806  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.12655134972412557  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.13915265996254866  - accuracy: 0.875\n",
      "At: 1917 [==========>] Loss 0.15998417335628584  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.1430816638732194  - accuracy: 0.8125\n",
      "At: 1919 [==========>] Loss 0.09387033288018162  - accuracy: 0.84375\n",
      "At: 1920 [==========>] Loss 0.10985505669416024  - accuracy: 0.8125\n",
      "At: 1921 [==========>] Loss 0.14759620246708088  - accuracy: 0.75\n",
      "At: 1922 [==========>] Loss 0.1166339525179404  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.17513750802362826  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.12780120358847602  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.1879504951017354  - accuracy: 0.75\n",
      "At: 1926 [==========>] Loss 0.08118252189164729  - accuracy: 0.90625\n",
      "At: 1927 [==========>] Loss 0.09138395118745599  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12764059018575702  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.16150541413323694  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.16388112234295044  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.09067280373930045  - accuracy: 0.875\n",
      "At: 1932 [==========>] Loss 0.1518483786358219  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.09262802019767351  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.14419155777049883  - accuracy: 0.84375\n",
      "At: 1935 [==========>] Loss 0.13106054040624057  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.12510709626616412  - accuracy: 0.8125\n",
      "At: 1937 [==========>] Loss 0.1366116918381186  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.161805533074859  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.08830312861275236  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.1319727283445149  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.12439072981638438  - accuracy: 0.8125\n",
      "At: 1942 [==========>] Loss 0.14733826575806938  - accuracy: 0.84375\n",
      "At: 1943 [==========>] Loss 0.11542131128259199  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.11101908330267463  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.15481537015471686  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.0822612801103967  - accuracy: 0.9375\n",
      "At: 1947 [==========>] Loss 0.12301733708371604  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.11112762784612622  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.08688325871806601  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.12872892721865808  - accuracy: 0.875\n",
      "At: 1951 [==========>] Loss 0.14584600896569877  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.08722557008521473  - accuracy: 0.90625\n",
      "At: 1953 [==========>] Loss 0.06047415699744617  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.18842587342413802  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.06903951528482125  - accuracy: 0.875\n",
      "At: 1956 [==========>] Loss 0.09898604983865862  - accuracy: 0.875\n",
      "At: 1957 [==========>] Loss 0.07840353374269417  - accuracy: 0.875\n",
      "At: 1958 [==========>] Loss 0.0904881094090311  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.1467836902189442  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.06272789134708386  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.17123853676333134  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.17214926335005  - accuracy: 0.84375\n",
      "At: 1963 [==========>] Loss 0.06102073566061461  - accuracy: 0.875\n",
      "At: 1964 [==========>] Loss 0.14807223710349188  - accuracy: 0.8125\n",
      "At: 1965 [==========>] Loss 0.1358404367856743  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.11856886846549708  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.14512525774208962  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.1891470592395384  - accuracy: 0.75\n",
      "At: 1969 [==========>] Loss 0.1334638784158489  - accuracy: 0.8125\n",
      "At: 1970 [==========>] Loss 0.11099576986661214  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.20538152825126355  - accuracy: 0.6875\n",
      "At: 1972 [==========>] Loss 0.0873240204529884  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.10774325703782665  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.1139325717792052  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.17502391830452685  - accuracy: 0.71875\n",
      "At: 1976 [==========>] Loss 0.08865339645330894  - accuracy: 0.84375\n",
      "At: 1977 [==========>] Loss 0.09751019039770406  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.1426297240111359  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.1265566527408576  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.13219428631066327  - accuracy: 0.8125\n",
      "At: 1981 [==========>] Loss 0.16733100358233005  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06999175465612484  - accuracy: 0.90625\n",
      "At: 1983 [==========>] Loss 0.12983541734646842  - accuracy: 0.84375\n",
      "At: 1984 [==========>] Loss 0.10080543922924831  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.14419346137595435  - accuracy: 0.8125\n",
      "At: 1986 [==========>] Loss 0.21310512969625395  - accuracy: 0.71875\n",
      "At: 1987 [==========>] Loss 0.10165688157546243  - accuracy: 0.875\n",
      "At: 1988 [==========>] Loss 0.09233981348313026  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.08041635972420294  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.08644210389047693  - accuracy: 0.90625\n",
      "At: 1991 [==========>] Loss 0.154625827857843  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.12458437847429668  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.14920966888020168  - accuracy: 0.75\n",
      "At: 1994 [==========>] Loss 0.0857238286348188  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.19095391009093238  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.10807577434960292  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.20572289253965703  - accuracy: 0.75\n",
      "At: 1998 [==========>] Loss 0.1760045865035313  - accuracy: 0.625\n",
      "At: 1999 [==========>] Loss 0.06763463891462627  - accuracy: 0.9375\n",
      "At: 2000 [==========>] Loss 0.1368009068205358  - accuracy: 0.84375\n",
      "At: 2001 [==========>] Loss 0.06888646157436461  - accuracy: 0.875\n",
      "At: 2002 [==========>] Loss 0.1051312840622666  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.12156253078046035  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.20318136126000838  - accuracy: 0.6875\n",
      "At: 2005 [==========>] Loss 0.11809640177143646  - accuracy: 0.84375\n",
      "At: 2006 [==========>] Loss 0.1405706769152859  - accuracy: 0.84375\n",
      "At: 2007 [==========>] Loss 0.11557211039630122  - accuracy: 0.8125\n",
      "At: 2008 [==========>] Loss 0.13397054156413182  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.1326905067926396  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.09713658767735558  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.1074141367119108  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.11305866373691258  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09065855626622829  - accuracy: 0.875\n",
      "At: 2014 [==========>] Loss 0.21354723058308506  - accuracy: 0.75\n",
      "At: 2015 [==========>] Loss 0.057966608307128104  - accuracy: 0.96875\n",
      "At: 2016 [==========>] Loss 0.1280246466717109  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.06423409955629135  - accuracy: 0.9375\n",
      "At: 2018 [==========>] Loss 0.09585721085662172  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.11004188024875536  - accuracy: 0.875\n",
      "At: 2020 [==========>] Loss 0.07302837627405849  - accuracy: 0.84375\n",
      "At: 2021 [==========>] Loss 0.11124421505835447  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.11052666673213793  - accuracy: 0.84375\n",
      "At: 2023 [==========>] Loss 0.09243118459035617  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.07799810661193657  - accuracy: 0.875\n",
      "At: 2025 [==========>] Loss 0.152382137092254  - accuracy: 0.71875\n",
      "At: 2026 [==========>] Loss 0.09697550023122166  - accuracy: 0.90625\n",
      "At: 2027 [==========>] Loss 0.13714722650248926  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.10495032531161105  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.10577767218722398  - accuracy: 0.84375\n",
      "At: 2030 [==========>] Loss 0.15184991949524285  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.15146421145348285  - accuracy: 0.875\n",
      "At: 2032 [==========>] Loss 0.11301282839981618  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.16749019357558298  - accuracy: 0.71875\n",
      "At: 2034 [==========>] Loss 0.21052412846605267  - accuracy: 0.625\n",
      "At: 2035 [==========>] Loss 0.08968193571557945  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.08288479551692027  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.1186980005287175  - accuracy: 0.78125\n",
      "At: 2038 [==========>] Loss 0.10899658001555206  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.09486631611838744  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.08402811331372442  - accuracy: 0.875\n",
      "At: 2041 [==========>] Loss 0.04859604622039419  - accuracy: 0.875\n",
      "At: 2042 [==========>] Loss 0.12234226585285542  - accuracy: 0.8125\n",
      "At: 2043 [==========>] Loss 0.09836041363798771  - accuracy: 0.90625\n",
      "At: 2044 [==========>] Loss 0.1061881346503944  - accuracy: 0.8125\n",
      "At: 2045 [==========>] Loss 0.23590713392470286  - accuracy: 0.65625\n",
      "At: 2046 [==========>] Loss 0.06488468250124899  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.08238062794177052  - accuracy: 0.875\n",
      "At: 2048 [==========>] Loss 0.11250893005874306  - accuracy: 0.84375\n",
      "At: 2049 [==========>] Loss 0.1359039971295335  - accuracy: 0.875\n",
      "At: 2050 [==========>] Loss 0.14488443650982014  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1638378800026584  - accuracy: 0.8125\n",
      "At: 2052 [==========>] Loss 0.06687298031059767  - accuracy: 0.9375\n",
      "At: 2053 [==========>] Loss 0.12091682204857224  - accuracy: 0.78125\n",
      "At: 2054 [==========>] Loss 0.11601369567192671  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.04554644093348075  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.12214125116253285  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.12262646035311908  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.12061962366468237  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.19583268398048415  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.11743853918590222  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.1564892531086437  - accuracy: 0.78125\n",
      "At: 2062 [==========>] Loss 0.14562461306609992  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.0999388634418521  - accuracy: 0.84375\n",
      "At: 2064 [==========>] Loss 0.18842817113585605  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.03441141167707562  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.1541103651032049  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.08157175842427333  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.10967688006256661  - accuracy: 0.84375\n",
      "At: 2069 [==========>] Loss 0.07368941477661828  - accuracy: 0.90625\n",
      "At: 2070 [==========>] Loss 0.15574635590591693  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.10587720154147554  - accuracy: 0.84375\n",
      "At: 2072 [==========>] Loss 0.07413651057627389  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.13018054129529621  - accuracy: 0.8125\n",
      "At: 2074 [==========>] Loss 0.07336826221833327  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.12536802553486065  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.11157563250595248  - accuracy: 0.84375\n",
      "At: 2077 [==========>] Loss 0.14437203507922297  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.09865484495643895  - accuracy: 0.875\n",
      "At: 2079 [==========>] Loss 0.06801261919146337  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.10822847341347457  - accuracy: 0.84375\n",
      "At: 2081 [==========>] Loss 0.13378370417668428  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.11084083905699173  - accuracy: 0.8125\n",
      "At: 2083 [==========>] Loss 0.16062691701225507  - accuracy: 0.75\n",
      "At: 2084 [==========>] Loss 0.11381580692293625  - accuracy: 0.8125\n",
      "At: 2085 [==========>] Loss 0.09350033293988153  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.08469002837637814  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.12782663977955694  - accuracy: 0.84375\n",
      "At: 2088 [==========>] Loss 0.08967338167860861  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.12287092607052111  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.12139407298810395  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.1323679271467442  - accuracy: 0.8125\n",
      "At: 2092 [==========>] Loss 0.08962385780844051  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.13262516633984955  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.12244330135054812  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.11973392551282667  - accuracy: 0.8125\n",
      "At: 2096 [==========>] Loss 0.15256257204543547  - accuracy: 0.8125\n",
      "At: 2097 [==========>] Loss 0.12028425070006787  - accuracy: 0.8125\n",
      "At: 2098 [==========>] Loss 0.11428040420435803  - accuracy: 0.84375\n",
      "At: 2099 [==========>] Loss 0.1091744755227067  - accuracy: 0.875\n",
      "At: 2100 [==========>] Loss 0.05065646481213891  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.1513256030509279  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.10143189472896309  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.16423709509748521  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.12621419560153174  - accuracy: 0.78125\n",
      "At: 2105 [==========>] Loss 0.1917275685708829  - accuracy: 0.65625\n",
      "At: 2106 [==========>] Loss 0.14603896033556715  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.09835928350534623  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.12712695734875157  - accuracy: 0.84375\n",
      "At: 2109 [==========>] Loss 0.10926536038303708  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.04602182506948717  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.11470602068583416  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.09143626671164024  - accuracy: 0.875\n",
      "At: 2113 [==========>] Loss 0.08574797142477919  - accuracy: 0.90625\n",
      "At: 2114 [==========>] Loss 0.13779881049554832  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.11163830987570068  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.09429427657101036  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.09931363698732866  - accuracy: 0.875\n",
      "At: 2118 [==========>] Loss 0.142045503748231  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.05097758812698734  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.12935534867181478  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.14714840086531572  - accuracy: 0.78125\n",
      "At: 2122 [==========>] Loss 0.13670307825511951  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.18287549081450127  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.11728439887891773  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.08113653804243623  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.05470882206087471  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.11193227250491646  - accuracy: 0.84375\n",
      "At: 2128 [==========>] Loss 0.10994527920261407  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.14885926463520058  - accuracy: 0.78125\n",
      "At: 2130 [==========>] Loss 0.06780452678710826  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.08853697528309115  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.19019445541155444  - accuracy: 0.71875\n",
      "At: 2133 [==========>] Loss 0.1473135140451759  - accuracy: 0.8125\n",
      "At: 2134 [==========>] Loss 0.13196203770377588  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.0893634184339773  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.10926098414336205  - accuracy: 0.875\n",
      "At: 2137 [==========>] Loss 0.11031990963808076  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.10262788653046226  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.16100454874372142  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.08869976803757919  - accuracy: 0.875\n",
      "At: 2141 [==========>] Loss 0.12120355386805001  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.1267353834878123  - accuracy: 0.8125\n",
      "At: 2143 [==========>] Loss 0.08905827795229188  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.07957912221611399  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.11182448720532637  - accuracy: 0.78125\n",
      "At: 2146 [==========>] Loss 0.17024240277077363  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.11137472130987128  - accuracy: 0.875\n",
      "At: 2148 [==========>] Loss 0.1734819919658845  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.11584846751040577  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.07033401789971565  - accuracy: 0.9375\n",
      "At: 2151 [==========>] Loss 0.09475877607429858  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.21639747576870377  - accuracy: 0.71875\n",
      "At: 2153 [==========>] Loss 0.1781782744305868  - accuracy: 0.71875\n",
      "At: 2154 [==========>] Loss 0.16323756007513987  - accuracy: 0.71875\n",
      "At: 2155 [==========>] Loss 0.12994938463519234  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.10354848486495413  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.1202369331982167  - accuracy: 0.8125\n",
      "At: 2158 [==========>] Loss 0.15098786772234474  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.07505547945895213  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.14127796326689523  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.10213753293021935  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.1071466721832316  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.12318684683728919  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.15627443384758208  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.08948732704735154  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.09656414072744157  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.10319317303400267  - accuracy: 0.84375\n",
      "At: 2168 [==========>] Loss 0.07349049685731389  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.12502149470258286  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.1155951651793978  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.1513405558437803  - accuracy: 0.8125\n",
      "At: 2172 [==========>] Loss 0.1008134677809619  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.1348325949547296  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.10698724139825551  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.13086226798451023  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.12870115456355155  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.14618859628738173  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.07524276294451315  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.12474904080725754  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.09215898103820969  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.15077100423600892  - accuracy: 0.78125\n",
      "At: 2182 [==========>] Loss 0.12129589197924265  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.23992523803778487  - accuracy: 0.625\n",
      "At: 2184 [==========>] Loss 0.06574018146129781  - accuracy: 0.9375\n",
      "At: 2185 [==========>] Loss 0.11456146266927256  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.13142676706386391  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.17663593886716505  - accuracy: 0.75\n",
      "At: 2188 [==========>] Loss 0.06448565155417535  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.09783765012563705  - accuracy: 0.84375\n",
      "At: 2190 [==========>] Loss 0.11725647280666528  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.11524291734813748  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.10021289887079626  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.15090538339660886  - accuracy: 0.71875\n",
      "At: 2194 [==========>] Loss 0.12274535766198645  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.160352978732005  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.14286630053192592  - accuracy: 0.8125\n",
      "At: 2197 [==========>] Loss 0.07527801347825319  - accuracy: 0.9375\n",
      "At: 2198 [==========>] Loss 0.08866419825589117  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.04104992680359537  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.05393793988740191  - accuracy: 0.90625\n",
      "At: 2201 [==========>] Loss 0.10906458343870859  - accuracy: 0.90625\n",
      "At: 2202 [==========>] Loss 0.06636430673562672  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.07954054324784399  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.12406032695966651  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.09805630698078105  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.1088635025839802  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.0697815984810228  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.16602832981413854  - accuracy: 0.75\n",
      "At: 2209 [==========>] Loss 0.15977566124096543  - accuracy: 0.8125\n",
      "At: 2210 [==========>] Loss 0.13223736961730653  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.13946217060020527  - accuracy: 0.8125\n",
      "At: 2212 [==========>] Loss 0.09328917818308875  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.11038679743898301  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.10190173272749095  - accuracy: 0.875\n",
      "At: 2215 [==========>] Loss 0.12349972937026105  - accuracy: 0.84375\n",
      "At: 2216 [==========>] Loss 0.11008067368257014  - accuracy: 0.875\n",
      "At: 2217 [==========>] Loss 0.12346683901438488  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.12817269594486438  - accuracy: 0.90625\n",
      "At: 2219 [==========>] Loss 0.06937185377402819  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.12776032937278115  - accuracy: 0.78125\n",
      "At: 2221 [==========>] Loss 0.1812124669018148  - accuracy: 0.75\n",
      "At: 2222 [==========>] Loss 0.10829693616499345  - accuracy: 0.875\n",
      "At: 2223 [==========>] Loss 0.16604242936745653  - accuracy: 0.6875\n",
      "At: 2224 [==========>] Loss 0.111098408701124  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.11905017196942586  - accuracy: 0.90625\n",
      "At: 2226 [==========>] Loss 0.12450295052235019  - accuracy: 0.84375\n",
      "At: 2227 [==========>] Loss 0.185173164880383  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.07167499494802453  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.13689031531387685  - accuracy: 0.8125\n",
      "At: 2230 [==========>] Loss 0.12110692975134052  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.16456377775185715  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.14808587791713762  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.14978675580376688  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.15633238167222036  - accuracy: 0.71875\n",
      "At: 2235 [==========>] Loss 0.1187511444183949  - accuracy: 0.84375\n",
      "At: 2236 [==========>] Loss 0.08434871209703047  - accuracy: 0.9375\n",
      "At: 2237 [==========>] Loss 0.09986193962104023  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.12340327504753713  - accuracy: 0.875\n",
      "At: 2239 [==========>] Loss 0.1588504149234818  - accuracy: 0.8125\n",
      "At: 2240 [==========>] Loss 0.1508311307029126  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.12843943494033747  - accuracy: 0.84375\n",
      "At: 2242 [==========>] Loss 0.18937202576062587  - accuracy: 0.6875\n",
      "At: 2243 [==========>] Loss 0.07310781855667652  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.09982566345532276  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.08104612561552973  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.12719960635066238  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.11374501163542877  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.15015503003608946  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.11889246014422292  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.08293696958722906  - accuracy: 0.875\n",
      "At: 2251 [==========>] Loss 0.0731054167297723  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.11139338786600864  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.14142430427530966  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.12225503503983606  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.13115119547163748  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.12727105436381464  - accuracy: 0.84375\n",
      "At: 2257 [==========>] Loss 0.10732879094757508  - accuracy: 0.8125\n",
      "At: 2258 [==========>] Loss 0.10327203887913053  - accuracy: 0.8125\n",
      "At: 2259 [==========>] Loss 0.11557630807638497  - accuracy: 0.84375\n",
      "At: 2260 [==========>] Loss 0.1650038940048052  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.08630746796059276  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.16121644783880756  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.1475663959853245  - accuracy: 0.78125\n",
      "At: 2264 [==========>] Loss 0.08546950666268055  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.08685920242019454  - accuracy: 0.90625\n",
      "At: 2266 [==========>] Loss 0.12684305661725706  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.0732229788896247  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.09368881907609772  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.039287086199968096  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.10275807045348048  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.13162790810886726  - accuracy: 0.8125\n",
      "At: 2272 [==========>] Loss 0.0893182929616066  - accuracy: 0.90625\n",
      "At: 2273 [==========>] Loss 0.1312919579845796  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.0911697274143386  - accuracy: 0.875\n",
      "At: 2275 [==========>] Loss 0.0977150989696143  - accuracy: 0.84375\n",
      "At: 2276 [==========>] Loss 0.09456254075394796  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.12246033401010745  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.09794613061019805  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.15963386975173915  - accuracy: 0.75\n",
      "At: 2280 [==========>] Loss 0.13862827409260023  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.10021504222333476  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.059202693151020856  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.14519843712613667  - accuracy: 0.78125\n",
      "At: 2284 [==========>] Loss 0.1193961015337241  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.10613505854570682  - accuracy: 0.84375\n",
      "At: 2286 [==========>] Loss 0.11063993520047849  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.12214320462802089  - accuracy: 0.875\n",
      "At: 2288 [==========>] Loss 0.09011993347037717  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.16248753936797347  - accuracy: 0.71875\n",
      "At: 2290 [==========>] Loss 0.0756513205191082  - accuracy: 0.90625\n",
      "At: 2291 [==========>] Loss 0.13236562762203977  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.0676652258926317  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.043643050679492076  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.05892091204030016  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.11960705514169462  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.16290799246259663  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.06266900480129432  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.113587518493211  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.1292820626738216  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.10385557814286223  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.2112682292333237  - accuracy: 0.6875\n",
      "At: 2302 [==========>] Loss 0.16175906617335623  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.0613873943752144  - accuracy: 0.9375\n",
      "At: 2304 [==========>] Loss 0.08669163221659186  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10722946170866458  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.11327646544310521  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.18907539361713466  - accuracy: 0.75\n",
      "At: 2308 [==========>] Loss 0.17768549793076704  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.1453779857492557  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.13554513650836114  - accuracy: 0.75\n",
      "At: 2311 [==========>] Loss 0.13773345018484143  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.09848691863615977  - accuracy: 0.90625\n",
      "At: 2313 [==========>] Loss 0.07607482891335789  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.10391322297037289  - accuracy: 0.875\n",
      "At: 2315 [==========>] Loss 0.1319325457333907  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.14876661695181181  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.1662375654131003  - accuracy: 0.8125\n",
      "At: 2318 [==========>] Loss 0.17213409236589108  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.09466739838634525  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.09653721974425696  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.13288423455672665  - accuracy: 0.8125\n",
      "At: 2322 [==========>] Loss 0.17292197256493644  - accuracy: 0.78125\n",
      "At: 2323 [==========>] Loss 0.11840650166388059  - accuracy: 0.875\n",
      "At: 2324 [==========>] Loss 0.16582879421080743  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.12545921423894515  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.07310819630443313  - accuracy: 0.90625\n",
      "At: 2327 [==========>] Loss 0.06066764202236086  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.11793135732007934  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.10619672255581537  - accuracy: 0.875\n",
      "At: 2330 [==========>] Loss 0.12070259370983621  - accuracy: 0.84375\n",
      "At: 2331 [==========>] Loss 0.08145160425231684  - accuracy: 0.90625\n",
      "At: 2332 [==========>] Loss 0.10400943653773279  - accuracy: 0.875\n",
      "At: 2333 [==========>] Loss 0.10154433900968321  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.1754661381706146  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.09293058736193152  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.114421446829083  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.12330727840293743  - accuracy: 0.84375\n",
      "At: 2338 [==========>] Loss 0.09317394604648341  - accuracy: 0.875\n",
      "At: 2339 [==========>] Loss 0.11524141216143677  - accuracy: 0.84375\n",
      "At: 2340 [==========>] Loss 0.1471942427362285  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.12997532839727724  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.177769720618456  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.08524973310366817  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.19825025277015368  - accuracy: 0.75\n",
      "At: 2345 [==========>] Loss 0.15541457882631582  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.0779221962181362  - accuracy: 0.9375\n",
      "At: 2347 [==========>] Loss 0.11555743621430259  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.05678773573964318  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.08873454243172821  - accuracy: 0.84375\n",
      "At: 2350 [==========>] Loss 0.11920416630747382  - accuracy: 0.84375\n",
      "At: 2351 [==========>] Loss 0.08426909738770444  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.1019196131195983  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.08542845925014689  - accuracy: 0.875\n",
      "At: 2354 [==========>] Loss 0.1547508670156848  - accuracy: 0.78125\n",
      "At: 2355 [==========>] Loss 0.04752348914010123  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.11564931230013696  - accuracy: 0.78125\n",
      "At: 2357 [==========>] Loss 0.15387095349711472  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.16150853983852725  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.17755245552769394  - accuracy: 0.6875\n",
      "At: 2360 [==========>] Loss 0.1527705307383979  - accuracy: 0.78125\n",
      "At: 2361 [==========>] Loss 0.14309731789065802  - accuracy: 0.84375\n",
      "At: 2362 [==========>] Loss 0.059088278866457  - accuracy: 0.96875\n",
      "At: 2363 [==========>] Loss 0.1771034363004162  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.08841395840594231  - accuracy: 0.875\n",
      "At: 2365 [==========>] Loss 0.08673318026457298  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.17204348830599592  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.10669518207614119  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.09277080384950712  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.09069780350784065  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.09396159959790247  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.08420304990853603  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.1475380630003208  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.1054144229998029  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.10834463118654714  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.04888166649593714  - accuracy: 0.90625\n",
      "At: 2376 [==========>] Loss 0.11790690124373374  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.09476043894664933  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.09794994204471044  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.15969059590423965  - accuracy: 0.8125\n",
      "At: 2380 [==========>] Loss 0.08983409420319517  - accuracy: 0.875\n",
      "At: 2381 [==========>] Loss 0.08650266594896339  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.09923677237522384  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.12181419373462343  - accuracy: 0.84375\n",
      "At: 2384 [==========>] Loss 0.09378518880532949  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.11397769585554117  - accuracy: 0.875\n",
      "At: 2386 [==========>] Loss 0.10261065188777252  - accuracy: 0.8125\n",
      "At: 2387 [==========>] Loss 0.06425450557890391  - accuracy: 0.96875\n",
      "At: 2388 [==========>] Loss 0.11789914979739119  - accuracy: 0.875\n",
      "At: 2389 [==========>] Loss 0.053262597414297105  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.07263690942004346  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.15700664134707662  - accuracy: 0.8125\n",
      "At: 2392 [==========>] Loss 0.13575018127514868  - accuracy: 0.78125\n",
      "At: 2393 [==========>] Loss 0.0946301474184496  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.07199321233178624  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.08168978791349858  - accuracy: 0.90625\n",
      "At: 2396 [==========>] Loss 0.07708852977367162  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.10203602073673285  - accuracy: 0.8125\n",
      "At: 2398 [==========>] Loss 0.11471221129865064  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.13322448039293716  - accuracy: 0.8125\n",
      "At: 2400 [==========>] Loss 0.0908402158548416  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.07951801261217888  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.07500531617926944  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.18597668863736933  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.13785664084722193  - accuracy: 0.875\n",
      "At: 2405 [==========>] Loss 0.049570499509043645  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.11603628996193326  - accuracy: 0.90625\n",
      "At: 2407 [==========>] Loss 0.10639630102984762  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.09390066751820886  - accuracy: 0.84375\n",
      "At: 2409 [==========>] Loss 0.1555104872834981  - accuracy: 0.78125\n",
      "At: 2410 [==========>] Loss 0.13461832653884367  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.09435417371096287  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.08057181062501796  - accuracy: 0.9375\n",
      "At: 2413 [==========>] Loss 0.07913451865297613  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.060943486140796206  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.1123445912753478  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.1062873065698749  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.15134644949356799  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.11791779124316219  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.14994621593276258  - accuracy: 0.78125\n",
      "At: 2420 [==========>] Loss 0.1417744921993182  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.0737823579002149  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.14179965012765375  - accuracy: 0.84375\n",
      "At: 2423 [==========>] Loss 0.12435456864701126  - accuracy: 0.78125\n",
      "At: 2424 [==========>] Loss 0.10375474869023912  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08279500154888711  - accuracy: 0.875\n",
      "At: 2426 [==========>] Loss 0.20364279856738815  - accuracy: 0.71875\n",
      "At: 2427 [==========>] Loss 0.13300281453581225  - accuracy: 0.84375\n",
      "At: 2428 [==========>] Loss 0.08599016427874377  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.09855068316807519  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.12322980183196408  - accuracy: 0.84375\n",
      "At: 2431 [==========>] Loss 0.15031751904111312  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.05928727429534973  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.06189025322719603  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.045138493928698814  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.1432807990789206  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.11188813714008464  - accuracy: 0.8125\n",
      "At: 2437 [==========>] Loss 0.19776972184271951  - accuracy: 0.6875\n",
      "At: 2438 [==========>] Loss 0.08774052889356328  - accuracy: 0.90625\n",
      "At: 2439 [==========>] Loss 0.14186501654719558  - accuracy: 0.8125\n",
      "At: 2440 [==========>] Loss 0.10386972926422784  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.11610322345218167  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.10934681228739815  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.11516860422426062  - accuracy: 0.8125\n",
      "At: 2444 [==========>] Loss 0.09192299639625215  - accuracy: 0.8125\n",
      "At: 2445 [==========>] Loss 0.06948734758092252  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.15704693063400788  - accuracy: 0.75\n",
      "At: 2447 [==========>] Loss 0.16266319713967903  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.10661390809881643  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.07213506282843046  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.06243446702977765  - accuracy: 0.9375\n",
      "At: 2451 [==========>] Loss 0.044755279628283816  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.13550431237141933  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.11167898707505956  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.13700681828920386  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1332294020397567  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.15492423379799192  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.17277559978687307  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.07648123038545546  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.140045245177476  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.060208159820344456  - accuracy: 0.9375\n",
      "At: 2461 [==========>] Loss 0.055888870773873375  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.12635558536655822  - accuracy: 0.78125\n",
      "At: 2463 [==========>] Loss 0.10756366598653982  - accuracy: 0.875\n",
      "At: 2464 [==========>] Loss 0.14058074231756845  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.128550424018736  - accuracy: 0.84375\n",
      "At: 2466 [==========>] Loss 0.07688515949246413  - accuracy: 0.9375\n",
      "At: 2467 [==========>] Loss 0.10389192379345387  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.07332417826080148  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.12886772310693148  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.12215152480255688  - accuracy: 0.8125\n",
      "At: 2471 [==========>] Loss 0.11852927745161589  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.08483579179792845  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.1121623956149463  - accuracy: 0.875\n",
      "At: 2474 [==========>] Loss 0.06888414733240589  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.12071157281766581  - accuracy: 0.84375\n",
      "At: 2476 [==========>] Loss 0.07389925340565309  - accuracy: 0.875\n",
      "At: 2477 [==========>] Loss 0.14420306239158653  - accuracy: 0.75\n",
      "At: 2478 [==========>] Loss 0.11466593402096228  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.06210462328277368  - accuracy: 0.9375\n",
      "At: 2480 [==========>] Loss 0.11880962952736265  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.05540954393243222  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.16174006780692823  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.09755729090826992  - accuracy: 0.875\n",
      "At: 2484 [==========>] Loss 0.08500999768927192  - accuracy: 0.90625\n",
      "At: 2485 [==========>] Loss 0.12678989499029877  - accuracy: 0.8125\n",
      "At: 2486 [==========>] Loss 0.12496268306762723  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.12694665046071052  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.15460446874975825  - accuracy: 0.8125\n",
      "At: 2489 [==========>] Loss 0.194855965461918  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.10419940854943999  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.12887440019764973  - accuracy: 0.75\n",
      "At: 2492 [==========>] Loss 0.12038612623273773  - accuracy: 0.78125\n",
      "At: 2493 [==========>] Loss 0.10533793455476965  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.08827430994211806  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.1090514824884507  - accuracy: 0.8125\n",
      "At: 2496 [==========>] Loss 0.06163422912691506  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.1884381891519485  - accuracy: 0.75\n",
      "At: 2498 [==========>] Loss 0.12038386469789508  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.05577189942210955  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.1730643056973894  - accuracy: 0.6875\n",
      "At: 2501 [==========>] Loss 0.13300893449056894  - accuracy: 0.8125\n",
      "At: 2502 [==========>] Loss 0.11963729470219639  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.13595621251812667  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.1640232400674875  - accuracy: 0.71875\n",
      "At: 2505 [==========>] Loss 0.1287853599371932  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.0868353862581564  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.1497333895455839  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.11350561754026303  - accuracy: 0.78125\n",
      "At: 2509 [==========>] Loss 0.12163142238820152  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.12545361475404826  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14306427571932784  - accuracy: 0.84375\n",
      "At: 2512 [==========>] Loss 0.07926385647485891  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.11391574389827565  - accuracy: 0.875\n",
      "At: 2514 [==========>] Loss 0.12824850042605573  - accuracy: 0.75\n",
      "At: 2515 [==========>] Loss 0.20843858945092555  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.1797401353111523  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.12548830942230577  - accuracy: 0.75\n",
      "At: 2518 [==========>] Loss 0.10226916605163229  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.129097298456061  - accuracy: 0.84375\n",
      "At: 2520 [==========>] Loss 0.13711180702305165  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.09814826089573173  - accuracy: 0.90625\n",
      "At: 2522 [==========>] Loss 0.22491175875221475  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.11022406424970731  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.17051993027601936  - accuracy: 0.71875\n",
      "At: 2525 [==========>] Loss 0.0739643269728613  - accuracy: 0.875\n",
      "At: 2526 [==========>] Loss 0.11315305912862553  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.13371950818363004  - accuracy: 0.75\n",
      "At: 2528 [==========>] Loss 0.07226967912569  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.14603278722719054  - accuracy: 0.8125\n",
      "At: 2530 [==========>] Loss 0.1448155245033192  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.04415519282026424  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.10119151329017034  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.11368065506093696  - accuracy: 0.84375\n",
      "At: 2534 [==========>] Loss 0.05863769786631398  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.07166241749946295  - accuracy: 0.90625\n",
      "At: 2536 [==========>] Loss 0.1373209391692338  - accuracy: 0.84375\n",
      "At: 2537 [==========>] Loss 0.08968659640069396  - accuracy: 0.90625\n",
      "At: 2538 [==========>] Loss 0.15813752333161274  - accuracy: 0.75\n",
      "At: 2539 [==========>] Loss 0.10099177194269948  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.11534360190042567  - accuracy: 0.875\n",
      "At: 2541 [==========>] Loss 0.061737114437836374  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.06487100936108137  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16986109824819667  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.12577647630879846  - accuracy: 0.8125\n",
      "At: 2545 [==========>] Loss 0.05958533114583453  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.13629943611162434  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.09942222574462227  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.08249215283298146  - accuracy: 0.875\n",
      "At: 2549 [==========>] Loss 0.09611862804822613  - accuracy: 0.90625\n",
      "At: 2550 [==========>] Loss 0.11577506831330922  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.13890259956329143  - accuracy: 0.75\n",
      "At: 2552 [==========>] Loss 0.1097709347028662  - accuracy: 0.8125\n",
      "At: 2553 [==========>] Loss 0.0791019677714136  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.11943020203189274  - accuracy: 0.84375\n",
      "At: 2555 [==========>] Loss 0.15101874077606758  - accuracy: 0.84375\n",
      "At: 2556 [==========>] Loss 0.06682639672268686  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.11283581871487255  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.09206570274022716  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.08611642817861632  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.1886578548209248  - accuracy: 0.6875\n",
      "At: 2561 [==========>] Loss 0.1031599451587514  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.09098713637965608  - accuracy: 0.90625\n",
      "At: 2563 [==========>] Loss 0.10111902185033868  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.07742104584607862  - accuracy: 0.9375\n",
      "At: 2565 [==========>] Loss 0.11339937336792003  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.20344733915119828  - accuracy: 0.71875\n",
      "At: 2567 [==========>] Loss 0.1096654105988929  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.10060360490569181  - accuracy: 0.875\n",
      "At: 2569 [==========>] Loss 0.06960637255904181  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.15904704354316224  - accuracy: 0.78125\n",
      "At: 2571 [==========>] Loss 0.09577857524270224  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.19562654682716557  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.17757037867194864  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.10552594386304556  - accuracy: 0.875\n",
      "At: 2575 [==========>] Loss 0.04407631315910358  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.1088095036943589  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.1994713034960799  - accuracy: 0.6875\n",
      "At: 2578 [==========>] Loss 0.1859595413869593  - accuracy: 0.71875\n",
      "At: 2579 [==========>] Loss 0.1912060843824174  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.1605812483326374  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.05432450456295497  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.19115011503380464  - accuracy: 0.71875\n",
      "At: 2583 [==========>] Loss 0.06315712192478143  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.16876124544866722  - accuracy: 0.75\n",
      "At: 2585 [==========>] Loss 0.08859444548677405  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.08494382219984556  - accuracy: 0.8125\n",
      "At: 2587 [==========>] Loss 0.19039309790194656  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.1187482783554098  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.09535785093827742  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.20550024803779687  - accuracy: 0.75\n",
      "At: 2591 [==========>] Loss 0.12417644548143421  - accuracy: 0.8125\n",
      "At: 2592 [==========>] Loss 0.07985576640362972  - accuracy: 0.90625\n",
      "At: 2593 [==========>] Loss 0.08828747894286679  - accuracy: 0.84375\n",
      "At: 2594 [==========>] Loss 0.08790924462965025  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.06765954407999022  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.11113064670453704  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.0750406712526071  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.12262695573490037  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.13285955420604972  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.14660810960104298  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.12624743726390567  - accuracy: 0.78125\n",
      "At: 2602 [==========>] Loss 0.055864891377162676  - accuracy: 0.9375\n",
      "At: 2603 [==========>] Loss 0.14022434695524172  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.06617720954408038  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.13200384797040093  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.0997410892829258  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.12384916513536466  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.06264438241024473  - accuracy: 0.96875\n",
      "At: 2609 [==========>] Loss 0.10722403216682616  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.10333829909412287  - accuracy: 0.84375\n",
      "At: 2611 [==========>] Loss 0.13462866683759014  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.0915662674583444  - accuracy: 0.84375\n",
      "At: 2613 [==========>] Loss 0.07795496560841264  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.10280191357211532  - accuracy: 0.90625\n",
      "At: 2615 [==========>] Loss 0.06652106789911066  - accuracy: 0.9375\n",
      "At: 2616 [==========>] Loss 0.05203876574735784  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.07952962289808743  - accuracy: 0.90625\n",
      "At: 2618 [==========>] Loss 0.07244004634736537  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.10057483270207779  - accuracy: 0.90625\n",
      "At: 2620 [==========>] Loss 0.10564881393546846  - accuracy: 0.84375\n",
      "At: 2621 [==========>] Loss 0.09404819101046355  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.08662224273835661  - accuracy: 0.90625\n",
      "At: 2623 [==========>] Loss 0.11167413787995992  - accuracy: 0.78125\n",
      "At: 2624 [==========>] Loss 0.12882710970644062  - accuracy: 0.84375\n",
      "At: 2625 [==========>] Loss 0.06347454818403124  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.04345517615343825  - accuracy: 0.96875\n",
      "At: 2627 [==========>] Loss 0.12243960371320622  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.05814159774116668  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.0625402786860374  - accuracy: 0.90625\n",
      "At: 2630 [==========>] Loss 0.10409966538463528  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.12585015849801223  - accuracy: 0.84375\n",
      "At: 2632 [==========>] Loss 0.12239404090715679  - accuracy: 0.84375\n",
      "At: 2633 [==========>] Loss 0.10706709893301637  - accuracy: 0.875\n",
      "At: 2634 [==========>] Loss 0.0564493004624123  - accuracy: 0.96875\n",
      "At: 2635 [==========>] Loss 0.18921123208505825  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.12712265811136225  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.13229373199242656  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.09288519948958025  - accuracy: 0.90625\n",
      "At: 2639 [==========>] Loss 0.06399746195671376  - accuracy: 0.96875\n",
      "At: 2640 [==========>] Loss 0.13133790680052967  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.13063056296752895  - accuracy: 0.84375\n",
      "At: 2642 [==========>] Loss 0.19976601176368428  - accuracy: 0.65625\n",
      "At: 2643 [==========>] Loss 0.07363121254735175  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.18203823857378598  - accuracy: 0.6875\n",
      "At: 2645 [==========>] Loss 0.06776181997386269  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.13674975809794662  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.1306392322260531  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.1549440423577677  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.12361162502660394  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.10421677879644989  - accuracy: 0.90625\n",
      "At: 2651 [==========>] Loss 0.16991782659661597  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.11215862639361372  - accuracy: 0.8125\n",
      "At: 2653 [==========>] Loss 0.12392652134245652  - accuracy: 0.8125\n",
      "At: 2654 [==========>] Loss 0.11339928095284564  - accuracy: 0.78125\n",
      "At: 2655 [==========>] Loss 0.2039091719321593  - accuracy: 0.71875\n",
      "At: 2656 [==========>] Loss 0.018764371025735024  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.1090744636069917  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.0787704981899372  - accuracy: 0.90625\n",
      "At: 2659 [==========>] Loss 0.044569294494562886  - accuracy: 0.96875\n",
      "At: 2660 [==========>] Loss 0.0897510922859247  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.08318549944374308  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.0668203101564821  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.08349320949832895  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.08759666198477073  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.0990128493814729  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.14608959461273457  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.1318876761678885  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.13057891291282994  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.14822119417107338  - accuracy: 0.75\n",
      "At: 2670 [==========>] Loss 0.10275251402727661  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.06654126457052911  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.12858417631667832  - accuracy: 0.78125\n",
      "At: 2673 [==========>] Loss 0.11360967247375037  - accuracy: 0.875\n",
      "At: 2674 [==========>] Loss 0.09895453987111225  - accuracy: 0.875\n",
      "At: 2675 [==========>] Loss 0.11980951478265482  - accuracy: 0.875\n",
      "At: 2676 [==========>] Loss 0.12847435187286832  - accuracy: 0.75\n",
      "At: 2677 [==========>] Loss 0.10898786957344396  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.05312799429572841  - accuracy: 0.875\n",
      "At: 2679 [==========>] Loss 0.08291186858727165  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.059853525238952286  - accuracy: 0.9375\n",
      "At: 2681 [==========>] Loss 0.09320912372956591  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.15546404474842496  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.19000893136213165  - accuracy: 0.78125\n",
      "At: 2684 [==========>] Loss 0.09396126981741634  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.09715590634437252  - accuracy: 0.8125\n",
      "At: 2686 [==========>] Loss 0.06772115494543021  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.1295763463824371  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.13311190573713846  - accuracy: 0.84375\n",
      "At: 2689 [==========>] Loss 0.12280547845805988  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.13436650086215352  - accuracy: 0.8125\n",
      "Epochs  6 / 10\n",
      "At: 1 [==========>] Loss 0.16625125725506262  - accuracy: 0.75\n",
      "At: 2 [==========>] Loss 0.20242787576809712  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.1687904294375602  - accuracy: 0.78125\n",
      "At: 4 [==========>] Loss 0.21081145374157875  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.08699221366273949  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.11425484560040866  - accuracy: 0.875\n",
      "At: 7 [==========>] Loss 0.17154023206793956  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.2513973727385753  - accuracy: 0.6875\n",
      "At: 9 [==========>] Loss 0.27407102813460044  - accuracy: 0.65625\n",
      "At: 10 [==========>] Loss 0.1875967178678254  - accuracy: 0.78125\n",
      "At: 11 [==========>] Loss 0.24258553810001884  - accuracy: 0.71875\n",
      "At: 12 [==========>] Loss 0.1690048391716776  - accuracy: 0.78125\n",
      "At: 13 [==========>] Loss 0.17618299495670225  - accuracy: 0.71875\n",
      "At: 14 [==========>] Loss 0.06886242915193301  - accuracy: 0.9375\n",
      "At: 15 [==========>] Loss 0.17664193141474394  - accuracy: 0.8125\n",
      "At: 16 [==========>] Loss 0.18645036357487996  - accuracy: 0.75\n",
      "At: 17 [==========>] Loss 0.12424219906000601  - accuracy: 0.8125\n",
      "At: 18 [==========>] Loss 0.21423072048046532  - accuracy: 0.71875\n",
      "At: 19 [==========>] Loss 0.15503042585990406  - accuracy: 0.75\n",
      "At: 20 [==========>] Loss 0.14260681667030606  - accuracy: 0.75\n",
      "At: 21 [==========>] Loss 0.1696967140252303  - accuracy: 0.71875\n",
      "At: 22 [==========>] Loss 0.19014375215272614  - accuracy: 0.75\n",
      "At: 23 [==========>] Loss 0.08702154320258748  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.2068239306909357  - accuracy: 0.75\n",
      "At: 25 [==========>] Loss 0.1825452144238222  - accuracy: 0.78125\n",
      "At: 26 [==========>] Loss 0.21637875757132274  - accuracy: 0.71875\n",
      "At: 27 [==========>] Loss 0.18528004696926664  - accuracy: 0.75\n",
      "At: 28 [==========>] Loss 0.16789076966823424  - accuracy: 0.71875\n",
      "At: 29 [==========>] Loss 0.15998280780167654  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.21366608488596334  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.24599795181150647  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.19241683587018593  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.12364135333540863  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.10191195113676695  - accuracy: 0.90625\n",
      "At: 35 [==========>] Loss 0.14881173364689096  - accuracy: 0.8125\n",
      "At: 36 [==========>] Loss 0.18197022817663924  - accuracy: 0.75\n",
      "At: 37 [==========>] Loss 0.17901150033831026  - accuracy: 0.75\n",
      "At: 38 [==========>] Loss 0.28228291571647407  - accuracy: 0.625\n",
      "At: 39 [==========>] Loss 0.18127162614730444  - accuracy: 0.75\n",
      "At: 40 [==========>] Loss 0.21675100722863025  - accuracy: 0.71875\n",
      "At: 41 [==========>] Loss 0.1008540149189717  - accuracy: 0.84375\n",
      "At: 42 [==========>] Loss 0.16221016335401034  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.1678700412641531  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.12951501860797768  - accuracy: 0.84375\n",
      "At: 45 [==========>] Loss 0.09800965567389266  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.173355630292497  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.19948512931813212  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.14857002367697889  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.15443672573386602  - accuracy: 0.78125\n",
      "At: 50 [==========>] Loss 0.22883532378237986  - accuracy: 0.71875\n",
      "At: 51 [==========>] Loss 0.19602888214165487  - accuracy: 0.71875\n",
      "At: 52 [==========>] Loss 0.27493799929929263  - accuracy: 0.6875\n",
      "At: 53 [==========>] Loss 0.12615862782421408  - accuracy: 0.875\n",
      "At: 54 [==========>] Loss 0.11226781478048335  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.18707306883551877  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.19962907827420842  - accuracy: 0.71875\n",
      "At: 57 [==========>] Loss 0.15150154660632958  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.21688830673737605  - accuracy: 0.6875\n",
      "At: 59 [==========>] Loss 0.21344003300312853  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.17722625822367494  - accuracy: 0.78125\n",
      "At: 61 [==========>] Loss 0.16813175111523415  - accuracy: 0.75\n",
      "At: 62 [==========>] Loss 0.15239830765276152  - accuracy: 0.8125\n",
      "At: 63 [==========>] Loss 0.1856279572633766  - accuracy: 0.8125\n",
      "At: 64 [==========>] Loss 0.22049139147102695  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.296687674702906  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.20801102576854547  - accuracy: 0.75\n",
      "At: 67 [==========>] Loss 0.21594180570087101  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.11688672572510765  - accuracy: 0.84375\n",
      "At: 69 [==========>] Loss 0.13507735069149845  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.166884956811979  - accuracy: 0.78125\n",
      "At: 71 [==========>] Loss 0.13405729470449002  - accuracy: 0.8125\n",
      "At: 72 [==========>] Loss 0.1386317533306709  - accuracy: 0.84375\n",
      "At: 73 [==========>] Loss 0.1266457128629971  - accuracy: 0.875\n",
      "At: 74 [==========>] Loss 0.1947449699493023  - accuracy: 0.6875\n",
      "At: 75 [==========>] Loss 0.19465790832778723  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.20365695738270867  - accuracy: 0.6875\n",
      "At: 77 [==========>] Loss 0.17774698326101324  - accuracy: 0.78125\n",
      "At: 78 [==========>] Loss 0.11487863492059362  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.14856153633832275  - accuracy: 0.78125\n",
      "At: 80 [==========>] Loss 0.16008378431567105  - accuracy: 0.78125\n",
      "At: 81 [==========>] Loss 0.11591042774742444  - accuracy: 0.84375\n",
      "At: 82 [==========>] Loss 0.2040328096832193  - accuracy: 0.71875\n",
      "At: 83 [==========>] Loss 0.1217906274334685  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.17139954306933092  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.19066772832013568  - accuracy: 0.78125\n",
      "At: 86 [==========>] Loss 0.16800457337577507  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.15396600052808046  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.33486835652738967  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.19331798397630212  - accuracy: 0.78125\n",
      "At: 90 [==========>] Loss 0.2398046674041241  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.2234857815689496  - accuracy: 0.71875\n",
      "At: 92 [==========>] Loss 0.06133221379317987  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.11291854310662727  - accuracy: 0.875\n",
      "At: 94 [==========>] Loss 0.12662870982054442  - accuracy: 0.875\n",
      "At: 95 [==========>] Loss 0.17326452669956732  - accuracy: 0.71875\n",
      "At: 96 [==========>] Loss 0.118756382651623  - accuracy: 0.90625\n",
      "At: 97 [==========>] Loss 0.09212585487661236  - accuracy: 0.84375\n",
      "At: 98 [==========>] Loss 0.24580305519364373  - accuracy: 0.59375\n",
      "At: 99 [==========>] Loss 0.09637211610610907  - accuracy: 0.875\n",
      "At: 100 [==========>] Loss 0.1344834840909142  - accuracy: 0.84375\n",
      "At: 101 [==========>] Loss 0.13189675215434332  - accuracy: 0.875\n",
      "At: 102 [==========>] Loss 0.21533045938969853  - accuracy: 0.6875\n",
      "At: 103 [==========>] Loss 0.09420667377855231  - accuracy: 0.875\n",
      "At: 104 [==========>] Loss 0.14662162391652706  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.17981098182042218  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.19372920177959882  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.17833630279631363  - accuracy: 0.78125\n",
      "At: 108 [==========>] Loss 0.20134103401096295  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.11694603009475521  - accuracy: 0.84375\n",
      "At: 110 [==========>] Loss 0.256019282766317  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.09884481653031907  - accuracy: 0.84375\n",
      "At: 112 [==========>] Loss 0.15791680923958604  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.25064155270783256  - accuracy: 0.65625\n",
      "At: 114 [==========>] Loss 0.15588375640032803  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.14943598844446515  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.17997788843394308  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.1635830749963862  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.1679091629377068  - accuracy: 0.71875\n",
      "At: 119 [==========>] Loss 0.13086650608209882  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.21152787193517963  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.13764022156542957  - accuracy: 0.78125\n",
      "At: 122 [==========>] Loss 0.20728719351386876  - accuracy: 0.71875\n",
      "At: 123 [==========>] Loss 0.1757536466697572  - accuracy: 0.78125\n",
      "At: 124 [==========>] Loss 0.19134043996945246  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.19048798335929495  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.18178861927806503  - accuracy: 0.75\n",
      "At: 127 [==========>] Loss 0.1447778112341631  - accuracy: 0.78125\n",
      "At: 128 [==========>] Loss 0.167201212635059  - accuracy: 0.75\n",
      "At: 129 [==========>] Loss 0.15345642261250922  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.24669670753090284  - accuracy: 0.6875\n",
      "At: 131 [==========>] Loss 0.170718252272637  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.11959576811109726  - accuracy: 0.875\n",
      "At: 133 [==========>] Loss 0.22372623152303073  - accuracy: 0.6875\n",
      "At: 134 [==========>] Loss 0.15525685266597936  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.17984267439941826  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.14812892650313045  - accuracy: 0.84375\n",
      "At: 137 [==========>] Loss 0.07695395353211607  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.1501814218082097  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.12153604184186695  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.1319486804805498  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.2721024589574134  - accuracy: 0.625\n",
      "At: 142 [==========>] Loss 0.16211979769950138  - accuracy: 0.75\n",
      "At: 143 [==========>] Loss 0.19307345935486048  - accuracy: 0.8125\n",
      "At: 144 [==========>] Loss 0.1239179066259243  - accuracy: 0.78125\n",
      "At: 145 [==========>] Loss 0.13648087639115  - accuracy: 0.8125\n",
      "At: 146 [==========>] Loss 0.1163144752237286  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.15507018639082737  - accuracy: 0.8125\n",
      "At: 148 [==========>] Loss 0.09903228470118058  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.14174955302227138  - accuracy: 0.8125\n",
      "At: 150 [==========>] Loss 0.17003277605577377  - accuracy: 0.75\n",
      "At: 151 [==========>] Loss 0.15575851201304353  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.18144417003267202  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.11297606515619714  - accuracy: 0.84375\n",
      "At: 154 [==========>] Loss 0.14285354029967673  - accuracy: 0.8125\n",
      "At: 155 [==========>] Loss 0.18316537725557158  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1123238178517928  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.1712655392365764  - accuracy: 0.78125\n",
      "At: 158 [==========>] Loss 0.1496824989822496  - accuracy: 0.84375\n",
      "At: 159 [==========>] Loss 0.1404371705543546  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.16801315920489251  - accuracy: 0.78125\n",
      "At: 161 [==========>] Loss 0.09160372508377869  - accuracy: 0.90625\n",
      "At: 162 [==========>] Loss 0.20841217747553523  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.20795320001708617  - accuracy: 0.71875\n",
      "At: 164 [==========>] Loss 0.21383600670898736  - accuracy: 0.71875\n",
      "At: 165 [==========>] Loss 0.22680181623108872  - accuracy: 0.6875\n",
      "At: 166 [==========>] Loss 0.14305203991393498  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.10408500282160132  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.21954626722079257  - accuracy: 0.71875\n",
      "At: 169 [==========>] Loss 0.1318075230198988  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.16082360632413117  - accuracy: 0.75\n",
      "At: 171 [==========>] Loss 0.17074268982496382  - accuracy: 0.75\n",
      "At: 172 [==========>] Loss 0.15317672234316104  - accuracy: 0.71875\n",
      "At: 173 [==========>] Loss 0.2549181780223138  - accuracy: 0.625\n",
      "At: 174 [==========>] Loss 0.1379558629208424  - accuracy: 0.8125\n",
      "At: 175 [==========>] Loss 0.12527309951972015  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.15926222222251515  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.0766833518459138  - accuracy: 0.90625\n",
      "At: 178 [==========>] Loss 0.20250141579154868  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.140450966903424  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.1503285422580959  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.06382682063105831  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.19621878690640554  - accuracy: 0.6875\n",
      "At: 183 [==========>] Loss 0.11474193459940125  - accuracy: 0.84375\n",
      "At: 184 [==========>] Loss 0.12355728274830702  - accuracy: 0.8125\n",
      "At: 185 [==========>] Loss 0.12244720751296853  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.18207330626355825  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.14756026681117176  - accuracy: 0.84375\n",
      "At: 188 [==========>] Loss 0.12522536528007985  - accuracy: 0.875\n",
      "At: 189 [==========>] Loss 0.2119388841958565  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.1233623544546826  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.2782432282537476  - accuracy: 0.65625\n",
      "At: 192 [==========>] Loss 0.1632348803775213  - accuracy: 0.78125\n",
      "At: 193 [==========>] Loss 0.18908648396718747  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.16436250573427447  - accuracy: 0.71875\n",
      "At: 195 [==========>] Loss 0.15715472080766218  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.13735940464263502  - accuracy: 0.84375\n",
      "At: 197 [==========>] Loss 0.14764010128271854  - accuracy: 0.75\n",
      "At: 198 [==========>] Loss 0.12772219888803255  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.10492819805637711  - accuracy: 0.90625\n",
      "At: 200 [==========>] Loss 0.2268479186205154  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.12423907006497267  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.11495012444668143  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.16434789364265578  - accuracy: 0.78125\n",
      "At: 204 [==========>] Loss 0.18310099497562835  - accuracy: 0.71875\n",
      "At: 205 [==========>] Loss 0.13991112586013618  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.07548632828756013  - accuracy: 0.9375\n",
      "At: 207 [==========>] Loss 0.10076341011573489  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.17836297380019434  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.22253016932108732  - accuracy: 0.6875\n",
      "At: 210 [==========>] Loss 0.05207005917698424  - accuracy: 0.9375\n",
      "At: 211 [==========>] Loss 0.1422440940325622  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.17543181192726703  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.19177088662362102  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.16361887064630903  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.08444556826288938  - accuracy: 0.875\n",
      "At: 216 [==========>] Loss 0.12813734312824038  - accuracy: 0.8125\n",
      "At: 217 [==========>] Loss 0.2048338582784327  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.19861774921180872  - accuracy: 0.75\n",
      "At: 219 [==========>] Loss 0.2564992698920018  - accuracy: 0.6875\n",
      "At: 220 [==========>] Loss 0.170249683160359  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.16026344137138615  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.08509382457853366  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.22124950342880964  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.16452144890520284  - accuracy: 0.8125\n",
      "At: 225 [==========>] Loss 0.1516222301331311  - accuracy: 0.8125\n",
      "At: 226 [==========>] Loss 0.1227488255277716  - accuracy: 0.8125\n",
      "At: 227 [==========>] Loss 0.13034613219154015  - accuracy: 0.78125\n",
      "At: 228 [==========>] Loss 0.17083088450668377  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.20923207170952143  - accuracy: 0.71875\n",
      "At: 230 [==========>] Loss 0.14794655836585763  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.2608804222873624  - accuracy: 0.625\n",
      "At: 232 [==========>] Loss 0.19312941782110793  - accuracy: 0.75\n",
      "At: 233 [==========>] Loss 0.16407685541624772  - accuracy: 0.78125\n",
      "At: 234 [==========>] Loss 0.11350432352032666  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.22574026163408872  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.17917717757424528  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.1378349770500857  - accuracy: 0.875\n",
      "At: 238 [==========>] Loss 0.10615622666965159  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.11933578232151076  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.2027309498192741  - accuracy: 0.78125\n",
      "At: 241 [==========>] Loss 0.15605696211446593  - accuracy: 0.75\n",
      "At: 242 [==========>] Loss 0.13918845665899876  - accuracy: 0.875\n",
      "At: 243 [==========>] Loss 0.12562907809122492  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.12971522489641657  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.11186747143819983  - accuracy: 0.84375\n",
      "At: 246 [==========>] Loss 0.1107522071862008  - accuracy: 0.875\n",
      "At: 247 [==========>] Loss 0.12439428995090115  - accuracy: 0.84375\n",
      "At: 248 [==========>] Loss 0.13921212099485458  - accuracy: 0.78125\n",
      "At: 249 [==========>] Loss 0.07185869439941237  - accuracy: 0.875\n",
      "At: 250 [==========>] Loss 0.2102550117258929  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.20293220923135788  - accuracy: 0.65625\n",
      "At: 252 [==========>] Loss 0.07751977265767808  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.18116603475466347  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.06861798076491304  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.16703116002203713  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.16488955736132235  - accuracy: 0.8125\n",
      "At: 257 [==========>] Loss 0.07900502672580043  - accuracy: 0.90625\n",
      "At: 258 [==========>] Loss 0.1848145334811157  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.12010691151093769  - accuracy: 0.875\n",
      "At: 260 [==========>] Loss 0.13116310050963484  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.09267565955182214  - accuracy: 0.875\n",
      "At: 262 [==========>] Loss 0.17939104452149682  - accuracy: 0.75\n",
      "At: 263 [==========>] Loss 0.08413814639658855  - accuracy: 0.9375\n",
      "At: 264 [==========>] Loss 0.10534283346167099  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.1919968907375357  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.23312422298952534  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.11708616694223725  - accuracy: 0.84375\n",
      "At: 268 [==========>] Loss 0.22656809213869494  - accuracy: 0.75\n",
      "At: 269 [==========>] Loss 0.10721642653015154  - accuracy: 0.78125\n",
      "At: 270 [==========>] Loss 0.2589539167447236  - accuracy: 0.65625\n",
      "At: 271 [==========>] Loss 0.16417573291507437  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.08077608672687925  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.1782465513123595  - accuracy: 0.8125\n",
      "At: 274 [==========>] Loss 0.1807981603196011  - accuracy: 0.71875\n",
      "At: 275 [==========>] Loss 0.05165754026576004  - accuracy: 0.96875\n",
      "At: 276 [==========>] Loss 0.1778117937352246  - accuracy: 0.78125\n",
      "At: 277 [==========>] Loss 0.1305309209019999  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.1176817019751774  - accuracy: 0.8125\n",
      "At: 279 [==========>] Loss 0.1353373550573022  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.13244153257697588  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.18160607407608037  - accuracy: 0.78125\n",
      "At: 282 [==========>] Loss 0.19713891332464067  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.13896295046698004  - accuracy: 0.84375\n",
      "At: 284 [==========>] Loss 0.08035152118180397  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.11567737618190146  - accuracy: 0.84375\n",
      "At: 286 [==========>] Loss 0.07423142200931271  - accuracy: 0.9375\n",
      "At: 287 [==========>] Loss 0.21523805373263488  - accuracy: 0.6875\n",
      "At: 288 [==========>] Loss 0.10027385984162154  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.08507519330316671  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.08246654068627934  - accuracy: 0.84375\n",
      "At: 291 [==========>] Loss 0.13332903834435994  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.13861399596455118  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.20868947623378664  - accuracy: 0.71875\n",
      "At: 294 [==========>] Loss 0.169304198273767  - accuracy: 0.78125\n",
      "At: 295 [==========>] Loss 0.12243115955823267  - accuracy: 0.8125\n",
      "At: 296 [==========>] Loss 0.07890280497265417  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.09491034594473001  - accuracy: 0.875\n",
      "At: 298 [==========>] Loss 0.13861632725687054  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.19681495083984546  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.16162901792692963  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.1644227980377319  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.1650207523059928  - accuracy: 0.78125\n",
      "At: 303 [==========>] Loss 0.08194683719827249  - accuracy: 0.90625\n",
      "At: 304 [==========>] Loss 0.1478921335921679  - accuracy: 0.84375\n",
      "At: 305 [==========>] Loss 0.19490347342550574  - accuracy: 0.78125\n",
      "At: 306 [==========>] Loss 0.12038769997034221  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.20917911046957494  - accuracy: 0.75\n",
      "At: 308 [==========>] Loss 0.12914806065438922  - accuracy: 0.8125\n",
      "At: 309 [==========>] Loss 0.06054904239782173  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.18239821859855937  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.07280128979793954  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.12087327914609122  - accuracy: 0.875\n",
      "At: 313 [==========>] Loss 0.08807820419126447  - accuracy: 0.90625\n",
      "At: 314 [==========>] Loss 0.1808325742770767  - accuracy: 0.75\n",
      "At: 315 [==========>] Loss 0.13956715064634642  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.18322625498558018  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.22346066777477414  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.12950090960897606  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.10160034591480593  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.19192703676125883  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.1803332002736262  - accuracy: 0.75\n",
      "At: 322 [==========>] Loss 0.08564139140989119  - accuracy: 0.9375\n",
      "At: 323 [==========>] Loss 0.1085722274101455  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.13093226038706002  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.11483643226985674  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.21949234889451327  - accuracy: 0.6875\n",
      "At: 327 [==========>] Loss 0.10457492221812559  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.08959702739227862  - accuracy: 0.875\n",
      "At: 329 [==========>] Loss 0.16105193422097835  - accuracy: 0.78125\n",
      "At: 330 [==========>] Loss 0.1821419029370494  - accuracy: 0.75\n",
      "At: 331 [==========>] Loss 0.18158947970119949  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.25449308413486116  - accuracy: 0.625\n",
      "At: 333 [==========>] Loss 0.14821742593183523  - accuracy: 0.75\n",
      "At: 334 [==========>] Loss 0.07075897179787301  - accuracy: 0.90625\n",
      "At: 335 [==========>] Loss 0.13593681316777287  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.12148189378468305  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.18225632793853858  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.12017794690378995  - accuracy: 0.875\n",
      "At: 339 [==========>] Loss 0.12155353541033709  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.0949100005641703  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.11717653058193933  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.11673740547715025  - accuracy: 0.8125\n",
      "At: 343 [==========>] Loss 0.23149613840225622  - accuracy: 0.71875\n",
      "At: 344 [==========>] Loss 0.15299938862223095  - accuracy: 0.78125\n",
      "At: 345 [==========>] Loss 0.176857955572359  - accuracy: 0.6875\n",
      "At: 346 [==========>] Loss 0.10496540703349241  - accuracy: 0.875\n",
      "At: 347 [==========>] Loss 0.11760694088080288  - accuracy: 0.875\n",
      "At: 348 [==========>] Loss 0.14108615041882905  - accuracy: 0.75\n",
      "At: 349 [==========>] Loss 0.1645673892621345  - accuracy: 0.78125\n",
      "At: 350 [==========>] Loss 0.0972054772882287  - accuracy: 0.90625\n",
      "At: 351 [==========>] Loss 0.18642359463340216  - accuracy: 0.6875\n",
      "At: 352 [==========>] Loss 0.09836389949700149  - accuracy: 0.90625\n",
      "At: 353 [==========>] Loss 0.15711392371574506  - accuracy: 0.78125\n",
      "At: 354 [==========>] Loss 0.22510753123564786  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.12232374089026628  - accuracy: 0.84375\n",
      "At: 356 [==========>] Loss 0.18989184892028874  - accuracy: 0.6875\n",
      "At: 357 [==========>] Loss 0.12888641132766515  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.11848430478186439  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.07236314285551161  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.14167288623798702  - accuracy: 0.8125\n",
      "At: 361 [==========>] Loss 0.06343872929260636  - accuracy: 0.9375\n",
      "At: 362 [==========>] Loss 0.12020927900037229  - accuracy: 0.8125\n",
      "At: 363 [==========>] Loss 0.11332600788556141  - accuracy: 0.84375\n",
      "At: 364 [==========>] Loss 0.1481859821227694  - accuracy: 0.75\n",
      "At: 365 [==========>] Loss 0.11276074077737767  - accuracy: 0.8125\n",
      "At: 366 [==========>] Loss 0.16183112600660454  - accuracy: 0.8125\n",
      "At: 367 [==========>] Loss 0.13876410952064314  - accuracy: 0.78125\n",
      "At: 368 [==========>] Loss 0.16257405671210043  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.13511989739419789  - accuracy: 0.84375\n",
      "At: 370 [==========>] Loss 0.1604520189865014  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.10954236035411133  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.10119168109122491  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.1935258444666229  - accuracy: 0.71875\n",
      "At: 374 [==========>] Loss 0.08867285523947516  - accuracy: 0.90625\n",
      "At: 375 [==========>] Loss 0.14034843147495468  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.096640287911845  - accuracy: 0.84375\n",
      "At: 377 [==========>] Loss 0.15237494805838142  - accuracy: 0.8125\n",
      "At: 378 [==========>] Loss 0.1670314648795025  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.1817670147614163  - accuracy: 0.6875\n",
      "At: 380 [==========>] Loss 0.14728027792056475  - accuracy: 0.84375\n",
      "At: 381 [==========>] Loss 0.16625046027690815  - accuracy: 0.78125\n",
      "At: 382 [==========>] Loss 0.05961554687799799  - accuracy: 0.9375\n",
      "At: 383 [==========>] Loss 0.15366095211518868  - accuracy: 0.8125\n",
      "At: 384 [==========>] Loss 0.16969225509484692  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.13338416846730683  - accuracy: 0.78125\n",
      "At: 386 [==========>] Loss 0.20614500966832727  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.07080315176088367  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.1664645519819043  - accuracy: 0.78125\n",
      "At: 389 [==========>] Loss 0.1762757306791401  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.11602657720415599  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.0837774811885571  - accuracy: 0.90625\n",
      "At: 392 [==========>] Loss 0.14510306310094617  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.23185707271695324  - accuracy: 0.59375\n",
      "At: 394 [==========>] Loss 0.09186522228284517  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.1413690378092256  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.13446528097871077  - accuracy: 0.84375\n",
      "At: 397 [==========>] Loss 0.0933225438710707  - accuracy: 0.84375\n",
      "At: 398 [==========>] Loss 0.1876142686417112  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.1579895596994459  - accuracy: 0.78125\n",
      "At: 400 [==========>] Loss 0.19453458020719566  - accuracy: 0.6875\n",
      "At: 401 [==========>] Loss 0.08985875611500053  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.10950280338617464  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.03193262053183226  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.0889702424638997  - accuracy: 0.90625\n",
      "At: 405 [==========>] Loss 0.15995754938453294  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.15593008703040956  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.14120736939318534  - accuracy: 0.84375\n",
      "At: 408 [==========>] Loss 0.17416422455936284  - accuracy: 0.78125\n",
      "At: 409 [==========>] Loss 0.20089927142543906  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.09817784004836382  - accuracy: 0.875\n",
      "At: 411 [==========>] Loss 0.083941957693983  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.1713132577546016  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.12607683637881312  - accuracy: 0.84375\n",
      "At: 414 [==========>] Loss 0.13759428263484658  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.12999174673405733  - accuracy: 0.875\n",
      "At: 416 [==========>] Loss 0.22537760456880046  - accuracy: 0.6875\n",
      "At: 417 [==========>] Loss 0.16756098767028899  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.1192820672211715  - accuracy: 0.84375\n",
      "At: 419 [==========>] Loss 0.09847547196753201  - accuracy: 0.875\n",
      "At: 420 [==========>] Loss 0.14838704458238913  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.1189354410857474  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.11884807350593841  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.11907438039063853  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.16228445425076918  - accuracy: 0.8125\n",
      "At: 425 [==========>] Loss 0.18631027917595297  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.1267794400857113  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.17530321857220776  - accuracy: 0.6875\n",
      "At: 428 [==========>] Loss 0.26664767228681724  - accuracy: 0.59375\n",
      "At: 429 [==========>] Loss 0.21193110306222968  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.12143298468744303  - accuracy: 0.84375\n",
      "At: 431 [==========>] Loss 0.10239465885462312  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.1367387400472307  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.10176894373706757  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.10698053127198033  - accuracy: 0.875\n",
      "At: 435 [==========>] Loss 0.16798887695277887  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.15343976041944818  - accuracy: 0.75\n",
      "At: 437 [==========>] Loss 0.13749756234952465  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.12214867558442145  - accuracy: 0.8125\n",
      "At: 439 [==========>] Loss 0.11122756574270726  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.07242121348568292  - accuracy: 0.9375\n",
      "At: 441 [==========>] Loss 0.143326344404003  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.12141122702517562  - accuracy: 0.84375\n",
      "At: 443 [==========>] Loss 0.12911435073094785  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.166222584152855  - accuracy: 0.78125\n",
      "At: 445 [==========>] Loss 0.12473787871769815  - accuracy: 0.875\n",
      "At: 446 [==========>] Loss 0.1732491955233157  - accuracy: 0.71875\n",
      "At: 447 [==========>] Loss 0.14826811231738374  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.16448804616650026  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.13455388576446276  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.08824749405602395  - accuracy: 0.875\n",
      "At: 451 [==========>] Loss 0.11908959709420261  - accuracy: 0.75\n",
      "At: 452 [==========>] Loss 0.12581594412576214  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.15892160346813894  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.1670712795160988  - accuracy: 0.8125\n",
      "At: 455 [==========>] Loss 0.18599586877247973  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.14204000921910626  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.11563175863567883  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.07528122151589528  - accuracy: 0.875\n",
      "At: 459 [==========>] Loss 0.22043716990153697  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.12598257221278011  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.18812421784325986  - accuracy: 0.78125\n",
      "At: 462 [==========>] Loss 0.16219481488260695  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.16572542873476678  - accuracy: 0.78125\n",
      "At: 464 [==========>] Loss 0.16939654704938778  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.16400513946345777  - accuracy: 0.8125\n",
      "At: 466 [==========>] Loss 0.11387824555931556  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.17864957111689095  - accuracy: 0.6875\n",
      "At: 468 [==========>] Loss 0.0735849560885608  - accuracy: 0.90625\n",
      "At: 469 [==========>] Loss 0.1313582765644511  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.10700511638991206  - accuracy: 0.78125\n",
      "At: 471 [==========>] Loss 0.1606194326841513  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.10576201305128993  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.1645607501834525  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.16539171179501022  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.1289771018059443  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.16797846371289857  - accuracy: 0.75\n",
      "At: 477 [==========>] Loss 0.09574886367268096  - accuracy: 0.875\n",
      "At: 478 [==========>] Loss 0.09894642384160894  - accuracy: 0.90625\n",
      "At: 479 [==========>] Loss 0.13383162123247558  - accuracy: 0.84375\n",
      "At: 480 [==========>] Loss 0.16492147213315828  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.08325555519578054  - accuracy: 0.9375\n",
      "At: 482 [==========>] Loss 0.0744661945248927  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.1307116681811454  - accuracy: 0.8125\n",
      "At: 484 [==========>] Loss 0.115230084975454  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.11524377925638285  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.2084698173905435  - accuracy: 0.78125\n",
      "At: 487 [==========>] Loss 0.14925892971674382  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.10683075570407011  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.15002731033411285  - accuracy: 0.8125\n",
      "At: 490 [==========>] Loss 0.12509609418106224  - accuracy: 0.84375\n",
      "At: 491 [==========>] Loss 0.12701595700040869  - accuracy: 0.84375\n",
      "At: 492 [==========>] Loss 0.18512830778542405  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.08969387472549134  - accuracy: 0.875\n",
      "At: 494 [==========>] Loss 0.16048702449751512  - accuracy: 0.75\n",
      "At: 495 [==========>] Loss 0.10636991255109313  - accuracy: 0.84375\n",
      "At: 496 [==========>] Loss 0.14736670359047413  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.14186105823196454  - accuracy: 0.8125\n",
      "At: 498 [==========>] Loss 0.09544708831258608  - accuracy: 0.8125\n",
      "At: 499 [==========>] Loss 0.15851965416145092  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.1257584715076342  - accuracy: 0.84375\n",
      "At: 501 [==========>] Loss 0.1378203664273772  - accuracy: 0.875\n",
      "At: 502 [==========>] Loss 0.1263284363293955  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.07329096645377692  - accuracy: 0.875\n",
      "At: 504 [==========>] Loss 0.11649837250772996  - accuracy: 0.84375\n",
      "At: 505 [==========>] Loss 0.14582382972955438  - accuracy: 0.8125\n",
      "At: 506 [==========>] Loss 0.25123259404745063  - accuracy: 0.625\n",
      "At: 507 [==========>] Loss 0.10561303555776871  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.079781328024422  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.13306600395851859  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.2057758033874798  - accuracy: 0.65625\n",
      "At: 511 [==========>] Loss 0.13561806736407161  - accuracy: 0.78125\n",
      "At: 512 [==========>] Loss 0.1796333001946133  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.14867069823439844  - accuracy: 0.78125\n",
      "At: 514 [==========>] Loss 0.1564929230860957  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.1517332295415501  - accuracy: 0.8125\n",
      "At: 516 [==========>] Loss 0.2118866162757424  - accuracy: 0.6875\n",
      "At: 517 [==========>] Loss 0.12737129624527532  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.07992798691019327  - accuracy: 0.9375\n",
      "At: 519 [==========>] Loss 0.10599790390185963  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.10771812495598715  - accuracy: 0.875\n",
      "At: 521 [==========>] Loss 0.15627532961710083  - accuracy: 0.75\n",
      "At: 522 [==========>] Loss 0.12593411425442544  - accuracy: 0.84375\n",
      "At: 523 [==========>] Loss 0.12619166598337656  - accuracy: 0.8125\n",
      "At: 524 [==========>] Loss 0.10724570173373928  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.12981305068846716  - accuracy: 0.75\n",
      "At: 526 [==========>] Loss 0.16479055664539366  - accuracy: 0.78125\n",
      "At: 527 [==========>] Loss 0.25168826375089737  - accuracy: 0.625\n",
      "At: 528 [==========>] Loss 0.20720778655884242  - accuracy: 0.75\n",
      "At: 529 [==========>] Loss 0.10131335149220441  - accuracy: 0.875\n",
      "At: 530 [==========>] Loss 0.18181833126708666  - accuracy: 0.75\n",
      "At: 531 [==========>] Loss 0.14172603671637055  - accuracy: 0.8125\n",
      "At: 532 [==========>] Loss 0.10421883576984448  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.10578363725370832  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.13828789370289418  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.12611503181529024  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.12124910374738143  - accuracy: 0.84375\n",
      "At: 537 [==========>] Loss 0.07108383390930367  - accuracy: 0.9375\n",
      "At: 538 [==========>] Loss 0.10544733393252477  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.08843949380632742  - accuracy: 0.90625\n",
      "At: 540 [==========>] Loss 0.21826129378846434  - accuracy: 0.78125\n",
      "At: 541 [==========>] Loss 0.1244973117352669  - accuracy: 0.8125\n",
      "At: 542 [==========>] Loss 0.13360572844002305  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.10372107601752921  - accuracy: 0.84375\n",
      "At: 544 [==========>] Loss 0.159951873270683  - accuracy: 0.8125\n",
      "At: 545 [==========>] Loss 0.08753345009327912  - accuracy: 0.90625\n",
      "At: 546 [==========>] Loss 0.1639513460077774  - accuracy: 0.8125\n",
      "At: 547 [==========>] Loss 0.12843295289606474  - accuracy: 0.9375\n",
      "At: 548 [==========>] Loss 0.10357228586415926  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.14412814044197522  - accuracy: 0.75\n",
      "At: 550 [==========>] Loss 0.11096093552789868  - accuracy: 0.8125\n",
      "At: 551 [==========>] Loss 0.16702187902095167  - accuracy: 0.78125\n",
      "At: 552 [==========>] Loss 0.12628684315735955  - accuracy: 0.84375\n",
      "At: 553 [==========>] Loss 0.12449975251455134  - accuracy: 0.78125\n",
      "At: 554 [==========>] Loss 0.08198014762078015  - accuracy: 0.90625\n",
      "At: 555 [==========>] Loss 0.12995105447591743  - accuracy: 0.8125\n",
      "At: 556 [==========>] Loss 0.14804070372879413  - accuracy: 0.8125\n",
      "At: 557 [==========>] Loss 0.09379391775551174  - accuracy: 0.875\n",
      "At: 558 [==========>] Loss 0.10922024452985538  - accuracy: 0.90625\n",
      "At: 559 [==========>] Loss 0.12839715707508811  - accuracy: 0.90625\n",
      "At: 560 [==========>] Loss 0.15072293591998193  - accuracy: 0.78125\n",
      "At: 561 [==========>] Loss 0.12942855592409092  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.06707238667986336  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.1457119585170793  - accuracy: 0.78125\n",
      "At: 564 [==========>] Loss 0.1408104253634938  - accuracy: 0.8125\n",
      "At: 565 [==========>] Loss 0.10507517994192987  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.13739143558735045  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.18918388079767354  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.1950260396334637  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.19923252687818746  - accuracy: 0.71875\n",
      "At: 570 [==========>] Loss 0.11615974392883098  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.09931912602887052  - accuracy: 0.875\n",
      "At: 572 [==========>] Loss 0.1068848886920191  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.11712007785508549  - accuracy: 0.8125\n",
      "At: 574 [==========>] Loss 0.11570306677662562  - accuracy: 0.84375\n",
      "At: 575 [==========>] Loss 0.11023279297957654  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.0584413270394957  - accuracy: 0.9375\n",
      "At: 577 [==========>] Loss 0.16704560081711636  - accuracy: 0.6875\n",
      "At: 578 [==========>] Loss 0.19581106363253553  - accuracy: 0.6875\n",
      "At: 579 [==========>] Loss 0.08633316416616525  - accuracy: 0.90625\n",
      "At: 580 [==========>] Loss 0.14285739752062723  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.17659980028383815  - accuracy: 0.78125\n",
      "At: 582 [==========>] Loss 0.1676731858317846  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.20176575649555323  - accuracy: 0.71875\n",
      "At: 584 [==========>] Loss 0.09650285058322128  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.1318655277502988  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.11839153306387165  - accuracy: 0.8125\n",
      "At: 587 [==========>] Loss 0.1490237673557343  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.13051347560461965  - accuracy: 0.84375\n",
      "At: 589 [==========>] Loss 0.13831022679185045  - accuracy: 0.8125\n",
      "At: 590 [==========>] Loss 0.0831334182426638  - accuracy: 0.9375\n",
      "At: 591 [==========>] Loss 0.14124102137447697  - accuracy: 0.78125\n",
      "At: 592 [==========>] Loss 0.0928962557025429  - accuracy: 0.9375\n",
      "At: 593 [==========>] Loss 0.13642337025872595  - accuracy: 0.84375\n",
      "At: 594 [==========>] Loss 0.17805345294984473  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.12707361022838587  - accuracy: 0.84375\n",
      "At: 596 [==========>] Loss 0.14274758659212294  - accuracy: 0.8125\n",
      "At: 597 [==========>] Loss 0.25733011189161215  - accuracy: 0.65625\n",
      "At: 598 [==========>] Loss 0.1440312455658565  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.13304345219272118  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.12967735210558637  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.11571049093128279  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.1406514961300475  - accuracy: 0.75\n",
      "At: 603 [==========>] Loss 0.12608279814316745  - accuracy: 0.84375\n",
      "At: 604 [==========>] Loss 0.15090992746917795  - accuracy: 0.71875\n",
      "At: 605 [==========>] Loss 0.10218194985868678  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.16835555727384302  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.10789866320417041  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.11857339098491286  - accuracy: 0.78125\n",
      "At: 609 [==========>] Loss 0.12858658619971142  - accuracy: 0.8125\n",
      "At: 610 [==========>] Loss 0.13003273935141027  - accuracy: 0.78125\n",
      "At: 611 [==========>] Loss 0.09780096407809567  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.13410223829020973  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.11433227621778372  - accuracy: 0.78125\n",
      "At: 614 [==========>] Loss 0.1527657327655047  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.17745067635284087  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.1181406592054578  - accuracy: 0.84375\n",
      "At: 617 [==========>] Loss 0.1000254060512358  - accuracy: 0.84375\n",
      "At: 618 [==========>] Loss 0.1961719823725408  - accuracy: 0.71875\n",
      "At: 619 [==========>] Loss 0.14290198349559063  - accuracy: 0.78125\n",
      "At: 620 [==========>] Loss 0.1538780198807161  - accuracy: 0.78125\n",
      "At: 621 [==========>] Loss 0.08505137834896392  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.16928129798512265  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.13622505845765903  - accuracy: 0.78125\n",
      "At: 624 [==========>] Loss 0.10385300210894108  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.12478167888145479  - accuracy: 0.84375\n",
      "At: 626 [==========>] Loss 0.09605314084765224  - accuracy: 0.84375\n",
      "At: 627 [==========>] Loss 0.11147500021911809  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.1427288097272629  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.17290229550620895  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.23634966866053403  - accuracy: 0.71875\n",
      "At: 631 [==========>] Loss 0.17469161586758786  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.1392544034496721  - accuracy: 0.84375\n",
      "At: 633 [==========>] Loss 0.15726166359519536  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.12585081027084852  - accuracy: 0.8125\n",
      "At: 635 [==========>] Loss 0.11229664516344946  - accuracy: 0.8125\n",
      "At: 636 [==========>] Loss 0.17576881334625366  - accuracy: 0.78125\n",
      "At: 637 [==========>] Loss 0.12764478324570552  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.10582351205033241  - accuracy: 0.875\n",
      "At: 639 [==========>] Loss 0.18755799463432368  - accuracy: 0.6875\n",
      "At: 640 [==========>] Loss 0.15113968305173414  - accuracy: 0.84375\n",
      "At: 641 [==========>] Loss 0.10713657372705995  - accuracy: 0.875\n",
      "At: 642 [==========>] Loss 0.16114856438315922  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.10891752325665587  - accuracy: 0.78125\n",
      "At: 644 [==========>] Loss 0.057029444896488896  - accuracy: 0.9375\n",
      "At: 645 [==========>] Loss 0.11663668530325597  - accuracy: 0.875\n",
      "At: 646 [==========>] Loss 0.14874971447335772  - accuracy: 0.71875\n",
      "At: 647 [==========>] Loss 0.11935267102957524  - accuracy: 0.875\n",
      "At: 648 [==========>] Loss 0.11906164525101466  - accuracy: 0.75\n",
      "At: 649 [==========>] Loss 0.13772350447625176  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.08749161275606349  - accuracy: 0.90625\n",
      "At: 651 [==========>] Loss 0.1871678529383627  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.08023454302476955  - accuracy: 0.90625\n",
      "At: 653 [==========>] Loss 0.13591771308919057  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.10553856188995649  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.14636818188584738  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.13288084020884838  - accuracy: 0.875\n",
      "At: 657 [==========>] Loss 0.14405744121374137  - accuracy: 0.75\n",
      "At: 658 [==========>] Loss 0.11978084032714496  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.11350496791028433  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.11682185634687921  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.12848368385175837  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.12630759367702826  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.09379811460670992  - accuracy: 0.84375\n",
      "At: 664 [==========>] Loss 0.12063548289550526  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.1798815764667835  - accuracy: 0.75\n",
      "At: 666 [==========>] Loss 0.12458467118814745  - accuracy: 0.84375\n",
      "At: 667 [==========>] Loss 0.1387912687824523  - accuracy: 0.8125\n",
      "At: 668 [==========>] Loss 0.11160484981945382  - accuracy: 0.84375\n",
      "At: 669 [==========>] Loss 0.13562706949363945  - accuracy: 0.78125\n",
      "At: 670 [==========>] Loss 0.1921376950678451  - accuracy: 0.75\n",
      "At: 671 [==========>] Loss 0.060812333244656805  - accuracy: 0.9375\n",
      "At: 672 [==========>] Loss 0.10406502107706347  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.054894086711986055  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.12265377389641438  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.07643678768820608  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.12674343043328767  - accuracy: 0.84375\n",
      "At: 677 [==========>] Loss 0.13986465029484324  - accuracy: 0.84375\n",
      "At: 678 [==========>] Loss 0.11139555224081825  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.08399974458932166  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.13665056927836677  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.11786049284526985  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.13274648945247  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.14390651625298848  - accuracy: 0.8125\n",
      "At: 684 [==========>] Loss 0.11837644690945828  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.12379591007974419  - accuracy: 0.84375\n",
      "At: 686 [==========>] Loss 0.1458675404616202  - accuracy: 0.84375\n",
      "At: 687 [==========>] Loss 0.06491296849019085  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.0854483556241125  - accuracy: 0.9375\n",
      "At: 689 [==========>] Loss 0.11126091862987411  - accuracy: 0.84375\n",
      "At: 690 [==========>] Loss 0.11968111130579032  - accuracy: 0.84375\n",
      "At: 691 [==========>] Loss 0.09405010296120862  - accuracy: 0.90625\n",
      "At: 692 [==========>] Loss 0.1129765451296342  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.14329508270733238  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.16997718665446349  - accuracy: 0.78125\n",
      "At: 695 [==========>] Loss 0.15298670361588035  - accuracy: 0.75\n",
      "At: 696 [==========>] Loss 0.16173833480749789  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.14620536482987528  - accuracy: 0.78125\n",
      "At: 698 [==========>] Loss 0.1173099779003238  - accuracy: 0.84375\n",
      "At: 699 [==========>] Loss 0.13040601026200416  - accuracy: 0.84375\n",
      "At: 700 [==========>] Loss 0.08397406842764302  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.10683152092917751  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.1123674169226395  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.16690504844923182  - accuracy: 0.84375\n",
      "At: 704 [==========>] Loss 0.16762054606218135  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.23448585383010234  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.1373850367506253  - accuracy: 0.71875\n",
      "At: 707 [==========>] Loss 0.09050177154380953  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.13605748369710455  - accuracy: 0.78125\n",
      "At: 709 [==========>] Loss 0.17977811778130037  - accuracy: 0.75\n",
      "At: 710 [==========>] Loss 0.13151260161908587  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.18774430121933172  - accuracy: 0.75\n",
      "At: 712 [==========>] Loss 0.14882317338935583  - accuracy: 0.8125\n",
      "At: 713 [==========>] Loss 0.14491003570765712  - accuracy: 0.8125\n",
      "At: 714 [==========>] Loss 0.2015467597795163  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.10556366840081277  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.09272805853662378  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.07031110891325808  - accuracy: 0.9375\n",
      "At: 718 [==========>] Loss 0.17357880380531415  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.10387736993844593  - accuracy: 0.875\n",
      "At: 720 [==========>] Loss 0.11053305391586536  - accuracy: 0.875\n",
      "At: 721 [==========>] Loss 0.11015531695474087  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.14339324688440286  - accuracy: 0.84375\n",
      "At: 723 [==========>] Loss 0.11266280421661351  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.10353524827857154  - accuracy: 0.8125\n",
      "At: 725 [==========>] Loss 0.1357046973141949  - accuracy: 0.84375\n",
      "At: 726 [==========>] Loss 0.1723075346404624  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.12905501713550618  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.15936561015957507  - accuracy: 0.78125\n",
      "At: 729 [==========>] Loss 0.19120059786359161  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.14479260806193484  - accuracy: 0.75\n",
      "At: 731 [==========>] Loss 0.16091553872076897  - accuracy: 0.75\n",
      "At: 732 [==========>] Loss 0.1806372440447154  - accuracy: 0.71875\n",
      "At: 733 [==========>] Loss 0.08665692683062834  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.13679676635560056  - accuracy: 0.84375\n",
      "At: 735 [==========>] Loss 0.11869232310753067  - accuracy: 0.78125\n",
      "At: 736 [==========>] Loss 0.09523012630491123  - accuracy: 0.90625\n",
      "At: 737 [==========>] Loss 0.12944013810400357  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.0932443597049844  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.05205351645762735  - accuracy: 0.9375\n",
      "At: 740 [==========>] Loss 0.1397699042867671  - accuracy: 0.78125\n",
      "At: 741 [==========>] Loss 0.09611000143905274  - accuracy: 0.875\n",
      "At: 742 [==========>] Loss 0.14003355935844328  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.13271139922771155  - accuracy: 0.84375\n",
      "At: 744 [==========>] Loss 0.13336521219528277  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.15041052783715  - accuracy: 0.75\n",
      "At: 746 [==========>] Loss 0.14171246508039786  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.10781645648575239  - accuracy: 0.90625\n",
      "At: 748 [==========>] Loss 0.1376732092147265  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.1517051398089836  - accuracy: 0.78125\n",
      "At: 750 [==========>] Loss 0.12141005042801462  - accuracy: 0.875\n",
      "At: 751 [==========>] Loss 0.17086637722655673  - accuracy: 0.8125\n",
      "At: 752 [==========>] Loss 0.09113345291096292  - accuracy: 0.84375\n",
      "At: 753 [==========>] Loss 0.17021336381994054  - accuracy: 0.75\n",
      "At: 754 [==========>] Loss 0.15363458646602918  - accuracy: 0.78125\n",
      "At: 755 [==========>] Loss 0.10188716364242786  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.20786163265653634  - accuracy: 0.6875\n",
      "At: 757 [==========>] Loss 0.08435705918424824  - accuracy: 0.84375\n",
      "At: 758 [==========>] Loss 0.11457759303696723  - accuracy: 0.875\n",
      "At: 759 [==========>] Loss 0.08393487616840509  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.1478285918037849  - accuracy: 0.78125\n",
      "At: 761 [==========>] Loss 0.138496160672279  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.11837387704120443  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.11865113693400581  - accuracy: 0.875\n",
      "At: 764 [==========>] Loss 0.10485156486579092  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.12424225329225831  - accuracy: 0.8125\n",
      "At: 766 [==========>] Loss 0.10637648894614363  - accuracy: 0.8125\n",
      "At: 767 [==========>] Loss 0.13717039453487143  - accuracy: 0.78125\n",
      "At: 768 [==========>] Loss 0.14700093733013203  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.16329339217720976  - accuracy: 0.71875\n",
      "At: 770 [==========>] Loss 0.12414406159207089  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.20460477440188338  - accuracy: 0.75\n",
      "At: 772 [==========>] Loss 0.08772063776229222  - accuracy: 0.90625\n",
      "At: 773 [==========>] Loss 0.08057310346100628  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.11959094183957841  - accuracy: 0.8125\n",
      "At: 775 [==========>] Loss 0.19374787496605128  - accuracy: 0.71875\n",
      "At: 776 [==========>] Loss 0.15469047968521937  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.06382694687647031  - accuracy: 0.9375\n",
      "At: 778 [==========>] Loss 0.16671878738044482  - accuracy: 0.8125\n",
      "At: 779 [==========>] Loss 0.11666816253774995  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.082452047264362  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.13669957499610036  - accuracy: 0.875\n",
      "At: 782 [==========>] Loss 0.1589424299038858  - accuracy: 0.71875\n",
      "At: 783 [==========>] Loss 0.1661206077896276  - accuracy: 0.78125\n",
      "At: 784 [==========>] Loss 0.1983795323663376  - accuracy: 0.65625\n",
      "At: 785 [==========>] Loss 0.16772833181602703  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.12074779719651225  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.14838887129548112  - accuracy: 0.8125\n",
      "At: 788 [==========>] Loss 0.07520693370050313  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.13005040278457128  - accuracy: 0.84375\n",
      "At: 790 [==========>] Loss 0.11684291141627477  - accuracy: 0.875\n",
      "At: 791 [==========>] Loss 0.13452347188934063  - accuracy: 0.8125\n",
      "At: 792 [==========>] Loss 0.17964919322804482  - accuracy: 0.75\n",
      "At: 793 [==========>] Loss 0.1445771652391475  - accuracy: 0.78125\n",
      "At: 794 [==========>] Loss 0.13774875674814707  - accuracy: 0.8125\n",
      "At: 795 [==========>] Loss 0.09316577007761975  - accuracy: 0.875\n",
      "At: 796 [==========>] Loss 0.14654544128611569  - accuracy: 0.78125\n",
      "At: 797 [==========>] Loss 0.15514460606963376  - accuracy: 0.75\n",
      "At: 798 [==========>] Loss 0.1330087075121126  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.06259833312451987  - accuracy: 0.9375\n",
      "At: 800 [==========>] Loss 0.17251145162417503  - accuracy: 0.75\n",
      "At: 801 [==========>] Loss 0.095971675440728  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.1718943830918507  - accuracy: 0.75\n",
      "At: 803 [==========>] Loss 0.16431014934911342  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.14873795310056603  - accuracy: 0.8125\n",
      "At: 805 [==========>] Loss 0.1211562497938937  - accuracy: 0.8125\n",
      "At: 806 [==========>] Loss 0.07639374413655116  - accuracy: 0.90625\n",
      "At: 807 [==========>] Loss 0.07126723900210069  - accuracy: 0.90625\n",
      "At: 808 [==========>] Loss 0.1311136702301084  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.1285888421751173  - accuracy: 0.78125\n",
      "At: 810 [==========>] Loss 0.1567306389150415  - accuracy: 0.8125\n",
      "At: 811 [==========>] Loss 0.0747872311824396  - accuracy: 0.875\n",
      "At: 812 [==========>] Loss 0.1256387495810994  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.16953901220126266  - accuracy: 0.6875\n",
      "At: 814 [==========>] Loss 0.13168275786752312  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.11267394556116539  - accuracy: 0.84375\n",
      "At: 816 [==========>] Loss 0.1402725798008871  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.08376949960986502  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.11470035465886909  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.14466044070299872  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.13469076218821296  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.13636200184481106  - accuracy: 0.84375\n",
      "At: 822 [==========>] Loss 0.11716148880050035  - accuracy: 0.8125\n",
      "At: 823 [==========>] Loss 0.16970602761686254  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.12977581124034354  - accuracy: 0.8125\n",
      "At: 825 [==========>] Loss 0.18304291200679323  - accuracy: 0.75\n",
      "At: 826 [==========>] Loss 0.07271382467433192  - accuracy: 0.90625\n",
      "At: 827 [==========>] Loss 0.08911340255077978  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.05364258211253001  - accuracy: 0.96875\n",
      "At: 829 [==========>] Loss 0.08167799891081531  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.09599111660075857  - accuracy: 0.875\n",
      "At: 831 [==========>] Loss 0.07136525772187219  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.07250922983060112  - accuracy: 0.90625\n",
      "At: 833 [==========>] Loss 0.18227143623210135  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.11438647842004088  - accuracy: 0.78125\n",
      "At: 835 [==========>] Loss 0.07494811459247178  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.10763918133622308  - accuracy: 0.90625\n",
      "At: 837 [==========>] Loss 0.09436876222193283  - accuracy: 0.90625\n",
      "At: 838 [==========>] Loss 0.11318028800636797  - accuracy: 0.875\n",
      "At: 839 [==========>] Loss 0.13430891835957146  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.13872873675284622  - accuracy: 0.75\n",
      "At: 841 [==========>] Loss 0.05760896885847942  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.06768560622151606  - accuracy: 0.96875\n",
      "At: 843 [==========>] Loss 0.140219619407685  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.10040475437305232  - accuracy: 0.90625\n",
      "At: 845 [==========>] Loss 0.11732616429220993  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.1466089048832473  - accuracy: 0.71875\n",
      "At: 847 [==========>] Loss 0.05602980811899519  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.10921197269947652  - accuracy: 0.90625\n",
      "At: 849 [==========>] Loss 0.11426174350145454  - accuracy: 0.84375\n",
      "At: 850 [==========>] Loss 0.10789650628444537  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.08344629711296778  - accuracy: 0.90625\n",
      "At: 852 [==========>] Loss 0.10637778436543756  - accuracy: 0.84375\n",
      "At: 853 [==========>] Loss 0.15715348474936097  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.22428484572285165  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.10611508873314351  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.0665174320218395  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.07054584216494214  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.26742373830402455  - accuracy: 0.625\n",
      "At: 859 [==========>] Loss 0.1125831796678928  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.10646564043080778  - accuracy: 0.84375\n",
      "At: 861 [==========>] Loss 0.06878403824547344  - accuracy: 0.90625\n",
      "At: 862 [==========>] Loss 0.08180544895939351  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.1449436446115035  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.16128780840024678  - accuracy: 0.75\n",
      "At: 865 [==========>] Loss 0.1963708357103507  - accuracy: 0.6875\n",
      "At: 866 [==========>] Loss 0.15886453063675354  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.08782014561011031  - accuracy: 0.8125\n",
      "At: 868 [==========>] Loss 0.20599591362242886  - accuracy: 0.6875\n",
      "At: 869 [==========>] Loss 0.1611927849557384  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.11987977438675611  - accuracy: 0.84375\n",
      "At: 871 [==========>] Loss 0.05420852635814345  - accuracy: 0.96875\n",
      "At: 872 [==========>] Loss 0.09978467125562436  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.1890286258213245  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.1760817291214024  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.10788436039973359  - accuracy: 0.84375\n",
      "At: 876 [==========>] Loss 0.12636512668781472  - accuracy: 0.78125\n",
      "At: 877 [==========>] Loss 0.13522825773541736  - accuracy: 0.84375\n",
      "At: 878 [==========>] Loss 0.05067055122405523  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.11566179835049695  - accuracy: 0.8125\n",
      "At: 880 [==========>] Loss 0.1070601723896068  - accuracy: 0.90625\n",
      "At: 881 [==========>] Loss 0.1336671243247821  - accuracy: 0.75\n",
      "At: 882 [==========>] Loss 0.08838669721083312  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.14423167902494938  - accuracy: 0.8125\n",
      "At: 884 [==========>] Loss 0.1182415239066605  - accuracy: 0.8125\n",
      "At: 885 [==========>] Loss 0.11621023990638432  - accuracy: 0.8125\n",
      "At: 886 [==========>] Loss 0.10579220267590023  - accuracy: 0.84375\n",
      "At: 887 [==========>] Loss 0.1450287008064601  - accuracy: 0.8125\n",
      "At: 888 [==========>] Loss 0.13421931722828412  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.11043517305101261  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.11669505059117899  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.08834918530022902  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.10035022550909703  - accuracy: 0.84375\n",
      "At: 893 [==========>] Loss 0.15496240458896549  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.16259689506192673  - accuracy: 0.75\n",
      "At: 895 [==========>] Loss 0.09403367019421166  - accuracy: 0.90625\n",
      "At: 896 [==========>] Loss 0.13714064027761794  - accuracy: 0.8125\n",
      "At: 897 [==========>] Loss 0.13962789170496692  - accuracy: 0.875\n",
      "At: 898 [==========>] Loss 0.11687670927881372  - accuracy: 0.84375\n",
      "At: 899 [==========>] Loss 0.07276538220423476  - accuracy: 0.9375\n",
      "At: 900 [==========>] Loss 0.1461043321604801  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.16835040029768644  - accuracy: 0.75\n",
      "At: 902 [==========>] Loss 0.12508928131947372  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.12525032093441235  - accuracy: 0.84375\n",
      "At: 904 [==========>] Loss 0.1317971516142215  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.06744661037592768  - accuracy: 0.9375\n",
      "At: 906 [==========>] Loss 0.09289770898597002  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.12856941150702367  - accuracy: 0.84375\n",
      "At: 908 [==========>] Loss 0.0984681051739324  - accuracy: 0.90625\n",
      "At: 909 [==========>] Loss 0.11033560194564057  - accuracy: 0.78125\n",
      "At: 910 [==========>] Loss 0.12942098586644557  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.11291826135530292  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.12670277087240533  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.10429182638199132  - accuracy: 0.84375\n",
      "At: 914 [==========>] Loss 0.1270813784745936  - accuracy: 0.78125\n",
      "At: 915 [==========>] Loss 0.13276305874517152  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.1619110863886301  - accuracy: 0.8125\n",
      "At: 917 [==========>] Loss 0.14680107593601094  - accuracy: 0.84375\n",
      "At: 918 [==========>] Loss 0.1663936787728792  - accuracy: 0.6875\n",
      "At: 919 [==========>] Loss 0.10007916247971803  - accuracy: 0.875\n",
      "At: 920 [==========>] Loss 0.11212577190585891  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.14458315165557206  - accuracy: 0.75\n",
      "At: 922 [==========>] Loss 0.1374908713860719  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.1138661717860268  - accuracy: 0.84375\n",
      "At: 924 [==========>] Loss 0.15497140882784588  - accuracy: 0.78125\n",
      "At: 925 [==========>] Loss 0.15806476556585852  - accuracy: 0.75\n",
      "At: 926 [==========>] Loss 0.12507173521700407  - accuracy: 0.8125\n",
      "At: 927 [==========>] Loss 0.15088949737370794  - accuracy: 0.78125\n",
      "At: 928 [==========>] Loss 0.09084340910123824  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.1643916700578611  - accuracy: 0.75\n",
      "At: 930 [==========>] Loss 0.09543646717633104  - accuracy: 0.84375\n",
      "At: 931 [==========>] Loss 0.15216688233908549  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.09603559743115288  - accuracy: 0.84375\n",
      "At: 933 [==========>] Loss 0.05876001018178588  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.14083307264202582  - accuracy: 0.8125\n",
      "At: 935 [==========>] Loss 0.059992465668015235  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.17193328312822093  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.1611032136717281  - accuracy: 0.78125\n",
      "At: 938 [==========>] Loss 0.14516183822993747  - accuracy: 0.8125\n",
      "At: 939 [==========>] Loss 0.11234969540557754  - accuracy: 0.78125\n",
      "At: 940 [==========>] Loss 0.18643526505766975  - accuracy: 0.75\n",
      "At: 941 [==========>] Loss 0.1056689100998098  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.14040853570070067  - accuracy: 0.75\n",
      "At: 943 [==========>] Loss 0.10665022597132406  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.0998926889499013  - accuracy: 0.90625\n",
      "At: 945 [==========>] Loss 0.08193676422328182  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.13082463009505355  - accuracy: 0.75\n",
      "At: 947 [==========>] Loss 0.11167787844818974  - accuracy: 0.84375\n",
      "At: 948 [==========>] Loss 0.1702670376998548  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.05515023719082997  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.1091576154360091  - accuracy: 0.84375\n",
      "At: 951 [==========>] Loss 0.09525308591974038  - accuracy: 0.9375\n",
      "At: 952 [==========>] Loss 0.058397160679371056  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.06973756995585836  - accuracy: 0.875\n",
      "At: 954 [==========>] Loss 0.13125305547868704  - accuracy: 0.8125\n",
      "At: 955 [==========>] Loss 0.12848992033930548  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.0767659145138777  - accuracy: 0.9375\n",
      "At: 957 [==========>] Loss 0.14755504181971352  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.08513747573817149  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.13104555184418276  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.09368697246562414  - accuracy: 0.90625\n",
      "At: 961 [==========>] Loss 0.11068841397439368  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.0937464779300641  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.12766451707329737  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.16441667633244092  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.09841891510518996  - accuracy: 0.90625\n",
      "At: 966 [==========>] Loss 0.14894586069564864  - accuracy: 0.8125\n",
      "At: 967 [==========>] Loss 0.11291143969630493  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.1558608840070122  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.12731783183997436  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.08024104173858532  - accuracy: 0.875\n",
      "At: 971 [==========>] Loss 0.12287416645453475  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.06861971739963199  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.11075096338623758  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.09259488644103334  - accuracy: 0.84375\n",
      "At: 975 [==========>] Loss 0.10136085743880502  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.10936152297775367  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.10509464366245329  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.1655264433531018  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.0942304727537702  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.12354141914956916  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.16352052559560076  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.06320518095053432  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.10251626604896066  - accuracy: 0.875\n",
      "At: 984 [==========>] Loss 0.12263594563374874  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.13407586013871178  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.12264907559891222  - accuracy: 0.75\n",
      "At: 987 [==========>] Loss 0.13556486130578801  - accuracy: 0.84375\n",
      "At: 988 [==========>] Loss 0.08861587499192167  - accuracy: 0.84375\n",
      "At: 989 [==========>] Loss 0.10548491782494512  - accuracy: 0.875\n",
      "At: 990 [==========>] Loss 0.14822310708976644  - accuracy: 0.75\n",
      "At: 991 [==========>] Loss 0.119455013234447  - accuracy: 0.84375\n",
      "At: 992 [==========>] Loss 0.2533501772151965  - accuracy: 0.625\n",
      "At: 993 [==========>] Loss 0.11963929031540099  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.13418506981200112  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.12299098747743027  - accuracy: 0.84375\n",
      "At: 996 [==========>] Loss 0.05212326279132086  - accuracy: 0.90625\n",
      "At: 997 [==========>] Loss 0.11634122240269022  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.10781120097595737  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.15716116584800022  - accuracy: 0.71875\n",
      "At: 1000 [==========>] Loss 0.19595896714524566  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.1418992507334928  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.21274286182657193  - accuracy: 0.75\n",
      "At: 1003 [==========>] Loss 0.11665299340801008  - accuracy: 0.78125\n",
      "At: 1004 [==========>] Loss 0.10814623178432053  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.06257432978458599  - accuracy: 0.96875\n",
      "At: 1006 [==========>] Loss 0.08484881371470446  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.13986954629102824  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.16517835345744813  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.1025997119166247  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.15138854688959494  - accuracy: 0.75\n",
      "At: 1011 [==========>] Loss 0.14522346128165903  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.09950696697249564  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.10296538687326592  - accuracy: 0.78125\n",
      "At: 1014 [==========>] Loss 0.10205989652696891  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.1763204788529147  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.13271071159165  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.16175592290789012  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.16483076171919883  - accuracy: 0.75\n",
      "At: 1019 [==========>] Loss 0.20040788236910556  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.14346858318781824  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.12782357042030684  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.1188943882509832  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.1756483997995982  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.1524115390030931  - accuracy: 0.78125\n",
      "At: 1025 [==========>] Loss 0.17443458909944748  - accuracy: 0.78125\n",
      "At: 1026 [==========>] Loss 0.11924236142791857  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.11655680873844074  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.24168567846851785  - accuracy: 0.625\n",
      "At: 1029 [==========>] Loss 0.10678521682625419  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.09074347895866541  - accuracy: 0.90625\n",
      "At: 1031 [==========>] Loss 0.15443628515926275  - accuracy: 0.78125\n",
      "At: 1032 [==========>] Loss 0.11303099636574343  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.11546588881961506  - accuracy: 0.875\n",
      "At: 1034 [==========>] Loss 0.07218874834085208  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.08137317209682016  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.13430162631588466  - accuracy: 0.78125\n",
      "At: 1037 [==========>] Loss 0.15044337632647983  - accuracy: 0.75\n",
      "At: 1038 [==========>] Loss 0.10051245694545655  - accuracy: 0.84375\n",
      "At: 1039 [==========>] Loss 0.08003108923407523  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.07196341065460067  - accuracy: 0.90625\n",
      "At: 1041 [==========>] Loss 0.14928538933310484  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.1269325348219696  - accuracy: 0.84375\n",
      "At: 1043 [==========>] Loss 0.17933507410517174  - accuracy: 0.75\n",
      "At: 1044 [==========>] Loss 0.13110164505115562  - accuracy: 0.84375\n",
      "At: 1045 [==========>] Loss 0.15472024529912948  - accuracy: 0.78125\n",
      "At: 1046 [==========>] Loss 0.1424145652124958  - accuracy: 0.8125\n",
      "At: 1047 [==========>] Loss 0.10991839049259663  - accuracy: 0.875\n",
      "At: 1048 [==========>] Loss 0.1889763540076635  - accuracy: 0.71875\n",
      "At: 1049 [==========>] Loss 0.14986925840308193  - accuracy: 0.8125\n",
      "At: 1050 [==========>] Loss 0.14483248097397272  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.09196497973346685  - accuracy: 0.84375\n",
      "At: 1052 [==========>] Loss 0.10769261533126312  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.09000009585387088  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.125324064130954  - accuracy: 0.8125\n",
      "At: 1055 [==========>] Loss 0.1586570112269906  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.1297530624656809  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.11900249562036871  - accuracy: 0.875\n",
      "At: 1058 [==========>] Loss 0.05917894188542841  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.0805768972598056  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.1128985694508453  - accuracy: 0.8125\n",
      "At: 1061 [==========>] Loss 0.08469201065573147  - accuracy: 0.84375\n",
      "At: 1062 [==========>] Loss 0.1702395941723241  - accuracy: 0.6875\n",
      "At: 1063 [==========>] Loss 0.09531307202199776  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.11301063017503346  - accuracy: 0.875\n",
      "At: 1065 [==========>] Loss 0.09267680942052224  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.09068160223004382  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.11568483378804964  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.1118033613889701  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.10849134160164886  - accuracy: 0.875\n",
      "At: 1070 [==========>] Loss 0.14547037008205968  - accuracy: 0.78125\n",
      "At: 1071 [==========>] Loss 0.07689677968068129  - accuracy: 0.9375\n",
      "At: 1072 [==========>] Loss 0.12202069095117202  - accuracy: 0.84375\n",
      "At: 1073 [==========>] Loss 0.15220181005261052  - accuracy: 0.84375\n",
      "At: 1074 [==========>] Loss 0.16398319994003074  - accuracy: 0.75\n",
      "At: 1075 [==========>] Loss 0.13169021247697726  - accuracy: 0.84375\n",
      "At: 1076 [==========>] Loss 0.1190213638241602  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.07914300811557869  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.06509435300053967  - accuracy: 0.9375\n",
      "At: 1079 [==========>] Loss 0.09967067789859242  - accuracy: 0.90625\n",
      "At: 1080 [==========>] Loss 0.11266187449419891  - accuracy: 0.8125\n",
      "At: 1081 [==========>] Loss 0.08744551030228773  - accuracy: 0.875\n",
      "At: 1082 [==========>] Loss 0.11446031928744535  - accuracy: 0.8125\n",
      "At: 1083 [==========>] Loss 0.1261909475556947  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.07714404651891325  - accuracy: 0.96875\n",
      "At: 1085 [==========>] Loss 0.12818578737160274  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.08720858945405402  - accuracy: 0.84375\n",
      "At: 1087 [==========>] Loss 0.11043168222038911  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.15276062712887079  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.0582200047879928  - accuracy: 0.96875\n",
      "At: 1090 [==========>] Loss 0.10092860826183658  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.19442720687546172  - accuracy: 0.71875\n",
      "At: 1092 [==========>] Loss 0.13625698021532076  - accuracy: 0.75\n",
      "At: 1093 [==========>] Loss 0.12084246278863843  - accuracy: 0.84375\n",
      "At: 1094 [==========>] Loss 0.14433328461966646  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.16767802994861708  - accuracy: 0.71875\n",
      "At: 1096 [==========>] Loss 0.08415138388218277  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.0636175730677299  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.1539661405499309  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.11750727558986669  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.056775231551665396  - accuracy: 0.9375\n",
      "At: 1101 [==========>] Loss 0.10860186005025063  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.11933785946524028  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.08992456637773943  - accuracy: 0.875\n",
      "At: 1104 [==========>] Loss 0.05425467908812577  - accuracy: 0.96875\n",
      "At: 1105 [==========>] Loss 0.07669651729786961  - accuracy: 0.84375\n",
      "At: 1106 [==========>] Loss 0.09850968714580169  - accuracy: 0.84375\n",
      "At: 1107 [==========>] Loss 0.18129208081955112  - accuracy: 0.65625\n",
      "At: 1108 [==========>] Loss 0.08317301925336458  - accuracy: 0.875\n",
      "At: 1109 [==========>] Loss 0.07390671081827689  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.0971651785371343  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.16107230455663013  - accuracy: 0.84375\n",
      "At: 1112 [==========>] Loss 0.11872486795168444  - accuracy: 0.84375\n",
      "At: 1113 [==========>] Loss 0.14650658768314737  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.07403120429440478  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.10397069359319538  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.10306007540758402  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.04851049681302133  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.13121607846710073  - accuracy: 0.8125\n",
      "At: 1119 [==========>] Loss 0.14303616038260208  - accuracy: 0.78125\n",
      "At: 1120 [==========>] Loss 0.07742365546898106  - accuracy: 0.90625\n",
      "At: 1121 [==========>] Loss 0.09858956888160841  - accuracy: 0.84375\n",
      "At: 1122 [==========>] Loss 0.09610330907578807  - accuracy: 0.875\n",
      "At: 1123 [==========>] Loss 0.11435921983534426  - accuracy: 0.8125\n",
      "At: 1124 [==========>] Loss 0.12305046935908809  - accuracy: 0.8125\n",
      "At: 1125 [==========>] Loss 0.16214373315260205  - accuracy: 0.71875\n",
      "At: 1126 [==========>] Loss 0.08031182112837328  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.12666904175443683  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.07235203090377446  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.1460521500699838  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.11184767816365075  - accuracy: 0.875\n",
      "At: 1131 [==========>] Loss 0.08863938715228219  - accuracy: 0.90625\n",
      "At: 1132 [==========>] Loss 0.08982081960467284  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.11747760363969763  - accuracy: 0.875\n",
      "At: 1134 [==========>] Loss 0.07881410073934406  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.11337522510111624  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.11448724202834665  - accuracy: 0.875\n",
      "At: 1137 [==========>] Loss 0.09286262010970253  - accuracy: 0.84375\n",
      "At: 1138 [==========>] Loss 0.1142690447962395  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.041541591234649365  - accuracy: 0.96875\n",
      "At: 1140 [==========>] Loss 0.19666792640791858  - accuracy: 0.59375\n",
      "At: 1141 [==========>] Loss 0.1252780007068248  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.13353743425215758  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.0799874306349842  - accuracy: 0.84375\n",
      "At: 1144 [==========>] Loss 0.12199436766629966  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.12687639481555962  - accuracy: 0.84375\n",
      "At: 1146 [==========>] Loss 0.10584907220555384  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.19958497776541484  - accuracy: 0.6875\n",
      "At: 1148 [==========>] Loss 0.07585858882693845  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.10601491934310667  - accuracy: 0.84375\n",
      "At: 1150 [==========>] Loss 0.13040373772057964  - accuracy: 0.78125\n",
      "At: 1151 [==========>] Loss 0.1509781422800486  - accuracy: 0.75\n",
      "At: 1152 [==========>] Loss 0.10940997578659739  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.16946577745294933  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.09572831012447655  - accuracy: 0.90625\n",
      "At: 1155 [==========>] Loss 0.10077011056146047  - accuracy: 0.90625\n",
      "At: 1156 [==========>] Loss 0.20846846737819247  - accuracy: 0.65625\n",
      "At: 1157 [==========>] Loss 0.10227728817251583  - accuracy: 0.90625\n",
      "At: 1158 [==========>] Loss 0.1506751587182894  - accuracy: 0.78125\n",
      "At: 1159 [==========>] Loss 0.11107256034103886  - accuracy: 0.875\n",
      "At: 1160 [==========>] Loss 0.08350020765885094  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.06384750259067358  - accuracy: 0.9375\n",
      "At: 1162 [==========>] Loss 0.12441991335381217  - accuracy: 0.84375\n",
      "At: 1163 [==========>] Loss 0.18529479367304613  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.08260788302045176  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.15136859377661893  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.0600245681821878  - accuracy: 0.90625\n",
      "At: 1167 [==========>] Loss 0.12279242870902744  - accuracy: 0.875\n",
      "At: 1168 [==========>] Loss 0.11182165098513439  - accuracy: 0.8125\n",
      "At: 1169 [==========>] Loss 0.1126531826692117  - accuracy: 0.78125\n",
      "At: 1170 [==========>] Loss 0.1596518973373814  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.06535678006368559  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.08598603769368868  - accuracy: 0.875\n",
      "At: 1173 [==========>] Loss 0.10136741533731519  - accuracy: 0.9375\n",
      "At: 1174 [==========>] Loss 0.17163877419626183  - accuracy: 0.78125\n",
      "At: 1175 [==========>] Loss 0.14039296094145487  - accuracy: 0.78125\n",
      "At: 1176 [==========>] Loss 0.12338646336587382  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.08102635405563163  - accuracy: 0.9375\n",
      "At: 1178 [==========>] Loss 0.10518389546002047  - accuracy: 0.875\n",
      "At: 1179 [==========>] Loss 0.09464588203630811  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.17451663055907413  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.09652491496902778  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.12815414922915705  - accuracy: 0.75\n",
      "At: 1183 [==========>] Loss 0.13018387179287577  - accuracy: 0.875\n",
      "At: 1184 [==========>] Loss 0.1289194115761747  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.09395693285445278  - accuracy: 0.90625\n",
      "At: 1186 [==========>] Loss 0.13938043372050307  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.11315232081696014  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.05551021168346705  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.1294082209196033  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.09017361053033189  - accuracy: 0.875\n",
      "At: 1191 [==========>] Loss 0.15548368578873512  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.061690600764851555  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.12209595431480309  - accuracy: 0.78125\n",
      "At: 1194 [==========>] Loss 0.10989481947760708  - accuracy: 0.90625\n",
      "At: 1195 [==========>] Loss 0.11472613573871654  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.11049098000891823  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.08018474671195729  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.09105621525567406  - accuracy: 0.84375\n",
      "At: 1199 [==========>] Loss 0.16832426654464552  - accuracy: 0.65625\n",
      "At: 1200 [==========>] Loss 0.07255448753387003  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.12097214777541439  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.19116665141327696  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.17239054589233077  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.10099842492222397  - accuracy: 0.875\n",
      "At: 1205 [==========>] Loss 0.0478853798643938  - accuracy: 0.96875\n",
      "At: 1206 [==========>] Loss 0.1010216582774933  - accuracy: 0.90625\n",
      "At: 1207 [==========>] Loss 0.16422553447901203  - accuracy: 0.75\n",
      "At: 1208 [==========>] Loss 0.09483022706823824  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.10593815837350537  - accuracy: 0.78125\n",
      "At: 1210 [==========>] Loss 0.1610066671768211  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.1431744730492072  - accuracy: 0.75\n",
      "At: 1212 [==========>] Loss 0.10073767801861355  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.17789286735262638  - accuracy: 0.78125\n",
      "At: 1214 [==========>] Loss 0.15051453508638651  - accuracy: 0.8125\n",
      "At: 1215 [==========>] Loss 0.13338515576717136  - accuracy: 0.8125\n",
      "At: 1216 [==========>] Loss 0.09864855441383188  - accuracy: 0.8125\n",
      "At: 1217 [==========>] Loss 0.07432538305552078  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.09930905072462874  - accuracy: 0.84375\n",
      "At: 1219 [==========>] Loss 0.1198369951643435  - accuracy: 0.8125\n",
      "At: 1220 [==========>] Loss 0.11557099085520134  - accuracy: 0.8125\n",
      "At: 1221 [==========>] Loss 0.11127431741471293  - accuracy: 0.84375\n",
      "At: 1222 [==========>] Loss 0.18858797539432218  - accuracy: 0.625\n",
      "At: 1223 [==========>] Loss 0.0793838061620602  - accuracy: 0.90625\n",
      "At: 1224 [==========>] Loss 0.060983775049998065  - accuracy: 0.9375\n",
      "At: 1225 [==========>] Loss 0.06846944342958826  - accuracy: 0.9375\n",
      "At: 1226 [==========>] Loss 0.085305512672738  - accuracy: 0.90625\n",
      "At: 1227 [==========>] Loss 0.15551940667208125  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.16062918211518828  - accuracy: 0.71875\n",
      "At: 1229 [==========>] Loss 0.11588032879251572  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.13010110196905594  - accuracy: 0.78125\n",
      "At: 1231 [==========>] Loss 0.12596972148371227  - accuracy: 0.84375\n",
      "At: 1232 [==========>] Loss 0.10158233946693682  - accuracy: 0.875\n",
      "At: 1233 [==========>] Loss 0.10261294686805428  - accuracy: 0.875\n",
      "At: 1234 [==========>] Loss 0.1177679358388433  - accuracy: 0.84375\n",
      "At: 1235 [==========>] Loss 0.07488720144948978  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.17307876579471027  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.10792010949448635  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.10782881238961864  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.14817001251933848  - accuracy: 0.84375\n",
      "At: 1240 [==========>] Loss 0.10774728732142869  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.13768454899085542  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.1427507214261003  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.1294472301768464  - accuracy: 0.84375\n",
      "At: 1244 [==========>] Loss 0.11578836076934519  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.09445273808092698  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.05294157450446037  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.16574086668230253  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.07149471456152359  - accuracy: 0.90625\n",
      "At: 1249 [==========>] Loss 0.10816902305864409  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.14052413805583558  - accuracy: 0.8125\n",
      "At: 1251 [==========>] Loss 0.11936178637776396  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.09317080879928996  - accuracy: 0.84375\n",
      "At: 1253 [==========>] Loss 0.09357251590087425  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.1531076613677899  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.07174584319308179  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.16127377998914538  - accuracy: 0.75\n",
      "At: 1257 [==========>] Loss 0.12897817871555564  - accuracy: 0.78125\n",
      "At: 1258 [==========>] Loss 0.08384401318340079  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.12170775711780413  - accuracy: 0.84375\n",
      "At: 1260 [==========>] Loss 0.0982694641227206  - accuracy: 0.90625\n",
      "At: 1261 [==========>] Loss 0.12223170874389094  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.13206605879297245  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.12192113706373175  - accuracy: 0.84375\n",
      "At: 1264 [==========>] Loss 0.08990491272847338  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.11924447401825063  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.12112217712341092  - accuracy: 0.78125\n",
      "At: 1267 [==========>] Loss 0.10072935626414989  - accuracy: 0.875\n",
      "At: 1268 [==========>] Loss 0.14829764317658595  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.10101346156413409  - accuracy: 0.90625\n",
      "At: 1270 [==========>] Loss 0.12129285275430354  - accuracy: 0.78125\n",
      "At: 1271 [==========>] Loss 0.18318560654878085  - accuracy: 0.6875\n",
      "At: 1272 [==========>] Loss 0.06500966119848336  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.2051053835393643  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.1598438592484877  - accuracy: 0.71875\n",
      "At: 1275 [==========>] Loss 0.08350245507223361  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.08940504410361383  - accuracy: 0.875\n",
      "At: 1277 [==========>] Loss 0.09681357836786768  - accuracy: 0.875\n",
      "At: 1278 [==========>] Loss 0.1333196821078876  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.10580396111006676  - accuracy: 0.84375\n",
      "At: 1280 [==========>] Loss 0.11064558990007745  - accuracy: 0.8125\n",
      "At: 1281 [==========>] Loss 0.1394253247593943  - accuracy: 0.8125\n",
      "At: 1282 [==========>] Loss 0.10822121234255844  - accuracy: 0.875\n",
      "At: 1283 [==========>] Loss 0.1220312284455366  - accuracy: 0.8125\n",
      "At: 1284 [==========>] Loss 0.17435340194593416  - accuracy: 0.75\n",
      "At: 1285 [==========>] Loss 0.10280792499181579  - accuracy: 0.8125\n",
      "At: 1286 [==========>] Loss 0.13756339913137827  - accuracy: 0.84375\n",
      "At: 1287 [==========>] Loss 0.1002279175244517  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.1831653230968041  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.1259470757957994  - accuracy: 0.78125\n",
      "At: 1290 [==========>] Loss 0.12669099858165975  - accuracy: 0.8125\n",
      "At: 1291 [==========>] Loss 0.11566081958723548  - accuracy: 0.875\n",
      "At: 1292 [==========>] Loss 0.10740790358815026  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.1701046279393289  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.1285834478621849  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.16132215360270263  - accuracy: 0.84375\n",
      "At: 1296 [==========>] Loss 0.14521017043907392  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.13753853981488628  - accuracy: 0.8125\n",
      "At: 1298 [==========>] Loss 0.08630770848618091  - accuracy: 0.90625\n",
      "At: 1299 [==========>] Loss 0.15114085978334335  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.12099561624980205  - accuracy: 0.8125\n",
      "At: 1301 [==========>] Loss 0.12740636620145013  - accuracy: 0.84375\n",
      "At: 1302 [==========>] Loss 0.0993550670223459  - accuracy: 0.875\n",
      "At: 1303 [==========>] Loss 0.09940905081632673  - accuracy: 0.84375\n",
      "At: 1304 [==========>] Loss 0.10184762081722662  - accuracy: 0.84375\n",
      "At: 1305 [==========>] Loss 0.12574012396283676  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.07049721015371391  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.18192804956708133  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.08360486714664453  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.15657941070075881  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.16139148674168882  - accuracy: 0.8125\n",
      "At: 1311 [==========>] Loss 0.13012954868952376  - accuracy: 0.78125\n",
      "At: 1312 [==========>] Loss 0.07724770090023342  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.1925990116520631  - accuracy: 0.75\n",
      "At: 1314 [==========>] Loss 0.05616297614499596  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.12532669906727154  - accuracy: 0.8125\n",
      "At: 1316 [==========>] Loss 0.12974528330339818  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.10094238347276675  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.1308390733048155  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.09854133085984314  - accuracy: 0.875\n",
      "At: 1320 [==========>] Loss 0.12700991833239808  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.07862030018458913  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.13697691448673432  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.06711194734165676  - accuracy: 0.96875\n",
      "At: 1324 [==========>] Loss 0.10910867223872268  - accuracy: 0.84375\n",
      "At: 1325 [==========>] Loss 0.09131183718059545  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.09552373909791355  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.1062769010961549  - accuracy: 0.84375\n",
      "At: 1328 [==========>] Loss 0.08680467469822245  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.04855945994012169  - accuracy: 1.0\n",
      "At: 1330 [==========>] Loss 0.09130871511918048  - accuracy: 0.875\n",
      "At: 1331 [==========>] Loss 0.1871168521487101  - accuracy: 0.6875\n",
      "At: 1332 [==========>] Loss 0.10636784230288732  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.14602475215612593  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.09439348080445331  - accuracy: 0.90625\n",
      "At: 1335 [==========>] Loss 0.11379815742152947  - accuracy: 0.875\n",
      "At: 1336 [==========>] Loss 0.0953796178121562  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.1453403915452267  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.13657688512932747  - accuracy: 0.8125\n",
      "At: 1339 [==========>] Loss 0.11973972192763122  - accuracy: 0.875\n",
      "At: 1340 [==========>] Loss 0.14026316363919605  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.07644552631396448  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.1655604072959706  - accuracy: 0.75\n",
      "At: 1343 [==========>] Loss 0.13275342671626866  - accuracy: 0.8125\n",
      "At: 1344 [==========>] Loss 0.1744329815995077  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.10578116731785549  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.10784279883708317  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.09221012041191914  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09785522425137846  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.1558963859324208  - accuracy: 0.75\n",
      "At: 1350 [==========>] Loss 0.09260083587752047  - accuracy: 0.875\n",
      "At: 1351 [==========>] Loss 0.09601614547054613  - accuracy: 0.8125\n",
      "At: 1352 [==========>] Loss 0.07532724151729647  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.1651240093855864  - accuracy: 0.75\n",
      "At: 1354 [==========>] Loss 0.1588177935099564  - accuracy: 0.75\n",
      "At: 1355 [==========>] Loss 0.08729111433855871  - accuracy: 0.875\n",
      "At: 1356 [==========>] Loss 0.0817418589827548  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.1223783057751168  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.12402335405527459  - accuracy: 0.875\n",
      "At: 1359 [==========>] Loss 0.07342344808324273  - accuracy: 0.875\n",
      "At: 1360 [==========>] Loss 0.1661796622566865  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.0983299595411412  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.13370525875334274  - accuracy: 0.875\n",
      "At: 1363 [==========>] Loss 0.08808223546596014  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.14458173211282815  - accuracy: 0.78125\n",
      "At: 1365 [==========>] Loss 0.13702898190194512  - accuracy: 0.78125\n",
      "At: 1366 [==========>] Loss 0.1255905447008489  - accuracy: 0.75\n",
      "At: 1367 [==========>] Loss 0.11173027784327189  - accuracy: 0.84375\n",
      "At: 1368 [==========>] Loss 0.1379312011490821  - accuracy: 0.8125\n",
      "At: 1369 [==========>] Loss 0.07855258384536079  - accuracy: 0.90625\n",
      "At: 1370 [==========>] Loss 0.10979014239491593  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.18542962066784952  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.09722527210013288  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.1482491853305722  - accuracy: 0.78125\n",
      "At: 1374 [==========>] Loss 0.12075791768299629  - accuracy: 0.84375\n",
      "At: 1375 [==========>] Loss 0.13643343427207444  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.09165540191330257  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.16166094637596617  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.11210062868530077  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.14461844367483853  - accuracy: 0.8125\n",
      "At: 1380 [==========>] Loss 0.13553971504024231  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.08853148998484099  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.12582979091567187  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.12298877100308549  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.11221781532945409  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.18670956728134752  - accuracy: 0.71875\n",
      "At: 1386 [==========>] Loss 0.16422294986040947  - accuracy: 0.78125\n",
      "At: 1387 [==========>] Loss 0.053220195372813285  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.15633992997087193  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.10584277891262871  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.12018438969189635  - accuracy: 0.875\n",
      "At: 1391 [==========>] Loss 0.11274108961355368  - accuracy: 0.84375\n",
      "At: 1392 [==========>] Loss 0.05700120855400668  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.14143517687256807  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.07824584539871042  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.1651721508934656  - accuracy: 0.78125\n",
      "At: 1396 [==========>] Loss 0.060658552781144924  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.1299356927881345  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.11054068409165752  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.07429296076005679  - accuracy: 0.9375\n",
      "At: 1400 [==========>] Loss 0.11775777151108349  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.09078593249831043  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.1793031364871254  - accuracy: 0.71875\n",
      "At: 1403 [==========>] Loss 0.13027450060088358  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.08786744551157571  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.07965580579417132  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.13566869310700058  - accuracy: 0.8125\n",
      "At: 1407 [==========>] Loss 0.09760766123472624  - accuracy: 0.875\n",
      "At: 1408 [==========>] Loss 0.14502894373665498  - accuracy: 0.71875\n",
      "At: 1409 [==========>] Loss 0.03458355695670835  - accuracy: 0.96875\n",
      "At: 1410 [==========>] Loss 0.11765896104171245  - accuracy: 0.84375\n",
      "At: 1411 [==========>] Loss 0.14737233990924561  - accuracy: 0.75\n",
      "At: 1412 [==========>] Loss 0.14500586341927302  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.09657942182189372  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.1819920777964405  - accuracy: 0.75\n",
      "At: 1415 [==========>] Loss 0.08983887765832287  - accuracy: 0.90625\n",
      "At: 1416 [==========>] Loss 0.12086678898910218  - accuracy: 0.84375\n",
      "At: 1417 [==========>] Loss 0.10556904448006807  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.1732155426546458  - accuracy: 0.78125\n",
      "At: 1419 [==========>] Loss 0.13023841635469072  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.08369997804450277  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.11079434955458684  - accuracy: 0.84375\n",
      "At: 1422 [==========>] Loss 0.13099586331048194  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.1323155426295871  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.14139364130827264  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.0920306179316788  - accuracy: 0.8125\n",
      "At: 1426 [==========>] Loss 0.1253914118901283  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.082271628088008  - accuracy: 0.875\n",
      "At: 1428 [==========>] Loss 0.09266636859220437  - accuracy: 0.90625\n",
      "At: 1429 [==========>] Loss 0.15528732968894557  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.10323366073742965  - accuracy: 0.8125\n",
      "At: 1431 [==========>] Loss 0.11227282473821168  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.09191037585390946  - accuracy: 0.875\n",
      "At: 1433 [==========>] Loss 0.1123379638915174  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.14765467613530003  - accuracy: 0.78125\n",
      "At: 1435 [==========>] Loss 0.09796410116707055  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.0662024209861681  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.14381123702374732  - accuracy: 0.78125\n",
      "At: 1438 [==========>] Loss 0.15483027025009954  - accuracy: 0.8125\n",
      "At: 1439 [==========>] Loss 0.12120744368316268  - accuracy: 0.875\n",
      "At: 1440 [==========>] Loss 0.10790661218600586  - accuracy: 0.78125\n",
      "At: 1441 [==========>] Loss 0.07262385850353115  - accuracy: 0.9375\n",
      "At: 1442 [==========>] Loss 0.11581271073494266  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.14898868795073647  - accuracy: 0.75\n",
      "At: 1444 [==========>] Loss 0.1151814238067331  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.18287409968443208  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.1817879057789062  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.15279855459296132  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.057893252662702274  - accuracy: 0.96875\n",
      "At: 1449 [==========>] Loss 0.1490382554927251  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.12561608098340077  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.1197437176103717  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.10180396293662389  - accuracy: 0.90625\n",
      "At: 1453 [==========>] Loss 0.0556406418370261  - accuracy: 0.96875\n",
      "At: 1454 [==========>] Loss 0.15635792374374066  - accuracy: 0.78125\n",
      "At: 1455 [==========>] Loss 0.11141508964890587  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.10663937897465922  - accuracy: 0.84375\n",
      "At: 1457 [==========>] Loss 0.08179066464476788  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.132261035839083  - accuracy: 0.84375\n",
      "At: 1459 [==========>] Loss 0.13885832438759693  - accuracy: 0.78125\n",
      "At: 1460 [==========>] Loss 0.15902270258763346  - accuracy: 0.8125\n",
      "At: 1461 [==========>] Loss 0.07691245758238754  - accuracy: 0.90625\n",
      "At: 1462 [==========>] Loss 0.14457737058907993  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.08400335781592883  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.16425783403223598  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.10593758859297207  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.09833301350301432  - accuracy: 0.84375\n",
      "At: 1467 [==========>] Loss 0.1805909028581788  - accuracy: 0.75\n",
      "At: 1468 [==========>] Loss 0.15669953879511916  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.16070582051199445  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.12505641391247732  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.11470070660812688  - accuracy: 0.875\n",
      "At: 1472 [==========>] Loss 0.07911688778390688  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.10704955680331456  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.16405136629510542  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.1323817362999006  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.10627009022091505  - accuracy: 0.78125\n",
      "At: 1477 [==========>] Loss 0.08335050210392321  - accuracy: 0.9375\n",
      "At: 1478 [==========>] Loss 0.09104435282840433  - accuracy: 0.875\n",
      "At: 1479 [==========>] Loss 0.14610154621756058  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.0725085873315134  - accuracy: 0.9375\n",
      "At: 1481 [==========>] Loss 0.1149432736487878  - accuracy: 0.8125\n",
      "At: 1482 [==========>] Loss 0.11407577715008986  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.18124929496935271  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.13206317910739515  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.13231134868387381  - accuracy: 0.78125\n",
      "At: 1486 [==========>] Loss 0.08930838365968637  - accuracy: 0.9375\n",
      "At: 1487 [==========>] Loss 0.07587799857757374  - accuracy: 0.875\n",
      "At: 1488 [==========>] Loss 0.1255773824262056  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.16346999111387464  - accuracy: 0.75\n",
      "At: 1490 [==========>] Loss 0.11288714216100681  - accuracy: 0.8125\n",
      "At: 1491 [==========>] Loss 0.1531316341047493  - accuracy: 0.84375\n",
      "At: 1492 [==========>] Loss 0.13348728935773266  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.15384950713529355  - accuracy: 0.78125\n",
      "At: 1494 [==========>] Loss 0.13178844699643633  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.12115064007713083  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.09190712712761046  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.16400712188887645  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.1575065381057616  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.10545096112072938  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.10470811532490468  - accuracy: 0.8125\n",
      "At: 1501 [==========>] Loss 0.0856015254699619  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.14590472783914654  - accuracy: 0.78125\n",
      "At: 1503 [==========>] Loss 0.1451241713004505  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.10012906473525587  - accuracy: 0.84375\n",
      "At: 1505 [==========>] Loss 0.12601389778640262  - accuracy: 0.78125\n",
      "At: 1506 [==========>] Loss 0.15742308168593558  - accuracy: 0.78125\n",
      "At: 1507 [==========>] Loss 0.0970126559722108  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.19794644387382673  - accuracy: 0.65625\n",
      "At: 1509 [==========>] Loss 0.09997104977556201  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.14250847912043113  - accuracy: 0.78125\n",
      "At: 1511 [==========>] Loss 0.11763954704406969  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.0547682889554511  - accuracy: 0.9375\n",
      "At: 1513 [==========>] Loss 0.14269075738900194  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.15464291748952452  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.1305603851293552  - accuracy: 0.71875\n",
      "At: 1516 [==========>] Loss 0.13230687451889966  - accuracy: 0.8125\n",
      "At: 1517 [==========>] Loss 0.1233430588821582  - accuracy: 0.875\n",
      "At: 1518 [==========>] Loss 0.08639368183695777  - accuracy: 0.875\n",
      "At: 1519 [==========>] Loss 0.11772613695581624  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.13664999051331453  - accuracy: 0.8125\n",
      "At: 1521 [==========>] Loss 0.08425257987090667  - accuracy: 0.90625\n",
      "At: 1522 [==========>] Loss 0.17842828737673647  - accuracy: 0.78125\n",
      "At: 1523 [==========>] Loss 0.09862565004801294  - accuracy: 0.90625\n",
      "At: 1524 [==========>] Loss 0.1440321967267111  - accuracy: 0.78125\n",
      "At: 1525 [==========>] Loss 0.11182027222731528  - accuracy: 0.875\n",
      "At: 1526 [==========>] Loss 0.0722394504650085  - accuracy: 0.9375\n",
      "At: 1527 [==========>] Loss 0.15468153198400314  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.1386144578704286  - accuracy: 0.84375\n",
      "At: 1529 [==========>] Loss 0.07258208282816311  - accuracy: 0.875\n",
      "At: 1530 [==========>] Loss 0.05780922757923021  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.10199074904315583  - accuracy: 0.875\n",
      "At: 1532 [==========>] Loss 0.19789619082935303  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.1941534076012459  - accuracy: 0.71875\n",
      "At: 1534 [==========>] Loss 0.10133862665381688  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.15241386350787453  - accuracy: 0.78125\n",
      "At: 1536 [==========>] Loss 0.1798879583207249  - accuracy: 0.78125\n",
      "At: 1537 [==========>] Loss 0.10802784152116375  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.12931072100221488  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.07639081782072644  - accuracy: 0.84375\n",
      "At: 1540 [==========>] Loss 0.17252811152172018  - accuracy: 0.75\n",
      "At: 1541 [==========>] Loss 0.11951000991691604  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.08321535941354355  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.10589192722857965  - accuracy: 0.875\n",
      "At: 1544 [==========>] Loss 0.12445274782912692  - accuracy: 0.84375\n",
      "At: 1545 [==========>] Loss 0.18820350025279015  - accuracy: 0.75\n",
      "At: 1546 [==========>] Loss 0.10918370402589583  - accuracy: 0.78125\n",
      "At: 1547 [==========>] Loss 0.1140449492558222  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.13696550850726757  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.16586134071668657  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.06359658567739526  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.17179621746163262  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.10936831202339596  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.0692305315560738  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.10427065603563876  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.1422300721377719  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.15510404642753217  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.1035036548751521  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.13354690277870915  - accuracy: 0.78125\n",
      "At: 1559 [==========>] Loss 0.0942458510673652  - accuracy: 0.875\n",
      "At: 1560 [==========>] Loss 0.13495270655906044  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.1307638064055688  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.07568246828944496  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.10014279922096975  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.0761622790739908  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.12629831300016423  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.13149508002800053  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.11761173117750195  - accuracy: 0.8125\n",
      "At: 1568 [==========>] Loss 0.06197878540179001  - accuracy: 0.90625\n",
      "At: 1569 [==========>] Loss 0.11387657366116881  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.08992408884920516  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.12528910273568158  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.1386920891661688  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.04783868228147474  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.1066648858003867  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.1094597489001516  - accuracy: 0.8125\n",
      "At: 1576 [==========>] Loss 0.12561689807402182  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.06943684077663625  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.08769757104510184  - accuracy: 0.9375\n",
      "At: 1579 [==========>] Loss 0.0975001299708673  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.10025732665812537  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.08651958867717308  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.14362858833008266  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.09604127360972958  - accuracy: 0.90625\n",
      "At: 1584 [==========>] Loss 0.11982052985636106  - accuracy: 0.8125\n",
      "At: 1585 [==========>] Loss 0.1096366781997209  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.1513506388116181  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.09888579352508395  - accuracy: 0.90625\n",
      "At: 1588 [==========>] Loss 0.12599361364677558  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.15585083592290647  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.133879072616999  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.0906026855936574  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.05714330419151976  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.1496633417653438  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.06823390147249205  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.14186048183939653  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.17926965859213218  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.13630143246012924  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.1624713834530767  - accuracy: 0.8125\n",
      "At: 1599 [==========>] Loss 0.2447475816775445  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.14984774674541035  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.06537289060838733  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.12822526048084448  - accuracy: 0.84375\n",
      "At: 1603 [==========>] Loss 0.16504157905829936  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.2212098506518021  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.07209502321220544  - accuracy: 0.9375\n",
      "At: 1606 [==========>] Loss 0.14044722084040134  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.1753137736609286  - accuracy: 0.78125\n",
      "At: 1608 [==========>] Loss 0.1461976021417893  - accuracy: 0.8125\n",
      "At: 1609 [==========>] Loss 0.15598534581557624  - accuracy: 0.75\n",
      "At: 1610 [==========>] Loss 0.15823361057324403  - accuracy: 0.8125\n",
      "At: 1611 [==========>] Loss 0.12857151438942788  - accuracy: 0.8125\n",
      "At: 1612 [==========>] Loss 0.07843092477486655  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.14192564835501814  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.15105004435739067  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.08350809117531441  - accuracy: 0.84375\n",
      "At: 1616 [==========>] Loss 0.12467564697917513  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.07782690828640829  - accuracy: 0.90625\n",
      "At: 1618 [==========>] Loss 0.11270216328281851  - accuracy: 0.8125\n",
      "At: 1619 [==========>] Loss 0.1714525056954319  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.10612495406300568  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.11220205210473717  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.15335360309196988  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.08396972134439734  - accuracy: 0.90625\n",
      "At: 1624 [==========>] Loss 0.14984742375883103  - accuracy: 0.6875\n",
      "At: 1625 [==========>] Loss 0.12311190557546339  - accuracy: 0.84375\n",
      "At: 1626 [==========>] Loss 0.10719472130631938  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.08025637878154812  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.16867055306853657  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.11991644302296603  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.08973335848585112  - accuracy: 0.9375\n",
      "At: 1631 [==========>] Loss 0.10917252998150692  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.10135524843759355  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.07550451547184  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.07416283794282871  - accuracy: 0.875\n",
      "At: 1635 [==========>] Loss 0.21222123329504566  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.11562020948704069  - accuracy: 0.8125\n",
      "At: 1637 [==========>] Loss 0.06927248608425911  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.1648640090317505  - accuracy: 0.75\n",
      "At: 1639 [==========>] Loss 0.1576793182567159  - accuracy: 0.71875\n",
      "At: 1640 [==========>] Loss 0.12341281288812227  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.08815028854869032  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.1147115079887695  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.10348730840133372  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.11749540090343305  - accuracy: 0.8125\n",
      "At: 1645 [==========>] Loss 0.07432235580238872  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.11996377829343571  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.17733560966034262  - accuracy: 0.75\n",
      "At: 1648 [==========>] Loss 0.12924534410543986  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.09365451515255849  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.08392503562403242  - accuracy: 0.90625\n",
      "At: 1651 [==========>] Loss 0.11859886277125423  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.07524118025528174  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.07309950144703109  - accuracy: 0.9375\n",
      "At: 1654 [==========>] Loss 0.046804464896290005  - accuracy: 1.0\n",
      "At: 1655 [==========>] Loss 0.15313896114285638  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.134037379698847  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.1325140663779806  - accuracy: 0.8125\n",
      "At: 1658 [==========>] Loss 0.18415872205882  - accuracy: 0.71875\n",
      "At: 1659 [==========>] Loss 0.0890732447291076  - accuracy: 0.90625\n",
      "At: 1660 [==========>] Loss 0.036480884775557984  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.07299791863962078  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.09531137271825002  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.1704557641738856  - accuracy: 0.78125\n",
      "At: 1664 [==========>] Loss 0.11706901883615145  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.10760657877542876  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.09439015366432219  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.1689759466118252  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.15391590684602152  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.11351159103306288  - accuracy: 0.84375\n",
      "At: 1670 [==========>] Loss 0.07209084292371606  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.1364744386615022  - accuracy: 0.84375\n",
      "At: 1672 [==========>] Loss 0.1492414942040487  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.07258560973892669  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.15049505260057106  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.1561079264220064  - accuracy: 0.8125\n",
      "At: 1676 [==========>] Loss 0.18480374996436666  - accuracy: 0.75\n",
      "At: 1677 [==========>] Loss 0.09528500328790622  - accuracy: 0.90625\n",
      "At: 1678 [==========>] Loss 0.18057705827037623  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.10389038002023154  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.14367417245288192  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.1303003898724866  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.0877231806425352  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.17120289554749418  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.09557711335036467  - accuracy: 0.90625\n",
      "At: 1685 [==========>] Loss 0.12092741550952392  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.13582752067451395  - accuracy: 0.8125\n",
      "At: 1687 [==========>] Loss 0.20604442589058597  - accuracy: 0.6875\n",
      "At: 1688 [==========>] Loss 0.04069551111121961  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.1415403459921571  - accuracy: 0.78125\n",
      "At: 1690 [==========>] Loss 0.11821365912517619  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.09421341692837727  - accuracy: 0.90625\n",
      "At: 1692 [==========>] Loss 0.15288450249097904  - accuracy: 0.8125\n",
      "At: 1693 [==========>] Loss 0.10080423669430491  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.10907662250967715  - accuracy: 0.84375\n",
      "At: 1695 [==========>] Loss 0.09496440252563096  - accuracy: 0.9375\n",
      "At: 1696 [==========>] Loss 0.16072807366047645  - accuracy: 0.78125\n",
      "At: 1697 [==========>] Loss 0.11839319089006159  - accuracy: 0.8125\n",
      "At: 1698 [==========>] Loss 0.08258363113033772  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.135401496667521  - accuracy: 0.78125\n",
      "At: 1700 [==========>] Loss 0.11885042174282598  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.09880297704795349  - accuracy: 0.84375\n",
      "At: 1702 [==========>] Loss 0.08089207478561947  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.17679868477857608  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.05842419616546984  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.09850914324354583  - accuracy: 0.875\n",
      "At: 1706 [==========>] Loss 0.1505708585914152  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.15897546214134534  - accuracy: 0.84375\n",
      "At: 1708 [==========>] Loss 0.0895521278612382  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.17167372334066486  - accuracy: 0.6875\n",
      "At: 1710 [==========>] Loss 0.15645879736481696  - accuracy: 0.78125\n",
      "At: 1711 [==========>] Loss 0.09789206054416041  - accuracy: 0.875\n",
      "At: 1712 [==========>] Loss 0.10365686446118368  - accuracy: 0.90625\n",
      "At: 1713 [==========>] Loss 0.08000306102052782  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.1715469183578831  - accuracy: 0.78125\n",
      "At: 1715 [==========>] Loss 0.10923943058799221  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06902964245275542  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.07085097847148013  - accuracy: 0.9375\n",
      "At: 1718 [==========>] Loss 0.11735287519643411  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.09906184180459891  - accuracy: 0.90625\n",
      "At: 1720 [==========>] Loss 0.08194869884177439  - accuracy: 0.875\n",
      "At: 1721 [==========>] Loss 0.1463764752288114  - accuracy: 0.8125\n",
      "At: 1722 [==========>] Loss 0.06247531450711534  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.1743563717770386  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.07032636673004049  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.14110335701619464  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.10772693444401038  - accuracy: 0.90625\n",
      "At: 1727 [==========>] Loss 0.12640345440507222  - accuracy: 0.78125\n",
      "At: 1728 [==========>] Loss 0.09331883980041122  - accuracy: 0.90625\n",
      "At: 1729 [==========>] Loss 0.17869795005424943  - accuracy: 0.78125\n",
      "At: 1730 [==========>] Loss 0.15047688151037847  - accuracy: 0.78125\n",
      "At: 1731 [==========>] Loss 0.08308134623098763  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.06553063365858211  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.14423461574761967  - accuracy: 0.84375\n",
      "At: 1734 [==========>] Loss 0.11503621881317166  - accuracy: 0.78125\n",
      "At: 1735 [==========>] Loss 0.1457852333532416  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.11129191104963394  - accuracy: 0.875\n",
      "At: 1737 [==========>] Loss 0.14804048359039318  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.11407161041588666  - accuracy: 0.84375\n",
      "At: 1739 [==========>] Loss 0.12280920573030935  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.09605759630324887  - accuracy: 0.8125\n",
      "At: 1741 [==========>] Loss 0.1262606043304937  - accuracy: 0.84375\n",
      "At: 1742 [==========>] Loss 0.036592250026428894  - accuracy: 1.0\n",
      "At: 1743 [==========>] Loss 0.1407230286357939  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.09975272421093934  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.10651604588953845  - accuracy: 0.84375\n",
      "At: 1746 [==========>] Loss 0.14892937289583535  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.14202714229161684  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.10964058134288257  - accuracy: 0.90625\n",
      "At: 1749 [==========>] Loss 0.10166822337529992  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.10561980412258143  - accuracy: 0.90625\n",
      "At: 1751 [==========>] Loss 0.17635408411035214  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.08644048092535186  - accuracy: 0.84375\n",
      "At: 1753 [==========>] Loss 0.08019158199389194  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.10730536284520761  - accuracy: 0.875\n",
      "At: 1755 [==========>] Loss 0.075468806690786  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.1417930450645279  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.1191359582183925  - accuracy: 0.875\n",
      "At: 1758 [==========>] Loss 0.06180017141646529  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.07629226987849608  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.07683436892305373  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.12263629842716557  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.15499454476337804  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.1117031204131538  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.09676777532856984  - accuracy: 0.875\n",
      "At: 1765 [==========>] Loss 0.12314734209945045  - accuracy: 0.8125\n",
      "At: 1766 [==========>] Loss 0.07383186130298378  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.08537218276953668  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.09269888364162399  - accuracy: 0.90625\n",
      "At: 1769 [==========>] Loss 0.0776601674534096  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.06107114683556168  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.1684329371858182  - accuracy: 0.71875\n",
      "At: 1772 [==========>] Loss 0.1290814395814488  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.09951065428679862  - accuracy: 0.90625\n",
      "At: 1774 [==========>] Loss 0.14932636740772684  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.10733724577773213  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.09953379534735086  - accuracy: 0.9375\n",
      "At: 1777 [==========>] Loss 0.11175925636568604  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.1031960867609405  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.07946196648161763  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.09217228379778156  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.2334491803143356  - accuracy: 0.65625\n",
      "At: 1782 [==========>] Loss 0.0792308378270917  - accuracy: 0.90625\n",
      "At: 1783 [==========>] Loss 0.1622859100019278  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.06384160792624455  - accuracy: 0.96875\n",
      "At: 1785 [==========>] Loss 0.07850512724606437  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.11646387694992091  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.15764868445004235  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.08381108758070661  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.10603116163306672  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.18744314389120775  - accuracy: 0.6875\n",
      "At: 1791 [==========>] Loss 0.07870605930026882  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.11874184383077911  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.10438400335532265  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.14618969142796392  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.07306123744473078  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.12847434791898443  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.09961979973445817  - accuracy: 0.90625\n",
      "At: 1798 [==========>] Loss 0.09664832918550986  - accuracy: 0.875\n",
      "At: 1799 [==========>] Loss 0.05100540669101771  - accuracy: 0.96875\n",
      "At: 1800 [==========>] Loss 0.12092586685741177  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.1737376394760664  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.13489799551445678  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.18061790697136204  - accuracy: 0.75\n",
      "At: 1804 [==========>] Loss 0.13589811689889142  - accuracy: 0.75\n",
      "At: 1805 [==========>] Loss 0.0320479990116029  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.16177859665016478  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.17183121129439857  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.15893581230482562  - accuracy: 0.84375\n",
      "At: 1809 [==========>] Loss 0.08025971459966158  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.13879992527391974  - accuracy: 0.71875\n",
      "At: 1811 [==========>] Loss 0.11722159763514718  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.10349615488175848  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.11867368638512231  - accuracy: 0.875\n",
      "At: 1814 [==========>] Loss 0.10323190302014107  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.17819518904287351  - accuracy: 0.71875\n",
      "At: 1816 [==========>] Loss 0.04036913960836416  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.13015075696957484  - accuracy: 0.84375\n",
      "At: 1818 [==========>] Loss 0.13864240306304476  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.17597304766984856  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.10790136581745917  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.10671728611996878  - accuracy: 0.8125\n",
      "At: 1822 [==========>] Loss 0.1318879672687464  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.16756354954594416  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.16310256610393442  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.11982694454059264  - accuracy: 0.875\n",
      "At: 1826 [==========>] Loss 0.06353227837875303  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11461864247532011  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.1347290698491716  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.1607255961735822  - accuracy: 0.75\n",
      "At: 1830 [==========>] Loss 0.1523938234078108  - accuracy: 0.75\n",
      "At: 1831 [==========>] Loss 0.13618701753803478  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.13575414656396081  - accuracy: 0.78125\n",
      "At: 1833 [==========>] Loss 0.09226218963246692  - accuracy: 0.9375\n",
      "At: 1834 [==========>] Loss 0.0904599592184064  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.14264037710979138  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.11522180288979322  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.04713952236142646  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.11167114563169982  - accuracy: 0.75\n",
      "At: 1839 [==========>] Loss 0.11604818189175023  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.14884608605182248  - accuracy: 0.78125\n",
      "At: 1841 [==========>] Loss 0.07398626695130836  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.11896064406363038  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.11498947562892187  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.08888185740045795  - accuracy: 0.90625\n",
      "At: 1845 [==========>] Loss 0.13962898466828416  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.12879489611518052  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.052802712609348934  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.0468668337842993  - accuracy: 0.96875\n",
      "At: 1849 [==========>] Loss 0.20060622623890004  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.03532261776854964  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.1595874996447353  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.07301338016951772  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.13584627777974587  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.13616602469830355  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.17651511598928765  - accuracy: 0.8125\n",
      "At: 1856 [==========>] Loss 0.12642017839788772  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.13831623427271178  - accuracy: 0.84375\n",
      "At: 1858 [==========>] Loss 0.10097873476791895  - accuracy: 0.875\n",
      "At: 1859 [==========>] Loss 0.15412467737948288  - accuracy: 0.75\n",
      "At: 1860 [==========>] Loss 0.13244550924099124  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.08958173014932763  - accuracy: 0.90625\n",
      "At: 1862 [==========>] Loss 0.19909273951937634  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.1169987553618098  - accuracy: 0.875\n",
      "At: 1864 [==========>] Loss 0.15101389992890685  - accuracy: 0.71875\n",
      "At: 1865 [==========>] Loss 0.09113757517118748  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.21964994121758696  - accuracy: 0.6875\n",
      "At: 1867 [==========>] Loss 0.11438218294561316  - accuracy: 0.84375\n",
      "At: 1868 [==========>] Loss 0.13974482589035767  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.17727613939645642  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.09385109431274542  - accuracy: 0.90625\n",
      "At: 1871 [==========>] Loss 0.12630201465519963  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.15699521641218084  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.10549496776907388  - accuracy: 0.8125\n",
      "At: 1874 [==========>] Loss 0.1338799611316756  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.1086719511425392  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.2150522100969156  - accuracy: 0.625\n",
      "At: 1877 [==========>] Loss 0.10296187792231237  - accuracy: 0.875\n",
      "At: 1878 [==========>] Loss 0.09817673748993046  - accuracy: 0.90625\n",
      "At: 1879 [==========>] Loss 0.12544923536779135  - accuracy: 0.8125\n",
      "At: 1880 [==========>] Loss 0.09333206533495059  - accuracy: 0.84375\n",
      "At: 1881 [==========>] Loss 0.09315757846128254  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.11898146442953797  - accuracy: 0.8125\n",
      "At: 1883 [==========>] Loss 0.1325480823937496  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.08474136658200734  - accuracy: 0.90625\n",
      "At: 1885 [==========>] Loss 0.08084404416055156  - accuracy: 0.90625\n",
      "At: 1886 [==========>] Loss 0.13970547565509905  - accuracy: 0.75\n",
      "At: 1887 [==========>] Loss 0.0923375019026692  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.12659616490436656  - accuracy: 0.84375\n",
      "At: 1889 [==========>] Loss 0.08033512271667187  - accuracy: 0.875\n",
      "At: 1890 [==========>] Loss 0.17221746572896895  - accuracy: 0.84375\n",
      "At: 1891 [==========>] Loss 0.07476570973251255  - accuracy: 0.875\n",
      "At: 1892 [==========>] Loss 0.08039490443471248  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.0699955662247294  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.07603877063589465  - accuracy: 0.9375\n",
      "At: 1895 [==========>] Loss 0.059957752797933785  - accuracy: 0.9375\n",
      "At: 1896 [==========>] Loss 0.12761602985801251  - accuracy: 0.8125\n",
      "At: 1897 [==========>] Loss 0.06883741678132697  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.09953563672516816  - accuracy: 0.90625\n",
      "At: 1899 [==========>] Loss 0.10657169312758301  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.1125044635379186  - accuracy: 0.8125\n",
      "At: 1901 [==========>] Loss 0.10363498488195932  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.15333994532716894  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.13042555116143442  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.05943146684703181  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.15447294565540082  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.08054158463803891  - accuracy: 0.875\n",
      "At: 1907 [==========>] Loss 0.06079036667019466  - accuracy: 0.9375\n",
      "At: 1908 [==========>] Loss 0.09165458192818661  - accuracy: 0.8125\n",
      "At: 1909 [==========>] Loss 0.09845660927100824  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.046629263780874494  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.1547696375599555  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.11367546611643578  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.16439452522133763  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.09650814940245792  - accuracy: 0.875\n",
      "At: 1915 [==========>] Loss 0.11726738077074787  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.15475038864056156  - accuracy: 0.78125\n",
      "At: 1917 [==========>] Loss 0.16643183970634418  - accuracy: 0.8125\n",
      "At: 1918 [==========>] Loss 0.16122014556095393  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.08121279004733706  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10520631764138702  - accuracy: 0.8125\n",
      "At: 1921 [==========>] Loss 0.12756712297442133  - accuracy: 0.84375\n",
      "At: 1922 [==========>] Loss 0.13103364436642453  - accuracy: 0.8125\n",
      "At: 1923 [==========>] Loss 0.16804902453913795  - accuracy: 0.75\n",
      "At: 1924 [==========>] Loss 0.15077545482663235  - accuracy: 0.75\n",
      "At: 1925 [==========>] Loss 0.16486991942830587  - accuracy: 0.75\n",
      "At: 1926 [==========>] Loss 0.08016184461755006  - accuracy: 0.96875\n",
      "At: 1927 [==========>] Loss 0.08114509863445957  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12609435292432106  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.18328160240810026  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.17300590313524492  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.12404101429374056  - accuracy: 0.78125\n",
      "At: 1932 [==========>] Loss 0.16286568851287914  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.07814337886058982  - accuracy: 0.875\n",
      "At: 1934 [==========>] Loss 0.148972122625359  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.13545609947306042  - accuracy: 0.84375\n",
      "At: 1936 [==========>] Loss 0.09816102686965869  - accuracy: 0.8125\n",
      "At: 1937 [==========>] Loss 0.13723486232912502  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.1718078542383012  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.09791226408615164  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.11471050463294381  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.1029404652381972  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.16392161856010534  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.12897648746192059  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.09408090911552296  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.17006972560425157  - accuracy: 0.78125\n",
      "At: 1946 [==========>] Loss 0.09276782776322803  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.09569857594473939  - accuracy: 0.875\n",
      "At: 1948 [==========>] Loss 0.11990950559083559  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.07157254831270438  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.14373855118891343  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.12271190480272523  - accuracy: 0.84375\n",
      "At: 1952 [==========>] Loss 0.0710846399087155  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.05298517641079285  - accuracy: 0.96875\n",
      "At: 1954 [==========>] Loss 0.18553540698375826  - accuracy: 0.78125\n",
      "At: 1955 [==========>] Loss 0.0457247835720166  - accuracy: 0.96875\n",
      "At: 1956 [==========>] Loss 0.11609942438283517  - accuracy: 0.8125\n",
      "At: 1957 [==========>] Loss 0.08052364084695152  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.09139548824809271  - accuracy: 0.84375\n",
      "At: 1959 [==========>] Loss 0.14107375141584005  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.07176253261639837  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.19369576019514628  - accuracy: 0.6875\n",
      "At: 1962 [==========>] Loss 0.20814364006267277  - accuracy: 0.6875\n",
      "At: 1963 [==========>] Loss 0.06023351511018768  - accuracy: 0.96875\n",
      "At: 1964 [==========>] Loss 0.1673186713833626  - accuracy: 0.71875\n",
      "At: 1965 [==========>] Loss 0.14094190940332313  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.1268934042680383  - accuracy: 0.78125\n",
      "At: 1967 [==========>] Loss 0.13427712388672072  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.18505159597672602  - accuracy: 0.75\n",
      "At: 1969 [==========>] Loss 0.16081300903160922  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.10897110497820972  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.1863508142771561  - accuracy: 0.75\n",
      "At: 1972 [==========>] Loss 0.0749691338000071  - accuracy: 0.9375\n",
      "At: 1973 [==========>] Loss 0.12503991167808265  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.1221809953234808  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.151028646671598  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.06124201211037476  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.09670460970687883  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.10640326002964645  - accuracy: 0.875\n",
      "At: 1979 [==========>] Loss 0.12303784848072877  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.12803631348010783  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.1332954673622934  - accuracy: 0.8125\n",
      "At: 1982 [==========>] Loss 0.06673294847254801  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.12039381205822026  - accuracy: 0.84375\n",
      "At: 1984 [==========>] Loss 0.09385544986618852  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.1399028052553176  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.1815540854848688  - accuracy: 0.71875\n",
      "At: 1987 [==========>] Loss 0.14567724605918214  - accuracy: 0.8125\n",
      "At: 1988 [==========>] Loss 0.10598246203112521  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.09168162599896006  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.08483732159772533  - accuracy: 0.9375\n",
      "At: 1991 [==========>] Loss 0.11861689411393786  - accuracy: 0.84375\n",
      "At: 1992 [==========>] Loss 0.11693746091505279  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.16541951370998048  - accuracy: 0.6875\n",
      "At: 1994 [==========>] Loss 0.11738427983000596  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.1819629223344671  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.08110340505830033  - accuracy: 0.875\n",
      "At: 1997 [==========>] Loss 0.17934765886589657  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.14985726081384648  - accuracy: 0.78125\n",
      "At: 1999 [==========>] Loss 0.08465877677725364  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.15698657849065167  - accuracy: 0.75\n",
      "At: 2001 [==========>] Loss 0.07158942971595297  - accuracy: 0.90625\n",
      "At: 2002 [==========>] Loss 0.10923371151228241  - accuracy: 0.8125\n",
      "At: 2003 [==========>] Loss 0.1336118428978969  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.15377855933824555  - accuracy: 0.8125\n",
      "At: 2005 [==========>] Loss 0.1322442014104549  - accuracy: 0.78125\n",
      "At: 2006 [==========>] Loss 0.11819478298510411  - accuracy: 0.84375\n",
      "At: 2007 [==========>] Loss 0.0956951986061522  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.13182457307794337  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.12774126174270256  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.09286656140427976  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.12397070224759428  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.10876822150741325  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.08759539358665036  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.2084819456944197  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.060454804184194404  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.1137745299361781  - accuracy: 0.875\n",
      "At: 2017 [==========>] Loss 0.0754712911791276  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.07908803493541468  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.13192702690226463  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.06976041009608484  - accuracy: 0.9375\n",
      "At: 2021 [==========>] Loss 0.10572047346843878  - accuracy: 0.84375\n",
      "At: 2022 [==========>] Loss 0.11695887456479181  - accuracy: 0.78125\n",
      "At: 2023 [==========>] Loss 0.09395958065410538  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.07983550097166628  - accuracy: 0.90625\n",
      "At: 2025 [==========>] Loss 0.16065512736336707  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.09096673079175722  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.13023846073473166  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.1268672087783796  - accuracy: 0.8125\n",
      "At: 2029 [==========>] Loss 0.11217176391607814  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.15409896780205284  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.1629281489333991  - accuracy: 0.71875\n",
      "At: 2032 [==========>] Loss 0.16042657411449882  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.17103015817146944  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.2415736328062255  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.11276644020602503  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09170222249879818  - accuracy: 0.84375\n",
      "At: 2037 [==========>] Loss 0.11853120817087971  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.1298563817134028  - accuracy: 0.78125\n",
      "At: 2039 [==========>] Loss 0.09446708862097916  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.09186021998972116  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.06099957118833698  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.13025923945478812  - accuracy: 0.8125\n",
      "At: 2043 [==========>] Loss 0.09670102430711516  - accuracy: 0.84375\n",
      "At: 2044 [==========>] Loss 0.06881700784945721  - accuracy: 0.9375\n",
      "At: 2045 [==========>] Loss 0.21577688540029843  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.07883109773278113  - accuracy: 0.875\n",
      "At: 2047 [==========>] Loss 0.0829153067507136  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.10156417335084467  - accuracy: 0.90625\n",
      "At: 2049 [==========>] Loss 0.14900503494823586  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.16803451975452038  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1868634006471172  - accuracy: 0.71875\n",
      "At: 2052 [==========>] Loss 0.050129680601522494  - accuracy: 0.96875\n",
      "At: 2053 [==========>] Loss 0.12423835816613728  - accuracy: 0.78125\n",
      "At: 2054 [==========>] Loss 0.12364995533370363  - accuracy: 0.75\n",
      "At: 2055 [==========>] Loss 0.07581059847354982  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.11552387622819695  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.13274419083740507  - accuracy: 0.875\n",
      "At: 2058 [==========>] Loss 0.1352897935859847  - accuracy: 0.78125\n",
      "At: 2059 [==========>] Loss 0.22351783439246956  - accuracy: 0.65625\n",
      "At: 2060 [==========>] Loss 0.11382929337418574  - accuracy: 0.90625\n",
      "At: 2061 [==========>] Loss 0.1711087270278328  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.12981999442849393  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.10870854547324536  - accuracy: 0.84375\n",
      "At: 2064 [==========>] Loss 0.1963754526042094  - accuracy: 0.71875\n",
      "At: 2065 [==========>] Loss 0.05386861050708053  - accuracy: 0.90625\n",
      "At: 2066 [==========>] Loss 0.20089082397538466  - accuracy: 0.71875\n",
      "At: 2067 [==========>] Loss 0.10575303870907421  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.12090911672531963  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.10719402705525452  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.11458446686845483  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.13315472633254966  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.07767576465615483  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.09871960400712229  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.09398684586275897  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.11040482088740294  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.15421826995930357  - accuracy: 0.75\n",
      "At: 2077 [==========>] Loss 0.1409598452760794  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.09035005762586636  - accuracy: 0.90625\n",
      "At: 2079 [==========>] Loss 0.08096702541373856  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.11936184470760627  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.13443238709142522  - accuracy: 0.75\n",
      "At: 2082 [==========>] Loss 0.10572069404631043  - accuracy: 0.875\n",
      "At: 2083 [==========>] Loss 0.1810966335581451  - accuracy: 0.6875\n",
      "At: 2084 [==========>] Loss 0.08020370859122575  - accuracy: 0.9375\n",
      "At: 2085 [==========>] Loss 0.09522176783073677  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.09431510608139486  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.17072657552477188  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.10081847882034072  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.11480047127722336  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.11373782313607994  - accuracy: 0.8125\n",
      "At: 2091 [==========>] Loss 0.13664368357319662  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.09978610771461788  - accuracy: 0.875\n",
      "At: 2093 [==========>] Loss 0.1399986888238414  - accuracy: 0.78125\n",
      "At: 2094 [==========>] Loss 0.12854687098702647  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.09870758171433949  - accuracy: 0.9375\n",
      "At: 2096 [==========>] Loss 0.14501110678121096  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.11967763291117925  - accuracy: 0.875\n",
      "At: 2098 [==========>] Loss 0.1222582058274275  - accuracy: 0.84375\n",
      "At: 2099 [==========>] Loss 0.10614052939416982  - accuracy: 0.875\n",
      "At: 2100 [==========>] Loss 0.06907321450514536  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.15716624124446793  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.08941261699488286  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.14954860063819422  - accuracy: 0.8125\n",
      "At: 2104 [==========>] Loss 0.12334504761727311  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.16387973641414208  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.139412025007224  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.08298362713796453  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.1422126759727798  - accuracy: 0.78125\n",
      "At: 2109 [==========>] Loss 0.1065003467117942  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.05396735265871394  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.09499290358314105  - accuracy: 0.875\n",
      "At: 2112 [==========>] Loss 0.1191456157314157  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.08240920722119541  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.14604464383387478  - accuracy: 0.78125\n",
      "At: 2115 [==========>] Loss 0.11532364006742911  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.09391033706076762  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.11353762804431515  - accuracy: 0.75\n",
      "At: 2118 [==========>] Loss 0.13847279972716267  - accuracy: 0.78125\n",
      "At: 2119 [==========>] Loss 0.07521080121808307  - accuracy: 0.84375\n",
      "At: 2120 [==========>] Loss 0.14165503657128897  - accuracy: 0.875\n",
      "At: 2121 [==========>] Loss 0.17264507097153828  - accuracy: 0.78125\n",
      "At: 2122 [==========>] Loss 0.11458727670753072  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.1711060786850153  - accuracy: 0.78125\n",
      "At: 2124 [==========>] Loss 0.13224402151426162  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.0870300481408944  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.03569932073332992  - accuracy: 1.0\n",
      "At: 2127 [==========>] Loss 0.107444741200181  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.11187454964563621  - accuracy: 0.78125\n",
      "At: 2129 [==========>] Loss 0.13603350431547742  - accuracy: 0.78125\n",
      "At: 2130 [==========>] Loss 0.04453964090188475  - accuracy: 0.96875\n",
      "At: 2131 [==========>] Loss 0.09639486374427676  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.19486911070465907  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.15836191758073123  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.11904980503338863  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.08125588201386849  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.13380252395756342  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.11605754213827454  - accuracy: 0.75\n",
      "At: 2138 [==========>] Loss 0.11334755518775273  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.19327582226466977  - accuracy: 0.71875\n",
      "At: 2140 [==========>] Loss 0.08458521109130893  - accuracy: 0.9375\n",
      "At: 2141 [==========>] Loss 0.12193185841004636  - accuracy: 0.84375\n",
      "At: 2142 [==========>] Loss 0.11025850886754959  - accuracy: 0.875\n",
      "At: 2143 [==========>] Loss 0.07748519983357648  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.07211289250457248  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.12605861961396786  - accuracy: 0.78125\n",
      "At: 2146 [==========>] Loss 0.15020622217892599  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.13826360687048606  - accuracy: 0.75\n",
      "At: 2148 [==========>] Loss 0.20813141686094416  - accuracy: 0.71875\n",
      "At: 2149 [==========>] Loss 0.12243205486598907  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.07812049466926901  - accuracy: 0.9375\n",
      "At: 2151 [==========>] Loss 0.08791113144579737  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.2176389163193201  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.17865746910811503  - accuracy: 0.71875\n",
      "At: 2154 [==========>] Loss 0.15989847079637703  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.1285479866331074  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.09954878056564795  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.1558881395942095  - accuracy: 0.78125\n",
      "At: 2158 [==========>] Loss 0.11276713901938235  - accuracy: 0.875\n",
      "At: 2159 [==========>] Loss 0.06932762262929616  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.15064901379720108  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.0933256708786127  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.0956965942753939  - accuracy: 0.8125\n",
      "At: 2163 [==========>] Loss 0.12506666285318896  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.15722599342922244  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.07314635067946304  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.10230568115152147  - accuracy: 0.875\n",
      "At: 2167 [==========>] Loss 0.10786294667777002  - accuracy: 0.875\n",
      "At: 2168 [==========>] Loss 0.08609072430376838  - accuracy: 0.875\n",
      "At: 2169 [==========>] Loss 0.1280703622642527  - accuracy: 0.84375\n",
      "At: 2170 [==========>] Loss 0.12921319096042555  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.16442204502337374  - accuracy: 0.6875\n",
      "At: 2172 [==========>] Loss 0.09225566540202576  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.14616779204228442  - accuracy: 0.8125\n",
      "At: 2174 [==========>] Loss 0.11584201856014262  - accuracy: 0.875\n",
      "At: 2175 [==========>] Loss 0.12656263560879627  - accuracy: 0.8125\n",
      "At: 2176 [==========>] Loss 0.12103548835295938  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.12472750331573054  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.09539622851444077  - accuracy: 0.84375\n",
      "At: 2179 [==========>] Loss 0.14006805464019462  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.11148674167879169  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.1710558868026889  - accuracy: 0.75\n",
      "At: 2182 [==========>] Loss 0.10528398359227795  - accuracy: 0.875\n",
      "At: 2183 [==========>] Loss 0.21055396396804754  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.09427598098080447  - accuracy: 0.875\n",
      "At: 2185 [==========>] Loss 0.1220332277576111  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.1318718650352037  - accuracy: 0.84375\n",
      "At: 2187 [==========>] Loss 0.17907417190375804  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.07707287479780645  - accuracy: 0.9375\n",
      "At: 2189 [==========>] Loss 0.05873542910476462  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.1164943279732318  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.10229962927446234  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.11250280385093758  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.12090904307267308  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.09768854058642114  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.17593620890543868  - accuracy: 0.71875\n",
      "At: 2196 [==========>] Loss 0.12901945561666178  - accuracy: 0.84375\n",
      "At: 2197 [==========>] Loss 0.0971408064260236  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.1059312898885711  - accuracy: 0.8125\n",
      "At: 2199 [==========>] Loss 0.04576057027286925  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.051128500409224495  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.13284362849619286  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.06682843926139768  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.09750439758132426  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.10399348946798456  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.11457750828731256  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.08839088797998937  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.08656714789037918  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.14651270284711135  - accuracy: 0.84375\n",
      "At: 2209 [==========>] Loss 0.18284882271507147  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.1105056309928497  - accuracy: 0.875\n",
      "At: 2211 [==========>] Loss 0.14768500451164504  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.08701518591888296  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.11409639088215814  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.15194199452636073  - accuracy: 0.75\n",
      "At: 2215 [==========>] Loss 0.11736912263007983  - accuracy: 0.90625\n",
      "At: 2216 [==========>] Loss 0.1494291585409658  - accuracy: 0.8125\n",
      "At: 2217 [==========>] Loss 0.1181830920439473  - accuracy: 0.84375\n",
      "At: 2218 [==========>] Loss 0.13911639179051738  - accuracy: 0.875\n",
      "At: 2219 [==========>] Loss 0.07286810257139949  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.131168976609856  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.1664602280617293  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.09420667231397419  - accuracy: 0.875\n",
      "At: 2223 [==========>] Loss 0.12784909967349434  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.1450031068197957  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.13344385244730156  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.144015490862203  - accuracy: 0.8125\n",
      "At: 2227 [==========>] Loss 0.19888760228336064  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.08376821636153517  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.14437324724062023  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.13577839449943307  - accuracy: 0.78125\n",
      "At: 2231 [==========>] Loss 0.14046614013298447  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.1617076892586418  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.14701585215976606  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.11706586454000914  - accuracy: 0.84375\n",
      "At: 2235 [==========>] Loss 0.12977477356036332  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.0875182395535458  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.15312906376979188  - accuracy: 0.78125\n",
      "At: 2238 [==========>] Loss 0.12942630299884533  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.1838591011018282  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.13929897304754774  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.12489030925083039  - accuracy: 0.84375\n",
      "At: 2242 [==========>] Loss 0.1662070073608049  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.07505824324385887  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.08822148383414488  - accuracy: 0.90625\n",
      "At: 2245 [==========>] Loss 0.08014561787769188  - accuracy: 0.8125\n",
      "At: 2246 [==========>] Loss 0.13912803253948808  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.1207055043611849  - accuracy: 0.78125\n",
      "At: 2248 [==========>] Loss 0.16800082128220728  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.09519571483507518  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.08230215237737427  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.07193140911787513  - accuracy: 0.90625\n",
      "At: 2252 [==========>] Loss 0.12342309545829469  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.13450070012780302  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.1050162921687767  - accuracy: 0.875\n",
      "At: 2255 [==========>] Loss 0.13335471709892383  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.1449528769051633  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.11093830581920103  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.13168941493105124  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.14504302799738575  - accuracy: 0.75\n",
      "At: 2260 [==========>] Loss 0.17821657363336726  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.09181233045748566  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.1468922764446401  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.1301813578900563  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.08202861772664971  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.1429276430194243  - accuracy: 0.75\n",
      "At: 2266 [==========>] Loss 0.1277471072948328  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.05193605735942426  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.10205535148928882  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.03462988591382618  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.11531050798919437  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.1175888404173215  - accuracy: 0.8125\n",
      "At: 2272 [==========>] Loss 0.0887656595691875  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.10265794492903792  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.07813393319742801  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.10234063744929328  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.09687629600631731  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.13435763415795535  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.09581398567862184  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.134844830986956  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.15178335255252529  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.10021146952871718  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.06249380759453961  - accuracy: 0.96875\n",
      "At: 2283 [==========>] Loss 0.16795585967503496  - accuracy: 0.71875\n",
      "At: 2284 [==========>] Loss 0.10578619434795408  - accuracy: 0.84375\n",
      "At: 2285 [==========>] Loss 0.13331758565149596  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.11876603467617941  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.11536924398420906  - accuracy: 0.84375\n",
      "At: 2288 [==========>] Loss 0.08789936329011008  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.15206547115244706  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.0735324047189167  - accuracy: 0.90625\n",
      "At: 2291 [==========>] Loss 0.13516377251759426  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.08545822806646672  - accuracy: 0.875\n",
      "At: 2293 [==========>] Loss 0.048716070823613525  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.05725748334240207  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.12834896246105323  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.1591365651671543  - accuracy: 0.8125\n",
      "At: 2297 [==========>] Loss 0.08869093629520483  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.09004143027801964  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.10982955470395175  - accuracy: 0.875\n",
      "At: 2300 [==========>] Loss 0.09880648554451751  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.20001234928944084  - accuracy: 0.65625\n",
      "At: 2302 [==========>] Loss 0.18258590724496992  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.039011520342750544  - accuracy: 0.96875\n",
      "At: 2304 [==========>] Loss 0.08890616612299271  - accuracy: 0.9375\n",
      "At: 2305 [==========>] Loss 0.07105332529117143  - accuracy: 0.90625\n",
      "At: 2306 [==========>] Loss 0.14114807138391627  - accuracy: 0.78125\n",
      "At: 2307 [==========>] Loss 0.1696804523953278  - accuracy: 0.71875\n",
      "At: 2308 [==========>] Loss 0.1692830460802689  - accuracy: 0.84375\n",
      "At: 2309 [==========>] Loss 0.13304652949392043  - accuracy: 0.78125\n",
      "At: 2310 [==========>] Loss 0.12609836107173963  - accuracy: 0.8125\n",
      "At: 2311 [==========>] Loss 0.13089817645125063  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.08868333269566295  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.05705895615326965  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.09647129835978557  - accuracy: 0.90625\n",
      "At: 2315 [==========>] Loss 0.1325152574905914  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.15241101769553386  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.16158795078066437  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.21925839835903305  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.12081607740521319  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.10199440501194004  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.12466446793863957  - accuracy: 0.84375\n",
      "At: 2322 [==========>] Loss 0.1559326215250859  - accuracy: 0.8125\n",
      "At: 2323 [==========>] Loss 0.1409519680704695  - accuracy: 0.8125\n",
      "At: 2324 [==========>] Loss 0.15143828512019392  - accuracy: 0.78125\n",
      "At: 2325 [==========>] Loss 0.12611000412520632  - accuracy: 0.875\n",
      "At: 2326 [==========>] Loss 0.05481628819404226  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.07284906607852946  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.09602832658612011  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.10464244419657298  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.1405201071385411  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.08436817366488823  - accuracy: 0.90625\n",
      "At: 2332 [==========>] Loss 0.1307852326730882  - accuracy: 0.78125\n",
      "At: 2333 [==========>] Loss 0.1040532079805922  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.1680618492152433  - accuracy: 0.78125\n",
      "At: 2335 [==========>] Loss 0.08425313241448025  - accuracy: 0.875\n",
      "At: 2336 [==========>] Loss 0.09210202213270222  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.15166001385987718  - accuracy: 0.75\n",
      "At: 2338 [==========>] Loss 0.10583751202259672  - accuracy: 0.875\n",
      "At: 2339 [==========>] Loss 0.09057257181024136  - accuracy: 0.84375\n",
      "At: 2340 [==========>] Loss 0.15242308543101815  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.11567140247830854  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.15381563011059177  - accuracy: 0.8125\n",
      "At: 2343 [==========>] Loss 0.11535933886270047  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.21819192154484957  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.1548404308235658  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.07189503408882733  - accuracy: 0.9375\n",
      "At: 2347 [==========>] Loss 0.12186887789981492  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.09263295582666223  - accuracy: 0.90625\n",
      "At: 2349 [==========>] Loss 0.08036099002106883  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.12391299178913645  - accuracy: 0.8125\n",
      "At: 2351 [==========>] Loss 0.09259342541759802  - accuracy: 0.90625\n",
      "At: 2352 [==========>] Loss 0.08418602376790889  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.0876998739716855  - accuracy: 0.90625\n",
      "At: 2354 [==========>] Loss 0.12678459730108424  - accuracy: 0.8125\n",
      "At: 2355 [==========>] Loss 0.05048256450148454  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.12194429026105054  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.13213403271533652  - accuracy: 0.875\n",
      "At: 2358 [==========>] Loss 0.16089505349435324  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.15903372796099846  - accuracy: 0.75\n",
      "At: 2360 [==========>] Loss 0.09833455888919007  - accuracy: 0.875\n",
      "At: 2361 [==========>] Loss 0.15361684439113704  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.07153412876544597  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.18009513667410337  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.06198456936992777  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.10240452070162682  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.16301158634257004  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.07083774044084176  - accuracy: 0.90625\n",
      "At: 2368 [==========>] Loss 0.10330889717085052  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.0914360322023112  - accuracy: 0.875\n",
      "At: 2370 [==========>] Loss 0.07775985000883404  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.09850264120174876  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.14269744354108144  - accuracy: 0.75\n",
      "At: 2373 [==========>] Loss 0.09459078092249919  - accuracy: 0.90625\n",
      "At: 2374 [==========>] Loss 0.09100116702548963  - accuracy: 0.90625\n",
      "At: 2375 [==========>] Loss 0.04896617259796551  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.09785516493823694  - accuracy: 0.875\n",
      "At: 2377 [==========>] Loss 0.0807654938269719  - accuracy: 0.9375\n",
      "At: 2378 [==========>] Loss 0.13111576484267173  - accuracy: 0.78125\n",
      "At: 2379 [==========>] Loss 0.16351653819363074  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.05473967838597253  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.08988126506922545  - accuracy: 0.84375\n",
      "At: 2382 [==========>] Loss 0.10591815155199515  - accuracy: 0.875\n",
      "At: 2383 [==========>] Loss 0.12083893884179411  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.10788293937505611  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.11323789603621418  - accuracy: 0.875\n",
      "At: 2386 [==========>] Loss 0.12196650170813463  - accuracy: 0.8125\n",
      "At: 2387 [==========>] Loss 0.07409178873004676  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.12220244471057604  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.053836782191274864  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.06200918863975565  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.13636308215706083  - accuracy: 0.875\n",
      "At: 2392 [==========>] Loss 0.1400878492778797  - accuracy: 0.78125\n",
      "At: 2393 [==========>] Loss 0.09198550514179274  - accuracy: 0.84375\n",
      "At: 2394 [==========>] Loss 0.06296157256435095  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.12697269425647292  - accuracy: 0.8125\n",
      "At: 2396 [==========>] Loss 0.07772382238514353  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.10657211473155356  - accuracy: 0.84375\n",
      "At: 2398 [==========>] Loss 0.093696255572918  - accuracy: 0.90625\n",
      "At: 2399 [==========>] Loss 0.1371441936857122  - accuracy: 0.84375\n",
      "At: 2400 [==========>] Loss 0.08691329432595656  - accuracy: 0.90625\n",
      "At: 2401 [==========>] Loss 0.10229912478600862  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.08526888017660414  - accuracy: 0.84375\n",
      "At: 2403 [==========>] Loss 0.19100882031781746  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.12680274138738085  - accuracy: 0.8125\n",
      "At: 2405 [==========>] Loss 0.06445257460511458  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.12117741942737706  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.09081984990374851  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.0868582029752328  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.15671535844160833  - accuracy: 0.84375\n",
      "At: 2410 [==========>] Loss 0.14690723341545942  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.11308345675464414  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.09661220090962218  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.08156979529452726  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.05169948287871111  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.10553244423210821  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.09795477481121409  - accuracy: 0.84375\n",
      "At: 2417 [==========>] Loss 0.16213082880919152  - accuracy: 0.8125\n",
      "At: 2418 [==========>] Loss 0.10845511527662922  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.15554759962671239  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.13697105100208667  - accuracy: 0.84375\n",
      "At: 2421 [==========>] Loss 0.08552764653861904  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.1639319809114555  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.12165519220638131  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.1033733687256335  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08902686388670467  - accuracy: 0.875\n",
      "At: 2426 [==========>] Loss 0.18926837935898133  - accuracy: 0.71875\n",
      "At: 2427 [==========>] Loss 0.12231841097335461  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.0838513017198958  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.13311917576274665  - accuracy: 0.78125\n",
      "At: 2430 [==========>] Loss 0.12240038599226125  - accuracy: 0.875\n",
      "At: 2431 [==========>] Loss 0.14977709016927263  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.058660058594985534  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.057573508492839896  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.044716317380998974  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.17930112376980073  - accuracy: 0.78125\n",
      "At: 2436 [==========>] Loss 0.10996500103299178  - accuracy: 0.90625\n",
      "At: 2437 [==========>] Loss 0.16558109381254094  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.09725498048753299  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.1334026444827721  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.10962553003969033  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.09134500392526716  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.10815439440467087  - accuracy: 0.875\n",
      "At: 2443 [==========>] Loss 0.10616575948478604  - accuracy: 0.84375\n",
      "At: 2444 [==========>] Loss 0.10428561917796432  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.0650159630644814  - accuracy: 0.9375\n",
      "At: 2446 [==========>] Loss 0.1662881945107857  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.12984454672801307  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.12454057714271438  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.07242227021162861  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.05920628735260755  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.03722437433872889  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.11523526183491788  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.1037562308801939  - accuracy: 0.875\n",
      "At: 2454 [==========>] Loss 0.16400787436502096  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.13610185798244862  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.14162017079702832  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.15947990877337545  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.06313926492361548  - accuracy: 0.9375\n",
      "At: 2459 [==========>] Loss 0.12419698268135737  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.07403511548487926  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.06787778787430179  - accuracy: 0.90625\n",
      "At: 2462 [==========>] Loss 0.15115798585554346  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.11832804556211154  - accuracy: 0.8125\n",
      "At: 2464 [==========>] Loss 0.14943571661795274  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.1229465252582291  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.08017777368873069  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.09945018794929114  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.09516578223722635  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.1120739638828266  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.12850922309581247  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.12403170510333582  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.09208910835217425  - accuracy: 0.9375\n",
      "At: 2473 [==========>] Loss 0.11369648633787251  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.05839464834185951  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.1090805414595021  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.07352606051776478  - accuracy: 0.875\n",
      "At: 2477 [==========>] Loss 0.1649827809880323  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.10585261431602662  - accuracy: 0.875\n",
      "At: 2479 [==========>] Loss 0.05588384389697878  - accuracy: 0.96875\n",
      "At: 2480 [==========>] Loss 0.14817998577917513  - accuracy: 0.71875\n",
      "At: 2481 [==========>] Loss 0.06612845696301473  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.17070952610865303  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.11720502265686479  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.09206803531933734  - accuracy: 0.8125\n",
      "At: 2485 [==========>] Loss 0.09673063861735508  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.11875111203946981  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.10624044411920441  - accuracy: 0.875\n",
      "At: 2488 [==========>] Loss 0.1752078658101033  - accuracy: 0.75\n",
      "At: 2489 [==========>] Loss 0.18010904801421107  - accuracy: 0.6875\n",
      "At: 2490 [==========>] Loss 0.11341034250671386  - accuracy: 0.90625\n",
      "At: 2491 [==========>] Loss 0.1355783335239089  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.1463935945163906  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.10426531042195332  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.08497952751895198  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.09591171701445582  - accuracy: 0.84375\n",
      "At: 2496 [==========>] Loss 0.06107951516287307  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.21292952026352557  - accuracy: 0.65625\n",
      "At: 2498 [==========>] Loss 0.11006714201481888  - accuracy: 0.90625\n",
      "At: 2499 [==========>] Loss 0.0555644472923253  - accuracy: 0.96875\n",
      "At: 2500 [==========>] Loss 0.18963382927707123  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.1301075552818878  - accuracy: 0.84375\n",
      "At: 2502 [==========>] Loss 0.12628718971368347  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.13190961174184465  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.1440752083354969  - accuracy: 0.78125\n",
      "At: 2505 [==========>] Loss 0.07319180277644097  - accuracy: 0.90625\n",
      "At: 2506 [==========>] Loss 0.10730186467736129  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.15521293906106212  - accuracy: 0.8125\n",
      "At: 2508 [==========>] Loss 0.11220191044808683  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.16622855286832125  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.12087901193988601  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.14907235642774005  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.11476373006054358  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.15682654491951264  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.129966147398764  - accuracy: 0.84375\n",
      "At: 2515 [==========>] Loss 0.20148818047251532  - accuracy: 0.78125\n",
      "At: 2516 [==========>] Loss 0.21374715636332878  - accuracy: 0.78125\n",
      "At: 2517 [==========>] Loss 0.11145350360246695  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.12249234378462126  - accuracy: 0.90625\n",
      "At: 2519 [==========>] Loss 0.08537579385429385  - accuracy: 0.90625\n",
      "At: 2520 [==========>] Loss 0.13625982557839647  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.10524414398932842  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.20145525066717102  - accuracy: 0.71875\n",
      "At: 2523 [==========>] Loss 0.11296819112570147  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.17167272554386512  - accuracy: 0.71875\n",
      "At: 2525 [==========>] Loss 0.08687233211245002  - accuracy: 0.84375\n",
      "At: 2526 [==========>] Loss 0.10067583513610759  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.11305893510000076  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.09802691754782823  - accuracy: 0.8125\n",
      "At: 2529 [==========>] Loss 0.14741086677963947  - accuracy: 0.75\n",
      "At: 2530 [==========>] Loss 0.13792712857110956  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.03171179997174099  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.10657416957693505  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.09353706597104136  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.0652965034064491  - accuracy: 0.90625\n",
      "At: 2535 [==========>] Loss 0.0632354414323167  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.16663593456245684  - accuracy: 0.78125\n",
      "At: 2537 [==========>] Loss 0.08905222638501324  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.11884339937818515  - accuracy: 0.75\n",
      "At: 2539 [==========>] Loss 0.06666506403401555  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.12728970759024327  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.08080989524518059  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.045278852454886755  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.15050598325192582  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.13658774011011232  - accuracy: 0.75\n",
      "At: 2545 [==========>] Loss 0.07103797696389336  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.1365072859689371  - accuracy: 0.8125\n",
      "At: 2547 [==========>] Loss 0.09979337631331907  - accuracy: 0.84375\n",
      "At: 2548 [==========>] Loss 0.09666926567008308  - accuracy: 0.84375\n",
      "At: 2549 [==========>] Loss 0.08876861864765262  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.11995181071070762  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.12071428649067328  - accuracy: 0.84375\n",
      "At: 2552 [==========>] Loss 0.0989382622367892  - accuracy: 0.90625\n",
      "At: 2553 [==========>] Loss 0.06570846282065379  - accuracy: 0.9375\n",
      "At: 2554 [==========>] Loss 0.14979311548893798  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.18555431551538673  - accuracy: 0.75\n",
      "At: 2556 [==========>] Loss 0.09292423831997966  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.12633342579693424  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.08787056492361961  - accuracy: 0.8125\n",
      "At: 2559 [==========>] Loss 0.10110123774782404  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.18938868894357586  - accuracy: 0.71875\n",
      "At: 2561 [==========>] Loss 0.10721808704169322  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.0856038436103805  - accuracy: 0.90625\n",
      "At: 2563 [==========>] Loss 0.12327959538847964  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09214254336540399  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.1125757237559564  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.16372791401167391  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.11546637330024337  - accuracy: 0.75\n",
      "At: 2568 [==========>] Loss 0.11110991999314668  - accuracy: 0.78125\n",
      "At: 2569 [==========>] Loss 0.09980618428334516  - accuracy: 0.875\n",
      "At: 2570 [==========>] Loss 0.19742920106569178  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.10209689820383258  - accuracy: 0.8125\n",
      "At: 2572 [==========>] Loss 0.16493182009363283  - accuracy: 0.8125\n",
      "At: 2573 [==========>] Loss 0.18999107587291306  - accuracy: 0.6875\n",
      "At: 2574 [==========>] Loss 0.13247501045514265  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.06979801342531193  - accuracy: 0.90625\n",
      "At: 2576 [==========>] Loss 0.1169519886900027  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.23001298393910702  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.1834872336225763  - accuracy: 0.71875\n",
      "At: 2579 [==========>] Loss 0.19209760895699968  - accuracy: 0.6875\n",
      "At: 2580 [==========>] Loss 0.15567941346372982  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.08000642312660435  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.1924079552265744  - accuracy: 0.71875\n",
      "At: 2583 [==========>] Loss 0.07530890630228038  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.19581658547894026  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.0847313068096098  - accuracy: 0.9375\n",
      "At: 2586 [==========>] Loss 0.09537767024977968  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.14893413078199882  - accuracy: 0.75\n",
      "At: 2588 [==========>] Loss 0.11341115531035086  - accuracy: 0.90625\n",
      "At: 2589 [==========>] Loss 0.09489111977013069  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.19721975058645183  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.12151290732411356  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.09815325130563834  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.09711467990732277  - accuracy: 0.8125\n",
      "At: 2594 [==========>] Loss 0.07969512524143191  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.08752847219997217  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.11519577538364975  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.08205844129365333  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.11976074156278807  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.11165213015881692  - accuracy: 0.84375\n",
      "At: 2600 [==========>] Loss 0.1488571481391308  - accuracy: 0.75\n",
      "At: 2601 [==========>] Loss 0.14380199108378444  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.07741126104094434  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.13903771375676724  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.10208280061709168  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.13865159671957333  - accuracy: 0.78125\n",
      "At: 2606 [==========>] Loss 0.12175067118237182  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.13241603136352204  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.06082578909407588  - accuracy: 0.96875\n",
      "At: 2609 [==========>] Loss 0.11217992741131816  - accuracy: 0.84375\n",
      "At: 2610 [==========>] Loss 0.13127473719281413  - accuracy: 0.8125\n",
      "At: 2611 [==========>] Loss 0.12821599359754496  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.09536020687162905  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.0885768626491965  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.10016198083932225  - accuracy: 0.90625\n",
      "At: 2615 [==========>] Loss 0.056156693095599446  - accuracy: 0.9375\n",
      "At: 2616 [==========>] Loss 0.056387872613884706  - accuracy: 0.96875\n",
      "At: 2617 [==========>] Loss 0.09598578402299957  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.06828264516408675  - accuracy: 0.9375\n",
      "At: 2619 [==========>] Loss 0.08432844336354642  - accuracy: 0.9375\n",
      "At: 2620 [==========>] Loss 0.12413241735446469  - accuracy: 0.78125\n",
      "At: 2621 [==========>] Loss 0.13169349165398436  - accuracy: 0.78125\n",
      "At: 2622 [==========>] Loss 0.11016781894892017  - accuracy: 0.8125\n",
      "At: 2623 [==========>] Loss 0.0937088482444538  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.1501715038783057  - accuracy: 0.78125\n",
      "At: 2625 [==========>] Loss 0.06800204219124326  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.053489111326882284  - accuracy: 0.96875\n",
      "At: 2627 [==========>] Loss 0.1305349251010612  - accuracy: 0.75\n",
      "At: 2628 [==========>] Loss 0.07399676529402034  - accuracy: 0.875\n",
      "At: 2629 [==========>] Loss 0.08909038717564467  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.11422800269749903  - accuracy: 0.84375\n",
      "At: 2631 [==========>] Loss 0.13838041584484495  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.14039509481705556  - accuracy: 0.84375\n",
      "At: 2633 [==========>] Loss 0.10450071018685456  - accuracy: 0.875\n",
      "At: 2634 [==========>] Loss 0.06824847231931024  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.19249083030687997  - accuracy: 0.71875\n",
      "At: 2636 [==========>] Loss 0.12370168273522658  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.14066856417110352  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.08197637085547965  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.06070253298195609  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.1353094479315957  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.12406973716739589  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.16313768411992768  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.07229225150526145  - accuracy: 0.90625\n",
      "At: 2644 [==========>] Loss 0.15656976554686164  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.059405761078263036  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.1494056576698109  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.12867435132066912  - accuracy: 0.78125\n",
      "At: 2648 [==========>] Loss 0.14693538151530194  - accuracy: 0.75\n",
      "At: 2649 [==========>] Loss 0.10125017965095784  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.11147665036928892  - accuracy: 0.90625\n",
      "At: 2651 [==========>] Loss 0.17889783442999058  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.08527251656240309  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.12975003261842766  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11058241958933075  - accuracy: 0.8125\n",
      "At: 2655 [==========>] Loss 0.21252224845585427  - accuracy: 0.6875\n",
      "At: 2656 [==========>] Loss 0.018824946226796105  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.08611748223960936  - accuracy: 0.90625\n",
      "At: 2658 [==========>] Loss 0.0834611141930333  - accuracy: 0.90625\n",
      "At: 2659 [==========>] Loss 0.05422951738588752  - accuracy: 0.96875\n",
      "At: 2660 [==========>] Loss 0.0878308571550185  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.08133304796829305  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.07636372617979431  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.09679720452202112  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.08617982594546471  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.08684385998240568  - accuracy: 0.90625\n",
      "At: 2666 [==========>] Loss 0.1303260743429792  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.11792750799619844  - accuracy: 0.8125\n",
      "At: 2668 [==========>] Loss 0.129462765972981  - accuracy: 0.8125\n",
      "At: 2669 [==========>] Loss 0.14009183232759526  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.09962612163856038  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.08164009010340853  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.11966026342058719  - accuracy: 0.78125\n",
      "At: 2673 [==========>] Loss 0.11082083144853044  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.09338368543798367  - accuracy: 0.875\n",
      "At: 2675 [==========>] Loss 0.1349638534564983  - accuracy: 0.75\n",
      "At: 2676 [==========>] Loss 0.13279726369569755  - accuracy: 0.8125\n",
      "At: 2677 [==========>] Loss 0.09956026592987421  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.04533541025863855  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.055946658480014  - accuracy: 0.9375\n",
      "At: 2680 [==========>] Loss 0.09166645780832094  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.08075580197211711  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.13988433436696898  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.1797316970067021  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.06874249272258166  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.11313688494378189  - accuracy: 0.8125\n",
      "At: 2686 [==========>] Loss 0.050672832043110405  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.12950928722391378  - accuracy: 0.8125\n",
      "At: 2688 [==========>] Loss 0.15086131248118453  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.12108842109485496  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.13201916193991767  - accuracy: 0.8125\n",
      "Epochs  7 / 10\n",
      "At: 1 [==========>] Loss 0.19849542717161908  - accuracy: 0.6875\n",
      "At: 2 [==========>] Loss 0.29574772663942783  - accuracy: 0.5625\n",
      "At: 3 [==========>] Loss 0.1538722666087497  - accuracy: 0.78125\n",
      "At: 4 [==========>] Loss 0.18633433801165034  - accuracy: 0.78125\n",
      "At: 5 [==========>] Loss 0.1401594203515255  - accuracy: 0.8125\n",
      "At: 6 [==========>] Loss 0.16675559984617588  - accuracy: 0.75\n",
      "At: 7 [==========>] Loss 0.203272445389367  - accuracy: 0.6875\n",
      "At: 8 [==========>] Loss 0.31572209648947214  - accuracy: 0.59375\n",
      "At: 9 [==========>] Loss 0.3505709725239608  - accuracy: 0.53125\n",
      "At: 10 [==========>] Loss 0.30673928612489443  - accuracy: 0.5625\n",
      "At: 11 [==========>] Loss 0.2818168431342566  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.24544877085468758  - accuracy: 0.6875\n",
      "At: 13 [==========>] Loss 0.22900439949058454  - accuracy: 0.71875\n",
      "At: 14 [==========>] Loss 0.13935267557914519  - accuracy: 0.8125\n",
      "At: 15 [==========>] Loss 0.19646344985142236  - accuracy: 0.75\n",
      "At: 16 [==========>] Loss 0.22702366291712678  - accuracy: 0.625\n",
      "At: 17 [==========>] Loss 0.27596424891547233  - accuracy: 0.5625\n",
      "At: 18 [==========>] Loss 0.27107953262151735  - accuracy: 0.65625\n",
      "At: 19 [==========>] Loss 0.23814311737178762  - accuracy: 0.75\n",
      "At: 20 [==========>] Loss 0.21046888123547028  - accuracy: 0.6875\n",
      "At: 21 [==========>] Loss 0.315264409632267  - accuracy: 0.59375\n",
      "At: 22 [==========>] Loss 0.24776543085187064  - accuracy: 0.65625\n",
      "At: 23 [==========>] Loss 0.09432916481471622  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.2782623418916563  - accuracy: 0.65625\n",
      "At: 25 [==========>] Loss 0.2939151586076026  - accuracy: 0.5\n",
      "At: 26 [==========>] Loss 0.3020217518626887  - accuracy: 0.59375\n",
      "At: 27 [==========>] Loss 0.279123349814015  - accuracy: 0.65625\n",
      "At: 28 [==========>] Loss 0.23087055944198392  - accuracy: 0.71875\n",
      "At: 29 [==========>] Loss 0.19513499366435938  - accuracy: 0.71875\n",
      "At: 30 [==========>] Loss 0.18029784560227952  - accuracy: 0.78125\n",
      "At: 31 [==========>] Loss 0.28614183270664173  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.2396872768753475  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.12325545436002286  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.15549275839510865  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.19857860042285838  - accuracy: 0.6875\n",
      "At: 36 [==========>] Loss 0.20968220307308674  - accuracy: 0.65625\n",
      "At: 37 [==========>] Loss 0.33467771568745497  - accuracy: 0.59375\n",
      "At: 38 [==========>] Loss 0.2945838027403656  - accuracy: 0.5625\n",
      "At: 39 [==========>] Loss 0.2433636446350126  - accuracy: 0.75\n",
      "At: 40 [==========>] Loss 0.2775747802501056  - accuracy: 0.65625\n",
      "At: 41 [==========>] Loss 0.10420501957209063  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.2435712412979611  - accuracy: 0.625\n",
      "At: 43 [==========>] Loss 0.1601593301965174  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.21499399534744473  - accuracy: 0.75\n",
      "At: 45 [==========>] Loss 0.1260143450811543  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.19723055092781408  - accuracy: 0.71875\n",
      "At: 47 [==========>] Loss 0.30381667971276083  - accuracy: 0.625\n",
      "At: 48 [==========>] Loss 0.1988171475572266  - accuracy: 0.71875\n",
      "At: 49 [==========>] Loss 0.18702553560230661  - accuracy: 0.71875\n",
      "At: 50 [==========>] Loss 0.26194204857258785  - accuracy: 0.65625\n",
      "At: 51 [==========>] Loss 0.276442476622336  - accuracy: 0.65625\n",
      "At: 52 [==========>] Loss 0.251461535589499  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.19827644840165695  - accuracy: 0.75\n",
      "At: 54 [==========>] Loss 0.19461071333322602  - accuracy: 0.75\n",
      "At: 55 [==========>] Loss 0.2427632749154507  - accuracy: 0.65625\n",
      "At: 56 [==========>] Loss 0.18515987611806073  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.20122216690485906  - accuracy: 0.75\n",
      "At: 58 [==========>] Loss 0.2294544246670997  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.20204857016569439  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.23783134994322244  - accuracy: 0.6875\n",
      "At: 61 [==========>] Loss 0.3035942462672978  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.17787035245275376  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.17094730680197262  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.21964074972257408  - accuracy: 0.75\n",
      "At: 65 [==========>] Loss 0.23835427089030523  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.2843274476935157  - accuracy: 0.59375\n",
      "At: 67 [==========>] Loss 0.2794969033498359  - accuracy: 0.59375\n",
      "At: 68 [==========>] Loss 0.17246882023186136  - accuracy: 0.78125\n",
      "At: 69 [==========>] Loss 0.1591755438342383  - accuracy: 0.78125\n",
      "At: 70 [==========>] Loss 0.23712621400974102  - accuracy: 0.71875\n",
      "At: 71 [==========>] Loss 0.2122537725275662  - accuracy: 0.71875\n",
      "At: 72 [==========>] Loss 0.20072003431585145  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.18305757144794016  - accuracy: 0.78125\n",
      "At: 74 [==========>] Loss 0.26543226787251095  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.23837802344685735  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.3138252486380657  - accuracy: 0.625\n",
      "At: 77 [==========>] Loss 0.28602408995946643  - accuracy: 0.625\n",
      "At: 78 [==========>] Loss 0.22287673583882436  - accuracy: 0.71875\n",
      "At: 79 [==========>] Loss 0.20726597923299056  - accuracy: 0.71875\n",
      "At: 80 [==========>] Loss 0.2664165799088355  - accuracy: 0.6875\n",
      "At: 81 [==========>] Loss 0.19737577982441185  - accuracy: 0.8125\n",
      "At: 82 [==========>] Loss 0.24892240356656054  - accuracy: 0.59375\n",
      "At: 83 [==========>] Loss 0.21668508384173454  - accuracy: 0.75\n",
      "At: 84 [==========>] Loss 0.2398290176544334  - accuracy: 0.6875\n",
      "At: 85 [==========>] Loss 0.22395587925618612  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.17755527912557914  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.17805378059850493  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.3895511619373766  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.1709271659587003  - accuracy: 0.71875\n",
      "At: 90 [==========>] Loss 0.21343067865864318  - accuracy: 0.75\n",
      "At: 91 [==========>] Loss 0.21453947618927843  - accuracy: 0.71875\n",
      "At: 92 [==========>] Loss 0.11943934788125621  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.1801215189349843  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.1297086846353444  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.24146353837972495  - accuracy: 0.75\n",
      "At: 96 [==========>] Loss 0.12529123312811752  - accuracy: 0.875\n",
      "At: 97 [==========>] Loss 0.1372727635413316  - accuracy: 0.875\n",
      "At: 98 [==========>] Loss 0.2603958304979294  - accuracy: 0.6875\n",
      "At: 99 [==========>] Loss 0.1626488604470949  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.15257755995454306  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.18565480197686896  - accuracy: 0.75\n",
      "At: 102 [==========>] Loss 0.2214282001017041  - accuracy: 0.6875\n",
      "At: 103 [==========>] Loss 0.20405006514565244  - accuracy: 0.78125\n",
      "At: 104 [==========>] Loss 0.1689693436153267  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.22226423250480304  - accuracy: 0.6875\n",
      "At: 106 [==========>] Loss 0.2587625726574485  - accuracy: 0.6875\n",
      "At: 107 [==========>] Loss 0.23857020060404538  - accuracy: 0.71875\n",
      "At: 108 [==========>] Loss 0.2711106417125998  - accuracy: 0.625\n",
      "At: 109 [==========>] Loss 0.1517030241011063  - accuracy: 0.78125\n",
      "At: 110 [==========>] Loss 0.31766904323890166  - accuracy: 0.625\n",
      "At: 111 [==========>] Loss 0.12147407416243935  - accuracy: 0.84375\n",
      "At: 112 [==========>] Loss 0.18015783034958538  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.22714270061792152  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.1678948675839534  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.22027948952037488  - accuracy: 0.75\n",
      "At: 116 [==========>] Loss 0.19467825128809674  - accuracy: 0.6875\n",
      "At: 117 [==========>] Loss 0.1745269946805494  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.28233856558539694  - accuracy: 0.625\n",
      "At: 119 [==========>] Loss 0.11708989106410272  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.19900456581155193  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.12426606875622281  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.14613635038871547  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.20395512585230924  - accuracy: 0.65625\n",
      "At: 124 [==========>] Loss 0.3030742646792627  - accuracy: 0.65625\n",
      "At: 125 [==========>] Loss 0.22649954776300846  - accuracy: 0.71875\n",
      "At: 126 [==========>] Loss 0.2578376873513767  - accuracy: 0.625\n",
      "At: 127 [==========>] Loss 0.21642327517317755  - accuracy: 0.6875\n",
      "At: 128 [==========>] Loss 0.3097704025882242  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.14797407979361396  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.2201240267983377  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.19744512534120034  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.22730649601042135  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.23649943097331821  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.14490380160763583  - accuracy: 0.78125\n",
      "At: 135 [==========>] Loss 0.20032339846787683  - accuracy: 0.71875\n",
      "At: 136 [==========>] Loss 0.18519009621811097  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.11968900031456284  - accuracy: 0.875\n",
      "At: 138 [==========>] Loss 0.2058924760984711  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.11781051654692912  - accuracy: 0.8125\n",
      "At: 140 [==========>] Loss 0.1706431948291414  - accuracy: 0.75\n",
      "At: 141 [==========>] Loss 0.32253548084155914  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.2237192640651346  - accuracy: 0.71875\n",
      "At: 143 [==========>] Loss 0.2007074841127541  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.12213846149332341  - accuracy: 0.875\n",
      "At: 145 [==========>] Loss 0.16953977219026417  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.17179925314139996  - accuracy: 0.78125\n",
      "At: 147 [==========>] Loss 0.20638646753645679  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.17706556261178347  - accuracy: 0.78125\n",
      "At: 149 [==========>] Loss 0.1923584983506801  - accuracy: 0.8125\n",
      "At: 150 [==========>] Loss 0.21067697727713486  - accuracy: 0.75\n",
      "At: 151 [==========>] Loss 0.20068318866006368  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.21818531341830638  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.26280409704141483  - accuracy: 0.6875\n",
      "At: 154 [==========>] Loss 0.2196490111956772  - accuracy: 0.75\n",
      "At: 155 [==========>] Loss 0.20724730795038196  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.15702393788054475  - accuracy: 0.8125\n",
      "At: 157 [==========>] Loss 0.2778730340980997  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.15923725545224454  - accuracy: 0.84375\n",
      "At: 159 [==========>] Loss 0.14561438930359882  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.14831615763466832  - accuracy: 0.8125\n",
      "At: 161 [==========>] Loss 0.1452541795342318  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.21452113840937528  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.17312855645015232  - accuracy: 0.78125\n",
      "At: 164 [==========>] Loss 0.1869621716040922  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.22563179990795817  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.18479657816156467  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.18021666967168049  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.21988179316276985  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.20599430393478965  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.20356005132172228  - accuracy: 0.75\n",
      "At: 171 [==========>] Loss 0.23160747812552904  - accuracy: 0.75\n",
      "At: 172 [==========>] Loss 0.21217415926061955  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.25916668207063603  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.22620212445182974  - accuracy: 0.71875\n",
      "At: 175 [==========>] Loss 0.19435878980108995  - accuracy: 0.75\n",
      "At: 176 [==========>] Loss 0.20854249116531262  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.12479831183195665  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.23210510909744825  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.15650052507885975  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.1639993493450943  - accuracy: 0.78125\n",
      "At: 181 [==========>] Loss 0.06021587967808164  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.2652481253617207  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.2148151336315713  - accuracy: 0.78125\n",
      "At: 184 [==========>] Loss 0.20953140450998872  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.10806804876266009  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.16436181080404924  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.17586689517899265  - accuracy: 0.78125\n",
      "At: 188 [==========>] Loss 0.2281857903572504  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.21140113581961614  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.15395181954887938  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.34524464925914283  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.13852294626656064  - accuracy: 0.8125\n",
      "At: 193 [==========>] Loss 0.2735184557854504  - accuracy: 0.59375\n",
      "At: 194 [==========>] Loss 0.20558721482979858  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.22041720907555307  - accuracy: 0.71875\n",
      "At: 196 [==========>] Loss 0.24016815304356445  - accuracy: 0.71875\n",
      "At: 197 [==========>] Loss 0.20187767704687126  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.15344631854154045  - accuracy: 0.8125\n",
      "At: 199 [==========>] Loss 0.15014972942166022  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.20401669703271672  - accuracy: 0.6875\n",
      "At: 201 [==========>] Loss 0.17841737302996913  - accuracy: 0.8125\n",
      "At: 202 [==========>] Loss 0.13578741347714693  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.16555412565081007  - accuracy: 0.78125\n",
      "At: 204 [==========>] Loss 0.20212571368166882  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.19524371697126158  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.13469497420531423  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.14217600358640636  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.2741543864227498  - accuracy: 0.625\n",
      "At: 209 [==========>] Loss 0.18482455699921146  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.13740803460143397  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.17200438172682847  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.24634394571838542  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.21125100054433835  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.227534198229376  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.13181101727933436  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.225680316918902  - accuracy: 0.71875\n",
      "At: 217 [==========>] Loss 0.23265362993463867  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.17429017097197191  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.19759228657698694  - accuracy: 0.75\n",
      "At: 220 [==========>] Loss 0.25789272661788876  - accuracy: 0.6875\n",
      "At: 221 [==========>] Loss 0.1854836422093904  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.09531450759186064  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.2921612815272884  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.17210214438715196  - accuracy: 0.8125\n",
      "At: 225 [==========>] Loss 0.16829622303957747  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.18650745821328046  - accuracy: 0.75\n",
      "At: 227 [==========>] Loss 0.22442134330241406  - accuracy: 0.71875\n",
      "At: 228 [==========>] Loss 0.19313318628594417  - accuracy: 0.71875\n",
      "At: 229 [==========>] Loss 0.21990268220739556  - accuracy: 0.71875\n",
      "At: 230 [==========>] Loss 0.17012720232770728  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.19625713106804588  - accuracy: 0.8125\n",
      "At: 232 [==========>] Loss 0.2779260235034386  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.23697013171742784  - accuracy: 0.71875\n",
      "At: 234 [==========>] Loss 0.09995469867721868  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.20780926743325775  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.2672100038956959  - accuracy: 0.6875\n",
      "At: 237 [==========>] Loss 0.13386730089825474  - accuracy: 0.8125\n",
      "At: 238 [==========>] Loss 0.1787162599272606  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.16191586283848702  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.25987610172242837  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.10866279128696012  - accuracy: 0.875\n",
      "At: 242 [==========>] Loss 0.23997623099692245  - accuracy: 0.75\n",
      "At: 243 [==========>] Loss 0.12641834417621478  - accuracy: 0.78125\n",
      "At: 244 [==========>] Loss 0.17837000581728554  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.2013412406026786  - accuracy: 0.75\n",
      "At: 246 [==========>] Loss 0.1578499698257912  - accuracy: 0.78125\n",
      "At: 247 [==========>] Loss 0.23755917153555364  - accuracy: 0.71875\n",
      "At: 248 [==========>] Loss 0.14398183111072596  - accuracy: 0.84375\n",
      "At: 249 [==========>] Loss 0.12598600710047378  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.26067117528777095  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.2474430333249852  - accuracy: 0.6875\n",
      "At: 252 [==========>] Loss 0.14501342482648127  - accuracy: 0.84375\n",
      "At: 253 [==========>] Loss 0.17389526106314931  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.11930030230403062  - accuracy: 0.84375\n",
      "At: 255 [==========>] Loss 0.1622519878116278  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.2295907689205805  - accuracy: 0.6875\n",
      "At: 257 [==========>] Loss 0.16663621599377457  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.1739390685248745  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.1734816226485892  - accuracy: 0.78125\n",
      "At: 260 [==========>] Loss 0.13361842476491242  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.09797407882263887  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.18407882940692988  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.11770492384347273  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.1577125548111222  - accuracy: 0.8125\n",
      "At: 265 [==========>] Loss 0.2795468929077063  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.33239555085673866  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1821219274820321  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.24733612555071005  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.15743977951211965  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.33225348631716944  - accuracy: 0.59375\n",
      "At: 271 [==========>] Loss 0.25871722421265325  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.09334999481427117  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.1966471317510903  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.21016941874285827  - accuracy: 0.71875\n",
      "At: 275 [==========>] Loss 0.08837233522613021  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.1992078927189571  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.13656800378247055  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.1577291289182195  - accuracy: 0.78125\n",
      "At: 279 [==========>] Loss 0.1906457533191539  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.14863928655822123  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.16131544391704905  - accuracy: 0.78125\n",
      "At: 282 [==========>] Loss 0.2210290433390178  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.13999720867876422  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.08666581167345083  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.1830391681195817  - accuracy: 0.78125\n",
      "At: 286 [==========>] Loss 0.10783550165312554  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.1925182851719619  - accuracy: 0.71875\n",
      "At: 288 [==========>] Loss 0.14604849198458741  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.11663158463194648  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.1130360622202927  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.118517727689364  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.17311281999114714  - accuracy: 0.75\n",
      "At: 293 [==========>] Loss 0.24155475375556518  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.20185686759280372  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.1649482919311747  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.17098383194219224  - accuracy: 0.8125\n",
      "At: 297 [==========>] Loss 0.17036852949113548  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.11405218682100415  - accuracy: 0.8125\n",
      "At: 299 [==========>] Loss 0.2655595863901655  - accuracy: 0.6875\n",
      "At: 300 [==========>] Loss 0.17784120240491424  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.20089030032586025  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.11643285761141116  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.1257232856565415  - accuracy: 0.84375\n",
      "At: 304 [==========>] Loss 0.22975835562457247  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.24716282490190167  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.09319345971024594  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.25612775483229655  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.20439506369908156  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.10554781271804023  - accuracy: 0.875\n",
      "At: 310 [==========>] Loss 0.22348110281595046  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.06365583501754898  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.14947603282875382  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.17064506401877505  - accuracy: 0.8125\n",
      "At: 314 [==========>] Loss 0.22235147738151512  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.14038792933846808  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.24342334160584955  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.30163416501521395  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.19148421339181906  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.11206560306829175  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.2093434844824943  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.2510167127998522  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.12455578666912961  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.07615520781950397  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.18321478436723132  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.1290152680638018  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.18664448025537095  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.14238311581936136  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.15027055980689938  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.15552150777123497  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.17536348365586368  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.21613148560113266  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.28805168486767085  - accuracy: 0.59375\n",
      "At: 333 [==========>] Loss 0.18198503239110114  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.13557404385520933  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.1682434985238238  - accuracy: 0.78125\n",
      "At: 336 [==========>] Loss 0.16144707813403386  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.24421611514533964  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.15375564911772432  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.2261953845421247  - accuracy: 0.75\n",
      "At: 340 [==========>] Loss 0.15481820738024854  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.11692777367616507  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.18614103119520684  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.2878085199747233  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.24080955593561182  - accuracy: 0.6875\n",
      "At: 345 [==========>] Loss 0.22150645544793668  - accuracy: 0.75\n",
      "At: 346 [==========>] Loss 0.15365682149688836  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.11574006190789952  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.148803271949544  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.18673512220925226  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.12690075318845645  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.2898962565626452  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.11883348826206987  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.18668411508762334  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.21213696742140575  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.13210496824561785  - accuracy: 0.84375\n",
      "At: 356 [==========>] Loss 0.2138514828404313  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.12357119756122754  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.1265532778075062  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.08248073827289243  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.16017503416166776  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.10466127908771033  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.2109706857517734  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.08400938277498428  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.24817497947180145  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.15183393555976893  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.24783077156256358  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.23738474983970975  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.16472666793636803  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.18277098454640256  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.16628047273507768  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.10742653113352041  - accuracy: 0.84375\n",
      "At: 372 [==========>] Loss 0.13372962887507658  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.24627243468701066  - accuracy: 0.65625\n",
      "At: 374 [==========>] Loss 0.07058539870669842  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.14380147019159906  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.11633037316401408  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.21269317798639487  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.21407096708610446  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.2119774464650222  - accuracy: 0.78125\n",
      "At: 380 [==========>] Loss 0.1585797214516179  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.19404190072787006  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.09210152414634273  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.2544033354573177  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.19248038301835876  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.14097770421503253  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.2595947290278588  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.08630013068727566  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.2512159120496763  - accuracy: 0.6875\n",
      "At: 389 [==========>] Loss 0.23744717736918936  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.14575042769635627  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.14052885761067  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.16234310012180908  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.2333545663226928  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.08738211082680383  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.2128145608601181  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.1680953212951796  - accuracy: 0.8125\n",
      "At: 397 [==========>] Loss 0.14887106533328348  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.23716069422199956  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.2526202228626142  - accuracy: 0.6875\n",
      "At: 400 [==========>] Loss 0.19842634384036978  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.14227255457416046  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.14034319579695664  - accuracy: 0.8125\n",
      "At: 403 [==========>] Loss 0.027662304469995916  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.11724021779094929  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.21826969253066938  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.1349867656341011  - accuracy: 0.8125\n",
      "At: 407 [==========>] Loss 0.1879573505801112  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.25607792540864543  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.2941230791017554  - accuracy: 0.65625\n",
      "At: 410 [==========>] Loss 0.14932485963023817  - accuracy: 0.78125\n",
      "At: 411 [==========>] Loss 0.1194534338685373  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.21343212283573604  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.1370919478464745  - accuracy: 0.8125\n",
      "At: 414 [==========>] Loss 0.20705766021067712  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.14242619653416322  - accuracy: 0.84375\n",
      "At: 416 [==========>] Loss 0.21773229721265658  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.17830318950130392  - accuracy: 0.71875\n",
      "At: 418 [==========>] Loss 0.10707577669060928  - accuracy: 0.84375\n",
      "At: 419 [==========>] Loss 0.17302315505015892  - accuracy: 0.78125\n",
      "At: 420 [==========>] Loss 0.21555694504738798  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.14576544926458596  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.12218922436798205  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.15005836522462632  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.3111099018297856  - accuracy: 0.65625\n",
      "At: 425 [==========>] Loss 0.2043869237140321  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.12098895102622931  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.1905256765131872  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.26335640392605725  - accuracy: 0.625\n",
      "At: 429 [==========>] Loss 0.21301948320378017  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.09838838776425465  - accuracy: 0.9375\n",
      "At: 431 [==========>] Loss 0.16961977631789146  - accuracy: 0.8125\n",
      "At: 432 [==========>] Loss 0.1789438803238273  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.13055447522536373  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.10656943425237175  - accuracy: 0.875\n",
      "At: 435 [==========>] Loss 0.22350064715190224  - accuracy: 0.71875\n",
      "At: 436 [==========>] Loss 0.16193146833613548  - accuracy: 0.8125\n",
      "At: 437 [==========>] Loss 0.15208151772952444  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.16851430369401132  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.12889357826040856  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.09601430443814685  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.23473555736378238  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.19234209790778622  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.18947064425263688  - accuracy: 0.78125\n",
      "At: 444 [==========>] Loss 0.18592875843692236  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.17610039065286692  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.27169971218337896  - accuracy: 0.65625\n",
      "At: 447 [==========>] Loss 0.15431770770300934  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.16639917364844561  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.1569978844304314  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.21904488003552558  - accuracy: 0.71875\n",
      "At: 451 [==========>] Loss 0.24594835527816744  - accuracy: 0.6875\n",
      "At: 452 [==========>] Loss 0.17125638797527482  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.17250276252706498  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.2699795284557383  - accuracy: 0.65625\n",
      "At: 455 [==========>] Loss 0.20128050847802353  - accuracy: 0.71875\n",
      "At: 456 [==========>] Loss 0.16599165309642674  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.2033259324171207  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.14688394102787428  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.2175515024171617  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.12439476094529302  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.23675750253116173  - accuracy: 0.65625\n",
      "At: 462 [==========>] Loss 0.19724010517815627  - accuracy: 0.8125\n",
      "At: 463 [==========>] Loss 0.1559651594325329  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.18399037207378147  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.24182491713089074  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.11913062080449452  - accuracy: 0.8125\n",
      "At: 467 [==========>] Loss 0.20402231555198855  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.1617172926632491  - accuracy: 0.8125\n",
      "At: 469 [==========>] Loss 0.15439768457145367  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.14154254795027968  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.21457171849711298  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.0984933045639369  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.16332782727354528  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.13112494758842566  - accuracy: 0.84375\n",
      "At: 475 [==========>] Loss 0.1631325897641139  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.16897397046107154  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.1526403661708559  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.1526000792137843  - accuracy: 0.8125\n",
      "At: 479 [==========>] Loss 0.18099984971154537  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.22220716673490082  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.1308384354491588  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.14096377378159508  - accuracy: 0.84375\n",
      "At: 483 [==========>] Loss 0.12307486132050424  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.10569047601574535  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.13350899074974465  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.22505252014431532  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.1448556803315182  - accuracy: 0.78125\n",
      "At: 488 [==========>] Loss 0.14325435465495845  - accuracy: 0.84375\n",
      "At: 489 [==========>] Loss 0.20190153201152433  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.15334456762068685  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.2063326500492514  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.25281529929788293  - accuracy: 0.59375\n",
      "At: 493 [==========>] Loss 0.14767544912870792  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.2662894804129447  - accuracy: 0.65625\n",
      "At: 495 [==========>] Loss 0.15013928612387512  - accuracy: 0.84375\n",
      "At: 496 [==========>] Loss 0.18949240932550732  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.2021440749078339  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.07759735361086786  - accuracy: 0.9375\n",
      "At: 499 [==========>] Loss 0.19591721125401393  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.16252727869176659  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.1803660777494711  - accuracy: 0.71875\n",
      "At: 502 [==========>] Loss 0.13995924661860165  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.16228070200254235  - accuracy: 0.8125\n",
      "At: 504 [==========>] Loss 0.13482416688435073  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.23203709312897963  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.2876657920060293  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.14634120021863184  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.08927947154752905  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.263997860329829  - accuracy: 0.65625\n",
      "At: 510 [==========>] Loss 0.23653035082336393  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.17293968841951474  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.1890821681707207  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.25613030700795286  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.1848550649831308  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.1286308763854328  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.20292150020361888  - accuracy: 0.71875\n",
      "At: 517 [==========>] Loss 0.14801542482593344  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.20345696389154852  - accuracy: 0.75\n",
      "At: 519 [==========>] Loss 0.15653168662368194  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.16434158012159295  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.17500579920720577  - accuracy: 0.71875\n",
      "At: 522 [==========>] Loss 0.14074433676741765  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.1883550528622796  - accuracy: 0.71875\n",
      "At: 524 [==========>] Loss 0.09695036030077231  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.18093656273865505  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.17377961535150482  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.32121080853319706  - accuracy: 0.53125\n",
      "At: 528 [==========>] Loss 0.17129142952804705  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.14166134575118602  - accuracy: 0.8125\n",
      "At: 530 [==========>] Loss 0.20195699477257403  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.17232890694085573  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.13727591224296615  - accuracy: 0.84375\n",
      "At: 533 [==========>] Loss 0.1205260335231698  - accuracy: 0.84375\n",
      "At: 534 [==========>] Loss 0.1702791539928977  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.212262807631026  - accuracy: 0.78125\n",
      "At: 536 [==========>] Loss 0.22386054917379075  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.1485008874611231  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.1646859782723431  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.148976921728947  - accuracy: 0.84375\n",
      "At: 540 [==========>] Loss 0.22105704390440822  - accuracy: 0.71875\n",
      "At: 541 [==========>] Loss 0.2118783118863841  - accuracy: 0.75\n",
      "At: 542 [==========>] Loss 0.13900572488177948  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.23998584658704836  - accuracy: 0.71875\n",
      "At: 544 [==========>] Loss 0.26043708148900463  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.10618728468343654  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.167558453177703  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.18911922252683847  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.11820775706869067  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.1704411929495545  - accuracy: 0.8125\n",
      "At: 550 [==========>] Loss 0.12544730542463864  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.15792077187776116  - accuracy: 0.84375\n",
      "At: 552 [==========>] Loss 0.19757505365123837  - accuracy: 0.75\n",
      "At: 553 [==========>] Loss 0.13293565773243815  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.10241417390861737  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.1885456158976756  - accuracy: 0.75\n",
      "At: 556 [==========>] Loss 0.1907820621575739  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.1927854479505312  - accuracy: 0.78125\n",
      "At: 558 [==========>] Loss 0.21274747768552502  - accuracy: 0.71875\n",
      "At: 559 [==========>] Loss 0.27353395562278987  - accuracy: 0.65625\n",
      "At: 560 [==========>] Loss 0.1221139004030993  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.1430280046663306  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.07891180219661986  - accuracy: 0.875\n",
      "At: 563 [==========>] Loss 0.1193649532757109  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.1424283890112849  - accuracy: 0.78125\n",
      "At: 565 [==========>] Loss 0.1426483235317507  - accuracy: 0.8125\n",
      "At: 566 [==========>] Loss 0.2279664994837235  - accuracy: 0.71875\n",
      "At: 567 [==========>] Loss 0.18287618373176096  - accuracy: 0.78125\n",
      "At: 568 [==========>] Loss 0.2722730002788018  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.20091531516556987  - accuracy: 0.78125\n",
      "At: 570 [==========>] Loss 0.1413510652829032  - accuracy: 0.78125\n",
      "At: 571 [==========>] Loss 0.1555607093832489  - accuracy: 0.78125\n",
      "At: 572 [==========>] Loss 0.17597812012630987  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.09074849338809943  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.19509068199005836  - accuracy: 0.78125\n",
      "At: 575 [==========>] Loss 0.17358194364427512  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.09037490222607826  - accuracy: 0.90625\n",
      "At: 577 [==========>] Loss 0.22417446682891665  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.21207252473325583  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.145770682395768  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.17657229383087206  - accuracy: 0.71875\n",
      "At: 581 [==========>] Loss 0.16617735537308453  - accuracy: 0.75\n",
      "At: 582 [==========>] Loss 0.1785227412249391  - accuracy: 0.71875\n",
      "At: 583 [==========>] Loss 0.22038372162405237  - accuracy: 0.6875\n",
      "At: 584 [==========>] Loss 0.10992377709615428  - accuracy: 0.875\n",
      "At: 585 [==========>] Loss 0.23113530940182092  - accuracy: 0.6875\n",
      "At: 586 [==========>] Loss 0.03662223147924026  - accuracy: 0.96875\n",
      "At: 587 [==========>] Loss 0.1657045911653559  - accuracy: 0.78125\n",
      "At: 588 [==========>] Loss 0.18153143521226728  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.17867811948005313  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.07210567822183966  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.1240224404382693  - accuracy: 0.84375\n",
      "At: 592 [==========>] Loss 0.13346243591073456  - accuracy: 0.84375\n",
      "At: 593 [==========>] Loss 0.22331746565949087  - accuracy: 0.6875\n",
      "At: 594 [==========>] Loss 0.13651086209779806  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.14154037227891952  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.16339341916358127  - accuracy: 0.78125\n",
      "At: 597 [==========>] Loss 0.22139888039772673  - accuracy: 0.6875\n",
      "At: 598 [==========>] Loss 0.13510901494999883  - accuracy: 0.78125\n",
      "At: 599 [==========>] Loss 0.2066474474638074  - accuracy: 0.71875\n",
      "At: 600 [==========>] Loss 0.08474467963328372  - accuracy: 0.9375\n",
      "At: 601 [==========>] Loss 0.1439951995238329  - accuracy: 0.8125\n",
      "At: 602 [==========>] Loss 0.16022891494421354  - accuracy: 0.78125\n",
      "At: 603 [==========>] Loss 0.23447212264605438  - accuracy: 0.65625\n",
      "At: 604 [==========>] Loss 0.2652055728774282  - accuracy: 0.59375\n",
      "At: 605 [==========>] Loss 0.14074064698132246  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.14772819860299807  - accuracy: 0.8125\n",
      "At: 607 [==========>] Loss 0.1281511177427193  - accuracy: 0.875\n",
      "At: 608 [==========>] Loss 0.153394083497086  - accuracy: 0.78125\n",
      "At: 609 [==========>] Loss 0.1599036692169177  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.18321178354542333  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.1203175192239849  - accuracy: 0.90625\n",
      "At: 612 [==========>] Loss 0.11911841338377696  - accuracy: 0.875\n",
      "At: 613 [==========>] Loss 0.20333193511170888  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.13361266706990965  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.18820019082194028  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.21230387076967727  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.15064723227715282  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.26998055737015636  - accuracy: 0.59375\n",
      "At: 619 [==========>] Loss 0.14903320523164051  - accuracy: 0.875\n",
      "At: 620 [==========>] Loss 0.19152685432568117  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.07735924387292323  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.17710137479869942  - accuracy: 0.78125\n",
      "At: 623 [==========>] Loss 0.1170872100572726  - accuracy: 0.875\n",
      "At: 624 [==========>] Loss 0.13538933645561774  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.1974526443121247  - accuracy: 0.75\n",
      "At: 626 [==========>] Loss 0.1791499557105787  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.11529384958631442  - accuracy: 0.875\n",
      "At: 628 [==========>] Loss 0.11751224285022074  - accuracy: 0.875\n",
      "At: 629 [==========>] Loss 0.22345717381655267  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.22476493837622374  - accuracy: 0.6875\n",
      "At: 631 [==========>] Loss 0.17023136001678157  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.1774434967472097  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.19281605824572068  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.17355579174560476  - accuracy: 0.75\n",
      "At: 635 [==========>] Loss 0.13457557585865454  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.15856831529081933  - accuracy: 0.78125\n",
      "At: 637 [==========>] Loss 0.14289875427438004  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.16813904250854936  - accuracy: 0.78125\n",
      "At: 639 [==========>] Loss 0.16333165306097663  - accuracy: 0.75\n",
      "At: 640 [==========>] Loss 0.2726433242292805  - accuracy: 0.625\n",
      "At: 641 [==========>] Loss 0.18256913007371262  - accuracy: 0.75\n",
      "At: 642 [==========>] Loss 0.21892693921105982  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.1908506920626891  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.11200976229293783  - accuracy: 0.875\n",
      "At: 645 [==========>] Loss 0.151538570555365  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.12494014356140548  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.23567855308395824  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.16694228080172419  - accuracy: 0.8125\n",
      "At: 649 [==========>] Loss 0.17083557660102555  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.11655913796961856  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.17737588488748374  - accuracy: 0.75\n",
      "At: 652 [==========>] Loss 0.11505635465691763  - accuracy: 0.8125\n",
      "At: 653 [==========>] Loss 0.12898828608314988  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.12196018734202765  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.12425421113088439  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.11156241126350608  - accuracy: 0.875\n",
      "At: 657 [==========>] Loss 0.14406982374874372  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.11706786951122428  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.17323384632110048  - accuracy: 0.71875\n",
      "At: 660 [==========>] Loss 0.1718292753974649  - accuracy: 0.78125\n",
      "At: 661 [==========>] Loss 0.13637373517390544  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.11823870886162215  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.09932636137366989  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.1675249294669296  - accuracy: 0.8125\n",
      "At: 665 [==========>] Loss 0.22625772446641146  - accuracy: 0.6875\n",
      "At: 666 [==========>] Loss 0.21961137879463685  - accuracy: 0.6875\n",
      "At: 667 [==========>] Loss 0.15937264085287411  - accuracy: 0.75\n",
      "At: 668 [==========>] Loss 0.15993230630922922  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.14770112235627478  - accuracy: 0.78125\n",
      "At: 670 [==========>] Loss 0.21261848817239015  - accuracy: 0.6875\n",
      "At: 671 [==========>] Loss 0.13529873214497803  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.11900064214459709  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.0853179451101835  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.11641869926736056  - accuracy: 0.875\n",
      "At: 675 [==========>] Loss 0.13437599719848553  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.2210873657296567  - accuracy: 0.71875\n",
      "At: 677 [==========>] Loss 0.14485601919252727  - accuracy: 0.71875\n",
      "At: 678 [==========>] Loss 0.16817189940918031  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.13458778322892478  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.11641768507245283  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.12128170289524906  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.1644364104774882  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.173877863795874  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.09594568166318654  - accuracy: 0.90625\n",
      "At: 685 [==========>] Loss 0.14919075805463922  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.08302419707894362  - accuracy: 0.84375\n",
      "At: 687 [==========>] Loss 0.07484034431386902  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.1277629419104253  - accuracy: 0.84375\n",
      "At: 689 [==========>] Loss 0.1978466010875256  - accuracy: 0.75\n",
      "At: 690 [==========>] Loss 0.16628265384575547  - accuracy: 0.75\n",
      "At: 691 [==========>] Loss 0.12525089719494445  - accuracy: 0.8125\n",
      "At: 692 [==========>] Loss 0.11693110066836787  - accuracy: 0.8125\n",
      "At: 693 [==========>] Loss 0.14701153434734687  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.1737190128147711  - accuracy: 0.8125\n",
      "At: 695 [==========>] Loss 0.18989672312609393  - accuracy: 0.75\n",
      "At: 696 [==========>] Loss 0.19589603556139684  - accuracy: 0.71875\n",
      "At: 697 [==========>] Loss 0.21672638959356927  - accuracy: 0.6875\n",
      "At: 698 [==========>] Loss 0.12797245031214743  - accuracy: 0.84375\n",
      "At: 699 [==========>] Loss 0.17978281622663417  - accuracy: 0.78125\n",
      "At: 700 [==========>] Loss 0.113759147907702  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.16558281191839663  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.08199866252841506  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.15169357195794805  - accuracy: 0.8125\n",
      "At: 704 [==========>] Loss 0.2297618258257124  - accuracy: 0.6875\n",
      "At: 705 [==========>] Loss 0.19309868708459554  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.14343566779386535  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.1545096350518634  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.18299397349211655  - accuracy: 0.75\n",
      "At: 709 [==========>] Loss 0.14895003540076468  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.22113024382612645  - accuracy: 0.71875\n",
      "At: 711 [==========>] Loss 0.2723835192366054  - accuracy: 0.65625\n",
      "At: 712 [==========>] Loss 0.2124071177962443  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.21893690068053898  - accuracy: 0.6875\n",
      "At: 714 [==========>] Loss 0.26833470706270857  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.1238104808568015  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.1278788958086841  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.07564560907254089  - accuracy: 0.9375\n",
      "At: 718 [==========>] Loss 0.24982991631524953  - accuracy: 0.6875\n",
      "At: 719 [==========>] Loss 0.10744056383115294  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.13826455636417437  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.10262630031861708  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.17433987234778792  - accuracy: 0.84375\n",
      "At: 723 [==========>] Loss 0.11953284793734928  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.11027614604038691  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.182460872502428  - accuracy: 0.6875\n",
      "At: 726 [==========>] Loss 0.2056224840504834  - accuracy: 0.71875\n",
      "At: 727 [==========>] Loss 0.198916302379609  - accuracy: 0.71875\n",
      "At: 728 [==========>] Loss 0.2149975259215795  - accuracy: 0.71875\n",
      "At: 729 [==========>] Loss 0.23341354473117903  - accuracy: 0.625\n",
      "At: 730 [==========>] Loss 0.1943988873380635  - accuracy: 0.75\n",
      "At: 731 [==========>] Loss 0.13407068147413714  - accuracy: 0.8125\n",
      "At: 732 [==========>] Loss 0.1743374233081369  - accuracy: 0.71875\n",
      "At: 733 [==========>] Loss 0.15839775024507663  - accuracy: 0.8125\n",
      "At: 734 [==========>] Loss 0.22549725563533252  - accuracy: 0.65625\n",
      "At: 735 [==========>] Loss 0.17838562420996337  - accuracy: 0.78125\n",
      "At: 736 [==========>] Loss 0.12702624129107604  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.15317046435367576  - accuracy: 0.8125\n",
      "At: 738 [==========>] Loss 0.13175243603854578  - accuracy: 0.8125\n",
      "At: 739 [==========>] Loss 0.09989386013295351  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.18196287896326774  - accuracy: 0.75\n",
      "At: 741 [==========>] Loss 0.13692891725678685  - accuracy: 0.84375\n",
      "At: 742 [==========>] Loss 0.1650202853994597  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.15916464282487397  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.16224293079850363  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.23530080781096407  - accuracy: 0.6875\n",
      "At: 746 [==========>] Loss 0.15020938683517407  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.15290326532091794  - accuracy: 0.78125\n",
      "At: 748 [==========>] Loss 0.1785626800811681  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.17935957145360243  - accuracy: 0.78125\n",
      "At: 750 [==========>] Loss 0.13847849120057784  - accuracy: 0.8125\n",
      "At: 751 [==========>] Loss 0.1811816846279267  - accuracy: 0.75\n",
      "At: 752 [==========>] Loss 0.09880971413691272  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.2549678443463139  - accuracy: 0.625\n",
      "At: 754 [==========>] Loss 0.19064632253930464  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.12441101505555685  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.1896001762430103  - accuracy: 0.78125\n",
      "At: 757 [==========>] Loss 0.10334819001532179  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.1280338985782922  - accuracy: 0.84375\n",
      "At: 759 [==========>] Loss 0.09004372943692505  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.2274799553803013  - accuracy: 0.6875\n",
      "At: 761 [==========>] Loss 0.15591639469372343  - accuracy: 0.75\n",
      "At: 762 [==========>] Loss 0.14718647995106976  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.12574855537733756  - accuracy: 0.8125\n",
      "At: 764 [==========>] Loss 0.13588513611458491  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.15980518128588025  - accuracy: 0.75\n",
      "At: 766 [==========>] Loss 0.15058707652591846  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.10754957125229786  - accuracy: 0.90625\n",
      "At: 768 [==========>] Loss 0.16460063534157057  - accuracy: 0.8125\n",
      "At: 769 [==========>] Loss 0.12704058896560277  - accuracy: 0.84375\n",
      "At: 770 [==========>] Loss 0.09039017727919786  - accuracy: 0.90625\n",
      "At: 771 [==========>] Loss 0.19681822413145256  - accuracy: 0.6875\n",
      "At: 772 [==========>] Loss 0.15743424659416155  - accuracy: 0.78125\n",
      "At: 773 [==========>] Loss 0.06586316778568606  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.18210394577554279  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.2258462643002502  - accuracy: 0.6875\n",
      "At: 776 [==========>] Loss 0.19847791052292815  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.11314748462045573  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.16683319643792344  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.13237494164513852  - accuracy: 0.84375\n",
      "At: 780 [==========>] Loss 0.08539963638688369  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.1842026977983107  - accuracy: 0.71875\n",
      "At: 782 [==========>] Loss 0.18919415056550085  - accuracy: 0.75\n",
      "At: 783 [==========>] Loss 0.23500176645897358  - accuracy: 0.6875\n",
      "At: 784 [==========>] Loss 0.14726277508961572  - accuracy: 0.78125\n",
      "At: 785 [==========>] Loss 0.21358242295851515  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.12159316295975577  - accuracy: 0.84375\n",
      "At: 787 [==========>] Loss 0.15806245214383316  - accuracy: 0.75\n",
      "At: 788 [==========>] Loss 0.08554922514891486  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.1654800606857404  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.13143509915546525  - accuracy: 0.78125\n",
      "At: 791 [==========>] Loss 0.1822355346244981  - accuracy: 0.71875\n",
      "At: 792 [==========>] Loss 0.1431176704730957  - accuracy: 0.78125\n",
      "At: 793 [==========>] Loss 0.11325714962389638  - accuracy: 0.84375\n",
      "At: 794 [==========>] Loss 0.17324782309238923  - accuracy: 0.75\n",
      "At: 795 [==========>] Loss 0.15518630218654522  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.1514975924951278  - accuracy: 0.75\n",
      "At: 797 [==========>] Loss 0.1772567601898622  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.1831102651558852  - accuracy: 0.75\n",
      "At: 799 [==========>] Loss 0.0859497890158605  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.13189629662438873  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.09594958719500193  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.21495955927874658  - accuracy: 0.6875\n",
      "At: 803 [==========>] Loss 0.16803099472924649  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.15028080070783179  - accuracy: 0.875\n",
      "At: 805 [==========>] Loss 0.15079668073344227  - accuracy: 0.8125\n",
      "At: 806 [==========>] Loss 0.09700303104509568  - accuracy: 0.875\n",
      "At: 807 [==========>] Loss 0.13414396228703893  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.15034758154382213  - accuracy: 0.75\n",
      "At: 809 [==========>] Loss 0.09066987510525418  - accuracy: 0.875\n",
      "At: 810 [==========>] Loss 0.20982765849782037  - accuracy: 0.71875\n",
      "At: 811 [==========>] Loss 0.16636943580572178  - accuracy: 0.78125\n",
      "At: 812 [==========>] Loss 0.13275117515302517  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.20286640687872004  - accuracy: 0.65625\n",
      "At: 814 [==========>] Loss 0.16253017177329926  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.17328733677535607  - accuracy: 0.75\n",
      "At: 816 [==========>] Loss 0.1445598524148335  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.07675675311845545  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.1205334272427797  - accuracy: 0.90625\n",
      "At: 819 [==========>] Loss 0.12123393635176993  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.12068560008711648  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.15207499825838883  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.17364842612918457  - accuracy: 0.71875\n",
      "At: 823 [==========>] Loss 0.14426652030566928  - accuracy: 0.78125\n",
      "At: 824 [==========>] Loss 0.1682173702357709  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.24108652393501293  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.18506436128891307  - accuracy: 0.75\n",
      "At: 827 [==========>] Loss 0.09968543416743308  - accuracy: 0.84375\n",
      "At: 828 [==========>] Loss 0.0679261198137488  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.12588197884061447  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.12555586333213883  - accuracy: 0.78125\n",
      "At: 831 [==========>] Loss 0.09533117111508775  - accuracy: 0.875\n",
      "At: 832 [==========>] Loss 0.12438127398511106  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.2155176860356218  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.12426553213123337  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.10558884155622617  - accuracy: 0.8125\n",
      "At: 836 [==========>] Loss 0.13927808632460534  - accuracy: 0.8125\n",
      "At: 837 [==========>] Loss 0.11984666966286749  - accuracy: 0.84375\n",
      "At: 838 [==========>] Loss 0.1313607043253307  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.12567616906501336  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.11591081610495643  - accuracy: 0.84375\n",
      "At: 841 [==========>] Loss 0.08395475563193171  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.10156911797832334  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.16678052135593693  - accuracy: 0.78125\n",
      "At: 844 [==========>] Loss 0.11317722416185841  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.16895346100941427  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.13493023050050357  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.11121411940019177  - accuracy: 0.875\n",
      "At: 848 [==========>] Loss 0.1905488680011865  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.20677592431029168  - accuracy: 0.65625\n",
      "At: 850 [==========>] Loss 0.0822850938320433  - accuracy: 0.9375\n",
      "At: 851 [==========>] Loss 0.09407789804157786  - accuracy: 0.84375\n",
      "At: 852 [==========>] Loss 0.1614084347273108  - accuracy: 0.8125\n",
      "At: 853 [==========>] Loss 0.16985503616960407  - accuracy: 0.75\n",
      "At: 854 [==========>] Loss 0.2248138759282965  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.09473973693880802  - accuracy: 0.90625\n",
      "At: 856 [==========>] Loss 0.06297287060775453  - accuracy: 0.96875\n",
      "At: 857 [==========>] Loss 0.09422994695057359  - accuracy: 0.90625\n",
      "At: 858 [==========>] Loss 0.2616737298102483  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.10505374908964663  - accuracy: 0.8125\n",
      "At: 860 [==========>] Loss 0.14170999882376856  - accuracy: 0.78125\n",
      "At: 861 [==========>] Loss 0.10405347641533842  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.1233741683793454  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.19743291054303097  - accuracy: 0.71875\n",
      "At: 864 [==========>] Loss 0.16808430382988052  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.21051590240941193  - accuracy: 0.65625\n",
      "At: 866 [==========>] Loss 0.17563187842167546  - accuracy: 0.75\n",
      "At: 867 [==========>] Loss 0.08721372984939976  - accuracy: 0.9375\n",
      "At: 868 [==========>] Loss 0.2064288422061426  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.18468515858880222  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.14078951252620323  - accuracy: 0.78125\n",
      "At: 871 [==========>] Loss 0.09519598221187914  - accuracy: 0.875\n",
      "At: 872 [==========>] Loss 0.06929106492399152  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.15432426318529396  - accuracy: 0.8125\n",
      "At: 874 [==========>] Loss 0.17058581252031269  - accuracy: 0.71875\n",
      "At: 875 [==========>] Loss 0.11759500434535382  - accuracy: 0.84375\n",
      "At: 876 [==========>] Loss 0.11271406778156154  - accuracy: 0.8125\n",
      "At: 877 [==========>] Loss 0.1799729324713224  - accuracy: 0.71875\n",
      "At: 878 [==========>] Loss 0.0778448821409721  - accuracy: 0.9375\n",
      "At: 879 [==========>] Loss 0.17208147407261173  - accuracy: 0.71875\n",
      "At: 880 [==========>] Loss 0.13606312132635506  - accuracy: 0.75\n",
      "At: 881 [==========>] Loss 0.15470696987825577  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.13118570878424834  - accuracy: 0.8125\n",
      "At: 883 [==========>] Loss 0.15492748155557085  - accuracy: 0.75\n",
      "At: 884 [==========>] Loss 0.16910041039682333  - accuracy: 0.75\n",
      "At: 885 [==========>] Loss 0.11350478907048742  - accuracy: 0.8125\n",
      "At: 886 [==========>] Loss 0.11775908140519765  - accuracy: 0.75\n",
      "At: 887 [==========>] Loss 0.16324323976873886  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.2024229128409869  - accuracy: 0.625\n",
      "At: 889 [==========>] Loss 0.08000914743098336  - accuracy: 0.90625\n",
      "At: 890 [==========>] Loss 0.14422889456906374  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.12487874364397139  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.1586450252240919  - accuracy: 0.78125\n",
      "At: 893 [==========>] Loss 0.16720728400316665  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.12530955680418612  - accuracy: 0.84375\n",
      "At: 895 [==========>] Loss 0.12946945434807622  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.1515736513486989  - accuracy: 0.71875\n",
      "At: 897 [==========>] Loss 0.17193576742748548  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.14662827138810358  - accuracy: 0.8125\n",
      "At: 899 [==========>] Loss 0.11851338562595548  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.17378503552370994  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.16768016957516008  - accuracy: 0.8125\n",
      "At: 902 [==========>] Loss 0.10667837602229205  - accuracy: 0.9375\n",
      "At: 903 [==========>] Loss 0.16762205558058546  - accuracy: 0.75\n",
      "At: 904 [==========>] Loss 0.09166538297793711  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.10405271960529419  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.12084916532574859  - accuracy: 0.8125\n",
      "At: 907 [==========>] Loss 0.15486690846872808  - accuracy: 0.8125\n",
      "At: 908 [==========>] Loss 0.1468044990805606  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.1159321256991575  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.1369075812488078  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.15285014533683988  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.2043376599384721  - accuracy: 0.71875\n",
      "At: 913 [==========>] Loss 0.13203154420860103  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.13149919375307045  - accuracy: 0.84375\n",
      "At: 915 [==========>] Loss 0.15075181257360748  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.19208509710607208  - accuracy: 0.75\n",
      "At: 917 [==========>] Loss 0.16819457799130058  - accuracy: 0.78125\n",
      "At: 918 [==========>] Loss 0.18558164732283175  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.13490242004926423  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.11895274807949346  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.1497138041299269  - accuracy: 0.75\n",
      "At: 922 [==========>] Loss 0.09721219070188473  - accuracy: 0.875\n",
      "At: 923 [==========>] Loss 0.09351043798899866  - accuracy: 0.90625\n",
      "At: 924 [==========>] Loss 0.18307573790029924  - accuracy: 0.6875\n",
      "At: 925 [==========>] Loss 0.1677472371895829  - accuracy: 0.75\n",
      "At: 926 [==========>] Loss 0.10848559881168157  - accuracy: 0.875\n",
      "At: 927 [==========>] Loss 0.13484982746992272  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.10756518748586927  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.1388120010487805  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.12298255729689543  - accuracy: 0.8125\n",
      "At: 931 [==========>] Loss 0.18241278518617834  - accuracy: 0.75\n",
      "At: 932 [==========>] Loss 0.10759824064202547  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.10728393275555112  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.12886328060776228  - accuracy: 0.875\n",
      "At: 935 [==========>] Loss 0.047929570644003536  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.1333726603642969  - accuracy: 0.8125\n",
      "At: 937 [==========>] Loss 0.1726754092226816  - accuracy: 0.8125\n",
      "At: 938 [==========>] Loss 0.13944544537948209  - accuracy: 0.78125\n",
      "At: 939 [==========>] Loss 0.09942054119308893  - accuracy: 0.84375\n",
      "At: 940 [==========>] Loss 0.29645529099283985  - accuracy: 0.625\n",
      "At: 941 [==========>] Loss 0.12533421117533483  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.2020372874922658  - accuracy: 0.71875\n",
      "At: 943 [==========>] Loss 0.1553459014474749  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.1266695428239668  - accuracy: 0.78125\n",
      "At: 945 [==========>] Loss 0.10948462098041148  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.1438247088439625  - accuracy: 0.8125\n",
      "At: 947 [==========>] Loss 0.18392075985641576  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.18192256450332778  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.08864665989176787  - accuracy: 0.875\n",
      "At: 950 [==========>] Loss 0.1700019841178343  - accuracy: 0.71875\n",
      "At: 951 [==========>] Loss 0.11753075920719555  - accuracy: 0.8125\n",
      "At: 952 [==========>] Loss 0.12030100066272696  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.0721865163046438  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.13918764498411482  - accuracy: 0.78125\n",
      "At: 955 [==========>] Loss 0.13956088670036018  - accuracy: 0.875\n",
      "At: 956 [==========>] Loss 0.07972669557041331  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.1486495081585429  - accuracy: 0.78125\n",
      "At: 958 [==========>] Loss 0.07950920765252266  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.13613990157115619  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.12726261528454066  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.1164200514748607  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.1196124804140609  - accuracy: 0.8125\n",
      "At: 963 [==========>] Loss 0.10234323083024563  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.16054372747540446  - accuracy: 0.78125\n",
      "At: 965 [==========>] Loss 0.14643075872950145  - accuracy: 0.78125\n",
      "At: 966 [==========>] Loss 0.1487461167479267  - accuracy: 0.75\n",
      "At: 967 [==========>] Loss 0.11739382599635732  - accuracy: 0.78125\n",
      "At: 968 [==========>] Loss 0.17019188707580912  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.13077051519237132  - accuracy: 0.78125\n",
      "At: 970 [==========>] Loss 0.1295255219106584  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.1299826895890331  - accuracy: 0.78125\n",
      "At: 972 [==========>] Loss 0.09591530151473036  - accuracy: 0.875\n",
      "At: 973 [==========>] Loss 0.1583530055889903  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.08485037909417181  - accuracy: 0.9375\n",
      "At: 975 [==========>] Loss 0.1401454149005081  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.15719144408209632  - accuracy: 0.78125\n",
      "At: 977 [==========>] Loss 0.13963640098137464  - accuracy: 0.8125\n",
      "At: 978 [==========>] Loss 0.17527696649815377  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.11250838778667249  - accuracy: 0.90625\n",
      "At: 980 [==========>] Loss 0.1351996961520061  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.20771017150310872  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.05759839107469243  - accuracy: 0.9375\n",
      "At: 983 [==========>] Loss 0.16580721631278192  - accuracy: 0.75\n",
      "At: 984 [==========>] Loss 0.11026323050340142  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.16722897874704953  - accuracy: 0.8125\n",
      "At: 986 [==========>] Loss 0.13030281336391714  - accuracy: 0.84375\n",
      "At: 987 [==========>] Loss 0.17709139500808602  - accuracy: 0.75\n",
      "At: 988 [==========>] Loss 0.12839145857454204  - accuracy: 0.84375\n",
      "At: 989 [==========>] Loss 0.11593290323239798  - accuracy: 0.84375\n",
      "At: 990 [==========>] Loss 0.14501536266488113  - accuracy: 0.71875\n",
      "At: 991 [==========>] Loss 0.1763140682543815  - accuracy: 0.75\n",
      "At: 992 [==========>] Loss 0.1881973380146814  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.1501805191882878  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.17579310749057372  - accuracy: 0.71875\n",
      "At: 995 [==========>] Loss 0.16665403309606971  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.10236700987138428  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.1638718460484182  - accuracy: 0.75\n",
      "At: 998 [==========>] Loss 0.12089836041501774  - accuracy: 0.8125\n",
      "At: 999 [==========>] Loss 0.15634986122917227  - accuracy: 0.75\n",
      "At: 1000 [==========>] Loss 0.1954571987185958  - accuracy: 0.71875\n",
      "At: 1001 [==========>] Loss 0.16904538495935612  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.23269640538235709  - accuracy: 0.6875\n",
      "At: 1003 [==========>] Loss 0.14533145661372118  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.12037980448639268  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.11980989811474768  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.1404244845813819  - accuracy: 0.84375\n",
      "At: 1007 [==========>] Loss 0.12057716363412124  - accuracy: 0.875\n",
      "At: 1008 [==========>] Loss 0.1666318644651117  - accuracy: 0.78125\n",
      "At: 1009 [==========>] Loss 0.10026400951414915  - accuracy: 0.90625\n",
      "At: 1010 [==========>] Loss 0.1849303889324564  - accuracy: 0.75\n",
      "At: 1011 [==========>] Loss 0.17836915759902486  - accuracy: 0.75\n",
      "At: 1012 [==========>] Loss 0.10649158589066629  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.12349494840008252  - accuracy: 0.84375\n",
      "At: 1014 [==========>] Loss 0.09190037529275785  - accuracy: 0.90625\n",
      "At: 1015 [==========>] Loss 0.21885502837488116  - accuracy: 0.71875\n",
      "At: 1016 [==========>] Loss 0.18453344193540883  - accuracy: 0.71875\n",
      "At: 1017 [==========>] Loss 0.13957000114555612  - accuracy: 0.875\n",
      "At: 1018 [==========>] Loss 0.1657729779161806  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.16074410082989077  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.16116828706428582  - accuracy: 0.8125\n",
      "At: 1021 [==========>] Loss 0.13707947558776767  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.11477909320214501  - accuracy: 0.875\n",
      "At: 1023 [==========>] Loss 0.16238588346473337  - accuracy: 0.75\n",
      "At: 1024 [==========>] Loss 0.17286623824917552  - accuracy: 0.75\n",
      "At: 1025 [==========>] Loss 0.21923251667547233  - accuracy: 0.65625\n",
      "At: 1026 [==========>] Loss 0.11823671543380307  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.14364569348082473  - accuracy: 0.78125\n",
      "At: 1028 [==========>] Loss 0.19595003049282603  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.10103993405070143  - accuracy: 0.875\n",
      "At: 1030 [==========>] Loss 0.1186736106753706  - accuracy: 0.90625\n",
      "At: 1031 [==========>] Loss 0.14331013404922954  - accuracy: 0.8125\n",
      "At: 1032 [==========>] Loss 0.12313400286412958  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.1305848667485876  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.09963990741487913  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.10127955477195066  - accuracy: 0.8125\n",
      "At: 1036 [==========>] Loss 0.17647944567998095  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.13942968079241547  - accuracy: 0.8125\n",
      "At: 1038 [==========>] Loss 0.10302432846566367  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.1412586203759901  - accuracy: 0.8125\n",
      "At: 1040 [==========>] Loss 0.1346876550072326  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.1540424515165101  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.1369876390317568  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.208259688773052  - accuracy: 0.71875\n",
      "At: 1044 [==========>] Loss 0.12935351957492927  - accuracy: 0.84375\n",
      "At: 1045 [==========>] Loss 0.1584941716891468  - accuracy: 0.75\n",
      "At: 1046 [==========>] Loss 0.1434290080072193  - accuracy: 0.78125\n",
      "At: 1047 [==========>] Loss 0.17151957445795676  - accuracy: 0.78125\n",
      "At: 1048 [==========>] Loss 0.171469605936666  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.17124590942289986  - accuracy: 0.71875\n",
      "At: 1050 [==========>] Loss 0.17914422023957233  - accuracy: 0.6875\n",
      "At: 1051 [==========>] Loss 0.11708063914835545  - accuracy: 0.84375\n",
      "At: 1052 [==========>] Loss 0.10878731241801842  - accuracy: 0.8125\n",
      "At: 1053 [==========>] Loss 0.1354771087152447  - accuracy: 0.8125\n",
      "At: 1054 [==========>] Loss 0.13359961018424774  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.19750991865151707  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.1366325207005812  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.16385069061323582  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.08488063264917656  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.08843536633416899  - accuracy: 0.90625\n",
      "At: 1060 [==========>] Loss 0.12657564644665176  - accuracy: 0.875\n",
      "At: 1061 [==========>] Loss 0.12682092143612023  - accuracy: 0.78125\n",
      "At: 1062 [==========>] Loss 0.12488068669230193  - accuracy: 0.8125\n",
      "At: 1063 [==========>] Loss 0.1496584199938628  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.13589804886006468  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.11312193076729773  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.13381152990184908  - accuracy: 0.78125\n",
      "At: 1067 [==========>] Loss 0.15160935851624033  - accuracy: 0.75\n",
      "At: 1068 [==========>] Loss 0.11283817010649541  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.1167580481433656  - accuracy: 0.875\n",
      "At: 1070 [==========>] Loss 0.12616501748175757  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.13558874787643718  - accuracy: 0.78125\n",
      "At: 1072 [==========>] Loss 0.13783670863854353  - accuracy: 0.8125\n",
      "At: 1073 [==========>] Loss 0.17229163367569678  - accuracy: 0.78125\n",
      "At: 1074 [==========>] Loss 0.18249397366519973  - accuracy: 0.65625\n",
      "At: 1075 [==========>] Loss 0.10931465031456428  - accuracy: 0.8125\n",
      "At: 1076 [==========>] Loss 0.14037732183417942  - accuracy: 0.84375\n",
      "At: 1077 [==========>] Loss 0.10021961709350227  - accuracy: 0.84375\n",
      "At: 1078 [==========>] Loss 0.0839229551831534  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.1510958436904194  - accuracy: 0.78125\n",
      "At: 1080 [==========>] Loss 0.14578194271495443  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.12905919853008657  - accuracy: 0.78125\n",
      "At: 1082 [==========>] Loss 0.10126447293462329  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.11133351029637015  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.10569195494615892  - accuracy: 0.875\n",
      "At: 1085 [==========>] Loss 0.12174084432070176  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.15655785592201077  - accuracy: 0.78125\n",
      "At: 1087 [==========>] Loss 0.15740961989898597  - accuracy: 0.8125\n",
      "At: 1088 [==========>] Loss 0.16820594226751157  - accuracy: 0.8125\n",
      "At: 1089 [==========>] Loss 0.0975803287850857  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.1158282524280564  - accuracy: 0.875\n",
      "At: 1091 [==========>] Loss 0.19740112885480443  - accuracy: 0.75\n",
      "At: 1092 [==========>] Loss 0.10141069637653856  - accuracy: 0.875\n",
      "At: 1093 [==========>] Loss 0.1577360972628179  - accuracy: 0.78125\n",
      "At: 1094 [==========>] Loss 0.1582978032519051  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.129038795635153  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.12496510161514264  - accuracy: 0.78125\n",
      "At: 1097 [==========>] Loss 0.08361582153014302  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.17340865377376324  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.15783502159430668  - accuracy: 0.75\n",
      "At: 1100 [==========>] Loss 0.08213561715253323  - accuracy: 0.875\n",
      "At: 1101 [==========>] Loss 0.10543561829607513  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.15724674918196088  - accuracy: 0.78125\n",
      "At: 1103 [==========>] Loss 0.11777915988298943  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.06437478987440995  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.08148850562897975  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.10271540111050137  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.216575302308459  - accuracy: 0.65625\n",
      "At: 1108 [==========>] Loss 0.14797859348261916  - accuracy: 0.75\n",
      "At: 1109 [==========>] Loss 0.062394735020736156  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.11198553220964809  - accuracy: 0.8125\n",
      "At: 1111 [==========>] Loss 0.169451833127171  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.1656041630513607  - accuracy: 0.71875\n",
      "At: 1113 [==========>] Loss 0.16392573038221958  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.10024437874225511  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.134691751672811  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.1251948762163475  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.07337727725387738  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.13944910203839558  - accuracy: 0.78125\n",
      "At: 1119 [==========>] Loss 0.14446488847215047  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.07749173657750918  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.13405384301588835  - accuracy: 0.78125\n",
      "At: 1122 [==========>] Loss 0.1217878657620328  - accuracy: 0.875\n",
      "At: 1123 [==========>] Loss 0.1461432464649539  - accuracy: 0.75\n",
      "At: 1124 [==========>] Loss 0.12747621096422346  - accuracy: 0.875\n",
      "At: 1125 [==========>] Loss 0.1496009696900374  - accuracy: 0.78125\n",
      "At: 1126 [==========>] Loss 0.10814474438559983  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.12387401866994843  - accuracy: 0.84375\n",
      "At: 1128 [==========>] Loss 0.08303270374517535  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.1319819435474644  - accuracy: 0.84375\n",
      "At: 1130 [==========>] Loss 0.14883316320606782  - accuracy: 0.875\n",
      "At: 1131 [==========>] Loss 0.09298275354993141  - accuracy: 0.8125\n",
      "At: 1132 [==========>] Loss 0.09994412109564135  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.13467709826996815  - accuracy: 0.75\n",
      "At: 1134 [==========>] Loss 0.0906308684370351  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.1523309749884522  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.17842619426991219  - accuracy: 0.78125\n",
      "At: 1137 [==========>] Loss 0.13996882207515415  - accuracy: 0.8125\n",
      "At: 1138 [==========>] Loss 0.11699951585943848  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.10219837223256951  - accuracy: 0.875\n",
      "At: 1140 [==========>] Loss 0.14666002835731295  - accuracy: 0.78125\n",
      "At: 1141 [==========>] Loss 0.1358632845278575  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.14762738229500488  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.08494400716662306  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.10586784954765784  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.16687504372938855  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.13443776259867107  - accuracy: 0.8125\n",
      "At: 1147 [==========>] Loss 0.18564910390090522  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.0952343705430888  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.1165387057566469  - accuracy: 0.84375\n",
      "At: 1150 [==========>] Loss 0.1549946979325469  - accuracy: 0.78125\n",
      "At: 1151 [==========>] Loss 0.15010110049359168  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.12856710409853883  - accuracy: 0.8125\n",
      "At: 1153 [==========>] Loss 0.16682912136648798  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.12563489910891373  - accuracy: 0.875\n",
      "At: 1155 [==========>] Loss 0.07498520345904963  - accuracy: 0.9375\n",
      "At: 1156 [==========>] Loss 0.16012710619707873  - accuracy: 0.78125\n",
      "At: 1157 [==========>] Loss 0.14080555591063174  - accuracy: 0.78125\n",
      "At: 1158 [==========>] Loss 0.14209588511033266  - accuracy: 0.84375\n",
      "At: 1159 [==========>] Loss 0.10863455587484287  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.09995345301451294  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.11861236290982533  - accuracy: 0.78125\n",
      "At: 1162 [==========>] Loss 0.13186932437255047  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.2009851871259424  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.11807799805262523  - accuracy: 0.90625\n",
      "At: 1165 [==========>] Loss 0.19610899835686935  - accuracy: 0.71875\n",
      "At: 1166 [==========>] Loss 0.08917102370518092  - accuracy: 0.875\n",
      "At: 1167 [==========>] Loss 0.1535487133398533  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.11322860158284874  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.1141785522177411  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.1680371794677074  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.09692420175459274  - accuracy: 0.84375\n",
      "At: 1172 [==========>] Loss 0.11714418125604958  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.16327242936537423  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.19832343541807024  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.13115352039889822  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.1123370019797417  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.1095950606411343  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.1632257928989086  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.1627145496928859  - accuracy: 0.75\n",
      "At: 1180 [==========>] Loss 0.22029968616027315  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.11773518315298298  - accuracy: 0.875\n",
      "At: 1182 [==========>] Loss 0.14548620983233934  - accuracy: 0.84375\n",
      "At: 1183 [==========>] Loss 0.15654818181628283  - accuracy: 0.78125\n",
      "At: 1184 [==========>] Loss 0.15579474965133194  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.11304266711859001  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.12566884247268567  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.12379022377575459  - accuracy: 0.8125\n",
      "At: 1188 [==========>] Loss 0.07714452571046573  - accuracy: 0.96875\n",
      "At: 1189 [==========>] Loss 0.15368235562383772  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.11410744656917379  - accuracy: 0.84375\n",
      "At: 1191 [==========>] Loss 0.17975225797326952  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.10867617809055419  - accuracy: 0.84375\n",
      "At: 1193 [==========>] Loss 0.16840146104058726  - accuracy: 0.8125\n",
      "At: 1194 [==========>] Loss 0.14607259636034703  - accuracy: 0.78125\n",
      "At: 1195 [==========>] Loss 0.15313903863016828  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.13761654841484275  - accuracy: 0.75\n",
      "At: 1197 [==========>] Loss 0.09353367475284854  - accuracy: 0.9375\n",
      "At: 1198 [==========>] Loss 0.08927095251879492  - accuracy: 0.90625\n",
      "At: 1199 [==========>] Loss 0.20730255875452244  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.08991086735317205  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.13133470012138462  - accuracy: 0.8125\n",
      "At: 1202 [==========>] Loss 0.18008667345838303  - accuracy: 0.71875\n",
      "At: 1203 [==========>] Loss 0.1696395138473042  - accuracy: 0.71875\n",
      "At: 1204 [==========>] Loss 0.08404310324205638  - accuracy: 0.875\n",
      "At: 1205 [==========>] Loss 0.07081559187441779  - accuracy: 0.90625\n",
      "At: 1206 [==========>] Loss 0.11641841009676494  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.17540180543142286  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.10767859825256446  - accuracy: 0.84375\n",
      "At: 1209 [==========>] Loss 0.1121795732175072  - accuracy: 0.84375\n",
      "At: 1210 [==========>] Loss 0.17853106587065642  - accuracy: 0.75\n",
      "At: 1211 [==========>] Loss 0.13781888105032675  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.1214728344049652  - accuracy: 0.8125\n",
      "At: 1213 [==========>] Loss 0.17975195420102877  - accuracy: 0.6875\n",
      "At: 1214 [==========>] Loss 0.1512952581902562  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.16059017149466334  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.1265212248436478  - accuracy: 0.8125\n",
      "At: 1217 [==========>] Loss 0.08024142989315561  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.09895556471351  - accuracy: 0.875\n",
      "At: 1219 [==========>] Loss 0.12804434128761208  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.12870250885550316  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.08762578836359675  - accuracy: 0.90625\n",
      "At: 1222 [==========>] Loss 0.17379369279668364  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.10946134786549429  - accuracy: 0.8125\n",
      "At: 1224 [==========>] Loss 0.08159380880185937  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.09679776896490555  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.11201625678931884  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.14433224203975645  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.1421556209494712  - accuracy: 0.78125\n",
      "At: 1229 [==========>] Loss 0.14433159937896747  - accuracy: 0.6875\n",
      "At: 1230 [==========>] Loss 0.17993755982579981  - accuracy: 0.75\n",
      "At: 1231 [==========>] Loss 0.14504615542082117  - accuracy: 0.84375\n",
      "At: 1232 [==========>] Loss 0.07449723419737965  - accuracy: 0.96875\n",
      "At: 1233 [==========>] Loss 0.08844256268798827  - accuracy: 0.875\n",
      "At: 1234 [==========>] Loss 0.14841025763466273  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.10310347069569153  - accuracy: 0.84375\n",
      "At: 1236 [==========>] Loss 0.16879869302931422  - accuracy: 0.75\n",
      "At: 1237 [==========>] Loss 0.08157693996849533  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.12153099218392589  - accuracy: 0.8125\n",
      "At: 1239 [==========>] Loss 0.16599247177151816  - accuracy: 0.71875\n",
      "At: 1240 [==========>] Loss 0.10250586042251289  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.1328997127991182  - accuracy: 0.75\n",
      "At: 1242 [==========>] Loss 0.1509223290417892  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.12495587097916677  - accuracy: 0.84375\n",
      "At: 1244 [==========>] Loss 0.14620022690315992  - accuracy: 0.71875\n",
      "At: 1245 [==========>] Loss 0.13584735403281556  - accuracy: 0.78125\n",
      "At: 1246 [==========>] Loss 0.08136136910123946  - accuracy: 0.875\n",
      "At: 1247 [==========>] Loss 0.16227976119847248  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.13046222260588963  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.1433220222266393  - accuracy: 0.78125\n",
      "At: 1250 [==========>] Loss 0.1202437961783881  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.128896223141373  - accuracy: 0.78125\n",
      "At: 1252 [==========>] Loss 0.07759703235185214  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.0951156911057127  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.1936382326857665  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.09571167981212488  - accuracy: 0.875\n",
      "At: 1256 [==========>] Loss 0.14052651623423174  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.14342192315067465  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.06094034445885925  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.13618680311764292  - accuracy: 0.8125\n",
      "At: 1260 [==========>] Loss 0.13166492661256132  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.14702635278675266  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.13393486354729667  - accuracy: 0.75\n",
      "At: 1263 [==========>] Loss 0.12195422120123038  - accuracy: 0.8125\n",
      "At: 1264 [==========>] Loss 0.0889209600983002  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.12112668854008092  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.09104658511431996  - accuracy: 0.90625\n",
      "At: 1267 [==========>] Loss 0.12477254106782208  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.16255256020348133  - accuracy: 0.75\n",
      "At: 1269 [==========>] Loss 0.15376502272246767  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.1337264499715432  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.14119231423483997  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.07702531278527984  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.18130085157000395  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.12264950193003427  - accuracy: 0.8125\n",
      "At: 1275 [==========>] Loss 0.10016773303849524  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.12168818251524594  - accuracy: 0.78125\n",
      "At: 1277 [==========>] Loss 0.11046754345021986  - accuracy: 0.8125\n",
      "At: 1278 [==========>] Loss 0.14993176291412783  - accuracy: 0.84375\n",
      "At: 1279 [==========>] Loss 0.10732518008589005  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.10064390394152108  - accuracy: 0.875\n",
      "At: 1281 [==========>] Loss 0.19472628558432503  - accuracy: 0.65625\n",
      "At: 1282 [==========>] Loss 0.1612861114162476  - accuracy: 0.75\n",
      "At: 1283 [==========>] Loss 0.1479022202937677  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.14749901438017132  - accuracy: 0.78125\n",
      "At: 1285 [==========>] Loss 0.09865289815147821  - accuracy: 0.84375\n",
      "At: 1286 [==========>] Loss 0.14449225061577153  - accuracy: 0.90625\n",
      "At: 1287 [==========>] Loss 0.10565991422483763  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.17672640808836823  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.10974887486582735  - accuracy: 0.8125\n",
      "At: 1290 [==========>] Loss 0.1647397125272152  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.15004243871034875  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.1208130612033497  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.16127228156901263  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.11630706424306905  - accuracy: 0.90625\n",
      "At: 1295 [==========>] Loss 0.16760070335526162  - accuracy: 0.75\n",
      "At: 1296 [==========>] Loss 0.16730396370370787  - accuracy: 0.75\n",
      "At: 1297 [==========>] Loss 0.16100783962012546  - accuracy: 0.75\n",
      "At: 1298 [==========>] Loss 0.12109116686287139  - accuracy: 0.8125\n",
      "At: 1299 [==========>] Loss 0.16628585114635555  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.1288526948977839  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.13331684788305947  - accuracy: 0.78125\n",
      "At: 1302 [==========>] Loss 0.09400750441097575  - accuracy: 0.875\n",
      "At: 1303 [==========>] Loss 0.10909901082118177  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.11589701504283187  - accuracy: 0.78125\n",
      "At: 1305 [==========>] Loss 0.15337040733978952  - accuracy: 0.75\n",
      "At: 1306 [==========>] Loss 0.07640615670085932  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.1691572193644313  - accuracy: 0.6875\n",
      "At: 1308 [==========>] Loss 0.06418380388982269  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.19072665363069485  - accuracy: 0.71875\n",
      "At: 1310 [==========>] Loss 0.1481180886886132  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.14185488071344493  - accuracy: 0.8125\n",
      "At: 1312 [==========>] Loss 0.06096078560348085  - accuracy: 0.96875\n",
      "At: 1313 [==========>] Loss 0.176034681441925  - accuracy: 0.6875\n",
      "At: 1314 [==========>] Loss 0.059050520503720665  - accuracy: 0.96875\n",
      "At: 1315 [==========>] Loss 0.16875699538222744  - accuracy: 0.71875\n",
      "At: 1316 [==========>] Loss 0.13833618687264668  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.11303307054407755  - accuracy: 0.84375\n",
      "At: 1318 [==========>] Loss 0.14757785446205696  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.11982441796278237  - accuracy: 0.90625\n",
      "At: 1320 [==========>] Loss 0.14201034497223797  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.0810557256607912  - accuracy: 0.90625\n",
      "At: 1322 [==========>] Loss 0.13739876415183078  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.08743223050671178  - accuracy: 0.90625\n",
      "At: 1324 [==========>] Loss 0.16028627237901155  - accuracy: 0.75\n",
      "At: 1325 [==========>] Loss 0.09280483381914947  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.09740126149138939  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.13691249541000608  - accuracy: 0.78125\n",
      "At: 1328 [==========>] Loss 0.0885971587868197  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.060077368269365425  - accuracy: 0.96875\n",
      "At: 1330 [==========>] Loss 0.13000681637751144  - accuracy: 0.8125\n",
      "At: 1331 [==========>] Loss 0.15760502625338219  - accuracy: 0.8125\n",
      "At: 1332 [==========>] Loss 0.1339646280496843  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.14920670322421106  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.10464830221668689  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.10599905944990044  - accuracy: 0.875\n",
      "At: 1336 [==========>] Loss 0.0794929389331778  - accuracy: 0.875\n",
      "At: 1337 [==========>] Loss 0.15715393044393744  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.16122648344734364  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.13739431719096673  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.14491527709907903  - accuracy: 0.78125\n",
      "At: 1341 [==========>] Loss 0.1344749908399402  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.10937100973028199  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.19208781600960073  - accuracy: 0.78125\n",
      "At: 1344 [==========>] Loss 0.18483876944207694  - accuracy: 0.65625\n",
      "At: 1345 [==========>] Loss 0.08581056537254746  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.1062772996291658  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.10875173426716882  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09812532824809593  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.12776482552068755  - accuracy: 0.8125\n",
      "At: 1350 [==========>] Loss 0.13175722574186405  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.09008579137035677  - accuracy: 0.875\n",
      "At: 1352 [==========>] Loss 0.09838028519491114  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.1662992707665667  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.1460006352901623  - accuracy: 0.8125\n",
      "At: 1355 [==========>] Loss 0.09280056416905204  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.1284281945859424  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.09869918958006255  - accuracy: 0.875\n",
      "At: 1358 [==========>] Loss 0.11771872719196363  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.06136497680185631  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.17285570892081567  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.11198593000717691  - accuracy: 0.8125\n",
      "At: 1362 [==========>] Loss 0.13590296931788476  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.10017727015493823  - accuracy: 0.90625\n",
      "At: 1364 [==========>] Loss 0.13576153694590626  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.09875128028938635  - accuracy: 0.875\n",
      "At: 1366 [==========>] Loss 0.12256791861669723  - accuracy: 0.8125\n",
      "At: 1367 [==========>] Loss 0.11494851379249969  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.17160378210915497  - accuracy: 0.71875\n",
      "At: 1369 [==========>] Loss 0.0767457425262559  - accuracy: 0.96875\n",
      "At: 1370 [==========>] Loss 0.11829683608081984  - accuracy: 0.84375\n",
      "At: 1371 [==========>] Loss 0.14854704127888718  - accuracy: 0.875\n",
      "At: 1372 [==========>] Loss 0.14209365275129254  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.12394302864120163  - accuracy: 0.84375\n",
      "At: 1374 [==========>] Loss 0.1476603391483981  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.13901735962718398  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.12323998431769645  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.16502968092183967  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.11640290886180304  - accuracy: 0.875\n",
      "At: 1379 [==========>] Loss 0.15157826842454988  - accuracy: 0.71875\n",
      "At: 1380 [==========>] Loss 0.12163408372613778  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.08441934685377966  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.14569588652057353  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.10029060277071761  - accuracy: 0.8125\n",
      "At: 1384 [==========>] Loss 0.09109766664586247  - accuracy: 0.84375\n",
      "At: 1385 [==========>] Loss 0.14529620963059353  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.16905339099032615  - accuracy: 0.75\n",
      "At: 1387 [==========>] Loss 0.06280181667673092  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.15787901003189936  - accuracy: 0.84375\n",
      "At: 1389 [==========>] Loss 0.11011272953781044  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.14058325225455387  - accuracy: 0.84375\n",
      "At: 1391 [==========>] Loss 0.12120332810590544  - accuracy: 0.84375\n",
      "At: 1392 [==========>] Loss 0.08026991936640349  - accuracy: 0.875\n",
      "At: 1393 [==========>] Loss 0.14455567746741071  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.09315115400305443  - accuracy: 0.84375\n",
      "At: 1395 [==========>] Loss 0.2060975121652836  - accuracy: 0.6875\n",
      "At: 1396 [==========>] Loss 0.060820111447175126  - accuracy: 0.96875\n",
      "At: 1397 [==========>] Loss 0.12409259742698356  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.11644407951960503  - accuracy: 0.78125\n",
      "At: 1399 [==========>] Loss 0.11927903973295018  - accuracy: 0.84375\n",
      "At: 1400 [==========>] Loss 0.14066426939995824  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.10766044600564825  - accuracy: 0.78125\n",
      "At: 1402 [==========>] Loss 0.12691072890968924  - accuracy: 0.875\n",
      "At: 1403 [==========>] Loss 0.1285445866510484  - accuracy: 0.78125\n",
      "At: 1404 [==========>] Loss 0.11665662295697682  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.09473550604619574  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.18010952637302574  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.14488378256594991  - accuracy: 0.75\n",
      "At: 1408 [==========>] Loss 0.14051573718097926  - accuracy: 0.78125\n",
      "At: 1409 [==========>] Loss 0.05844381275491879  - accuracy: 0.9375\n",
      "At: 1410 [==========>] Loss 0.16021249786053604  - accuracy: 0.75\n",
      "At: 1411 [==========>] Loss 0.14441307500475864  - accuracy: 0.78125\n",
      "At: 1412 [==========>] Loss 0.10568580970761732  - accuracy: 0.875\n",
      "At: 1413 [==========>] Loss 0.10199781085771616  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.17714433011836792  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.09671495744304295  - accuracy: 0.90625\n",
      "At: 1416 [==========>] Loss 0.14520737556547955  - accuracy: 0.78125\n",
      "At: 1417 [==========>] Loss 0.10518495638852712  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.16600595936943713  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.11867014973693829  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.0972566066305047  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.10291301496095653  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.12638102106201285  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.1682277324967021  - accuracy: 0.75\n",
      "At: 1424 [==========>] Loss 0.12981750310753093  - accuracy: 0.875\n",
      "At: 1425 [==========>] Loss 0.07511534424465  - accuracy: 0.9375\n",
      "At: 1426 [==========>] Loss 0.11951452257699198  - accuracy: 0.84375\n",
      "At: 1427 [==========>] Loss 0.11745970807564875  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.08714358115591957  - accuracy: 0.90625\n",
      "At: 1429 [==========>] Loss 0.19282086020661826  - accuracy: 0.78125\n",
      "At: 1430 [==========>] Loss 0.07364273892260614  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.11481818359551864  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.11369735489537447  - accuracy: 0.78125\n",
      "At: 1433 [==========>] Loss 0.1092932463612626  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.14890844285330898  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.12537940452267388  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.09053266638648255  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.11607648949862875  - accuracy: 0.90625\n",
      "At: 1438 [==========>] Loss 0.18412763330926615  - accuracy: 0.71875\n",
      "At: 1439 [==========>] Loss 0.133875044961975  - accuracy: 0.84375\n",
      "At: 1440 [==========>] Loss 0.09918038134895686  - accuracy: 0.875\n",
      "At: 1441 [==========>] Loss 0.10048271826272348  - accuracy: 0.8125\n",
      "At: 1442 [==========>] Loss 0.09796785863256857  - accuracy: 0.875\n",
      "At: 1443 [==========>] Loss 0.11967233436853704  - accuracy: 0.84375\n",
      "At: 1444 [==========>] Loss 0.09634252833198895  - accuracy: 0.84375\n",
      "At: 1445 [==========>] Loss 0.18369142839644362  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.18081692239984593  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.17019807766854506  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.09391099822015773  - accuracy: 0.875\n",
      "At: 1449 [==========>] Loss 0.13560504768669202  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.12663711589743806  - accuracy: 0.78125\n",
      "At: 1451 [==========>] Loss 0.09934459008189361  - accuracy: 0.875\n",
      "At: 1452 [==========>] Loss 0.1163458566402084  - accuracy: 0.8125\n",
      "At: 1453 [==========>] Loss 0.06089802267887104  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.15698296572527873  - accuracy: 0.78125\n",
      "At: 1455 [==========>] Loss 0.1133314395991073  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.11548928606583904  - accuracy: 0.84375\n",
      "At: 1457 [==========>] Loss 0.06945546997041838  - accuracy: 0.9375\n",
      "At: 1458 [==========>] Loss 0.1374627126635277  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.11158034041067032  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.15469002741645604  - accuracy: 0.8125\n",
      "At: 1461 [==========>] Loss 0.12026044052419664  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.17009228058531894  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.0870992264001384  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.17226468396984623  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.10501335711952822  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.0956610230802798  - accuracy: 0.875\n",
      "At: 1467 [==========>] Loss 0.20380509645796935  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.16242542382377362  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.18071079806568585  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.13921791273923906  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.15193981475929486  - accuracy: 0.71875\n",
      "At: 1472 [==========>] Loss 0.08043772432410701  - accuracy: 0.875\n",
      "At: 1473 [==========>] Loss 0.1259003072089594  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.2039169877622849  - accuracy: 0.65625\n",
      "At: 1475 [==========>] Loss 0.17416845009003987  - accuracy: 0.75\n",
      "At: 1476 [==========>] Loss 0.12934701239491347  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.07578237547087993  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.0973193151567871  - accuracy: 0.875\n",
      "At: 1479 [==========>] Loss 0.11016492594159015  - accuracy: 0.875\n",
      "At: 1480 [==========>] Loss 0.07761174448826928  - accuracy: 0.90625\n",
      "At: 1481 [==========>] Loss 0.12888498865144873  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.11756709661945222  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.24272684394713906  - accuracy: 0.65625\n",
      "At: 1484 [==========>] Loss 0.14001684512279322  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.1733076104455322  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.09157409923677903  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.07193230254682531  - accuracy: 0.90625\n",
      "At: 1488 [==========>] Loss 0.14297719324758523  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.1906989371893139  - accuracy: 0.78125\n",
      "At: 1490 [==========>] Loss 0.09486347061205219  - accuracy: 0.8125\n",
      "At: 1491 [==========>] Loss 0.16185265592178658  - accuracy: 0.78125\n",
      "At: 1492 [==========>] Loss 0.1514254153947795  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.17804370061043767  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.16162947136765315  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.1381378293387691  - accuracy: 0.75\n",
      "At: 1496 [==========>] Loss 0.07680096162720916  - accuracy: 0.9375\n",
      "At: 1497 [==========>] Loss 0.15383286921305975  - accuracy: 0.78125\n",
      "At: 1498 [==========>] Loss 0.16803084046648814  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.11343114675599447  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.10566922501215054  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.11473611693660313  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.13939554196548815  - accuracy: 0.78125\n",
      "At: 1503 [==========>] Loss 0.12615254337694254  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.14212916438255258  - accuracy: 0.8125\n",
      "At: 1505 [==========>] Loss 0.13760796309851808  - accuracy: 0.84375\n",
      "At: 1506 [==========>] Loss 0.12308791261416815  - accuracy: 0.8125\n",
      "At: 1507 [==========>] Loss 0.14264443820045217  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.18910638271092145  - accuracy: 0.75\n",
      "At: 1509 [==========>] Loss 0.10395084462302748  - accuracy: 0.875\n",
      "At: 1510 [==========>] Loss 0.13021157651947968  - accuracy: 0.8125\n",
      "At: 1511 [==========>] Loss 0.12206229009682007  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.09611068895239505  - accuracy: 0.875\n",
      "At: 1513 [==========>] Loss 0.14363468795950204  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.13112526628349144  - accuracy: 0.84375\n",
      "At: 1515 [==========>] Loss 0.1426400367595937  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.10598005156289084  - accuracy: 0.84375\n",
      "At: 1517 [==========>] Loss 0.1666788127777764  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.10362796456577415  - accuracy: 0.875\n",
      "At: 1519 [==========>] Loss 0.1330991519931776  - accuracy: 0.84375\n",
      "At: 1520 [==========>] Loss 0.08315244277679479  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.07797004746186428  - accuracy: 0.90625\n",
      "At: 1522 [==========>] Loss 0.1805297128223689  - accuracy: 0.71875\n",
      "At: 1523 [==========>] Loss 0.12331603018759454  - accuracy: 0.8125\n",
      "At: 1524 [==========>] Loss 0.13016096998571736  - accuracy: 0.84375\n",
      "At: 1525 [==========>] Loss 0.1259392473055112  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.12012344691609762  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.1483995284215026  - accuracy: 0.78125\n",
      "At: 1528 [==========>] Loss 0.14315593613533603  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.09003813529178381  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.05755348018574703  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.13228514573318986  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.16934784407830855  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.1544689865651881  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.10784680388320231  - accuracy: 0.875\n",
      "At: 1535 [==========>] Loss 0.16598301464164772  - accuracy: 0.75\n",
      "At: 1536 [==========>] Loss 0.161277783982735  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.11521679808637827  - accuracy: 0.875\n",
      "At: 1538 [==========>] Loss 0.1415044836790183  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.07328813826783206  - accuracy: 0.875\n",
      "At: 1540 [==========>] Loss 0.15356372023421805  - accuracy: 0.6875\n",
      "At: 1541 [==========>] Loss 0.10615763040304294  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.07484540413642592  - accuracy: 0.90625\n",
      "At: 1543 [==========>] Loss 0.12171059797454169  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.1447234949984959  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.20240225120218996  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.13351570166720195  - accuracy: 0.8125\n",
      "At: 1547 [==========>] Loss 0.15146576255139763  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.1354235470259934  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.14022838868068174  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.08610792507916196  - accuracy: 0.84375\n",
      "At: 1551 [==========>] Loss 0.16083696598738026  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.10031336755949527  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.0815470840004202  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.1238741427126608  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.12432675673575068  - accuracy: 0.875\n",
      "At: 1556 [==========>] Loss 0.15697657515641317  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.08735565926708005  - accuracy: 0.8125\n",
      "At: 1558 [==========>] Loss 0.15976411335070262  - accuracy: 0.78125\n",
      "At: 1559 [==========>] Loss 0.07415291623768402  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.14322332752757422  - accuracy: 0.78125\n",
      "At: 1561 [==========>] Loss 0.14029324872343735  - accuracy: 0.84375\n",
      "At: 1562 [==========>] Loss 0.09466223454542302  - accuracy: 0.84375\n",
      "At: 1563 [==========>] Loss 0.11521196073302145  - accuracy: 0.875\n",
      "At: 1564 [==========>] Loss 0.10913098990274556  - accuracy: 0.8125\n",
      "At: 1565 [==========>] Loss 0.14258957060220578  - accuracy: 0.78125\n",
      "At: 1566 [==========>] Loss 0.1111287318332704  - accuracy: 0.90625\n",
      "At: 1567 [==========>] Loss 0.15698468073234104  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.07950234979477365  - accuracy: 0.90625\n",
      "At: 1569 [==========>] Loss 0.12109833021334279  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.11377956753722568  - accuracy: 0.8125\n",
      "At: 1571 [==========>] Loss 0.14759310799298586  - accuracy: 0.71875\n",
      "At: 1572 [==========>] Loss 0.13076878937803982  - accuracy: 0.875\n",
      "At: 1573 [==========>] Loss 0.07493397730073767  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.1094157572852918  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.12796886991998235  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.13656111249182656  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.07441708257900106  - accuracy: 0.9375\n",
      "At: 1578 [==========>] Loss 0.0656883606986043  - accuracy: 0.875\n",
      "At: 1579 [==========>] Loss 0.1098347895357519  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.1059389269506814  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.11602487768844942  - accuracy: 0.8125\n",
      "At: 1582 [==========>] Loss 0.16649418561436832  - accuracy: 0.8125\n",
      "At: 1583 [==========>] Loss 0.10568430574310157  - accuracy: 0.8125\n",
      "At: 1584 [==========>] Loss 0.1267915610642944  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.10317224248224971  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.1428315118586219  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.10552361414935642  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.13372773108645772  - accuracy: 0.78125\n",
      "At: 1589 [==========>] Loss 0.15919790500410014  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.1556262794096826  - accuracy: 0.75\n",
      "At: 1591 [==========>] Loss 0.11695004508302659  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.08841121877870545  - accuracy: 0.90625\n",
      "At: 1593 [==========>] Loss 0.16781218706929985  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.10145032257007996  - accuracy: 0.84375\n",
      "At: 1595 [==========>] Loss 0.14749784839200558  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.1847068797819147  - accuracy: 0.71875\n",
      "At: 1597 [==========>] Loss 0.11753836538896995  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.17779125827214698  - accuracy: 0.71875\n",
      "At: 1599 [==========>] Loss 0.21705800044807982  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.15541785361688454  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.07970296043888733  - accuracy: 0.96875\n",
      "At: 1602 [==========>] Loss 0.15365795869757043  - accuracy: 0.71875\n",
      "At: 1603 [==========>] Loss 0.1867847001395891  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.2718349828454272  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.09023918899929383  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.13013810910976717  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.1809289104407984  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.15872452032539075  - accuracy: 0.75\n",
      "At: 1609 [==========>] Loss 0.15432173850683092  - accuracy: 0.75\n",
      "At: 1610 [==========>] Loss 0.16454643603948177  - accuracy: 0.84375\n",
      "At: 1611 [==========>] Loss 0.08130626375048623  - accuracy: 0.90625\n",
      "At: 1612 [==========>] Loss 0.09484625182148589  - accuracy: 0.875\n",
      "At: 1613 [==========>] Loss 0.14241073540469842  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.14994729066810056  - accuracy: 0.75\n",
      "At: 1615 [==========>] Loss 0.0943904272935116  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.14362650603363575  - accuracy: 0.71875\n",
      "At: 1617 [==========>] Loss 0.09983027996096568  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11282721630805066  - accuracy: 0.8125\n",
      "At: 1619 [==========>] Loss 0.18748269554206243  - accuracy: 0.71875\n",
      "At: 1620 [==========>] Loss 0.11341553998436268  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.12721426954311235  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.1744503142064187  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.0880454222795026  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.1639024081285868  - accuracy: 0.78125\n",
      "At: 1625 [==========>] Loss 0.13248152789167328  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.11070486245926354  - accuracy: 0.78125\n",
      "At: 1627 [==========>] Loss 0.08594919143517038  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.1402584471895598  - accuracy: 0.84375\n",
      "At: 1629 [==========>] Loss 0.1362496232268509  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.08568866380352194  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.11838927199785225  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.11015010043659487  - accuracy: 0.84375\n",
      "At: 1633 [==========>] Loss 0.07271017000501465  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.10810654509562073  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.2037837082637256  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.11178764130075555  - accuracy: 0.78125\n",
      "At: 1637 [==========>] Loss 0.08663165951814064  - accuracy: 0.9375\n",
      "At: 1638 [==========>] Loss 0.18355016058404325  - accuracy: 0.71875\n",
      "At: 1639 [==========>] Loss 0.15882496447968802  - accuracy: 0.6875\n",
      "At: 1640 [==========>] Loss 0.12916955278416548  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.10719467545648345  - accuracy: 0.8125\n",
      "At: 1642 [==========>] Loss 0.11334569241460253  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.11222104965093897  - accuracy: 0.8125\n",
      "At: 1644 [==========>] Loss 0.11830969480827143  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.06372895841819441  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.14182912104605416  - accuracy: 0.78125\n",
      "At: 1647 [==========>] Loss 0.17270989013890986  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.13081458005989158  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.07858342550242031  - accuracy: 0.9375\n",
      "At: 1650 [==========>] Loss 0.11951923724577618  - accuracy: 0.8125\n",
      "At: 1651 [==========>] Loss 0.11144408148314232  - accuracy: 0.90625\n",
      "At: 1652 [==========>] Loss 0.07815038047878253  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.08190248905485328  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.057999813826601396  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.15989845785350396  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.10575184389032907  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.12783154739540625  - accuracy: 0.84375\n",
      "At: 1658 [==========>] Loss 0.2089399967782816  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.12164840316943659  - accuracy: 0.8125\n",
      "At: 1660 [==========>] Loss 0.0438077960746131  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.08690669724146093  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.10753414396857285  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.17464999062229036  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.10769389972513269  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.11038459865942973  - accuracy: 0.90625\n",
      "At: 1666 [==========>] Loss 0.08858983168421325  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.14643833091933994  - accuracy: 0.78125\n",
      "At: 1668 [==========>] Loss 0.12446757702401093  - accuracy: 0.78125\n",
      "At: 1669 [==========>] Loss 0.12964411838594445  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.05678431053068406  - accuracy: 0.9375\n",
      "At: 1671 [==========>] Loss 0.1363270885154007  - accuracy: 0.78125\n",
      "At: 1672 [==========>] Loss 0.15377623208622954  - accuracy: 0.71875\n",
      "At: 1673 [==========>] Loss 0.073185714120556  - accuracy: 0.9375\n",
      "At: 1674 [==========>] Loss 0.1679685979398491  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.1771946055855605  - accuracy: 0.78125\n",
      "At: 1676 [==========>] Loss 0.19759332177052916  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.09817389271152481  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.1802409795364519  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.11344414357495838  - accuracy: 0.8125\n",
      "At: 1680 [==========>] Loss 0.142070761087748  - accuracy: 0.8125\n",
      "At: 1681 [==========>] Loss 0.10747973169025501  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.0990975237020506  - accuracy: 0.90625\n",
      "At: 1683 [==========>] Loss 0.16415367520511617  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.11511756939019913  - accuracy: 0.90625\n",
      "At: 1685 [==========>] Loss 0.1070795768599906  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.14693996365725293  - accuracy: 0.8125\n",
      "At: 1687 [==========>] Loss 0.13860063186433746  - accuracy: 0.78125\n",
      "At: 1688 [==========>] Loss 0.06750228403857955  - accuracy: 0.90625\n",
      "At: 1689 [==========>] Loss 0.13085684306860892  - accuracy: 0.78125\n",
      "At: 1690 [==========>] Loss 0.1343927052760921  - accuracy: 0.75\n",
      "At: 1691 [==========>] Loss 0.1226522347819973  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.2042693274936059  - accuracy: 0.65625\n",
      "At: 1693 [==========>] Loss 0.10660457632931641  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.09568152789688836  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.16021849778353764  - accuracy: 0.71875\n",
      "At: 1696 [==========>] Loss 0.14124248317352045  - accuracy: 0.8125\n",
      "At: 1697 [==========>] Loss 0.0935284778740534  - accuracy: 0.84375\n",
      "At: 1698 [==========>] Loss 0.10017018431396323  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.11254271005596543  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.11533242735303655  - accuracy: 0.875\n",
      "At: 1701 [==========>] Loss 0.10643833666004729  - accuracy: 0.84375\n",
      "At: 1702 [==========>] Loss 0.10441904268873434  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.171000910878487  - accuracy: 0.78125\n",
      "At: 1704 [==========>] Loss 0.06723739510379409  - accuracy: 0.9375\n",
      "At: 1705 [==========>] Loss 0.12691626524327815  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.13124120054629246  - accuracy: 0.84375\n",
      "At: 1707 [==========>] Loss 0.18448693167798147  - accuracy: 0.71875\n",
      "At: 1708 [==========>] Loss 0.09809764938166698  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.16785247301135875  - accuracy: 0.75\n",
      "At: 1710 [==========>] Loss 0.16253343960812197  - accuracy: 0.78125\n",
      "At: 1711 [==========>] Loss 0.07665480022329585  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.13638937146816746  - accuracy: 0.78125\n",
      "At: 1713 [==========>] Loss 0.10678952087470614  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.1627071612110801  - accuracy: 0.75\n",
      "At: 1715 [==========>] Loss 0.1146156630315934  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06137279273518496  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.09095057639836643  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.13691856141222833  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.12275241934772296  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.05984497400371326  - accuracy: 0.90625\n",
      "At: 1721 [==========>] Loss 0.16242036975432583  - accuracy: 0.8125\n",
      "At: 1722 [==========>] Loss 0.08428213154092973  - accuracy: 0.875\n",
      "At: 1723 [==========>] Loss 0.2037969906613076  - accuracy: 0.65625\n",
      "At: 1724 [==========>] Loss 0.09612269463764014  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.15819463271479584  - accuracy: 0.71875\n",
      "At: 1726 [==========>] Loss 0.1365789637210942  - accuracy: 0.8125\n",
      "At: 1727 [==========>] Loss 0.10865453738541164  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.1194317210395325  - accuracy: 0.75\n",
      "At: 1729 [==========>] Loss 0.1657494691287334  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.14441701114390126  - accuracy: 0.78125\n",
      "At: 1731 [==========>] Loss 0.11577273623832679  - accuracy: 0.8125\n",
      "At: 1732 [==========>] Loss 0.08768531935686338  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.17432785835693881  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.10265951765442424  - accuracy: 0.875\n",
      "At: 1735 [==========>] Loss 0.1527467914045545  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.12532529744609272  - accuracy: 0.875\n",
      "At: 1737 [==========>] Loss 0.14008780896805817  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.11311691934731964  - accuracy: 0.84375\n",
      "At: 1739 [==========>] Loss 0.1316133843315597  - accuracy: 0.8125\n",
      "At: 1740 [==========>] Loss 0.16469656528309784  - accuracy: 0.75\n",
      "At: 1741 [==========>] Loss 0.1486625193809923  - accuracy: 0.75\n",
      "At: 1742 [==========>] Loss 0.0370341832163773  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.15989767741260258  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.09785547190731676  - accuracy: 0.9375\n",
      "At: 1745 [==========>] Loss 0.12133522137699315  - accuracy: 0.8125\n",
      "At: 1746 [==========>] Loss 0.16195644792674538  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.1058282955482565  - accuracy: 0.875\n",
      "At: 1748 [==========>] Loss 0.12094301002627922  - accuracy: 0.90625\n",
      "At: 1749 [==========>] Loss 0.12494269372306901  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.13419352180689298  - accuracy: 0.84375\n",
      "At: 1751 [==========>] Loss 0.17253196812965932  - accuracy: 0.8125\n",
      "At: 1752 [==========>] Loss 0.1146740336550374  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.08025652405653433  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.09954603843438109  - accuracy: 0.90625\n",
      "At: 1755 [==========>] Loss 0.08338592788544824  - accuracy: 0.84375\n",
      "At: 1756 [==========>] Loss 0.1449834319743875  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.16061837263196507  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.060376038468257696  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.11594622837208804  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.09987258450824621  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.13495922432094343  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.14918250311202574  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.10143272375946269  - accuracy: 0.875\n",
      "At: 1764 [==========>] Loss 0.12385720414436141  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.11569118353981422  - accuracy: 0.875\n",
      "At: 1766 [==========>] Loss 0.06991495841336987  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.07011364449586718  - accuracy: 0.9375\n",
      "At: 1768 [==========>] Loss 0.11422258632626944  - accuracy: 0.78125\n",
      "At: 1769 [==========>] Loss 0.06620851200576969  - accuracy: 0.9375\n",
      "At: 1770 [==========>] Loss 0.06918765482886297  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.16627725376395036  - accuracy: 0.8125\n",
      "At: 1772 [==========>] Loss 0.16470372950860973  - accuracy: 0.78125\n",
      "At: 1773 [==========>] Loss 0.09742261329102195  - accuracy: 0.84375\n",
      "At: 1774 [==========>] Loss 0.16060434454796232  - accuracy: 0.8125\n",
      "At: 1775 [==========>] Loss 0.10514489025121994  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.11134974694995105  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11364391877170711  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.0968692577007191  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.09875279450902241  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.10664133432474732  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.16932490998265687  - accuracy: 0.78125\n",
      "At: 1782 [==========>] Loss 0.12436235602151792  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.13818626548889917  - accuracy: 0.78125\n",
      "At: 1784 [==========>] Loss 0.08962452661506543  - accuracy: 0.875\n",
      "At: 1785 [==========>] Loss 0.09817038568868394  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.13723643315755307  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.13422066545941208  - accuracy: 0.75\n",
      "At: 1788 [==========>] Loss 0.11487047639255243  - accuracy: 0.8125\n",
      "At: 1789 [==========>] Loss 0.11122465695720377  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.1696734163099627  - accuracy: 0.71875\n",
      "At: 1791 [==========>] Loss 0.0785686855664632  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.13976349075694536  - accuracy: 0.8125\n",
      "At: 1793 [==========>] Loss 0.08864096657261736  - accuracy: 0.90625\n",
      "At: 1794 [==========>] Loss 0.1615544423823197  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.07713821734706507  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.14103485064243815  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.09962896106136157  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.12993859567484056  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.07408376224743943  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.0844415501609986  - accuracy: 0.90625\n",
      "At: 1801 [==========>] Loss 0.17667067097052122  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.15334685726647782  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.1578968639265247  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.11863282817811782  - accuracy: 0.84375\n",
      "At: 1805 [==========>] Loss 0.048925784197467515  - accuracy: 0.9375\n",
      "At: 1806 [==========>] Loss 0.15916772135732013  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.18299702819144475  - accuracy: 0.75\n",
      "At: 1808 [==========>] Loss 0.15538783511531223  - accuracy: 0.84375\n",
      "At: 1809 [==========>] Loss 0.08464055713560881  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.12352731972106436  - accuracy: 0.8125\n",
      "At: 1811 [==========>] Loss 0.1342028522031819  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.11931530167698308  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.12729090079398467  - accuracy: 0.8125\n",
      "At: 1814 [==========>] Loss 0.10360709823818129  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.15142632736162792  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.04888971201530097  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.13570527476255237  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.11757536066096466  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.1814073584758038  - accuracy: 0.71875\n",
      "At: 1820 [==========>] Loss 0.11693700273391996  - accuracy: 0.8125\n",
      "At: 1821 [==========>] Loss 0.10462232343202921  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.1678067971149254  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.16800128286566285  - accuracy: 0.78125\n",
      "At: 1824 [==========>] Loss 0.18334395403826415  - accuracy: 0.6875\n",
      "At: 1825 [==========>] Loss 0.1320252101381601  - accuracy: 0.78125\n",
      "At: 1826 [==========>] Loss 0.06315758345321444  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.0965006624434602  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.14767526229685574  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.15475127482544376  - accuracy: 0.84375\n",
      "At: 1830 [==========>] Loss 0.13057145813125492  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.1428067478184713  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.1234302598600527  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.11522795514045617  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.07544066797199295  - accuracy: 0.9375\n",
      "At: 1835 [==========>] Loss 0.15855218733010956  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.10050171371207274  - accuracy: 0.8125\n",
      "At: 1837 [==========>] Loss 0.047409503057204906  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.09402386503985108  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.10873682460152914  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.12284693587302087  - accuracy: 0.8125\n",
      "At: 1841 [==========>] Loss 0.10774694454303246  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.1151788897265241  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.13833227429730754  - accuracy: 0.78125\n",
      "At: 1844 [==========>] Loss 0.10185297871774943  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.17795589903604897  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.13429080836157029  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05609106458717122  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.056984579740127654  - accuracy: 0.90625\n",
      "At: 1849 [==========>] Loss 0.1718470299462626  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.037046536928633425  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.16837338174220035  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.0871316563849657  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.11435565073962206  - accuracy: 0.90625\n",
      "At: 1854 [==========>] Loss 0.12475364566208197  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.18334638574748888  - accuracy: 0.75\n",
      "At: 1856 [==========>] Loss 0.14134840304840846  - accuracy: 0.8125\n",
      "At: 1857 [==========>] Loss 0.15729041333646934  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.11481686196008403  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.1538641437932849  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.15539425305092536  - accuracy: 0.75\n",
      "At: 1861 [==========>] Loss 0.0899246770324009  - accuracy: 0.875\n",
      "At: 1862 [==========>] Loss 0.1677721414047948  - accuracy: 0.78125\n",
      "At: 1863 [==========>] Loss 0.14972900865526517  - accuracy: 0.78125\n",
      "At: 1864 [==========>] Loss 0.14331508275155538  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.11432191965737191  - accuracy: 0.84375\n",
      "At: 1866 [==========>] Loss 0.19309733529588824  - accuracy: 0.65625\n",
      "At: 1867 [==========>] Loss 0.10437386184687392  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.15261968614106705  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.14858381614510086  - accuracy: 0.78125\n",
      "At: 1870 [==========>] Loss 0.09446511140690486  - accuracy: 0.875\n",
      "At: 1871 [==========>] Loss 0.14316750876286044  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.15038081763558203  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.0889378618158907  - accuracy: 0.9375\n",
      "At: 1874 [==========>] Loss 0.16307735756745392  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.10822177481209687  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.21442539674450817  - accuracy: 0.71875\n",
      "At: 1877 [==========>] Loss 0.06582202781764829  - accuracy: 0.9375\n",
      "At: 1878 [==========>] Loss 0.11657669992381879  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.1262985214984079  - accuracy: 0.90625\n",
      "At: 1880 [==========>] Loss 0.10905930058023286  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.09664270532456054  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.1044465351983955  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.1544810521753166  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.10968220113658246  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.1071908532512234  - accuracy: 0.78125\n",
      "At: 1886 [==========>] Loss 0.14538842927060916  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.09118908637878106  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.15117640700683577  - accuracy: 0.78125\n",
      "At: 1889 [==========>] Loss 0.094614041146985  - accuracy: 0.875\n",
      "At: 1890 [==========>] Loss 0.14991804046174156  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.08106523110402607  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.08523572769246222  - accuracy: 0.9375\n",
      "At: 1893 [==========>] Loss 0.07952839885306634  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.10188329560960946  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.07287938903242593  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.12788947104085296  - accuracy: 0.84375\n",
      "At: 1897 [==========>] Loss 0.06808388096235084  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.1239708415470892  - accuracy: 0.84375\n",
      "At: 1899 [==========>] Loss 0.0886329833983219  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.1251703875487672  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.11524967542712511  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.14873276100332924  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.12063055442908253  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.058862317782458574  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.12520958951421576  - accuracy: 0.8125\n",
      "At: 1906 [==========>] Loss 0.1130833149726251  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.0808734590522185  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.08183871175209709  - accuracy: 0.875\n",
      "At: 1909 [==========>] Loss 0.09860999526626546  - accuracy: 0.90625\n",
      "At: 1910 [==========>] Loss 0.07067563609152556  - accuracy: 0.84375\n",
      "At: 1911 [==========>] Loss 0.11545250170211739  - accuracy: 0.875\n",
      "At: 1912 [==========>] Loss 0.14866966334150689  - accuracy: 0.8125\n",
      "At: 1913 [==========>] Loss 0.17168682036576238  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.07765111992328036  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.12334802307926943  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.13034116739486604  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.16280207342669809  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.16983690209884483  - accuracy: 0.75\n",
      "At: 1919 [==========>] Loss 0.09396379792425856  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.09082261860661835  - accuracy: 0.875\n",
      "At: 1921 [==========>] Loss 0.11594150355488714  - accuracy: 0.875\n",
      "At: 1922 [==========>] Loss 0.14421074950316937  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.1864715274875322  - accuracy: 0.6875\n",
      "At: 1924 [==========>] Loss 0.1273019127225736  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.1563913129088891  - accuracy: 0.84375\n",
      "At: 1926 [==========>] Loss 0.09884467391831374  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.11569165700757701  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.1002654093199137  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.1691009067079729  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.1639906321369352  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.11062038004513762  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.1549931540549091  - accuracy: 0.8125\n",
      "At: 1933 [==========>] Loss 0.09426252396467005  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.14518923466585157  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.11665002376146097  - accuracy: 0.875\n",
      "At: 1936 [==========>] Loss 0.08288441513055028  - accuracy: 0.9375\n",
      "At: 1937 [==========>] Loss 0.14933427273378108  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.16205947427826023  - accuracy: 0.78125\n",
      "At: 1939 [==========>] Loss 0.09766219693048625  - accuracy: 0.8125\n",
      "At: 1940 [==========>] Loss 0.12057478264298553  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.13062195907909216  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.1488095501838998  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.1393114053885468  - accuracy: 0.8125\n",
      "At: 1944 [==========>] Loss 0.13046961598641105  - accuracy: 0.78125\n",
      "At: 1945 [==========>] Loss 0.14626146988853989  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.111118427547619  - accuracy: 0.8125\n",
      "At: 1947 [==========>] Loss 0.13701943097191055  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.10704387390563241  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.059748951878988585  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.14255380325711633  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.12407414844462203  - accuracy: 0.75\n",
      "At: 1952 [==========>] Loss 0.09043839027808043  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.07246576348768867  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.18001787896188676  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.0772559221424949  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.10509214638397812  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.06596860136768085  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.09176737031368615  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.11448305826597832  - accuracy: 0.84375\n",
      "At: 1960 [==========>] Loss 0.05602004452052359  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.19548481697239894  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.2188594470013631  - accuracy: 0.71875\n",
      "At: 1963 [==========>] Loss 0.10709586831404334  - accuracy: 0.875\n",
      "At: 1964 [==========>] Loss 0.18782074445504313  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.13873706263086422  - accuracy: 0.8125\n",
      "At: 1966 [==========>] Loss 0.118764339040906  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.11365378637364514  - accuracy: 0.84375\n",
      "At: 1968 [==========>] Loss 0.16913180338226932  - accuracy: 0.75\n",
      "At: 1969 [==========>] Loss 0.1599062867229673  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.09926880909375269  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.20870536124474842  - accuracy: 0.6875\n",
      "At: 1972 [==========>] Loss 0.08395073870202735  - accuracy: 0.84375\n",
      "At: 1973 [==========>] Loss 0.138411365917185  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.11902960381543064  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.14935057005363578  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.06963185063199212  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.10095113350194018  - accuracy: 0.90625\n",
      "At: 1978 [==========>] Loss 0.15017724524069648  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.11674078937299905  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.10105759880573076  - accuracy: 0.90625\n",
      "At: 1981 [==========>] Loss 0.1587784238424537  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06747744614840742  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.12955055177818353  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.09031917642390863  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.12396130672985234  - accuracy: 0.875\n",
      "At: 1986 [==========>] Loss 0.1769555134620726  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.11929592249314241  - accuracy: 0.8125\n",
      "At: 1988 [==========>] Loss 0.10312396791335876  - accuracy: 0.84375\n",
      "At: 1989 [==========>] Loss 0.11139551334343897  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.12134290759026323  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.1519547539455575  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.12379414037135966  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.12933646013738867  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.12811408260270957  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.17004998678249122  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.0993350929438902  - accuracy: 0.90625\n",
      "At: 1997 [==========>] Loss 0.18057136111868563  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.12946862450826027  - accuracy: 0.78125\n",
      "At: 1999 [==========>] Loss 0.0893329822999061  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.16706239715550947  - accuracy: 0.71875\n",
      "At: 2001 [==========>] Loss 0.08522841538688941  - accuracy: 0.84375\n",
      "At: 2002 [==========>] Loss 0.08426498555983714  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.1099293692963351  - accuracy: 0.875\n",
      "At: 2004 [==========>] Loss 0.16382300978312225  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.14144572481733675  - accuracy: 0.78125\n",
      "At: 2006 [==========>] Loss 0.14009433973905477  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.10144718753492138  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.1269855287297877  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.15938797987229364  - accuracy: 0.8125\n",
      "At: 2010 [==========>] Loss 0.13688324287997616  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.1230439053383228  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.1109758330063583  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09805144851025269  - accuracy: 0.875\n",
      "At: 2014 [==========>] Loss 0.20919663469280886  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.06933416717312853  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.11138633141853116  - accuracy: 0.875\n",
      "At: 2017 [==========>] Loss 0.09362908950268344  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.09203282241347636  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.11519270572125001  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.08465148709866109  - accuracy: 0.875\n",
      "At: 2021 [==========>] Loss 0.12437006097139759  - accuracy: 0.8125\n",
      "At: 2022 [==========>] Loss 0.12901876601928638  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.1074895865390148  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.11940491107024406  - accuracy: 0.78125\n",
      "At: 2025 [==========>] Loss 0.1686866418252207  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.10293027024237603  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.15858877780400665  - accuracy: 0.71875\n",
      "At: 2028 [==========>] Loss 0.11397928599174871  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.14251130479835136  - accuracy: 0.78125\n",
      "At: 2030 [==========>] Loss 0.1607831342705266  - accuracy: 0.78125\n",
      "At: 2031 [==========>] Loss 0.16560178682409776  - accuracy: 0.6875\n",
      "At: 2032 [==========>] Loss 0.15333444995955478  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.15145344027734617  - accuracy: 0.8125\n",
      "At: 2034 [==========>] Loss 0.20014184510436656  - accuracy: 0.71875\n",
      "At: 2035 [==========>] Loss 0.12148948229204674  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.10160604774824901  - accuracy: 0.84375\n",
      "At: 2037 [==========>] Loss 0.11680218392530632  - accuracy: 0.8125\n",
      "At: 2038 [==========>] Loss 0.10300376512688196  - accuracy: 0.875\n",
      "At: 2039 [==========>] Loss 0.10309558895015608  - accuracy: 0.875\n",
      "At: 2040 [==========>] Loss 0.10981222289120979  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.07256837870292432  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.12059940239447924  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.08344940242891954  - accuracy: 0.90625\n",
      "At: 2044 [==========>] Loss 0.0747934378657336  - accuracy: 0.90625\n",
      "At: 2045 [==========>] Loss 0.24861245199604262  - accuracy: 0.65625\n",
      "At: 2046 [==========>] Loss 0.06464080552666003  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08384219464157239  - accuracy: 0.9375\n",
      "At: 2048 [==========>] Loss 0.10241134254955775  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.15400904044950478  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.14678066787418664  - accuracy: 0.71875\n",
      "At: 2051 [==========>] Loss 0.15837674511350347  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.0803864052880102  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.11113100581736599  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.12553242456952146  - accuracy: 0.875\n",
      "At: 2055 [==========>] Loss 0.06878221193022871  - accuracy: 0.96875\n",
      "At: 2056 [==========>] Loss 0.13284704464284147  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.11743979272071228  - accuracy: 0.75\n",
      "At: 2058 [==========>] Loss 0.14928558019929777  - accuracy: 0.75\n",
      "At: 2059 [==========>] Loss 0.20112827194741145  - accuracy: 0.625\n",
      "At: 2060 [==========>] Loss 0.13775010993536804  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.1605464843398742  - accuracy: 0.84375\n",
      "At: 2062 [==========>] Loss 0.11646969999442275  - accuracy: 0.84375\n",
      "At: 2063 [==========>] Loss 0.1077158262497731  - accuracy: 0.90625\n",
      "At: 2064 [==========>] Loss 0.16918048362222787  - accuracy: 0.78125\n",
      "At: 2065 [==========>] Loss 0.0516350341241438  - accuracy: 0.90625\n",
      "At: 2066 [==========>] Loss 0.15525351906236676  - accuracy: 0.75\n",
      "At: 2067 [==========>] Loss 0.09516847487098595  - accuracy: 0.8125\n",
      "At: 2068 [==========>] Loss 0.10051205860023829  - accuracy: 0.84375\n",
      "At: 2069 [==========>] Loss 0.11778174268844332  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.11486307227234355  - accuracy: 0.875\n",
      "At: 2071 [==========>] Loss 0.12332085413769601  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.0663180309440118  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.11901038359913653  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.10379671405708822  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.11683436737630902  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.12003394420667983  - accuracy: 0.84375\n",
      "At: 2077 [==========>] Loss 0.12168419994107388  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.08256705953857295  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07690325840406675  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.10703087205232825  - accuracy: 0.84375\n",
      "At: 2081 [==========>] Loss 0.14363259966672842  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.1282558189757081  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.18172250972922116  - accuracy: 0.65625\n",
      "At: 2084 [==========>] Loss 0.09101389462573087  - accuracy: 0.9375\n",
      "At: 2085 [==========>] Loss 0.09085977750472088  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.10013957187481115  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.18384813536226374  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.08159147094227966  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.1470854861074296  - accuracy: 0.71875\n",
      "At: 2090 [==========>] Loss 0.10203819598143615  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.12068696943035437  - accuracy: 0.8125\n",
      "At: 2092 [==========>] Loss 0.10658078361614128  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.14720691282922488  - accuracy: 0.78125\n",
      "At: 2094 [==========>] Loss 0.14166977906638972  - accuracy: 0.78125\n",
      "At: 2095 [==========>] Loss 0.10889950287254004  - accuracy: 0.90625\n",
      "At: 2096 [==========>] Loss 0.1560754698241184  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.11648994878914966  - accuracy: 0.84375\n",
      "At: 2098 [==========>] Loss 0.12960845888780723  - accuracy: 0.78125\n",
      "At: 2099 [==========>] Loss 0.11742516750591324  - accuracy: 0.84375\n",
      "At: 2100 [==========>] Loss 0.07612381033627104  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.17087388829680478  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.10143879036806736  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.15683496031067884  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.09636949043847864  - accuracy: 0.90625\n",
      "At: 2105 [==========>] Loss 0.17440981486950297  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.12357112425553105  - accuracy: 0.84375\n",
      "At: 2107 [==========>] Loss 0.08578925976205387  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.13659142838631783  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.10309211435531165  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.06244251139731459  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.11828832808008727  - accuracy: 0.78125\n",
      "At: 2112 [==========>] Loss 0.12210536229648294  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.09779423442402375  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.15822661341342908  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.1254665215008858  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.11561758463313843  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.10654534749865559  - accuracy: 0.8125\n",
      "At: 2118 [==========>] Loss 0.11838529838522226  - accuracy: 0.8125\n",
      "At: 2119 [==========>] Loss 0.09539204205071704  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.15620254965745992  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.15704362968804908  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.13887296265170138  - accuracy: 0.75\n",
      "At: 2123 [==========>] Loss 0.1718606168970285  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.1449513875865567  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.07745329000802605  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.06113865760772834  - accuracy: 0.90625\n",
      "At: 2127 [==========>] Loss 0.11328903804787982  - accuracy: 0.90625\n",
      "At: 2128 [==========>] Loss 0.1129124071321326  - accuracy: 0.875\n",
      "At: 2129 [==========>] Loss 0.13182402269948162  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.0722648506380103  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.09660551078453994  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.21300329527539186  - accuracy: 0.6875\n",
      "At: 2133 [==========>] Loss 0.1323965879512656  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.11517737518838475  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.08117860665924656  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.13512517248155712  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.12781738543071727  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.10709184950359336  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.17447782479201435  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.09140672961807059  - accuracy: 0.84375\n",
      "At: 2141 [==========>] Loss 0.12466767532681813  - accuracy: 0.8125\n",
      "At: 2142 [==========>] Loss 0.09972776984962711  - accuracy: 0.875\n",
      "At: 2143 [==========>] Loss 0.11153007021219605  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.07854501262491831  - accuracy: 0.9375\n",
      "At: 2145 [==========>] Loss 0.09005746921688304  - accuracy: 0.875\n",
      "At: 2146 [==========>] Loss 0.14392724398236748  - accuracy: 0.75\n",
      "At: 2147 [==========>] Loss 0.13463568381650876  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.18034412231021557  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.10682386325327825  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.10647238645545609  - accuracy: 0.8125\n",
      "At: 2151 [==========>] Loss 0.0883891482837538  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.210730661913348  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.1722242924575465  - accuracy: 0.65625\n",
      "At: 2154 [==========>] Loss 0.13968806772118297  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.14301587358445866  - accuracy: 0.84375\n",
      "At: 2156 [==========>] Loss 0.10644577564061194  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.1332965550973677  - accuracy: 0.78125\n",
      "At: 2158 [==========>] Loss 0.1287404544671841  - accuracy: 0.8125\n",
      "At: 2159 [==========>] Loss 0.08102539830204832  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.14602800156741655  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.08946797391008472  - accuracy: 0.90625\n",
      "At: 2162 [==========>] Loss 0.09480490913536269  - accuracy: 0.9375\n",
      "At: 2163 [==========>] Loss 0.12873989190908952  - accuracy: 0.875\n",
      "At: 2164 [==========>] Loss 0.17096124604637405  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.09797699854188145  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.10683699179733895  - accuracy: 0.90625\n",
      "At: 2167 [==========>] Loss 0.08982674231257283  - accuracy: 0.9375\n",
      "At: 2168 [==========>] Loss 0.09604819265504151  - accuracy: 0.8125\n",
      "At: 2169 [==========>] Loss 0.13076374350457726  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11624391189856839  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.1423074307968485  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.1058993576501192  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.13103823079216664  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.10711666744382917  - accuracy: 0.875\n",
      "At: 2175 [==========>] Loss 0.12656379713885954  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.12174490290680892  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.13858878556948528  - accuracy: 0.78125\n",
      "At: 2178 [==========>] Loss 0.10821105342114987  - accuracy: 0.8125\n",
      "At: 2179 [==========>] Loss 0.09964590254880337  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.09956824568133062  - accuracy: 0.90625\n",
      "At: 2181 [==========>] Loss 0.14559318515881628  - accuracy: 0.8125\n",
      "At: 2182 [==========>] Loss 0.12554421754203643  - accuracy: 0.71875\n",
      "At: 2183 [==========>] Loss 0.23350647466712282  - accuracy: 0.65625\n",
      "At: 2184 [==========>] Loss 0.0860593451690906  - accuracy: 0.9375\n",
      "At: 2185 [==========>] Loss 0.12446580900969531  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.15216127511601246  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.14478884692875488  - accuracy: 0.8125\n",
      "At: 2188 [==========>] Loss 0.06894844172582981  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.0793226019166441  - accuracy: 0.875\n",
      "At: 2190 [==========>] Loss 0.11808096297024374  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.11802456862003526  - accuracy: 0.84375\n",
      "At: 2192 [==========>] Loss 0.1053103595054119  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.15134119252131645  - accuracy: 0.75\n",
      "At: 2194 [==========>] Loss 0.11260802810504611  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.17034745343685065  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.1403337522772983  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.09963623860354251  - accuracy: 0.84375\n",
      "At: 2198 [==========>] Loss 0.08800040031862394  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.050707303787257116  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.048783651009721554  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.0889942340630073  - accuracy: 0.90625\n",
      "At: 2202 [==========>] Loss 0.08625965566533393  - accuracy: 0.8125\n",
      "At: 2203 [==========>] Loss 0.09936185201186043  - accuracy: 0.84375\n",
      "At: 2204 [==========>] Loss 0.11149617815889439  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.10240162790290988  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.11079453930866301  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.10879935581274525  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.1520599616069254  - accuracy: 0.84375\n",
      "At: 2209 [==========>] Loss 0.18644450691047354  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.11260496121078868  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.14395075137317997  - accuracy: 0.8125\n",
      "At: 2212 [==========>] Loss 0.08830338748560002  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.1158450269123717  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.13604912548259634  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.13818842611789306  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.12887851512954443  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.1231884984474978  - accuracy: 0.84375\n",
      "At: 2218 [==========>] Loss 0.16732972429955725  - accuracy: 0.84375\n",
      "At: 2219 [==========>] Loss 0.08459267185860611  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.13818283402092008  - accuracy: 0.8125\n",
      "At: 2221 [==========>] Loss 0.1688511971064719  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.1033471977700477  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.15775854599164765  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.14708631966023755  - accuracy: 0.78125\n",
      "At: 2225 [==========>] Loss 0.14893458468480814  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.125658132260099  - accuracy: 0.875\n",
      "At: 2227 [==========>] Loss 0.17413748539963997  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.08479046915259017  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.1703730049541552  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.10705185716625995  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.14201362337372514  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.16258861097268842  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.1593632196481338  - accuracy: 0.8125\n",
      "At: 2234 [==========>] Loss 0.15418265677818388  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.1275708463414323  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.08588825180348278  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.1457652321808939  - accuracy: 0.78125\n",
      "At: 2238 [==========>] Loss 0.15456613415322273  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.17672600461544907  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.1506172525989696  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.1377122070006862  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.15817952264192836  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.07572752216071944  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.09306869806844498  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.06858222781739323  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.14563572985161116  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.10479161153989063  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.16967454750868685  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.11440817787798527  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.06971392057586695  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.07685989678099733  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.13330338221456167  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.13196865108429162  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.14646282644122782  - accuracy: 0.75\n",
      "At: 2255 [==========>] Loss 0.11960146741058869  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.15885101580527922  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.10314826626365212  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.13378619321916052  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.11782506864190742  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.1840179140185933  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.12621307563820997  - accuracy: 0.8125\n",
      "At: 2262 [==========>] Loss 0.13469046926762843  - accuracy: 0.84375\n",
      "At: 2263 [==========>] Loss 0.15646671430258843  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.10509612753986107  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.1270657947140712  - accuracy: 0.75\n",
      "At: 2266 [==========>] Loss 0.1310173760304716  - accuracy: 0.78125\n",
      "At: 2267 [==========>] Loss 0.07362611541936517  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.09574032123880283  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.03646678787686486  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.10476727888660418  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.14526193185634567  - accuracy: 0.78125\n",
      "At: 2272 [==========>] Loss 0.0770644494824117  - accuracy: 0.9375\n",
      "At: 2273 [==========>] Loss 0.11263047382846939  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.10815377970220749  - accuracy: 0.84375\n",
      "At: 2275 [==========>] Loss 0.09887795802628888  - accuracy: 0.8125\n",
      "At: 2276 [==========>] Loss 0.09326717076472807  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.1411109629275576  - accuracy: 0.84375\n",
      "At: 2278 [==========>] Loss 0.10467052900624696  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.14717545536420196  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.15564669939383982  - accuracy: 0.8125\n",
      "At: 2281 [==========>] Loss 0.12532941822649266  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.0729197804841493  - accuracy: 0.90625\n",
      "At: 2283 [==========>] Loss 0.14817452858778468  - accuracy: 0.75\n",
      "At: 2284 [==========>] Loss 0.13128331122714698  - accuracy: 0.78125\n",
      "At: 2285 [==========>] Loss 0.12458495504374587  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.1200785740964882  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.12362222527090831  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.09563313759221706  - accuracy: 0.84375\n",
      "At: 2289 [==========>] Loss 0.1703582507992754  - accuracy: 0.75\n",
      "At: 2290 [==========>] Loss 0.08805476254083446  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.1323477778924978  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.07267265072214507  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.050152179154838536  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.05784080797345775  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.16495833331611542  - accuracy: 0.6875\n",
      "At: 2296 [==========>] Loss 0.1572688149181128  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.07360584601097228  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.10392560823989525  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.09616785372020865  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.13532567528509087  - accuracy: 0.75\n",
      "At: 2301 [==========>] Loss 0.2153928998303605  - accuracy: 0.65625\n",
      "At: 2302 [==========>] Loss 0.1683686227620458  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.06417574330553447  - accuracy: 0.875\n",
      "At: 2304 [==========>] Loss 0.08290209422023274  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10860880980505709  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.12588338180266573  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.1546423200503023  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.18456784862826114  - accuracy: 0.71875\n",
      "At: 2309 [==========>] Loss 0.14385248736181666  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.15247485449199039  - accuracy: 0.8125\n",
      "At: 2311 [==========>] Loss 0.15181586317047194  - accuracy: 0.84375\n",
      "At: 2312 [==========>] Loss 0.08287448938207405  - accuracy: 0.90625\n",
      "At: 2313 [==========>] Loss 0.07627010656111835  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.12203446172375468  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.10195890882430804  - accuracy: 0.84375\n",
      "At: 2316 [==========>] Loss 0.12210715376883947  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.16625509914877928  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.1844953305277649  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.12687231656685677  - accuracy: 0.78125\n",
      "At: 2320 [==========>] Loss 0.09915003535919703  - accuracy: 0.90625\n",
      "At: 2321 [==========>] Loss 0.13799897998421945  - accuracy: 0.8125\n",
      "At: 2322 [==========>] Loss 0.20031727369523802  - accuracy: 0.75\n",
      "At: 2323 [==========>] Loss 0.13150634339373113  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.13803217104519777  - accuracy: 0.84375\n",
      "At: 2325 [==========>] Loss 0.15154554449895624  - accuracy: 0.75\n",
      "At: 2326 [==========>] Loss 0.06618684807740163  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.05158414060848124  - accuracy: 0.96875\n",
      "At: 2328 [==========>] Loss 0.1354795994615113  - accuracy: 0.8125\n",
      "At: 2329 [==========>] Loss 0.11310347217377516  - accuracy: 0.8125\n",
      "At: 2330 [==========>] Loss 0.1526588109900256  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.09624413506276666  - accuracy: 0.90625\n",
      "At: 2332 [==========>] Loss 0.09876180988272804  - accuracy: 0.875\n",
      "At: 2333 [==========>] Loss 0.1115873858705175  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.15584160755268042  - accuracy: 0.8125\n",
      "At: 2335 [==========>] Loss 0.09508544504570274  - accuracy: 0.875\n",
      "At: 2336 [==========>] Loss 0.08436405994204602  - accuracy: 0.90625\n",
      "At: 2337 [==========>] Loss 0.14700809087690433  - accuracy: 0.8125\n",
      "At: 2338 [==========>] Loss 0.09186955703508506  - accuracy: 0.90625\n",
      "At: 2339 [==========>] Loss 0.08195680521693322  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.15622197941660987  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.1287306946445596  - accuracy: 0.8125\n",
      "At: 2342 [==========>] Loss 0.14650880475236985  - accuracy: 0.8125\n",
      "At: 2343 [==========>] Loss 0.11586658775606432  - accuracy: 0.8125\n",
      "At: 2344 [==========>] Loss 0.20861576531156925  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.16708548578609178  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.09135959901989041  - accuracy: 0.875\n",
      "At: 2347 [==========>] Loss 0.14791750943263077  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.05992504546511508  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.08509790443970981  - accuracy: 0.90625\n",
      "At: 2350 [==========>] Loss 0.12486154771888583  - accuracy: 0.8125\n",
      "At: 2351 [==========>] Loss 0.09053496444470865  - accuracy: 0.90625\n",
      "At: 2352 [==========>] Loss 0.09419204583001747  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.08103325575736336  - accuracy: 0.90625\n",
      "At: 2354 [==========>] Loss 0.13412792104238644  - accuracy: 0.8125\n",
      "At: 2355 [==========>] Loss 0.05076234131266111  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.12092566489887271  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.14639342656138515  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.16905865131266204  - accuracy: 0.75\n",
      "At: 2359 [==========>] Loss 0.19444346880411695  - accuracy: 0.59375\n",
      "At: 2360 [==========>] Loss 0.12408625752645175  - accuracy: 0.875\n",
      "At: 2361 [==========>] Loss 0.1516095109082033  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.06857688532274658  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.16591773486650374  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.06692800168062969  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.0915674764962976  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.14642214299728967  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.09176643630371269  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.09858259059665656  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.09251371385397891  - accuracy: 0.875\n",
      "At: 2370 [==========>] Loss 0.0942697961677776  - accuracy: 0.84375\n",
      "At: 2371 [==========>] Loss 0.09735878901582216  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.146559213885818  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.09653969628969573  - accuracy: 0.90625\n",
      "At: 2374 [==========>] Loss 0.12797119287742748  - accuracy: 0.78125\n",
      "At: 2375 [==========>] Loss 0.05011590949010196  - accuracy: 1.0\n",
      "At: 2376 [==========>] Loss 0.09932092944910478  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.10570206035221302  - accuracy: 0.8125\n",
      "At: 2378 [==========>] Loss 0.13647654897438533  - accuracy: 0.78125\n",
      "At: 2379 [==========>] Loss 0.16785751638402602  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.06965945790260632  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.1074704167764805  - accuracy: 0.84375\n",
      "At: 2382 [==========>] Loss 0.08716199052831691  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1446994593703672  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.12239077581014735  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.12336619052153673  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.10043614825797079  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.081411961294698  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.12246317140469479  - accuracy: 0.875\n",
      "At: 2389 [==========>] Loss 0.048376727178434845  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.0721624843560364  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.1548136812190555  - accuracy: 0.8125\n",
      "At: 2392 [==========>] Loss 0.14146165984632922  - accuracy: 0.78125\n",
      "At: 2393 [==========>] Loss 0.0766348689538427  - accuracy: 0.90625\n",
      "At: 2394 [==========>] Loss 0.08121912892650998  - accuracy: 0.875\n",
      "At: 2395 [==========>] Loss 0.10819392004853902  - accuracy: 0.84375\n",
      "At: 2396 [==========>] Loss 0.08578546792314516  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.12099223072089063  - accuracy: 0.84375\n",
      "At: 2398 [==========>] Loss 0.12891203091215836  - accuracy: 0.8125\n",
      "At: 2399 [==========>] Loss 0.16464196015348814  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.10394973422829551  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.09685483984951171  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.10236021588501396  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.1813226059960694  - accuracy: 0.71875\n",
      "At: 2404 [==========>] Loss 0.14186567508785547  - accuracy: 0.8125\n",
      "At: 2405 [==========>] Loss 0.06727885606460983  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.1332154198674929  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.11718616297517437  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.09581622473901752  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.14522299580113993  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.15993738371169058  - accuracy: 0.75\n",
      "At: 2411 [==========>] Loss 0.08683998081474512  - accuracy: 0.90625\n",
      "At: 2412 [==========>] Loss 0.09035490165285259  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.0750819642850069  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.06528534391955715  - accuracy: 0.9375\n",
      "At: 2415 [==========>] Loss 0.08570568415031969  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.11053757441030579  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.15468131050686204  - accuracy: 0.8125\n",
      "At: 2418 [==========>] Loss 0.1286306153353498  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.14469507972467768  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.12185288771482175  - accuracy: 0.84375\n",
      "At: 2421 [==========>] Loss 0.0962822476664468  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.13662818441603536  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.149364173673186  - accuracy: 0.78125\n",
      "At: 2424 [==========>] Loss 0.08523675784107596  - accuracy: 0.9375\n",
      "At: 2425 [==========>] Loss 0.09035563182386255  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.16669180014218427  - accuracy: 0.6875\n",
      "At: 2427 [==========>] Loss 0.12971644626845436  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.0719867344769046  - accuracy: 0.9375\n",
      "At: 2429 [==========>] Loss 0.14755334913575896  - accuracy: 0.8125\n",
      "At: 2430 [==========>] Loss 0.12898523330190628  - accuracy: 0.875\n",
      "At: 2431 [==========>] Loss 0.14227014031482463  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.0619106377748352  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.0678954890018523  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.039936140996349054  - accuracy: 1.0\n",
      "At: 2435 [==========>] Loss 0.14314584677277553  - accuracy: 0.8125\n",
      "At: 2436 [==========>] Loss 0.11641003362893129  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.20870327293272806  - accuracy: 0.65625\n",
      "At: 2438 [==========>] Loss 0.10263677835623286  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.1757819517741717  - accuracy: 0.65625\n",
      "At: 2440 [==========>] Loss 0.11197739532321827  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.11047886769315846  - accuracy: 0.875\n",
      "At: 2442 [==========>] Loss 0.1349639040500044  - accuracy: 0.84375\n",
      "At: 2443 [==========>] Loss 0.12759265102126605  - accuracy: 0.71875\n",
      "At: 2444 [==========>] Loss 0.10434958853059587  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.0828421641287357  - accuracy: 0.875\n",
      "At: 2446 [==========>] Loss 0.18460489826337972  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.17561634614251387  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.1264682021708185  - accuracy: 0.8125\n",
      "At: 2449 [==========>] Loss 0.09007313546127864  - accuracy: 0.875\n",
      "At: 2450 [==========>] Loss 0.06405453807572262  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.046775252434357706  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.1238043384233542  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.13732870653352644  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.1539675583809807  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1411077249033477  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.12867809316862514  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.16433641381244274  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.08009805542495767  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.1362276843008886  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.0621286237398321  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.06779129481883103  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.15111999176397445  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.11261275492262571  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.127493089754395  - accuracy: 0.8125\n",
      "At: 2465 [==========>] Loss 0.1329183707119576  - accuracy: 0.84375\n",
      "At: 2466 [==========>] Loss 0.09487034753645161  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.10501831724837224  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.07885912980217014  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.14855923807058385  - accuracy: 0.75\n",
      "At: 2470 [==========>] Loss 0.12243597287419206  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.10992712186773265  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.1013117861366121  - accuracy: 0.90625\n",
      "At: 2473 [==========>] Loss 0.12676699105263597  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.061652307201430064  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10621358259928404  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.0813541284855095  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.15289613430427323  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.10319846315054726  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.09039482547640318  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.15136613789950282  - accuracy: 0.75\n",
      "At: 2481 [==========>] Loss 0.07177009379739681  - accuracy: 0.875\n",
      "At: 2482 [==========>] Loss 0.15940231300591776  - accuracy: 0.75\n",
      "At: 2483 [==========>] Loss 0.10539512418138569  - accuracy: 0.875\n",
      "At: 2484 [==========>] Loss 0.08008506040705016  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.13976688193256795  - accuracy: 0.78125\n",
      "At: 2486 [==========>] Loss 0.12130541947667534  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.09981350587476448  - accuracy: 0.875\n",
      "At: 2488 [==========>] Loss 0.14860344627431332  - accuracy: 0.8125\n",
      "At: 2489 [==========>] Loss 0.1788979872320699  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.12202166027774645  - accuracy: 0.84375\n",
      "At: 2491 [==========>] Loss 0.11516304880542544  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.14019897235070458  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.09782749927979834  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.11279789395033449  - accuracy: 0.84375\n",
      "At: 2495 [==========>] Loss 0.10816693275637666  - accuracy: 0.84375\n",
      "At: 2496 [==========>] Loss 0.06927027105481809  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.20933231699763974  - accuracy: 0.625\n",
      "At: 2498 [==========>] Loss 0.11063551986463604  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.06652118541768078  - accuracy: 0.875\n",
      "At: 2500 [==========>] Loss 0.17245867899617529  - accuracy: 0.71875\n",
      "At: 2501 [==========>] Loss 0.17704006193602526  - accuracy: 0.71875\n",
      "At: 2502 [==========>] Loss 0.11357225894554597  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.12282364444809436  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.15240932056610612  - accuracy: 0.8125\n",
      "At: 2505 [==========>] Loss 0.12410249953648769  - accuracy: 0.75\n",
      "At: 2506 [==========>] Loss 0.10707833182899126  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.15795262574221505  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.11644370657168725  - accuracy: 0.875\n",
      "At: 2509 [==========>] Loss 0.16686723003707876  - accuracy: 0.75\n",
      "At: 2510 [==========>] Loss 0.12137136762759848  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14117835216927815  - accuracy: 0.84375\n",
      "At: 2512 [==========>] Loss 0.12033557061781726  - accuracy: 0.8125\n",
      "At: 2513 [==========>] Loss 0.1427244355101163  - accuracy: 0.8125\n",
      "At: 2514 [==========>] Loss 0.1336392920980504  - accuracy: 0.8125\n",
      "At: 2515 [==========>] Loss 0.20524227956320604  - accuracy: 0.6875\n",
      "At: 2516 [==========>] Loss 0.17251970848187403  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.11168829162854309  - accuracy: 0.84375\n",
      "At: 2518 [==========>] Loss 0.12024905526517829  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.09754450574052034  - accuracy: 0.875\n",
      "At: 2520 [==========>] Loss 0.1545711763660247  - accuracy: 0.75\n",
      "At: 2521 [==========>] Loss 0.1159215663796356  - accuracy: 0.8125\n",
      "At: 2522 [==========>] Loss 0.22100343233895997  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.10949526860743751  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.14993107356756957  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.06214847876299985  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.11246729639671083  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.135740032227517  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.10082241326908624  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.13676689427597605  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.1343075544416976  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.051443149186453485  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.14755543500837917  - accuracy: 0.71875\n",
      "At: 2533 [==========>] Loss 0.09581571695031987  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.05602111372072628  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.09082214265287536  - accuracy: 0.84375\n",
      "At: 2536 [==========>] Loss 0.16319853104479967  - accuracy: 0.78125\n",
      "At: 2537 [==========>] Loss 0.09331402729087827  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.13590145323011332  - accuracy: 0.8125\n",
      "At: 2539 [==========>] Loss 0.08780208150534223  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.11652883258602004  - accuracy: 0.84375\n",
      "At: 2541 [==========>] Loss 0.06616767127943889  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.05589173532284723  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16506140879541942  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.14300495300401533  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.05799853902185425  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.13667276757042518  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.12236330298471716  - accuracy: 0.8125\n",
      "At: 2548 [==========>] Loss 0.1060216647160293  - accuracy: 0.8125\n",
      "At: 2549 [==========>] Loss 0.11156946950146654  - accuracy: 0.84375\n",
      "At: 2550 [==========>] Loss 0.12999311896952542  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.15109924244121664  - accuracy: 0.75\n",
      "At: 2552 [==========>] Loss 0.1146725472354258  - accuracy: 0.84375\n",
      "At: 2553 [==========>] Loss 0.08323683971839567  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.1398758122049795  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.17926077972126653  - accuracy: 0.71875\n",
      "At: 2556 [==========>] Loss 0.09026819767617328  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.12817012734491995  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.08155652524741666  - accuracy: 0.90625\n",
      "At: 2559 [==========>] Loss 0.09656139932338223  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.1782486262498325  - accuracy: 0.75\n",
      "At: 2561 [==========>] Loss 0.09174921464155299  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.10728957334948944  - accuracy: 0.90625\n",
      "At: 2563 [==========>] Loss 0.11653240537069347  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09360249257790688  - accuracy: 0.84375\n",
      "At: 2565 [==========>] Loss 0.10912968538722505  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1949206430951908  - accuracy: 0.75\n",
      "At: 2567 [==========>] Loss 0.10523204192479746  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.08816045340742254  - accuracy: 0.875\n",
      "At: 2569 [==========>] Loss 0.06972337207497668  - accuracy: 0.96875\n",
      "At: 2570 [==========>] Loss 0.17633605168538669  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.089871562966442  - accuracy: 0.8125\n",
      "At: 2572 [==========>] Loss 0.19646026937079708  - accuracy: 0.71875\n",
      "At: 2573 [==========>] Loss 0.18417084628323654  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.11531737607014661  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.050564260129203856  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.102632018520643  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.18130513917464647  - accuracy: 0.78125\n",
      "At: 2578 [==========>] Loss 0.19393465582473737  - accuracy: 0.6875\n",
      "At: 2579 [==========>] Loss 0.20326867144089153  - accuracy: 0.65625\n",
      "At: 2580 [==========>] Loss 0.1576127739821862  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.04881836829957638  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.17123935294964387  - accuracy: 0.8125\n",
      "At: 2583 [==========>] Loss 0.08140384516482585  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.18512343311052165  - accuracy: 0.75\n",
      "At: 2585 [==========>] Loss 0.08592171659254048  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.098700323577748  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.1634137524681873  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.13041019658247638  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.12472236847448132  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.19284960009952856  - accuracy: 0.71875\n",
      "At: 2591 [==========>] Loss 0.13679617471953154  - accuracy: 0.78125\n",
      "At: 2592 [==========>] Loss 0.08528913886586639  - accuracy: 0.875\n",
      "At: 2593 [==========>] Loss 0.10589307144174172  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.085018526778593  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.09421654887308661  - accuracy: 0.84375\n",
      "At: 2596 [==========>] Loss 0.11590766213250454  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.07502713989139494  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.1382549185020291  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.13354159414468406  - accuracy: 0.8125\n",
      "At: 2600 [==========>] Loss 0.15560726534944364  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.15199689844213998  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.0973494007928112  - accuracy: 0.84375\n",
      "At: 2603 [==========>] Loss 0.12701256122448717  - accuracy: 0.84375\n",
      "At: 2604 [==========>] Loss 0.1249700728906307  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.1363449838356866  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.10831279148699202  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.12300235418322006  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.0718602754641233  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.11367752501077241  - accuracy: 0.8125\n",
      "At: 2610 [==========>] Loss 0.14221017568155295  - accuracy: 0.75\n",
      "At: 2611 [==========>] Loss 0.12507810173178052  - accuracy: 0.84375\n",
      "At: 2612 [==========>] Loss 0.08629268072296323  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.11167092508607523  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.08915787992713298  - accuracy: 0.875\n",
      "At: 2615 [==========>] Loss 0.0687434880202601  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06525343609980674  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.10257863820048299  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.07793635650838954  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.08374839709543724  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.09283239608165765  - accuracy: 0.90625\n",
      "At: 2621 [==========>] Loss 0.11029853908808408  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.10749958587076122  - accuracy: 0.875\n",
      "At: 2623 [==========>] Loss 0.0825140020564416  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.12514002898525373  - accuracy: 0.875\n",
      "At: 2625 [==========>] Loss 0.06811601390584067  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.08043060436488685  - accuracy: 0.875\n",
      "At: 2627 [==========>] Loss 0.13755720692312273  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.07586221634587903  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.1066787033052121  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.101314449203015  - accuracy: 0.84375\n",
      "At: 2631 [==========>] Loss 0.14493902750936874  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.11810120669973888  - accuracy: 0.875\n",
      "At: 2633 [==========>] Loss 0.09944781412693349  - accuracy: 0.84375\n",
      "At: 2634 [==========>] Loss 0.08163554243584735  - accuracy: 0.84375\n",
      "At: 2635 [==========>] Loss 0.19438058776701894  - accuracy: 0.71875\n",
      "At: 2636 [==========>] Loss 0.12511898906039598  - accuracy: 0.84375\n",
      "At: 2637 [==========>] Loss 0.1561410792536598  - accuracy: 0.75\n",
      "At: 2638 [==========>] Loss 0.07767351235768594  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.09492363025597644  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.1360214617099625  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.15105177886999377  - accuracy: 0.84375\n",
      "At: 2642 [==========>] Loss 0.18301141567450957  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.08645159237603497  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.16632483408490434  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.08156341679204968  - accuracy: 0.875\n",
      "At: 2646 [==========>] Loss 0.15013276194906616  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.11085447274554419  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.13875814302257197  - accuracy: 0.84375\n",
      "At: 2649 [==========>] Loss 0.13233367899744725  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.11283751864740993  - accuracy: 0.8125\n",
      "At: 2651 [==========>] Loss 0.17534776481561976  - accuracy: 0.75\n",
      "At: 2652 [==========>] Loss 0.09519570705602695  - accuracy: 0.875\n",
      "At: 2653 [==========>] Loss 0.12199000950962943  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11817551116509867  - accuracy: 0.8125\n",
      "At: 2655 [==========>] Loss 0.23814617921890963  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.02749493196632231  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.10920551848054497  - accuracy: 0.8125\n",
      "At: 2658 [==========>] Loss 0.096057178377976  - accuracy: 0.90625\n",
      "At: 2659 [==========>] Loss 0.08401290271909898  - accuracy: 0.84375\n",
      "At: 2660 [==========>] Loss 0.0948967163883623  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.10060523607615784  - accuracy: 0.84375\n",
      "At: 2662 [==========>] Loss 0.07941550528860723  - accuracy: 0.90625\n",
      "At: 2663 [==========>] Loss 0.0946028735256228  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.08980975071479404  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.10539389792334929  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.12329280905364312  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.1267537966932118  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.1491722562429043  - accuracy: 0.78125\n",
      "At: 2669 [==========>] Loss 0.15700492723170584  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.10595923689236798  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.0902533671040974  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.11942285601839725  - accuracy: 0.78125\n",
      "At: 2673 [==========>] Loss 0.10870932600342204  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.09502432428904663  - accuracy: 0.8125\n",
      "At: 2675 [==========>] Loss 0.12903547000165927  - accuracy: 0.84375\n",
      "At: 2676 [==========>] Loss 0.11227480694274256  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.11002164324707653  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.055963443807042684  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.07079316377266004  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.08756357140680116  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.10352369662246604  - accuracy: 0.875\n",
      "At: 2682 [==========>] Loss 0.13741892108020587  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.17626503384503767  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09856526072916544  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.10213329609906918  - accuracy: 0.84375\n",
      "At: 2686 [==========>] Loss 0.06272942921233055  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.12749520297835462  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.14468063976756287  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.11317486055236134  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.1154644862537345  - accuracy: 0.78125\n",
      "Epochs  8 / 10\n",
      "At: 1 [==========>] Loss 0.14990762234741464  - accuracy: 0.84375\n",
      "At: 2 [==========>] Loss 0.24393110957498826  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.18601524452000306  - accuracy: 0.78125\n",
      "At: 4 [==========>] Loss 0.20413620592753928  - accuracy: 0.6875\n",
      "At: 5 [==========>] Loss 0.08330829645026067  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.12443927551111914  - accuracy: 0.875\n",
      "At: 7 [==========>] Loss 0.21483183873870806  - accuracy: 0.75\n",
      "At: 8 [==========>] Loss 0.2419286610641038  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.30452932896061363  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.25219411961477717  - accuracy: 0.71875\n",
      "At: 11 [==========>] Loss 0.22868980708835995  - accuracy: 0.71875\n",
      "At: 12 [==========>] Loss 0.1874597194707633  - accuracy: 0.75\n",
      "At: 13 [==========>] Loss 0.1958566832341701  - accuracy: 0.78125\n",
      "At: 14 [==========>] Loss 0.1024466326748133  - accuracy: 0.84375\n",
      "At: 15 [==========>] Loss 0.14070092441809554  - accuracy: 0.84375\n",
      "At: 16 [==========>] Loss 0.17582171306170533  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.2212573741075265  - accuracy: 0.6875\n",
      "At: 18 [==========>] Loss 0.2574200464236489  - accuracy: 0.65625\n",
      "At: 19 [==========>] Loss 0.16626330919492585  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.1016695390133133  - accuracy: 0.875\n",
      "At: 21 [==========>] Loss 0.2119096029846358  - accuracy: 0.75\n",
      "At: 22 [==========>] Loss 0.17458615269290884  - accuracy: 0.8125\n",
      "At: 23 [==========>] Loss 0.09258213332165932  - accuracy: 0.90625\n",
      "At: 24 [==========>] Loss 0.259409422970079  - accuracy: 0.71875\n",
      "At: 25 [==========>] Loss 0.2544252025192891  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.25853027629446124  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.23654499478626353  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.18883263234840603  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.19694628522749652  - accuracy: 0.78125\n",
      "At: 30 [==========>] Loss 0.2475493808508268  - accuracy: 0.6875\n",
      "At: 31 [==========>] Loss 0.26914278009109627  - accuracy: 0.6875\n",
      "At: 32 [==========>] Loss 0.2243594273807252  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.1418877461380352  - accuracy: 0.84375\n",
      "At: 34 [==========>] Loss 0.17764150537901363  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.214346852128492  - accuracy: 0.75\n",
      "At: 36 [==========>] Loss 0.22076842844180322  - accuracy: 0.75\n",
      "At: 37 [==========>] Loss 0.23428707790648742  - accuracy: 0.75\n",
      "At: 38 [==========>] Loss 0.25139870314610363  - accuracy: 0.6875\n",
      "At: 39 [==========>] Loss 0.18912306319433608  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.29571141595916883  - accuracy: 0.625\n",
      "At: 41 [==========>] Loss 0.08482781011523816  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.17642154997511686  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.19378246788207287  - accuracy: 0.75\n",
      "At: 44 [==========>] Loss 0.16507587450183986  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.12178178057435456  - accuracy: 0.84375\n",
      "At: 46 [==========>] Loss 0.2545012954568926  - accuracy: 0.71875\n",
      "At: 47 [==========>] Loss 0.21626821337620605  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.18003682696882067  - accuracy: 0.84375\n",
      "At: 49 [==========>] Loss 0.12805315406745382  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.2272626448854695  - accuracy: 0.71875\n",
      "At: 51 [==========>] Loss 0.1978171076620412  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.28733417301765796  - accuracy: 0.6875\n",
      "At: 53 [==========>] Loss 0.19377777513044853  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.1623612759118791  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.25398822510072083  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.17033572057384805  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.15137178476612226  - accuracy: 0.78125\n",
      "At: 58 [==========>] Loss 0.1897127306472143  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.22993672575729696  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.18975299803026624  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.28586270086362253  - accuracy: 0.625\n",
      "At: 62 [==========>] Loss 0.208652134167292  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.22205151962538905  - accuracy: 0.6875\n",
      "At: 64 [==========>] Loss 0.18956277010528397  - accuracy: 0.78125\n",
      "At: 65 [==========>] Loss 0.2624423686836329  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.23620893139778104  - accuracy: 0.6875\n",
      "At: 67 [==========>] Loss 0.24161242390153526  - accuracy: 0.6875\n",
      "At: 68 [==========>] Loss 0.14420591920139264  - accuracy: 0.84375\n",
      "At: 69 [==========>] Loss 0.13833519235250835  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.20719830188264585  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.18158297595779274  - accuracy: 0.8125\n",
      "At: 72 [==========>] Loss 0.18079241932092444  - accuracy: 0.78125\n",
      "At: 73 [==========>] Loss 0.16935530254520545  - accuracy: 0.78125\n",
      "At: 74 [==========>] Loss 0.1993955034527399  - accuracy: 0.6875\n",
      "At: 75 [==========>] Loss 0.20834394799979866  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.2657067682118156  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.22434224721294854  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.17031942369908626  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.18454485009620805  - accuracy: 0.75\n",
      "At: 80 [==========>] Loss 0.23332688131568782  - accuracy: 0.75\n",
      "At: 81 [==========>] Loss 0.2082042454708239  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.21941654021344908  - accuracy: 0.75\n",
      "At: 83 [==========>] Loss 0.17078834997573988  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.19630363702595005  - accuracy: 0.78125\n",
      "At: 85 [==========>] Loss 0.21673802074918996  - accuracy: 0.65625\n",
      "At: 86 [==========>] Loss 0.1882314113050154  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.185308723401445  - accuracy: 0.75\n",
      "At: 88 [==========>] Loss 0.34611627036028647  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.22037302935150674  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.2545669988838455  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.20025672812403003  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.08380280338670915  - accuracy: 0.90625\n",
      "At: 93 [==========>] Loss 0.1511612756889208  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.15745910105259647  - accuracy: 0.84375\n",
      "At: 95 [==========>] Loss 0.18743896735523408  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.1456402860688995  - accuracy: 0.8125\n",
      "At: 97 [==========>] Loss 0.099533378226595  - accuracy: 0.9375\n",
      "At: 98 [==========>] Loss 0.2642199903685619  - accuracy: 0.6875\n",
      "At: 99 [==========>] Loss 0.12691778436515996  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.13501647446334886  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.17255601663981443  - accuracy: 0.8125\n",
      "At: 102 [==========>] Loss 0.18237355421556  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.1368578208661197  - accuracy: 0.84375\n",
      "At: 104 [==========>] Loss 0.1405702863049458  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.17288940474366174  - accuracy: 0.71875\n",
      "At: 106 [==========>] Loss 0.2233798906883513  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.20702037840490728  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.2679598148665164  - accuracy: 0.65625\n",
      "At: 109 [==========>] Loss 0.1221324228761392  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.2563898021803613  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.10886726034342187  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.18029847416108258  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.21174481510493176  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.18399203027486538  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.20725167900181363  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.20124139202697694  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.16083952287429684  - accuracy: 0.84375\n",
      "At: 118 [==========>] Loss 0.26629079924874255  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.14015345885898278  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.19655605668057474  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.1543765973514843  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.22016932560197372  - accuracy: 0.75\n",
      "At: 123 [==========>] Loss 0.22515749801212936  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.2787845633314202  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.19602855756686122  - accuracy: 0.6875\n",
      "At: 126 [==========>] Loss 0.2532057846085751  - accuracy: 0.6875\n",
      "At: 127 [==========>] Loss 0.23243258325141608  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.29677213255859136  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.11618754323671046  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.18817790384583077  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.20488366610901199  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.26971344285200893  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.20666220554222037  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.20047483864941015  - accuracy: 0.75\n",
      "At: 135 [==========>] Loss 0.232063423483955  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.1994908750699974  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.09916837701335265  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.18395544799474156  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.13202786324123175  - accuracy: 0.84375\n",
      "At: 140 [==========>] Loss 0.16122480028651873  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.33504976913837725  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.1705420944194592  - accuracy: 0.8125\n",
      "At: 143 [==========>] Loss 0.20888951742460438  - accuracy: 0.71875\n",
      "At: 144 [==========>] Loss 0.16406977010007714  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.10649227636928799  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.15150410494602026  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.21454556890303578  - accuracy: 0.6875\n",
      "At: 148 [==========>] Loss 0.14507492751233197  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.20567407611796357  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.16693999623918457  - accuracy: 0.78125\n",
      "At: 151 [==========>] Loss 0.1904667095950625  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.2267785197305337  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.20575427122913317  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.20484783151619346  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.1842815279453232  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.13980121610686225  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.22932101342966665  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.1728917490561252  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.15444833858637247  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.12915465511901686  - accuracy: 0.875\n",
      "At: 161 [==========>] Loss 0.10564406601873623  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.24210779083579503  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.21859777880265788  - accuracy: 0.78125\n",
      "At: 164 [==========>] Loss 0.17438080913179854  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.24716764023400067  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.20019866722648602  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.15910197374168253  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.20199477086708845  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.1775800077136354  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.1629179043631126  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.24530482066365633  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.19216898878013772  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.3094391564091168  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.21200899824030006  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.15851903708800913  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.17710491986116184  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.13758343999289624  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.22484248757864328  - accuracy: 0.71875\n",
      "At: 179 [==========>] Loss 0.1460874013232446  - accuracy: 0.84375\n",
      "At: 180 [==========>] Loss 0.1821697752313401  - accuracy: 0.78125\n",
      "At: 181 [==========>] Loss 0.05582313215344249  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.1917706401104347  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.18456071204902527  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.17483127479711374  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.14047306524033087  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.18392155232232285  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.1738733204076947  - accuracy: 0.75\n",
      "At: 188 [==========>] Loss 0.19654147700908203  - accuracy: 0.71875\n",
      "At: 189 [==========>] Loss 0.24492815550164626  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.1522086228287004  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.32797743291136816  - accuracy: 0.625\n",
      "At: 192 [==========>] Loss 0.19376849825658382  - accuracy: 0.75\n",
      "At: 193 [==========>] Loss 0.22957935719934536  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.16738387796302456  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.18124450514349852  - accuracy: 0.75\n",
      "At: 196 [==========>] Loss 0.18743190512351532  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.1670083486939761  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.14490701627599295  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.13556003230732133  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.2049074162332403  - accuracy: 0.6875\n",
      "At: 201 [==========>] Loss 0.10603392566922352  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.1266477415316939  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.1873124855921872  - accuracy: 0.75\n",
      "At: 204 [==========>] Loss 0.207149280837451  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.15060616299394253  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.10613839491526604  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.12605679373735512  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.2755079418541409  - accuracy: 0.625\n",
      "At: 209 [==========>] Loss 0.21021141421290046  - accuracy: 0.71875\n",
      "At: 210 [==========>] Loss 0.11907868011038034  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.17554921133522916  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.2212632746706819  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.19672289921169  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.2286140620726056  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.11621414868770666  - accuracy: 0.90625\n",
      "At: 216 [==========>] Loss 0.20514085826027237  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.25288549225479584  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.16974009343819074  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.19070180360103292  - accuracy: 0.75\n",
      "At: 220 [==========>] Loss 0.250486572409999  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.15947180839509328  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.09415786166673824  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.258733654311659  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.1892968483378037  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.14817890535058897  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.17330280617988386  - accuracy: 0.75\n",
      "At: 227 [==========>] Loss 0.23813641786777906  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.21705559110495765  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.18061322369918031  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.17732773462881812  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.23440679427690742  - accuracy: 0.71875\n",
      "At: 232 [==========>] Loss 0.2538525176934897  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.1894984499031721  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.11963369286388195  - accuracy: 0.84375\n",
      "At: 235 [==========>] Loss 0.2460066213194357  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.20465683015542227  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.11337775999641561  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.1394182027523587  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.1753688951583996  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.25746189621762616  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.13717945066714338  - accuracy: 0.8125\n",
      "At: 242 [==========>] Loss 0.17399357956745712  - accuracy: 0.78125\n",
      "At: 243 [==========>] Loss 0.13970865263518098  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.17375133613074806  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.16715338022582438  - accuracy: 0.78125\n",
      "At: 246 [==========>] Loss 0.1480644282587147  - accuracy: 0.78125\n",
      "At: 247 [==========>] Loss 0.14946616983344102  - accuracy: 0.84375\n",
      "At: 248 [==========>] Loss 0.12604859792980969  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.08497308916977653  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.1911871975870163  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.24365017316805704  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.1395784075271546  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.18740117810143875  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.10034228164161363  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.1587831308675305  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.21057269689887775  - accuracy: 0.6875\n",
      "At: 257 [==========>] Loss 0.12211239114675466  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.16143317498854529  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.1532814695200178  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.13508492409740908  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.09450701138106067  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.14680264041195373  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.12276209136942764  - accuracy: 0.90625\n",
      "At: 264 [==========>] Loss 0.10894619633191013  - accuracy: 0.875\n",
      "At: 265 [==========>] Loss 0.17769411244306876  - accuracy: 0.75\n",
      "At: 266 [==========>] Loss 0.2503222263168926  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1914236745488682  - accuracy: 0.71875\n",
      "At: 268 [==========>] Loss 0.24378676785134518  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.15432541008634085  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.25707318638468457  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.17795638699839228  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.10705356754017349  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.251565710614828  - accuracy: 0.6875\n",
      "At: 274 [==========>] Loss 0.17624711071192012  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.09622822945984137  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.2177935492725962  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.1118131975104864  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.12681397712744388  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.1854894805266279  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.18857814806931827  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.14725669205686231  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.23544404618744752  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.14251061110147079  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.10074330505720468  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.11614342710781313  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.0876645330256488  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.18357557522829782  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.15393896549829544  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.11205383846894178  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.12888128693108883  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.12937718350499625  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.1487286112005956  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.1838661046339426  - accuracy: 0.71875\n",
      "At: 294 [==========>] Loss 0.17708458074441422  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.1742109542705554  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.13806604569465303  - accuracy: 0.84375\n",
      "At: 297 [==========>] Loss 0.16777330148196046  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.15081732690102057  - accuracy: 0.8125\n",
      "At: 299 [==========>] Loss 0.22248076516280124  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.18528366611294927  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.18085689402302352  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.12022898768528034  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.11359482355636154  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.1915289952970344  - accuracy: 0.78125\n",
      "At: 305 [==========>] Loss 0.2332411440643798  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.1709518548401177  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.26886375385977207  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.18595288872938406  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.08514279260307009  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.23743075775205064  - accuracy: 0.65625\n",
      "At: 311 [==========>] Loss 0.10085030450858952  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.1359935272798684  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.12157076575680932  - accuracy: 0.875\n",
      "At: 314 [==========>] Loss 0.24587049449130904  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.14610486023583275  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.20035311335975217  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.2946959607924883  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.1611627207774668  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.13485472484028574  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.2273331639026794  - accuracy: 0.65625\n",
      "At: 321 [==========>] Loss 0.2457018857530093  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.11898926598101052  - accuracy: 0.84375\n",
      "At: 323 [==========>] Loss 0.1264520278163982  - accuracy: 0.875\n",
      "At: 324 [==========>] Loss 0.1695003725764318  - accuracy: 0.75\n",
      "At: 325 [==========>] Loss 0.09593117742597193  - accuracy: 0.90625\n",
      "At: 326 [==========>] Loss 0.17863414531010158  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.12375800691747982  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.1480153102752311  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.1305514194294261  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.20758363054865542  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.2156448597196733  - accuracy: 0.71875\n",
      "At: 332 [==========>] Loss 0.26854350227712864  - accuracy: 0.625\n",
      "At: 333 [==========>] Loss 0.1798874092469624  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.10297503873300437  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.16533294302270402  - accuracy: 0.78125\n",
      "At: 336 [==========>] Loss 0.12960474091818283  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.18777682652558253  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.14316434286864244  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.156444965329799  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.13997923550773012  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.10608920999479486  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.15138851622398997  - accuracy: 0.8125\n",
      "At: 343 [==========>] Loss 0.24941505333980338  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.18674588427410196  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.22299425552408939  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.18559621801442144  - accuracy: 0.78125\n",
      "At: 347 [==========>] Loss 0.108054309928512  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.1639227137783778  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.1725258591754488  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.11532421124395764  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.2671355560222148  - accuracy: 0.65625\n",
      "At: 352 [==========>] Loss 0.13597662451053816  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.13738970456362673  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.21576781863981026  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.11129245666467864  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.19319683690246048  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.1247413656337242  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.15957477646805884  - accuracy: 0.8125\n",
      "At: 359 [==========>] Loss 0.1154346332465118  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.15829319506002731  - accuracy: 0.71875\n",
      "At: 361 [==========>] Loss 0.10217521469833007  - accuracy: 0.875\n",
      "At: 362 [==========>] Loss 0.17073480465048313  - accuracy: 0.71875\n",
      "At: 363 [==========>] Loss 0.13981836998924782  - accuracy: 0.84375\n",
      "At: 364 [==========>] Loss 0.1998566593483569  - accuracy: 0.75\n",
      "At: 365 [==========>] Loss 0.14713411790710967  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.185565661949865  - accuracy: 0.6875\n",
      "At: 367 [==========>] Loss 0.17584737283319657  - accuracy: 0.78125\n",
      "At: 368 [==========>] Loss 0.20350135731014737  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.17091751659104218  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.17851148488249846  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.10291002261220507  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.10977695284987438  - accuracy: 0.8125\n",
      "At: 373 [==========>] Loss 0.22216658894915797  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.08386358206385934  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.15375386752626474  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.09428762316763047  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.1749974779387224  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.1455989552012077  - accuracy: 0.78125\n",
      "At: 379 [==========>] Loss 0.1908591270516825  - accuracy: 0.71875\n",
      "At: 380 [==========>] Loss 0.15185531643221353  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.18944384151122706  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.10440018758869117  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.20558974008941017  - accuracy: 0.65625\n",
      "At: 384 [==========>] Loss 0.18063223953348312  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.1466349161351781  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.20935519956128518  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.10730056761511572  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.19504605238287342  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.20146659728675934  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.10745851139806581  - accuracy: 0.875\n",
      "At: 391 [==========>] Loss 0.12835594887179147  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.1412480447977535  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.24334272070291396  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.11257101843671932  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.20456727644811534  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.1722903359308332  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.14162034444524918  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.20038528921662158  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.23784417011436093  - accuracy: 0.625\n",
      "At: 400 [==========>] Loss 0.16899760579847053  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.14622777930174508  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.09837275522124969  - accuracy: 0.84375\n",
      "At: 403 [==========>] Loss 0.05255566856939789  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.1201021880043397  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.1333617246843727  - accuracy: 0.84375\n",
      "At: 406 [==========>] Loss 0.17726742785828464  - accuracy: 0.71875\n",
      "At: 407 [==========>] Loss 0.19782405461923785  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.22468769937419525  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.22750411410902274  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.15352316939338762  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.1009612805547992  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.19689561885635665  - accuracy: 0.71875\n",
      "At: 413 [==========>] Loss 0.13554308165290704  - accuracy: 0.84375\n",
      "At: 414 [==========>] Loss 0.17378599609018985  - accuracy: 0.71875\n",
      "At: 415 [==========>] Loss 0.1523710519718809  - accuracy: 0.84375\n",
      "At: 416 [==========>] Loss 0.2419966495958938  - accuracy: 0.6875\n",
      "At: 417 [==========>] Loss 0.16792718066392628  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.15050401107617067  - accuracy: 0.84375\n",
      "At: 419 [==========>] Loss 0.15925190374475207  - accuracy: 0.78125\n",
      "At: 420 [==========>] Loss 0.2006725893258406  - accuracy: 0.65625\n",
      "At: 421 [==========>] Loss 0.12254059490701986  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.12916981447636172  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.1537976476677786  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.23439620500670322  - accuracy: 0.6875\n",
      "At: 425 [==========>] Loss 0.20648059331982055  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.1172601071272428  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.17231620157393035  - accuracy: 0.78125\n",
      "At: 428 [==========>] Loss 0.20138097407944194  - accuracy: 0.6875\n",
      "At: 429 [==========>] Loss 0.1682699800761217  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.10892849906806339  - accuracy: 0.90625\n",
      "At: 431 [==========>] Loss 0.1313345283285686  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.1458937782962483  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.08778873848925858  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.13819480351160707  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.18743135162436242  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.14867288704454462  - accuracy: 0.78125\n",
      "At: 437 [==========>] Loss 0.12806195325981007  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.11733304664708386  - accuracy: 0.84375\n",
      "At: 439 [==========>] Loss 0.10793854068004315  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.1035497122809341  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.1975363646623205  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.1897122720702648  - accuracy: 0.75\n",
      "At: 443 [==========>] Loss 0.10971390695615141  - accuracy: 0.84375\n",
      "At: 444 [==========>] Loss 0.22737716672660274  - accuracy: 0.71875\n",
      "At: 445 [==========>] Loss 0.16758373101388496  - accuracy: 0.75\n",
      "At: 446 [==========>] Loss 0.22836013539327593  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.17981263972239658  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.19775162395554025  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.11986809658988849  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.1555920067479168  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.17381180023675885  - accuracy: 0.75\n",
      "At: 452 [==========>] Loss 0.12224913471745152  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.1696889635763721  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.24093067479935643  - accuracy: 0.65625\n",
      "At: 455 [==========>] Loss 0.16392038530623645  - accuracy: 0.8125\n",
      "At: 456 [==========>] Loss 0.14556230183261484  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.11299898435221711  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.11882856349330524  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.21328195575874487  - accuracy: 0.65625\n",
      "At: 460 [==========>] Loss 0.14493107522866097  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.2222201835018902  - accuracy: 0.625\n",
      "At: 462 [==========>] Loss 0.1830243291590538  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.1408211401140486  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.20903980882659137  - accuracy: 0.6875\n",
      "At: 465 [==========>] Loss 0.17752109411601147  - accuracy: 0.71875\n",
      "At: 466 [==========>] Loss 0.12801637799784113  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.16219964741422044  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.0982555090345552  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.1346696212824514  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.1343588106166737  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.14809123767071536  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.14804302759089433  - accuracy: 0.78125\n",
      "At: 473 [==========>] Loss 0.17517975319623197  - accuracy: 0.75\n",
      "At: 474 [==========>] Loss 0.17950341407106674  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.15222794616660376  - accuracy: 0.78125\n",
      "At: 476 [==========>] Loss 0.17970026751864082  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.14943409694623977  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.13835393054127165  - accuracy: 0.78125\n",
      "At: 479 [==========>] Loss 0.21788598757738348  - accuracy: 0.71875\n",
      "At: 480 [==========>] Loss 0.2199669769757985  - accuracy: 0.6875\n",
      "At: 481 [==========>] Loss 0.11106175322330789  - accuracy: 0.8125\n",
      "At: 482 [==========>] Loss 0.08630403038468293  - accuracy: 0.9375\n",
      "At: 483 [==========>] Loss 0.12049071277291688  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.0944845204639716  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.10752226344895943  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.1675920666892564  - accuracy: 0.75\n",
      "At: 487 [==========>] Loss 0.14422665883754443  - accuracy: 0.84375\n",
      "At: 488 [==========>] Loss 0.12697729847008668  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.14576580104303896  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.15679643397588358  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.18135275343880797  - accuracy: 0.78125\n",
      "At: 492 [==========>] Loss 0.18125372419589464  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.15225762113613686  - accuracy: 0.8125\n",
      "At: 494 [==========>] Loss 0.17819688447795923  - accuracy: 0.71875\n",
      "At: 495 [==========>] Loss 0.154611259045578  - accuracy: 0.78125\n",
      "At: 496 [==========>] Loss 0.17670301783963294  - accuracy: 0.75\n",
      "At: 497 [==========>] Loss 0.16569106511539927  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.10935555001701508  - accuracy: 0.84375\n",
      "At: 499 [==========>] Loss 0.16801605093951574  - accuracy: 0.8125\n",
      "At: 500 [==========>] Loss 0.16067613251488005  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.19080796724265964  - accuracy: 0.8125\n",
      "At: 502 [==========>] Loss 0.1191497871854685  - accuracy: 0.875\n",
      "At: 503 [==========>] Loss 0.12384402317988409  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.10446497004195704  - accuracy: 0.875\n",
      "At: 505 [==========>] Loss 0.1984357648570147  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.2671311384172626  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.1343345348243707  - accuracy: 0.8125\n",
      "At: 508 [==========>] Loss 0.1211268922078624  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.1525555479322665  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.1792326887982231  - accuracy: 0.65625\n",
      "At: 511 [==========>] Loss 0.1139320170863391  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.16152956568811444  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.23858244635202291  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.16519954504321394  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.09983144704403919  - accuracy: 0.875\n",
      "At: 516 [==========>] Loss 0.15621550963827144  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.10815068799200407  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.15586551013580113  - accuracy: 0.875\n",
      "At: 519 [==========>] Loss 0.11183195033966714  - accuracy: 0.90625\n",
      "At: 520 [==========>] Loss 0.14298759419596976  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.14297791872492116  - accuracy: 0.84375\n",
      "At: 522 [==========>] Loss 0.1572834997838566  - accuracy: 0.78125\n",
      "At: 523 [==========>] Loss 0.1558608637884913  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.10899709368032152  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.19695268539523506  - accuracy: 0.75\n",
      "At: 526 [==========>] Loss 0.17920802230599356  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.2640260171154929  - accuracy: 0.59375\n",
      "At: 528 [==========>] Loss 0.17121530380791947  - accuracy: 0.78125\n",
      "At: 529 [==========>] Loss 0.1054723326993718  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.19565617094385937  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.1852773222886775  - accuracy: 0.6875\n",
      "At: 532 [==========>] Loss 0.11716652476805232  - accuracy: 0.90625\n",
      "At: 533 [==========>] Loss 0.08255558154043274  - accuracy: 0.84375\n",
      "At: 534 [==========>] Loss 0.1508684986264321  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.13130024712290933  - accuracy: 0.84375\n",
      "At: 536 [==========>] Loss 0.1782274451589831  - accuracy: 0.78125\n",
      "At: 537 [==========>] Loss 0.09225603238299085  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.15316802831886484  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.07918770628540915  - accuracy: 0.90625\n",
      "At: 540 [==========>] Loss 0.2190031777065794  - accuracy: 0.625\n",
      "At: 541 [==========>] Loss 0.1538003391945435  - accuracy: 0.78125\n",
      "At: 542 [==========>] Loss 0.14915750793438162  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.15871643959842882  - accuracy: 0.6875\n",
      "At: 544 [==========>] Loss 0.20947657483088497  - accuracy: 0.71875\n",
      "At: 545 [==========>] Loss 0.13409633106988014  - accuracy: 0.90625\n",
      "At: 546 [==========>] Loss 0.166522020546771  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.11819016219613565  - accuracy: 0.84375\n",
      "At: 548 [==========>] Loss 0.14088921888400693  - accuracy: 0.84375\n",
      "At: 549 [==========>] Loss 0.12382561452221993  - accuracy: 0.84375\n",
      "At: 550 [==========>] Loss 0.10056570051656867  - accuracy: 0.90625\n",
      "At: 551 [==========>] Loss 0.13906520202291933  - accuracy: 0.78125\n",
      "At: 552 [==========>] Loss 0.1671670328053197  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.15872272873421814  - accuracy: 0.78125\n",
      "At: 554 [==========>] Loss 0.08607060352976234  - accuracy: 0.9375\n",
      "At: 555 [==========>] Loss 0.14740361447218256  - accuracy: 0.71875\n",
      "At: 556 [==========>] Loss 0.14820197005081745  - accuracy: 0.8125\n",
      "At: 557 [==========>] Loss 0.12485977799803877  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.1332864419391459  - accuracy: 0.84375\n",
      "At: 559 [==========>] Loss 0.2112928703030767  - accuracy: 0.71875\n",
      "At: 560 [==========>] Loss 0.12329429934896252  - accuracy: 0.875\n",
      "At: 561 [==========>] Loss 0.14274471396998184  - accuracy: 0.78125\n",
      "At: 562 [==========>] Loss 0.0967868949395446  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.1104625054255058  - accuracy: 0.90625\n",
      "At: 564 [==========>] Loss 0.1965715153328332  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.10967571722449199  - accuracy: 0.875\n",
      "At: 566 [==========>] Loss 0.18534826902199822  - accuracy: 0.71875\n",
      "At: 567 [==========>] Loss 0.20637094589295052  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.22984650260331874  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.20050624507911133  - accuracy: 0.6875\n",
      "At: 570 [==========>] Loss 0.08357654286959494  - accuracy: 0.9375\n",
      "At: 571 [==========>] Loss 0.14127388635095423  - accuracy: 0.71875\n",
      "At: 572 [==========>] Loss 0.10407087558717777  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.10232532673018493  - accuracy: 0.84375\n",
      "At: 574 [==========>] Loss 0.15466227710946556  - accuracy: 0.875\n",
      "At: 575 [==========>] Loss 0.12837941849959136  - accuracy: 0.78125\n",
      "At: 576 [==========>] Loss 0.15919981512493167  - accuracy: 0.78125\n",
      "At: 577 [==========>] Loss 0.1746418903637998  - accuracy: 0.8125\n",
      "At: 578 [==========>] Loss 0.16199959705138467  - accuracy: 0.75\n",
      "At: 579 [==========>] Loss 0.12333793093960606  - accuracy: 0.84375\n",
      "At: 580 [==========>] Loss 0.15528506352780996  - accuracy: 0.75\n",
      "At: 581 [==========>] Loss 0.1592628480900055  - accuracy: 0.8125\n",
      "At: 582 [==========>] Loss 0.1594867345290938  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.1915348641737759  - accuracy: 0.6875\n",
      "At: 584 [==========>] Loss 0.11952620558303668  - accuracy: 0.8125\n",
      "At: 585 [==========>] Loss 0.1669137186609113  - accuracy: 0.75\n",
      "At: 586 [==========>] Loss 0.10365219958871863  - accuracy: 0.8125\n",
      "At: 587 [==========>] Loss 0.149523753516901  - accuracy: 0.78125\n",
      "At: 588 [==========>] Loss 0.15896763400872457  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.17785266053093995  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.08310417647025106  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.17879871969109584  - accuracy: 0.75\n",
      "At: 592 [==========>] Loss 0.09440476352232555  - accuracy: 0.9375\n",
      "At: 593 [==========>] Loss 0.2099086481027495  - accuracy: 0.65625\n",
      "At: 594 [==========>] Loss 0.17413087937977173  - accuracy: 0.71875\n",
      "At: 595 [==========>] Loss 0.16873623527290757  - accuracy: 0.78125\n",
      "At: 596 [==========>] Loss 0.11277483156684286  - accuracy: 0.875\n",
      "At: 597 [==========>] Loss 0.19849949855312699  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.16079183788854745  - accuracy: 0.78125\n",
      "At: 599 [==========>] Loss 0.14927831057492397  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.08355122116100148  - accuracy: 0.90625\n",
      "At: 601 [==========>] Loss 0.1342664961044593  - accuracy: 0.78125\n",
      "At: 602 [==========>] Loss 0.1383746877331813  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.14082625966650972  - accuracy: 0.75\n",
      "At: 604 [==========>] Loss 0.24272214489539357  - accuracy: 0.65625\n",
      "At: 605 [==========>] Loss 0.10176867536970202  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.1635243248341104  - accuracy: 0.71875\n",
      "At: 607 [==========>] Loss 0.16499664335148445  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.12201187150300902  - accuracy: 0.84375\n",
      "At: 609 [==========>] Loss 0.13173577718772067  - accuracy: 0.8125\n",
      "At: 610 [==========>] Loss 0.1218466452802432  - accuracy: 0.84375\n",
      "At: 611 [==========>] Loss 0.12091695274760339  - accuracy: 0.8125\n",
      "At: 612 [==========>] Loss 0.13362154098954843  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.18859738997328973  - accuracy: 0.75\n",
      "At: 614 [==========>] Loss 0.13254350957889155  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.17742464533420288  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.1859527463422652  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.11831367689841937  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.20479770697676797  - accuracy: 0.71875\n",
      "At: 619 [==========>] Loss 0.1292640865407295  - accuracy: 0.84375\n",
      "At: 620 [==========>] Loss 0.15687389175606364  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.08920670203216272  - accuracy: 0.96875\n",
      "At: 622 [==========>] Loss 0.20009299015468332  - accuracy: 0.65625\n",
      "At: 623 [==========>] Loss 0.14467933020582036  - accuracy: 0.75\n",
      "At: 624 [==========>] Loss 0.13041387368248542  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.14762721650039168  - accuracy: 0.875\n",
      "At: 626 [==========>] Loss 0.1371099662933144  - accuracy: 0.78125\n",
      "At: 627 [==========>] Loss 0.11017466752383676  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.11374111880782849  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.16859094729299257  - accuracy: 0.78125\n",
      "At: 630 [==========>] Loss 0.22870668287756293  - accuracy: 0.625\n",
      "At: 631 [==========>] Loss 0.1589157831652367  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.13986812735064114  - accuracy: 0.75\n",
      "At: 633 [==========>] Loss 0.15957295974378438  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.13379608538286097  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.13714341500763766  - accuracy: 0.78125\n",
      "At: 636 [==========>] Loss 0.1403256485202664  - accuracy: 0.84375\n",
      "At: 637 [==========>] Loss 0.12930235958903036  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.1288663591418279  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.14248629652326467  - accuracy: 0.84375\n",
      "At: 640 [==========>] Loss 0.21233096210012847  - accuracy: 0.71875\n",
      "At: 641 [==========>] Loss 0.1420681648688372  - accuracy: 0.78125\n",
      "At: 642 [==========>] Loss 0.18193368641704882  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.1328251561568492  - accuracy: 0.71875\n",
      "At: 644 [==========>] Loss 0.07584094700759633  - accuracy: 0.90625\n",
      "At: 645 [==========>] Loss 0.1138834341729752  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.12323726565284024  - accuracy: 0.875\n",
      "At: 647 [==========>] Loss 0.16804963580098614  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.16970820181460483  - accuracy: 0.75\n",
      "At: 649 [==========>] Loss 0.17848461628841158  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.12179661963383648  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.18422019215814434  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.10881468123117279  - accuracy: 0.84375\n",
      "At: 653 [==========>] Loss 0.10770306590591236  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.08142610802473377  - accuracy: 0.90625\n",
      "At: 655 [==========>] Loss 0.13511812339897059  - accuracy: 0.78125\n",
      "At: 656 [==========>] Loss 0.11915420286588185  - accuracy: 0.8125\n",
      "At: 657 [==========>] Loss 0.15245339718917272  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.12293016871861429  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.15228924972204094  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.12473496064578811  - accuracy: 0.8125\n",
      "At: 661 [==========>] Loss 0.1674577195603012  - accuracy: 0.71875\n",
      "At: 662 [==========>] Loss 0.09653899412701825  - accuracy: 0.875\n",
      "At: 663 [==========>] Loss 0.10861024184938886  - accuracy: 0.84375\n",
      "At: 664 [==========>] Loss 0.139869343606273  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.1972101902009777  - accuracy: 0.6875\n",
      "At: 666 [==========>] Loss 0.19222983133272117  - accuracy: 0.75\n",
      "At: 667 [==========>] Loss 0.1398422591843216  - accuracy: 0.8125\n",
      "At: 668 [==========>] Loss 0.18832464017098588  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.15507486881455085  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.19861210392187384  - accuracy: 0.6875\n",
      "At: 671 [==========>] Loss 0.14491027596886819  - accuracy: 0.8125\n",
      "At: 672 [==========>] Loss 0.10320426228340357  - accuracy: 0.875\n",
      "At: 673 [==========>] Loss 0.06249526496558396  - accuracy: 0.96875\n",
      "At: 674 [==========>] Loss 0.1222158882639284  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.09415532860476195  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.1641181929684877  - accuracy: 0.75\n",
      "At: 677 [==========>] Loss 0.12769230870433956  - accuracy: 0.875\n",
      "At: 678 [==========>] Loss 0.13410035221893585  - accuracy: 0.78125\n",
      "At: 679 [==========>] Loss 0.11132955457185834  - accuracy: 0.875\n",
      "At: 680 [==========>] Loss 0.11454233724348495  - accuracy: 0.75\n",
      "At: 681 [==========>] Loss 0.12120022604727049  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.135006316733692  - accuracy: 0.84375\n",
      "At: 683 [==========>] Loss 0.17257693387197157  - accuracy: 0.75\n",
      "At: 684 [==========>] Loss 0.09122103853676894  - accuracy: 0.90625\n",
      "At: 685 [==========>] Loss 0.18946834340898944  - accuracy: 0.75\n",
      "At: 686 [==========>] Loss 0.10448417513308178  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.08612017614529381  - accuracy: 0.875\n",
      "At: 688 [==========>] Loss 0.10776999920340194  - accuracy: 0.90625\n",
      "At: 689 [==========>] Loss 0.13711625103422553  - accuracy: 0.84375\n",
      "At: 690 [==========>] Loss 0.10949974383216543  - accuracy: 0.875\n",
      "At: 691 [==========>] Loss 0.0954295039774759  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.11757806334214074  - accuracy: 0.84375\n",
      "At: 693 [==========>] Loss 0.17600558196914526  - accuracy: 0.75\n",
      "At: 694 [==========>] Loss 0.17792281538735774  - accuracy: 0.78125\n",
      "At: 695 [==========>] Loss 0.13869541965477422  - accuracy: 0.8125\n",
      "At: 696 [==========>] Loss 0.1639410624608996  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.19761234579391307  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.104037618165779  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.10582205365869163  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.10188139653047822  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.1210509968040371  - accuracy: 0.84375\n",
      "At: 702 [==========>] Loss 0.10744408279830181  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.19794667050521952  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.1822028262260738  - accuracy: 0.6875\n",
      "At: 705 [==========>] Loss 0.23512560011726058  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.19730057686530622  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.11080915809464717  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.18179250387624346  - accuracy: 0.71875\n",
      "At: 709 [==========>] Loss 0.21471604554500814  - accuracy: 0.75\n",
      "At: 710 [==========>] Loss 0.11822324719544686  - accuracy: 0.84375\n",
      "At: 711 [==========>] Loss 0.18252233197747264  - accuracy: 0.71875\n",
      "At: 712 [==========>] Loss 0.166836876410775  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.17496397257472526  - accuracy: 0.6875\n",
      "At: 714 [==========>] Loss 0.19188280318570053  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.11795370728848151  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.11679743038578012  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.08883581253060932  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.18237674070069163  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.13098842387848508  - accuracy: 0.8125\n",
      "At: 720 [==========>] Loss 0.1259612425280378  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.11870227117902574  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.15026867942129069  - accuracy: 0.78125\n",
      "At: 723 [==========>] Loss 0.14862430974560525  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.09010102907526962  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.14585482198288283  - accuracy: 0.8125\n",
      "At: 726 [==========>] Loss 0.21187954307762746  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.19035245155739405  - accuracy: 0.71875\n",
      "At: 728 [==========>] Loss 0.13209087647310688  - accuracy: 0.8125\n",
      "At: 729 [==========>] Loss 0.21171773343503314  - accuracy: 0.625\n",
      "At: 730 [==========>] Loss 0.18673233096433528  - accuracy: 0.75\n",
      "At: 731 [==========>] Loss 0.13894792382617283  - accuracy: 0.8125\n",
      "At: 732 [==========>] Loss 0.158488975117151  - accuracy: 0.78125\n",
      "At: 733 [==========>] Loss 0.10351312311613084  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.16948207474448723  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.12627134638115894  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.11605607505334595  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.1676670632013187  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.08775870098470434  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.09704549526517578  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.17319664784524885  - accuracy: 0.78125\n",
      "At: 741 [==========>] Loss 0.11277383986697895  - accuracy: 0.84375\n",
      "At: 742 [==========>] Loss 0.11887240813225303  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.12917375678426055  - accuracy: 0.84375\n",
      "At: 744 [==========>] Loss 0.12965159202061513  - accuracy: 0.875\n",
      "At: 745 [==========>] Loss 0.20359222988536202  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.12394031249181611  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.14167275078954697  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.15074486299486536  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.1602196821044215  - accuracy: 0.78125\n",
      "At: 750 [==========>] Loss 0.12275468808209655  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.1689740270701799  - accuracy: 0.8125\n",
      "At: 752 [==========>] Loss 0.11221130807571006  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.1720981127900469  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.13752079423167435  - accuracy: 0.71875\n",
      "At: 755 [==========>] Loss 0.09338589538966435  - accuracy: 0.90625\n",
      "At: 756 [==========>] Loss 0.20770736220857552  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.08695449926702803  - accuracy: 0.90625\n",
      "At: 758 [==========>] Loss 0.10482998337261908  - accuracy: 0.84375\n",
      "At: 759 [==========>] Loss 0.1078959992088787  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.21174054170900114  - accuracy: 0.65625\n",
      "At: 761 [==========>] Loss 0.16097406135800277  - accuracy: 0.84375\n",
      "At: 762 [==========>] Loss 0.13620881495430318  - accuracy: 0.75\n",
      "At: 763 [==========>] Loss 0.12688927984610096  - accuracy: 0.78125\n",
      "At: 764 [==========>] Loss 0.10403115523780233  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.15712801433732618  - accuracy: 0.8125\n",
      "At: 766 [==========>] Loss 0.137932591658395  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.11692407367974267  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.1661564269406302  - accuracy: 0.75\n",
      "At: 769 [==========>] Loss 0.1710690405241011  - accuracy: 0.71875\n",
      "At: 770 [==========>] Loss 0.11692844827946156  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.18720957508639152  - accuracy: 0.75\n",
      "At: 772 [==========>] Loss 0.14076120943911677  - accuracy: 0.8125\n",
      "At: 773 [==========>] Loss 0.10526444359986253  - accuracy: 0.84375\n",
      "At: 774 [==========>] Loss 0.1549110925717421  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.1709284999350028  - accuracy: 0.75\n",
      "At: 776 [==========>] Loss 0.19955018384255785  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.08814402093660992  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.1581699278341459  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.13954246049867053  - accuracy: 0.75\n",
      "At: 780 [==========>] Loss 0.09068605685240262  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.1358085040126041  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.1541938394192783  - accuracy: 0.8125\n",
      "At: 783 [==========>] Loss 0.15873443762448472  - accuracy: 0.78125\n",
      "At: 784 [==========>] Loss 0.17759465755494502  - accuracy: 0.75\n",
      "At: 785 [==========>] Loss 0.2218241861250011  - accuracy: 0.65625\n",
      "At: 786 [==========>] Loss 0.1409206939070775  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.1918075411181411  - accuracy: 0.6875\n",
      "At: 788 [==========>] Loss 0.0947162774813239  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.16179258575038827  - accuracy: 0.71875\n",
      "At: 790 [==========>] Loss 0.12173011884097143  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.1628145952764363  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.15729559138118293  - accuracy: 0.75\n",
      "At: 793 [==========>] Loss 0.11370291655646962  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.11223047659771399  - accuracy: 0.875\n",
      "At: 795 [==========>] Loss 0.1541843708749349  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.1572797397470102  - accuracy: 0.75\n",
      "At: 797 [==========>] Loss 0.1668031763938651  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.16525850644585435  - accuracy: 0.71875\n",
      "At: 799 [==========>] Loss 0.0690881399540518  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.1635114081251556  - accuracy: 0.71875\n",
      "At: 801 [==========>] Loss 0.12465622424232624  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.16242842648534178  - accuracy: 0.84375\n",
      "At: 803 [==========>] Loss 0.1871743205788432  - accuracy: 0.71875\n",
      "At: 804 [==========>] Loss 0.12895266340282635  - accuracy: 0.8125\n",
      "At: 805 [==========>] Loss 0.13653284025767953  - accuracy: 0.84375\n",
      "At: 806 [==========>] Loss 0.09615251457067173  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.10518983672740682  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.1430227538597371  - accuracy: 0.78125\n",
      "At: 809 [==========>] Loss 0.12295158937081578  - accuracy: 0.8125\n",
      "At: 810 [==========>] Loss 0.16210024090532849  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.12571576375470794  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.12553369387273572  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.2010686945486072  - accuracy: 0.75\n",
      "At: 814 [==========>] Loss 0.20777519638224834  - accuracy: 0.71875\n",
      "At: 815 [==========>] Loss 0.15137855925507923  - accuracy: 0.78125\n",
      "At: 816 [==========>] Loss 0.15691062995438979  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.08217894343283666  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.11615137230830053  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.12342501077589553  - accuracy: 0.875\n",
      "At: 820 [==========>] Loss 0.14616186118831181  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.16085217087699405  - accuracy: 0.71875\n",
      "At: 822 [==========>] Loss 0.1525579994346893  - accuracy: 0.75\n",
      "At: 823 [==========>] Loss 0.16691340573330443  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.15702954368980357  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.19026621392274712  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.13495141174692848  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.07956146812977499  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.0686596538054593  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09057459539736824  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.09867552178939037  - accuracy: 0.90625\n",
      "At: 831 [==========>] Loss 0.10648010254031468  - accuracy: 0.875\n",
      "At: 832 [==========>] Loss 0.11326870823069879  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.17285728580973236  - accuracy: 0.78125\n",
      "At: 834 [==========>] Loss 0.11287399747525435  - accuracy: 0.84375\n",
      "At: 835 [==========>] Loss 0.08514193113188036  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.1327640668347528  - accuracy: 0.875\n",
      "At: 837 [==========>] Loss 0.12796282316541224  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.1045971904807887  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.13244489918889668  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.11774052426108284  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.06330135330987596  - accuracy: 0.96875\n",
      "At: 842 [==========>] Loss 0.058883580920851886  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.18289014671767273  - accuracy: 0.8125\n",
      "At: 844 [==========>] Loss 0.13258406321465419  - accuracy: 0.75\n",
      "At: 845 [==========>] Loss 0.1591554577335514  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.146573425701574  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.09724437654326759  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.15968718102187807  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.1429144674920606  - accuracy: 0.875\n",
      "At: 850 [==========>] Loss 0.08065672929502846  - accuracy: 0.90625\n",
      "At: 851 [==========>] Loss 0.08912469356882889  - accuracy: 0.84375\n",
      "At: 852 [==========>] Loss 0.16032194975687006  - accuracy: 0.75\n",
      "At: 853 [==========>] Loss 0.14158943922153333  - accuracy: 0.84375\n",
      "At: 854 [==========>] Loss 0.19563362321561573  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.09227142160898129  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.08214446059671296  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.08313833780944455  - accuracy: 0.90625\n",
      "At: 858 [==========>] Loss 0.26865751392388326  - accuracy: 0.625\n",
      "At: 859 [==========>] Loss 0.11576254338226338  - accuracy: 0.8125\n",
      "At: 860 [==========>] Loss 0.13667080489160308  - accuracy: 0.8125\n",
      "At: 861 [==========>] Loss 0.11607796561341022  - accuracy: 0.84375\n",
      "At: 862 [==========>] Loss 0.09564486322574932  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.12955658794688732  - accuracy: 0.84375\n",
      "At: 864 [==========>] Loss 0.1837264076613647  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.17985666183869856  - accuracy: 0.78125\n",
      "At: 866 [==========>] Loss 0.17661194475644787  - accuracy: 0.71875\n",
      "At: 867 [==========>] Loss 0.10704418079101008  - accuracy: 0.84375\n",
      "At: 868 [==========>] Loss 0.1588907191950885  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.15196003497040778  - accuracy: 0.8125\n",
      "At: 870 [==========>] Loss 0.14948030661456735  - accuracy: 0.78125\n",
      "At: 871 [==========>] Loss 0.09227397214823055  - accuracy: 0.84375\n",
      "At: 872 [==========>] Loss 0.12308172198829784  - accuracy: 0.84375\n",
      "At: 873 [==========>] Loss 0.19845718565053463  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.16241684304679985  - accuracy: 0.84375\n",
      "At: 875 [==========>] Loss 0.12995520442476097  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.14394128303403214  - accuracy: 0.71875\n",
      "At: 877 [==========>] Loss 0.17040145081124258  - accuracy: 0.78125\n",
      "At: 878 [==========>] Loss 0.06962218274418708  - accuracy: 0.875\n",
      "At: 879 [==========>] Loss 0.16721897271467018  - accuracy: 0.71875\n",
      "At: 880 [==========>] Loss 0.10614464146808651  - accuracy: 0.8125\n",
      "At: 881 [==========>] Loss 0.12954840828217312  - accuracy: 0.8125\n",
      "At: 882 [==========>] Loss 0.12338365423418046  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.125794873955775  - accuracy: 0.78125\n",
      "At: 884 [==========>] Loss 0.16472299534702564  - accuracy: 0.71875\n",
      "At: 885 [==========>] Loss 0.13233848331261142  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.1065219093055772  - accuracy: 0.875\n",
      "At: 887 [==========>] Loss 0.13864624562120373  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.17751528984306622  - accuracy: 0.71875\n",
      "At: 889 [==========>] Loss 0.10961502170114476  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.14903099868830902  - accuracy: 0.75\n",
      "At: 891 [==========>] Loss 0.10514262525254717  - accuracy: 0.8125\n",
      "At: 892 [==========>] Loss 0.12770411721353864  - accuracy: 0.78125\n",
      "At: 893 [==========>] Loss 0.13584467357183305  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.11657899765693051  - accuracy: 0.84375\n",
      "At: 895 [==========>] Loss 0.1253669944261223  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.12328442181693683  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.1374042200368364  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.16142211610745621  - accuracy: 0.84375\n",
      "At: 899 [==========>] Loss 0.10961542159083364  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.11932087811121057  - accuracy: 0.84375\n",
      "At: 901 [==========>] Loss 0.17165971948850323  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.09282090712065885  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.1376003111332042  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.13662459839660035  - accuracy: 0.78125\n",
      "At: 905 [==========>] Loss 0.09098651782391726  - accuracy: 0.875\n",
      "At: 906 [==========>] Loss 0.09342912665343434  - accuracy: 0.84375\n",
      "At: 907 [==========>] Loss 0.15463201076494723  - accuracy: 0.78125\n",
      "At: 908 [==========>] Loss 0.14315247469371564  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.0829917070688877  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.12562628859685626  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.16612136596848034  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.1593714093844717  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.12169498854632058  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.12376596433031518  - accuracy: 0.78125\n",
      "At: 915 [==========>] Loss 0.15550845733335916  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.18680735866414094  - accuracy: 0.65625\n",
      "At: 917 [==========>] Loss 0.16819590750124086  - accuracy: 0.71875\n",
      "At: 918 [==========>] Loss 0.18422796380033415  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.13128720609887448  - accuracy: 0.71875\n",
      "At: 920 [==========>] Loss 0.1328581049077531  - accuracy: 0.8125\n",
      "At: 921 [==========>] Loss 0.14424877323912844  - accuracy: 0.8125\n",
      "At: 922 [==========>] Loss 0.13336569907574117  - accuracy: 0.84375\n",
      "At: 923 [==========>] Loss 0.11706580063292937  - accuracy: 0.8125\n",
      "At: 924 [==========>] Loss 0.19195311128488643  - accuracy: 0.71875\n",
      "At: 925 [==========>] Loss 0.1308625179708635  - accuracy: 0.84375\n",
      "At: 926 [==========>] Loss 0.16210698508434138  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.11474767908684239  - accuracy: 0.90625\n",
      "At: 928 [==========>] Loss 0.14700442120696464  - accuracy: 0.78125\n",
      "At: 929 [==========>] Loss 0.14467301088489812  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.12851621010653463  - accuracy: 0.8125\n",
      "At: 931 [==========>] Loss 0.1614187611260859  - accuracy: 0.75\n",
      "At: 932 [==========>] Loss 0.09597991741133399  - accuracy: 0.84375\n",
      "At: 933 [==========>] Loss 0.10087783580905257  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.14912758479290272  - accuracy: 0.75\n",
      "At: 935 [==========>] Loss 0.06259400210656968  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.1604070503331744  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.14582795959459857  - accuracy: 0.8125\n",
      "At: 938 [==========>] Loss 0.14132950620166837  - accuracy: 0.8125\n",
      "At: 939 [==========>] Loss 0.11099557934626039  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.21948316006778937  - accuracy: 0.71875\n",
      "At: 941 [==========>] Loss 0.10141596248165943  - accuracy: 0.875\n",
      "At: 942 [==========>] Loss 0.1395742240672747  - accuracy: 0.84375\n",
      "At: 943 [==========>] Loss 0.09314627422239236  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.10832335014926851  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.09359481212050552  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.1333460856075806  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.13948828679956649  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.19072276616682057  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.08248948705113285  - accuracy: 0.84375\n",
      "At: 950 [==========>] Loss 0.09996593569110664  - accuracy: 0.875\n",
      "At: 951 [==========>] Loss 0.09403275108806847  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.08661054863670689  - accuracy: 0.9375\n",
      "At: 953 [==========>] Loss 0.08277971396653089  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.11163991368596866  - accuracy: 0.8125\n",
      "At: 955 [==========>] Loss 0.15443020678437686  - accuracy: 0.8125\n",
      "At: 956 [==========>] Loss 0.10518878781764841  - accuracy: 0.84375\n",
      "At: 957 [==========>] Loss 0.13188916210378712  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.10050893156468366  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.09807510731035657  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.11113583340580463  - accuracy: 0.8125\n",
      "At: 961 [==========>] Loss 0.11579047528444668  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.1064514757927465  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.09741312660559183  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.17259562961546962  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.11694767335706716  - accuracy: 0.90625\n",
      "At: 966 [==========>] Loss 0.17812284841277382  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.11782266224448064  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.14749104992113393  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.13566490767988934  - accuracy: 0.78125\n",
      "At: 970 [==========>] Loss 0.10902898332983929  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.11343729853477527  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.07134203245021717  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.11107227246111548  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.09641040082634837  - accuracy: 0.875\n",
      "At: 975 [==========>] Loss 0.1598198622528818  - accuracy: 0.78125\n",
      "At: 976 [==========>] Loss 0.13007922690197432  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.10479687604066013  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.16614324422614052  - accuracy: 0.71875\n",
      "At: 979 [==========>] Loss 0.11295255184530402  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.14090531571335327  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.1795592729678433  - accuracy: 0.6875\n",
      "At: 982 [==========>] Loss 0.07298438093513275  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.13177121477352677  - accuracy: 0.84375\n",
      "At: 984 [==========>] Loss 0.07561398825655472  - accuracy: 0.9375\n",
      "At: 985 [==========>] Loss 0.17165266687359143  - accuracy: 0.75\n",
      "At: 986 [==========>] Loss 0.0962745420436434  - accuracy: 0.90625\n",
      "At: 987 [==========>] Loss 0.11542824500978023  - accuracy: 0.84375\n",
      "At: 988 [==========>] Loss 0.11484775434720233  - accuracy: 0.84375\n",
      "At: 989 [==========>] Loss 0.1423833829016346  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.14279925119224313  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.13802808802103428  - accuracy: 0.8125\n",
      "At: 992 [==========>] Loss 0.22614344165444084  - accuracy: 0.6875\n",
      "At: 993 [==========>] Loss 0.14451573809497445  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.1594806453536834  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.16937064559216564  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.05834416252500715  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.1434073981629466  - accuracy: 0.75\n",
      "At: 998 [==========>] Loss 0.12525827425093433  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.14742326800433847  - accuracy: 0.84375\n",
      "At: 1000 [==========>] Loss 0.20611433945419905  - accuracy: 0.6875\n",
      "At: 1001 [==========>] Loss 0.16256706196534562  - accuracy: 0.75\n",
      "At: 1002 [==========>] Loss 0.1723965991094502  - accuracy: 0.78125\n",
      "At: 1003 [==========>] Loss 0.12358799285469671  - accuracy: 0.84375\n",
      "At: 1004 [==========>] Loss 0.13920505820952034  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.0803626992055592  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.11231401494456483  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.13894065242990877  - accuracy: 0.75\n",
      "At: 1008 [==========>] Loss 0.16992183852311216  - accuracy: 0.6875\n",
      "At: 1009 [==========>] Loss 0.14048345242077392  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.1256519623609748  - accuracy: 0.84375\n",
      "At: 1011 [==========>] Loss 0.15524924810543925  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.082759758797076  - accuracy: 0.90625\n",
      "At: 1013 [==========>] Loss 0.092628944173754  - accuracy: 0.875\n",
      "At: 1014 [==========>] Loss 0.08401746878631719  - accuracy: 0.90625\n",
      "At: 1015 [==========>] Loss 0.17248676383109934  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.12535986171666333  - accuracy: 0.8125\n",
      "At: 1017 [==========>] Loss 0.17876610780131427  - accuracy: 0.71875\n",
      "At: 1018 [==========>] Loss 0.15726273225947168  - accuracy: 0.75\n",
      "At: 1019 [==========>] Loss 0.1878628910300323  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.14081561894299677  - accuracy: 0.8125\n",
      "At: 1021 [==========>] Loss 0.11502550584883182  - accuracy: 0.875\n",
      "At: 1022 [==========>] Loss 0.14170089529509206  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.1773789674393908  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.2046872512178069  - accuracy: 0.6875\n",
      "At: 1025 [==========>] Loss 0.1950420162234146  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.09873496864894643  - accuracy: 0.90625\n",
      "At: 1027 [==========>] Loss 0.1191092892638525  - accuracy: 0.875\n",
      "At: 1028 [==========>] Loss 0.21231296391356277  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.10588104831989247  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.122038645009046  - accuracy: 0.875\n",
      "At: 1031 [==========>] Loss 0.13636792050513888  - accuracy: 0.8125\n",
      "At: 1032 [==========>] Loss 0.13386766681693124  - accuracy: 0.78125\n",
      "At: 1033 [==========>] Loss 0.12076404420688755  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.09715300349895548  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.11032450623560144  - accuracy: 0.84375\n",
      "At: 1036 [==========>] Loss 0.1904392502074258  - accuracy: 0.71875\n",
      "At: 1037 [==========>] Loss 0.14594229064081943  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.09636625956558259  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.10367497791748022  - accuracy: 0.9375\n",
      "At: 1040 [==========>] Loss 0.1075659480828853  - accuracy: 0.875\n",
      "At: 1041 [==========>] Loss 0.142195180767238  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.11671932701263094  - accuracy: 0.75\n",
      "At: 1043 [==========>] Loss 0.17982516544274196  - accuracy: 0.71875\n",
      "At: 1044 [==========>] Loss 0.12480477228714983  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.154454036846453  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.19362658792987347  - accuracy: 0.65625\n",
      "At: 1047 [==========>] Loss 0.12255259744826827  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.17609004939886186  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.13786330300044336  - accuracy: 0.8125\n",
      "At: 1050 [==========>] Loss 0.13094401098330222  - accuracy: 0.84375\n",
      "At: 1051 [==========>] Loss 0.07955432633792227  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.13221730232082415  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.09705407366268379  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.0981925617162228  - accuracy: 0.90625\n",
      "At: 1055 [==========>] Loss 0.1628218264677324  - accuracy: 0.75\n",
      "At: 1056 [==========>] Loss 0.11924478293609586  - accuracy: 0.84375\n",
      "At: 1057 [==========>] Loss 0.13559490710010072  - accuracy: 0.8125\n",
      "At: 1058 [==========>] Loss 0.06958435361446365  - accuracy: 0.9375\n",
      "At: 1059 [==========>] Loss 0.08163266588588869  - accuracy: 0.9375\n",
      "At: 1060 [==========>] Loss 0.11720265430640284  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.07630053991879657  - accuracy: 0.96875\n",
      "At: 1062 [==========>] Loss 0.17234742086187874  - accuracy: 0.71875\n",
      "At: 1063 [==========>] Loss 0.11260723748572138  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.1600739299343675  - accuracy: 0.71875\n",
      "At: 1065 [==========>] Loss 0.07317797089772832  - accuracy: 0.96875\n",
      "At: 1066 [==========>] Loss 0.1141439428811728  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.1120378891876209  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.08978256226941586  - accuracy: 0.9375\n",
      "At: 1069 [==========>] Loss 0.1267354340179913  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.12811929723999071  - accuracy: 0.84375\n",
      "At: 1071 [==========>] Loss 0.09933587718549164  - accuracy: 0.875\n",
      "At: 1072 [==========>] Loss 0.1296097775892607  - accuracy: 0.875\n",
      "At: 1073 [==========>] Loss 0.15014960187749  - accuracy: 0.84375\n",
      "At: 1074 [==========>] Loss 0.1718209985077443  - accuracy: 0.71875\n",
      "At: 1075 [==========>] Loss 0.12386843034514447  - accuracy: 0.8125\n",
      "At: 1076 [==========>] Loss 0.14317006084033532  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.08497553692179281  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.10482050304302881  - accuracy: 0.84375\n",
      "At: 1079 [==========>] Loss 0.1124028142510538  - accuracy: 0.875\n",
      "At: 1080 [==========>] Loss 0.14776466505354607  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.09855229138336043  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.12047485397497718  - accuracy: 0.84375\n",
      "At: 1083 [==========>] Loss 0.11130684702371926  - accuracy: 0.875\n",
      "At: 1084 [==========>] Loss 0.10409901375398367  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.1293572689975171  - accuracy: 0.78125\n",
      "At: 1086 [==========>] Loss 0.13053343653835847  - accuracy: 0.84375\n",
      "At: 1087 [==========>] Loss 0.1375370391705613  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.1608053096504328  - accuracy: 0.71875\n",
      "At: 1089 [==========>] Loss 0.09431943600912124  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.0919947975495893  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.18743585179897804  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.09060619197024812  - accuracy: 0.90625\n",
      "At: 1093 [==========>] Loss 0.16461676562528899  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.13899859351886104  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.1550450926746596  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.11545416597182917  - accuracy: 0.84375\n",
      "At: 1097 [==========>] Loss 0.0553092943411336  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.146365376822226  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.12683609093388148  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.09552240163907036  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.1089801707090034  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.14415903301735686  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.09545637407511322  - accuracy: 0.8125\n",
      "At: 1104 [==========>] Loss 0.07451420798741977  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.08031056187165209  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.07364292082466965  - accuracy: 0.84375\n",
      "At: 1107 [==========>] Loss 0.18493623636301926  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.10043517510751626  - accuracy: 0.84375\n",
      "At: 1109 [==========>] Loss 0.058016534408332696  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.12252371364171005  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.18994462562310313  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.1645738896784268  - accuracy: 0.75\n",
      "At: 1113 [==========>] Loss 0.16936315110651812  - accuracy: 0.75\n",
      "At: 1114 [==========>] Loss 0.07519460568922251  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.11375100652232939  - accuracy: 0.875\n",
      "At: 1116 [==========>] Loss 0.11480041990262513  - accuracy: 0.875\n",
      "At: 1117 [==========>] Loss 0.0545569633498838  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.13062207761272715  - accuracy: 0.75\n",
      "At: 1119 [==========>] Loss 0.13379355662015185  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.09815211816411393  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.10731247501277655  - accuracy: 0.78125\n",
      "At: 1122 [==========>] Loss 0.10290891900874599  - accuracy: 0.875\n",
      "At: 1123 [==========>] Loss 0.12476171805507012  - accuracy: 0.84375\n",
      "At: 1124 [==========>] Loss 0.13717792453201189  - accuracy: 0.75\n",
      "At: 1125 [==========>] Loss 0.14376433257978394  - accuracy: 0.8125\n",
      "At: 1126 [==========>] Loss 0.10534886486679283  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.11594478969917363  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.07371867817026204  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.15156146842252874  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.11445088600277886  - accuracy: 0.875\n",
      "At: 1131 [==========>] Loss 0.1016173137673003  - accuracy: 0.90625\n",
      "At: 1132 [==========>] Loss 0.11983449011134081  - accuracy: 0.78125\n",
      "At: 1133 [==========>] Loss 0.15459326631777548  - accuracy: 0.8125\n",
      "At: 1134 [==========>] Loss 0.10829571089523679  - accuracy: 0.90625\n",
      "At: 1135 [==========>] Loss 0.12706735292873  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.14643486092899194  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.11873097262579703  - accuracy: 0.78125\n",
      "At: 1138 [==========>] Loss 0.11117362278077667  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.0901777345066494  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.17296065632322571  - accuracy: 0.71875\n",
      "At: 1141 [==========>] Loss 0.13188017413576092  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.14411447566405025  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.06803980395783009  - accuracy: 0.90625\n",
      "At: 1144 [==========>] Loss 0.1122305466974104  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.15291347882966957  - accuracy: 0.75\n",
      "At: 1146 [==========>] Loss 0.11396595137397479  - accuracy: 0.875\n",
      "At: 1147 [==========>] Loss 0.1921804736297157  - accuracy: 0.75\n",
      "At: 1148 [==========>] Loss 0.08334864172303103  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.11312329758536172  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.13563108065782653  - accuracy: 0.75\n",
      "At: 1151 [==========>] Loss 0.15610934896361353  - accuracy: 0.75\n",
      "At: 1152 [==========>] Loss 0.12137241060193177  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.17337107013537204  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.08630695141062997  - accuracy: 0.9375\n",
      "At: 1155 [==========>] Loss 0.09972591512248662  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.14695710144498425  - accuracy: 0.8125\n",
      "At: 1157 [==========>] Loss 0.1149577559537919  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.18903386944849004  - accuracy: 0.71875\n",
      "At: 1159 [==========>] Loss 0.10432604325948666  - accuracy: 0.875\n",
      "At: 1160 [==========>] Loss 0.1020118999102975  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.07006262973718592  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.1473662191664407  - accuracy: 0.78125\n",
      "At: 1163 [==========>] Loss 0.15808630342600555  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.0832086580328216  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.14991217204847218  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.08760399538096317  - accuracy: 0.96875\n",
      "At: 1167 [==========>] Loss 0.149565590118924  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.11689610695415763  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.11858307832272497  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.1523084031694003  - accuracy: 0.8125\n",
      "At: 1171 [==========>] Loss 0.06620348349545768  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.1260274750086026  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.11213300567874018  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.18353973504460758  - accuracy: 0.71875\n",
      "At: 1175 [==========>] Loss 0.13738139035692162  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.1281980751022966  - accuracy: 0.75\n",
      "At: 1177 [==========>] Loss 0.07325872920373208  - accuracy: 0.9375\n",
      "At: 1178 [==========>] Loss 0.16634954768521232  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.12168414460482922  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.21468293452041767  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.12166406250995875  - accuracy: 0.8125\n",
      "At: 1182 [==========>] Loss 0.1233607846246375  - accuracy: 0.78125\n",
      "At: 1183 [==========>] Loss 0.15614321738594356  - accuracy: 0.8125\n",
      "At: 1184 [==========>] Loss 0.14374174290044434  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.09723814990554884  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.14984801108639723  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.12652334748916105  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.07453071354665514  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.14978553089064878  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.128732777814911  - accuracy: 0.8125\n",
      "At: 1191 [==========>] Loss 0.1754950353651824  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.10437494202042824  - accuracy: 0.875\n",
      "At: 1193 [==========>] Loss 0.14420005024676563  - accuracy: 0.71875\n",
      "At: 1194 [==========>] Loss 0.11291809548334297  - accuracy: 0.8125\n",
      "At: 1195 [==========>] Loss 0.119695832038173  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.11617739605733284  - accuracy: 0.8125\n",
      "At: 1197 [==========>] Loss 0.10849219600437818  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.08318936667644591  - accuracy: 0.90625\n",
      "At: 1199 [==========>] Loss 0.190693636509456  - accuracy: 0.65625\n",
      "At: 1200 [==========>] Loss 0.0826436459466154  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.13284219506542186  - accuracy: 0.78125\n",
      "At: 1202 [==========>] Loss 0.15666636114730573  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.1342079485295058  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.08507273977254814  - accuracy: 0.9375\n",
      "At: 1205 [==========>] Loss 0.06562442737457383  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.12203763890306227  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.15921892345255115  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.09643690000889234  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.1276956204803119  - accuracy: 0.75\n",
      "At: 1210 [==========>] Loss 0.13565272808171605  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.19041122283380504  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.0828424377186219  - accuracy: 0.9375\n",
      "At: 1213 [==========>] Loss 0.19878678185823734  - accuracy: 0.71875\n",
      "At: 1214 [==========>] Loss 0.15120465546227999  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.16182090210744338  - accuracy: 0.71875\n",
      "At: 1216 [==========>] Loss 0.10879998250451067  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.08286236515974207  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.11776264076765325  - accuracy: 0.78125\n",
      "At: 1219 [==========>] Loss 0.13835950291543858  - accuracy: 0.8125\n",
      "At: 1220 [==========>] Loss 0.1433772183019201  - accuracy: 0.78125\n",
      "At: 1221 [==========>] Loss 0.08131766736012497  - accuracy: 0.9375\n",
      "At: 1222 [==========>] Loss 0.19001031638733967  - accuracy: 0.625\n",
      "At: 1223 [==========>] Loss 0.09787561665870533  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.06578582447643079  - accuracy: 0.96875\n",
      "At: 1225 [==========>] Loss 0.09029185290875594  - accuracy: 0.9375\n",
      "At: 1226 [==========>] Loss 0.10697249068321142  - accuracy: 0.8125\n",
      "At: 1227 [==========>] Loss 0.1707475755654606  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.14179964302476372  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.13323664797944446  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.1550697891633103  - accuracy: 0.8125\n",
      "At: 1231 [==========>] Loss 0.13063744140776112  - accuracy: 0.8125\n",
      "At: 1232 [==========>] Loss 0.07880512077036235  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.11940412558804994  - accuracy: 0.75\n",
      "At: 1234 [==========>] Loss 0.14776968265618523  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.07458649038371709  - accuracy: 0.9375\n",
      "At: 1236 [==========>] Loss 0.1315621409700406  - accuracy: 0.84375\n",
      "At: 1237 [==========>] Loss 0.0738312165360281  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.10301146288519553  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.17222965620057235  - accuracy: 0.78125\n",
      "At: 1240 [==========>] Loss 0.09950928801585804  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.10894508914630552  - accuracy: 0.78125\n",
      "At: 1242 [==========>] Loss 0.1520618270546253  - accuracy: 0.84375\n",
      "At: 1243 [==========>] Loss 0.12644911151539825  - accuracy: 0.875\n",
      "At: 1244 [==========>] Loss 0.15889882165809935  - accuracy: 0.75\n",
      "At: 1245 [==========>] Loss 0.12375429603639726  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.07730089428266207  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.16377940144291006  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.08105959709156173  - accuracy: 0.90625\n",
      "At: 1249 [==========>] Loss 0.1265980410156818  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.1182822911087105  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.14756930016691466  - accuracy: 0.78125\n",
      "At: 1252 [==========>] Loss 0.0879667768868959  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.10543432014346191  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.18155027099526855  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.1054347786836562  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.1386282530950152  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.10997804615338337  - accuracy: 0.875\n",
      "At: 1258 [==========>] Loss 0.09114129815794758  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.1147209435838084  - accuracy: 0.84375\n",
      "At: 1260 [==========>] Loss 0.11748010167464902  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.13652589030759477  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.13093987061498613  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.10015458198662569  - accuracy: 0.875\n",
      "At: 1264 [==========>] Loss 0.10713185041634403  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.14791784076954606  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.12182807778030731  - accuracy: 0.84375\n",
      "At: 1267 [==========>] Loss 0.1156464833889885  - accuracy: 0.84375\n",
      "At: 1268 [==========>] Loss 0.13592980061056242  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.1069672873167489  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.13817243272180785  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.15474480736823398  - accuracy: 0.8125\n",
      "At: 1272 [==========>] Loss 0.06315901434351494  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.20680758276707428  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.13286999859467485  - accuracy: 0.84375\n",
      "At: 1275 [==========>] Loss 0.10360227377962178  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.08564639150458701  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.08161161523603205  - accuracy: 0.875\n",
      "At: 1278 [==========>] Loss 0.1375086741824183  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.09581748776916549  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.09777767579003328  - accuracy: 0.875\n",
      "At: 1281 [==========>] Loss 0.14292578355566268  - accuracy: 0.875\n",
      "At: 1282 [==========>] Loss 0.11671882889961942  - accuracy: 0.875\n",
      "At: 1283 [==========>] Loss 0.1375325341929772  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.16372568297948809  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.08463350822458503  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.10172905244777623  - accuracy: 0.90625\n",
      "At: 1287 [==========>] Loss 0.12740233149000357  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.1807002355164635  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.12535099227939664  - accuracy: 0.78125\n",
      "At: 1290 [==========>] Loss 0.13276632620424006  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.15085678511431566  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.11486225397073493  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.16884637173174033  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.17396489606865928  - accuracy: 0.75\n",
      "At: 1295 [==========>] Loss 0.17634035026670283  - accuracy: 0.78125\n",
      "At: 1296 [==========>] Loss 0.13948404489775548  - accuracy: 0.8125\n",
      "At: 1297 [==========>] Loss 0.12183250945105845  - accuracy: 0.875\n",
      "At: 1298 [==========>] Loss 0.10404078994193831  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.16072411001774284  - accuracy: 0.71875\n",
      "At: 1300 [==========>] Loss 0.12661732453714272  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.12787362805023322  - accuracy: 0.8125\n",
      "At: 1302 [==========>] Loss 0.08452487163353373  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.10442312244875954  - accuracy: 0.90625\n",
      "At: 1304 [==========>] Loss 0.11557337253709013  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.13871402252935938  - accuracy: 0.78125\n",
      "At: 1306 [==========>] Loss 0.06896390490529758  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.16170951980488693  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.0709245626218413  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.185048108981926  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.15321206283387423  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.16850415132759936  - accuracy: 0.71875\n",
      "At: 1312 [==========>] Loss 0.08876896738426263  - accuracy: 0.875\n",
      "At: 1313 [==========>] Loss 0.15244049578090627  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.06845696176895377  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.13365147658613316  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.13556948831943702  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.10397590759282006  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.1090420085951327  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.1284636641180294  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.11804283407440747  - accuracy: 0.78125\n",
      "At: 1321 [==========>] Loss 0.09474676908065763  - accuracy: 0.875\n",
      "At: 1322 [==========>] Loss 0.16018251132149003  - accuracy: 0.71875\n",
      "At: 1323 [==========>] Loss 0.09539493958608553  - accuracy: 0.90625\n",
      "At: 1324 [==========>] Loss 0.15223980795193678  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.09262717706064302  - accuracy: 0.84375\n",
      "At: 1326 [==========>] Loss 0.06582147628938714  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.1362886514401971  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.08236607125234154  - accuracy: 0.875\n",
      "At: 1329 [==========>] Loss 0.09303046307520366  - accuracy: 0.84375\n",
      "At: 1330 [==========>] Loss 0.13964558291122103  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.15430438906747018  - accuracy: 0.75\n",
      "At: 1332 [==========>] Loss 0.10461270522867237  - accuracy: 0.84375\n",
      "At: 1333 [==========>] Loss 0.14213622091339917  - accuracy: 0.84375\n",
      "At: 1334 [==========>] Loss 0.11713038084448656  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.0957321870559135  - accuracy: 0.875\n",
      "At: 1336 [==========>] Loss 0.09993091797486671  - accuracy: 0.90625\n",
      "At: 1337 [==========>] Loss 0.1599673982724747  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.13500396472428658  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.11178584808864807  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.15361948452491936  - accuracy: 0.78125\n",
      "At: 1341 [==========>] Loss 0.08541468459295368  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.13668923004184313  - accuracy: 0.8125\n",
      "At: 1343 [==========>] Loss 0.17115683180787558  - accuracy: 0.8125\n",
      "At: 1344 [==========>] Loss 0.1647552118848467  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.0976122131932563  - accuracy: 0.8125\n",
      "At: 1346 [==========>] Loss 0.08780577260320614  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.10566253658895779  - accuracy: 0.8125\n",
      "At: 1348 [==========>] Loss 0.11705451882126994  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.13199666326612258  - accuracy: 0.8125\n",
      "At: 1350 [==========>] Loss 0.13028903299306224  - accuracy: 0.875\n",
      "At: 1351 [==========>] Loss 0.10730129381977893  - accuracy: 0.8125\n",
      "At: 1352 [==========>] Loss 0.09893337055975555  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.15563818846798194  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.18203072042332263  - accuracy: 0.71875\n",
      "At: 1355 [==========>] Loss 0.07438790293439251  - accuracy: 0.9375\n",
      "At: 1356 [==========>] Loss 0.0860442400752458  - accuracy: 0.9375\n",
      "At: 1357 [==========>] Loss 0.10722332116514534  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.13094428982066528  - accuracy: 0.8125\n",
      "At: 1359 [==========>] Loss 0.08115419752174728  - accuracy: 0.875\n",
      "At: 1360 [==========>] Loss 0.16705169192542416  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.10223042184203901  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.12750935307513153  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.13265072169797548  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.12029335908186718  - accuracy: 0.84375\n",
      "At: 1365 [==========>] Loss 0.13171815012847343  - accuracy: 0.84375\n",
      "At: 1366 [==========>] Loss 0.10562338011338343  - accuracy: 0.875\n",
      "At: 1367 [==========>] Loss 0.10871390354281063  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.16552769634489278  - accuracy: 0.8125\n",
      "At: 1369 [==========>] Loss 0.0771812005237702  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.11724216483368148  - accuracy: 0.78125\n",
      "At: 1371 [==========>] Loss 0.21822297600024965  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.12520478533689072  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.13184843105916208  - accuracy: 0.875\n",
      "At: 1374 [==========>] Loss 0.1675714704866394  - accuracy: 0.75\n",
      "At: 1375 [==========>] Loss 0.1267814517852543  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.11768929996532429  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.1607390023454608  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.12236475700237645  - accuracy: 0.8125\n",
      "At: 1379 [==========>] Loss 0.17035440802633633  - accuracy: 0.65625\n",
      "At: 1380 [==========>] Loss 0.14767015606886374  - accuracy: 0.75\n",
      "At: 1381 [==========>] Loss 0.09299780221331888  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.12755243951697315  - accuracy: 0.84375\n",
      "At: 1383 [==========>] Loss 0.10561585492390951  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.1076621176244888  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.1874817233428815  - accuracy: 0.71875\n",
      "At: 1386 [==========>] Loss 0.1558338677667238  - accuracy: 0.75\n",
      "At: 1387 [==========>] Loss 0.07968504566877363  - accuracy: 0.9375\n",
      "At: 1388 [==========>] Loss 0.16635080666162189  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.092412506572797  - accuracy: 0.84375\n",
      "At: 1390 [==========>] Loss 0.14440271070365918  - accuracy: 0.8125\n",
      "At: 1391 [==========>] Loss 0.10326665773130125  - accuracy: 0.875\n",
      "At: 1392 [==========>] Loss 0.10836747540444153  - accuracy: 0.78125\n",
      "At: 1393 [==========>] Loss 0.17060199833199946  - accuracy: 0.75\n",
      "At: 1394 [==========>] Loss 0.09790505406264739  - accuracy: 0.84375\n",
      "At: 1395 [==========>] Loss 0.23656287870224635  - accuracy: 0.625\n",
      "At: 1396 [==========>] Loss 0.06228376434118235  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.12921109322754856  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.11227948522181656  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.1298661499832629  - accuracy: 0.8125\n",
      "At: 1400 [==========>] Loss 0.15299692506581508  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.09021825480730677  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.15532871342394533  - accuracy: 0.75\n",
      "At: 1403 [==========>] Loss 0.12866738851128945  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.12801468160046217  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.09943760063674073  - accuracy: 0.84375\n",
      "At: 1406 [==========>] Loss 0.16109249144758236  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.11296180345301556  - accuracy: 0.875\n",
      "At: 1408 [==========>] Loss 0.16014893030888755  - accuracy: 0.71875\n",
      "At: 1409 [==========>] Loss 0.03214532156659844  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.1145293979347543  - accuracy: 0.8125\n",
      "At: 1411 [==========>] Loss 0.13553815515082768  - accuracy: 0.8125\n",
      "At: 1412 [==========>] Loss 0.12099227217715017  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.11675377279092887  - accuracy: 0.78125\n",
      "At: 1414 [==========>] Loss 0.171145465615404  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.08486393326967237  - accuracy: 0.90625\n",
      "At: 1416 [==========>] Loss 0.17450022432654666  - accuracy: 0.71875\n",
      "At: 1417 [==========>] Loss 0.11533099798305971  - accuracy: 0.8125\n",
      "At: 1418 [==========>] Loss 0.16304246357770907  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.12273251769161903  - accuracy: 0.875\n",
      "At: 1420 [==========>] Loss 0.08604580733397904  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.10324389569031195  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.14188190931475214  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.13669224851372058  - accuracy: 0.875\n",
      "At: 1424 [==========>] Loss 0.13904880801546773  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.0809272034488305  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.12437547080617198  - accuracy: 0.875\n",
      "At: 1427 [==========>] Loss 0.10904653421939223  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.11200782697141567  - accuracy: 0.8125\n",
      "At: 1429 [==========>] Loss 0.14601716006124077  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.08330920674931008  - accuracy: 0.9375\n",
      "At: 1431 [==========>] Loss 0.11407393117249928  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.10292484230884133  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.12300475055633023  - accuracy: 0.8125\n",
      "At: 1434 [==========>] Loss 0.1543042830011747  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.11328413043907798  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.07103511064301088  - accuracy: 0.875\n",
      "At: 1437 [==========>] Loss 0.12460390886673062  - accuracy: 0.84375\n",
      "At: 1438 [==========>] Loss 0.158894268987162  - accuracy: 0.8125\n",
      "At: 1439 [==========>] Loss 0.1111794274442907  - accuracy: 0.84375\n",
      "At: 1440 [==========>] Loss 0.11110296149658198  - accuracy: 0.8125\n",
      "At: 1441 [==========>] Loss 0.0871449589739669  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.11568724368069376  - accuracy: 0.84375\n",
      "At: 1443 [==========>] Loss 0.11069900641244412  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.09934405442484795  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.20141543009058419  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.21014970645635828  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.14191072481483236  - accuracy: 0.84375\n",
      "At: 1448 [==========>] Loss 0.07130997963288675  - accuracy: 0.9375\n",
      "At: 1449 [==========>] Loss 0.13201032476700703  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.12993918886083908  - accuracy: 0.875\n",
      "At: 1451 [==========>] Loss 0.10933783264013365  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.10132851200892615  - accuracy: 0.90625\n",
      "At: 1453 [==========>] Loss 0.06307064078579225  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.16793445445054855  - accuracy: 0.78125\n",
      "At: 1455 [==========>] Loss 0.09687098879009773  - accuracy: 0.9375\n",
      "At: 1456 [==========>] Loss 0.0871515927369467  - accuracy: 0.90625\n",
      "At: 1457 [==========>] Loss 0.0908735326775998  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.1429887045042753  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.12838359321262227  - accuracy: 0.84375\n",
      "At: 1460 [==========>] Loss 0.14980036402476468  - accuracy: 0.8125\n",
      "At: 1461 [==========>] Loss 0.10527719926547505  - accuracy: 0.90625\n",
      "At: 1462 [==========>] Loss 0.14446288325566026  - accuracy: 0.78125\n",
      "At: 1463 [==========>] Loss 0.08701873228815789  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.1327252098915528  - accuracy: 0.84375\n",
      "At: 1465 [==========>] Loss 0.11670340972586932  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.07171176116238107  - accuracy: 0.9375\n",
      "At: 1467 [==========>] Loss 0.17831467722771074  - accuracy: 0.75\n",
      "At: 1468 [==========>] Loss 0.16663149378374334  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.1555644237165102  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.14438703060491112  - accuracy: 0.8125\n",
      "At: 1471 [==========>] Loss 0.14804051793406747  - accuracy: 0.75\n",
      "At: 1472 [==========>] Loss 0.07371304909248683  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.12155996737433741  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.2106737144529549  - accuracy: 0.6875\n",
      "At: 1475 [==========>] Loss 0.1196054169264038  - accuracy: 0.8125\n",
      "At: 1476 [==========>] Loss 0.12916348131032065  - accuracy: 0.84375\n",
      "At: 1477 [==========>] Loss 0.09476665138355596  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.10252954478827905  - accuracy: 0.96875\n",
      "At: 1479 [==========>] Loss 0.15168390001224413  - accuracy: 0.8125\n",
      "At: 1480 [==========>] Loss 0.07721365952808934  - accuracy: 0.875\n",
      "At: 1481 [==========>] Loss 0.15490874217570835  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.10547301787981278  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.18533491472587366  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.14860640240412992  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.16177708294206686  - accuracy: 0.78125\n",
      "At: 1486 [==========>] Loss 0.0760621607683075  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.06700441018714445  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.13274054349869344  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.19791020756187364  - accuracy: 0.78125\n",
      "At: 1490 [==========>] Loss 0.09459221533160153  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.16417526274287056  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.12248328790378464  - accuracy: 0.875\n",
      "At: 1493 [==========>] Loss 0.18092166029538653  - accuracy: 0.8125\n",
      "At: 1494 [==========>] Loss 0.15623085081864396  - accuracy: 0.75\n",
      "At: 1495 [==========>] Loss 0.1121158016077629  - accuracy: 0.84375\n",
      "At: 1496 [==========>] Loss 0.11357433105993765  - accuracy: 0.8125\n",
      "At: 1497 [==========>] Loss 0.16704183802989503  - accuracy: 0.71875\n",
      "At: 1498 [==========>] Loss 0.16138437180686388  - accuracy: 0.71875\n",
      "At: 1499 [==========>] Loss 0.11121326900291179  - accuracy: 0.8125\n",
      "At: 1500 [==========>] Loss 0.07515235440942042  - accuracy: 0.90625\n",
      "At: 1501 [==========>] Loss 0.09790187088605251  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.14338999243973574  - accuracy: 0.75\n",
      "At: 1503 [==========>] Loss 0.12371105142695471  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.11945690919657272  - accuracy: 0.84375\n",
      "At: 1505 [==========>] Loss 0.13338370451972664  - accuracy: 0.75\n",
      "At: 1506 [==========>] Loss 0.12879092893084992  - accuracy: 0.75\n",
      "At: 1507 [==========>] Loss 0.15036465195591936  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.2166611059757551  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.09046759246883967  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.12408222357613378  - accuracy: 0.75\n",
      "At: 1511 [==========>] Loss 0.11491859435163901  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.07953043090988873  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.1584648359198068  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.1459729374471327  - accuracy: 0.8125\n",
      "At: 1515 [==========>] Loss 0.12355900056594464  - accuracy: 0.8125\n",
      "At: 1516 [==========>] Loss 0.12897060456322543  - accuracy: 0.8125\n",
      "At: 1517 [==========>] Loss 0.16904340924483308  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.13201122997767167  - accuracy: 0.78125\n",
      "At: 1519 [==========>] Loss 0.1501199386714788  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.0833552271964709  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.08525836656110203  - accuracy: 0.84375\n",
      "At: 1522 [==========>] Loss 0.17683783373296075  - accuracy: 0.8125\n",
      "At: 1523 [==========>] Loss 0.11752328036697907  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.14824663840798627  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.13040964024575694  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.10994106606421222  - accuracy: 0.90625\n",
      "At: 1527 [==========>] Loss 0.12339533275654281  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.13432757348885538  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.08695521749465483  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.05331924062443562  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.1341277742725839  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.17116261082083672  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.14211809980403045  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.09454672653639148  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.13836455548323742  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.14402718901304992  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10056102003247461  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.11536703349688726  - accuracy: 0.84375\n",
      "At: 1539 [==========>] Loss 0.07332211998006613  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1420058768948333  - accuracy: 0.8125\n",
      "At: 1541 [==========>] Loss 0.1323689249546021  - accuracy: 0.8125\n",
      "At: 1542 [==========>] Loss 0.07452135035743773  - accuracy: 0.9375\n",
      "At: 1543 [==========>] Loss 0.1474619619306059  - accuracy: 0.78125\n",
      "At: 1544 [==========>] Loss 0.12292148670946719  - accuracy: 0.84375\n",
      "At: 1545 [==========>] Loss 0.2081494839713906  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.13389753980689295  - accuracy: 0.78125\n",
      "At: 1547 [==========>] Loss 0.1686440087904812  - accuracy: 0.75\n",
      "At: 1548 [==========>] Loss 0.1338468340856287  - accuracy: 0.875\n",
      "At: 1549 [==========>] Loss 0.14804501498321676  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.06982409852580003  - accuracy: 0.9375\n",
      "At: 1551 [==========>] Loss 0.16086093851114813  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.10185605772728631  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.07741236326141857  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.13366582784603398  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.121845519297129  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1655487589076614  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.07880234756426241  - accuracy: 0.9375\n",
      "At: 1558 [==========>] Loss 0.12844601821385246  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.08646020510745342  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.1507631076281965  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.1623257063374439  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.08975417616670645  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.11426846508480207  - accuracy: 0.875\n",
      "At: 1564 [==========>] Loss 0.08813981137555627  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.13062981236441962  - accuracy: 0.875\n",
      "At: 1566 [==========>] Loss 0.14506289460422983  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.1389405863554728  - accuracy: 0.8125\n",
      "At: 1568 [==========>] Loss 0.10244197141912567  - accuracy: 0.84375\n",
      "At: 1569 [==========>] Loss 0.10978270975359003  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.08675014779287  - accuracy: 0.875\n",
      "At: 1571 [==========>] Loss 0.13625575333858708  - accuracy: 0.78125\n",
      "At: 1572 [==========>] Loss 0.13130194546822105  - accuracy: 0.875\n",
      "At: 1573 [==========>] Loss 0.04646749701989922  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.13670746071288198  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.11591385800361262  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.14057043889396664  - accuracy: 0.75\n",
      "At: 1577 [==========>] Loss 0.07898367217214505  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.07346520859618805  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.10801597117198256  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.09742196995678856  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.09966248190104503  - accuracy: 0.875\n",
      "At: 1582 [==========>] Loss 0.15443856989841087  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.06547709108494015  - accuracy: 0.9375\n",
      "At: 1584 [==========>] Loss 0.13311495587608208  - accuracy: 0.84375\n",
      "At: 1585 [==========>] Loss 0.10720870916512962  - accuracy: 0.8125\n",
      "At: 1586 [==========>] Loss 0.12291090935968924  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.10282996999135119  - accuracy: 0.90625\n",
      "At: 1588 [==========>] Loss 0.13546150887026617  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.1422531702493628  - accuracy: 0.8125\n",
      "At: 1590 [==========>] Loss 0.13592657744972192  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.1238964128215324  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.08358795664693036  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.16826822287121776  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.09456689165231408  - accuracy: 0.875\n",
      "At: 1595 [==========>] Loss 0.14884171424975756  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.1820499326314799  - accuracy: 0.78125\n",
      "At: 1597 [==========>] Loss 0.17457018596000595  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.17887709901071364  - accuracy: 0.71875\n",
      "At: 1599 [==========>] Loss 0.2182294096380863  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.14925693996972972  - accuracy: 0.875\n",
      "At: 1601 [==========>] Loss 0.06371160387182309  - accuracy: 0.96875\n",
      "At: 1602 [==========>] Loss 0.11560027832984177  - accuracy: 0.78125\n",
      "At: 1603 [==========>] Loss 0.1746560549756129  - accuracy: 0.78125\n",
      "At: 1604 [==========>] Loss 0.21980278297492362  - accuracy: 0.6875\n",
      "At: 1605 [==========>] Loss 0.09943307875854636  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.12188728963004022  - accuracy: 0.875\n",
      "At: 1607 [==========>] Loss 0.18409367606397453  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.11209736846433752  - accuracy: 0.875\n",
      "At: 1609 [==========>] Loss 0.15697047860012453  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.17022762738440456  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.07230551906729898  - accuracy: 0.875\n",
      "At: 1612 [==========>] Loss 0.061737943442848234  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.14649454041254756  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.12691516990319301  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.0955597985463396  - accuracy: 0.875\n",
      "At: 1616 [==========>] Loss 0.1210314666627887  - accuracy: 0.78125\n",
      "At: 1617 [==========>] Loss 0.09282128838731503  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11020409431270199  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.18058184326395316  - accuracy: 0.75\n",
      "At: 1620 [==========>] Loss 0.09699537074358655  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.12492006041635333  - accuracy: 0.78125\n",
      "At: 1622 [==========>] Loss 0.15040854551089405  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.10295699272756319  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.1595999728026081  - accuracy: 0.71875\n",
      "At: 1625 [==========>] Loss 0.12131917315183299  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.12229264555347907  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.09768165516262145  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.14701703171372246  - accuracy: 0.78125\n",
      "At: 1629 [==========>] Loss 0.14493449853412887  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.08624296158288178  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.09707444012807694  - accuracy: 0.90625\n",
      "At: 1632 [==========>] Loss 0.060679056721196625  - accuracy: 0.9375\n",
      "At: 1633 [==========>] Loss 0.06341889853054468  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.12957370472695467  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.18916947946414814  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.12150204866242595  - accuracy: 0.875\n",
      "At: 1637 [==========>] Loss 0.08367890892410315  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.16506544451067023  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.14957379339395613  - accuracy: 0.78125\n",
      "At: 1640 [==========>] Loss 0.1289682121022514  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.1115818390611841  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.13481400494674414  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.11791064300068833  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.11362599077525226  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.05028840595650083  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.14372497872887013  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.15401121231832843  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1526121577167464  - accuracy: 0.78125\n",
      "At: 1649 [==========>] Loss 0.09067244837763919  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.0969615647709757  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.12249591045114411  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.08068839311627016  - accuracy: 0.875\n",
      "At: 1653 [==========>] Loss 0.0964581454399479  - accuracy: 0.90625\n",
      "At: 1654 [==========>] Loss 0.07963072832092868  - accuracy: 0.875\n",
      "At: 1655 [==========>] Loss 0.15950978465487142  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.1276453944042849  - accuracy: 0.8125\n",
      "At: 1657 [==========>] Loss 0.15423546148269635  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.20131869292715357  - accuracy: 0.71875\n",
      "At: 1659 [==========>] Loss 0.08734347915213013  - accuracy: 0.90625\n",
      "At: 1660 [==========>] Loss 0.04794379361878981  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.07934484748464603  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.11362321990754434  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.17185266784311928  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.10396761185997533  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.11502747861683796  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.09822037756846896  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.1308656640358702  - accuracy: 0.8125\n",
      "At: 1668 [==========>] Loss 0.1454324301752295  - accuracy: 0.75\n",
      "At: 1669 [==========>] Loss 0.12351723209377152  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.08565934512050642  - accuracy: 0.875\n",
      "At: 1671 [==========>] Loss 0.12466269223242918  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.1297942560039985  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.07159012017342127  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.1701852790957201  - accuracy: 0.8125\n",
      "At: 1675 [==========>] Loss 0.1654825310813956  - accuracy: 0.78125\n",
      "At: 1676 [==========>] Loss 0.21534058793055832  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.10092679684703323  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.1757508618122621  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.10395028162711836  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.17534636957338154  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.1288499399617985  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.06700916477332777  - accuracy: 0.9375\n",
      "At: 1683 [==========>] Loss 0.17243685181504653  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.11349359076388892  - accuracy: 0.84375\n",
      "At: 1685 [==========>] Loss 0.09165558936234856  - accuracy: 0.90625\n",
      "At: 1686 [==========>] Loss 0.1354255946878148  - accuracy: 0.875\n",
      "At: 1687 [==========>] Loss 0.1395754315303361  - accuracy: 0.75\n",
      "At: 1688 [==========>] Loss 0.04120771590702986  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.11469904414855435  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.13171467544121693  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.07954808566187362  - accuracy: 0.9375\n",
      "At: 1692 [==========>] Loss 0.19424147512799855  - accuracy: 0.75\n",
      "At: 1693 [==========>] Loss 0.09514149248599121  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.09435557935121702  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.13752562309231553  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.15103304407604318  - accuracy: 0.78125\n",
      "At: 1697 [==========>] Loss 0.0994209718265216  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.08018339662573147  - accuracy: 0.9375\n",
      "At: 1699 [==========>] Loss 0.10502604536849938  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.12177880051981005  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.09571627641763492  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.09638990114128132  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.20832408993920343  - accuracy: 0.6875\n",
      "At: 1704 [==========>] Loss 0.07893049320072279  - accuracy: 0.84375\n",
      "At: 1705 [==========>] Loss 0.09617671861316193  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.15366103740825057  - accuracy: 0.84375\n",
      "At: 1707 [==========>] Loss 0.19096336645594908  - accuracy: 0.71875\n",
      "At: 1708 [==========>] Loss 0.0858269029947556  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.181929733826659  - accuracy: 0.78125\n",
      "At: 1710 [==========>] Loss 0.15479090910090001  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.06602454932344445  - accuracy: 0.9375\n",
      "At: 1712 [==========>] Loss 0.10893569034473292  - accuracy: 0.875\n",
      "At: 1713 [==========>] Loss 0.09495246150141107  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.17561637392348178  - accuracy: 0.71875\n",
      "At: 1715 [==========>] Loss 0.11454476494900095  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.058401218202085325  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.09908115939712059  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.12295343974296033  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.12671072841723657  - accuracy: 0.8125\n",
      "At: 1720 [==========>] Loss 0.04509247838321717  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.15977367613745197  - accuracy: 0.8125\n",
      "At: 1722 [==========>] Loss 0.06497952798904494  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.2113673526370813  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.10045499966506655  - accuracy: 0.84375\n",
      "At: 1725 [==========>] Loss 0.135300361246312  - accuracy: 0.8125\n",
      "At: 1726 [==========>] Loss 0.11725136754817905  - accuracy: 0.8125\n",
      "At: 1727 [==========>] Loss 0.11396163130730798  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.08814116039688374  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.2001949927128821  - accuracy: 0.65625\n",
      "At: 1730 [==========>] Loss 0.16591182352361836  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.10725962667857133  - accuracy: 0.84375\n",
      "At: 1732 [==========>] Loss 0.09274129320585718  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.1724693290144787  - accuracy: 0.71875\n",
      "At: 1734 [==========>] Loss 0.09351524881001187  - accuracy: 0.875\n",
      "At: 1735 [==========>] Loss 0.13151044056260824  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.12329590126701348  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.16414230159731175  - accuracy: 0.75\n",
      "At: 1738 [==========>] Loss 0.13047070652704332  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.12890352761012527  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.12599345058242517  - accuracy: 0.8125\n",
      "At: 1741 [==========>] Loss 0.1634983677823818  - accuracy: 0.75\n",
      "At: 1742 [==========>] Loss 0.04016369657307306  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.15138635390601632  - accuracy: 0.75\n",
      "At: 1744 [==========>] Loss 0.073636787406751  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.11512809733202883  - accuracy: 0.84375\n",
      "At: 1746 [==========>] Loss 0.16263987874578872  - accuracy: 0.75\n",
      "At: 1747 [==========>] Loss 0.12380884611926361  - accuracy: 0.78125\n",
      "At: 1748 [==========>] Loss 0.10690869734580148  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.1053726727225976  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.10735760397436711  - accuracy: 0.84375\n",
      "At: 1751 [==========>] Loss 0.18875717783341656  - accuracy: 0.8125\n",
      "At: 1752 [==========>] Loss 0.10096830594867337  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.0904239668846041  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.12194556112352167  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.061363049335015324  - accuracy: 0.9375\n",
      "At: 1756 [==========>] Loss 0.15759475088998287  - accuracy: 0.8125\n",
      "At: 1757 [==========>] Loss 0.15570114927785259  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.07481436409664813  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.098069550278769  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.08466793257768582  - accuracy: 0.875\n",
      "At: 1761 [==========>] Loss 0.13564012679078122  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.17012106159597903  - accuracy: 0.78125\n",
      "At: 1763 [==========>] Loss 0.11549545397242592  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.12385110899653426  - accuracy: 0.875\n",
      "At: 1765 [==========>] Loss 0.1616354022777753  - accuracy: 0.78125\n",
      "At: 1766 [==========>] Loss 0.07984318560192036  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.07019921964295482  - accuracy: 0.9375\n",
      "At: 1768 [==========>] Loss 0.09822648187297356  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.06317292751969505  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.07552198585543735  - accuracy: 0.90625\n",
      "At: 1771 [==========>] Loss 0.14556314276657523  - accuracy: 0.8125\n",
      "At: 1772 [==========>] Loss 0.12426255309565533  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.09880637861100441  - accuracy: 0.84375\n",
      "At: 1774 [==========>] Loss 0.17224171543569378  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.11011638349265133  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.10514873506906801  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11099458794660066  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.1122329372892684  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.10425608925480268  - accuracy: 0.9375\n",
      "At: 1780 [==========>] Loss 0.10958423919404725  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.18418601534874476  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.10075211222729463  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.12086029885287487  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.07433991696476655  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.08315094014022852  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.12953462297049054  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.12181099253507469  - accuracy: 0.875\n",
      "At: 1788 [==========>] Loss 0.09674396651319322  - accuracy: 0.875\n",
      "At: 1789 [==========>] Loss 0.11672122547609826  - accuracy: 0.84375\n",
      "At: 1790 [==========>] Loss 0.1575475142583903  - accuracy: 0.8125\n",
      "At: 1791 [==========>] Loss 0.07893145161940365  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.12109275241421018  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.07679215215580576  - accuracy: 0.96875\n",
      "At: 1794 [==========>] Loss 0.15653420707405674  - accuracy: 0.78125\n",
      "At: 1795 [==========>] Loss 0.0599929463190791  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.12047595522754254  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.10331796303170589  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.12354196973153697  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.0904337772331482  - accuracy: 0.9375\n",
      "At: 1800 [==========>] Loss 0.11240715857759866  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.17945945905311844  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.1383949497339149  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.1898658349452139  - accuracy: 0.71875\n",
      "At: 1804 [==========>] Loss 0.13333719444423414  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.04298346375301636  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.15786755376370792  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.15017811457923555  - accuracy: 0.8125\n",
      "At: 1808 [==========>] Loss 0.14893903710776663  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.0854607482981442  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.13922828511878715  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.14350586671507734  - accuracy: 0.75\n",
      "At: 1812 [==========>] Loss 0.09892632404795029  - accuracy: 0.875\n",
      "At: 1813 [==========>] Loss 0.12640840817922075  - accuracy: 0.875\n",
      "At: 1814 [==========>] Loss 0.11384752892739347  - accuracy: 0.875\n",
      "At: 1815 [==========>] Loss 0.15452512177880173  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.04713074611520382  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.14295931186758626  - accuracy: 0.6875\n",
      "At: 1818 [==========>] Loss 0.12852359519056897  - accuracy: 0.78125\n",
      "At: 1819 [==========>] Loss 0.179642344253017  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.11618545950331985  - accuracy: 0.875\n",
      "At: 1821 [==========>] Loss 0.11555146004422179  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.1479810703930309  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.19076277405419934  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.16346452214557158  - accuracy: 0.75\n",
      "At: 1825 [==========>] Loss 0.11294242801262944  - accuracy: 0.75\n",
      "At: 1826 [==========>] Loss 0.0757178131464638  - accuracy: 0.875\n",
      "At: 1827 [==========>] Loss 0.10528578993101059  - accuracy: 0.8125\n",
      "At: 1828 [==========>] Loss 0.15832130556113122  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.14712790250892116  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.1410159980816328  - accuracy: 0.8125\n",
      "At: 1831 [==========>] Loss 0.11900722489246249  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.1328613379103886  - accuracy: 0.78125\n",
      "At: 1833 [==========>] Loss 0.1164059550506556  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.06880897790105941  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.18003990505732567  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.10793338302554534  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.05278495316424925  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.09894746844776403  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.09553816253093567  - accuracy: 0.84375\n",
      "At: 1840 [==========>] Loss 0.1074563276620761  - accuracy: 0.875\n",
      "At: 1841 [==========>] Loss 0.09882788849212093  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.1420456097448093  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.11282722325435673  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.11364531210103218  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.18515573759721646  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.1379345203791971  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05594711612570208  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.04099853152886058  - accuracy: 1.0\n",
      "At: 1849 [==========>] Loss 0.17092495142467434  - accuracy: 0.8125\n",
      "At: 1850 [==========>] Loss 0.04023102636210864  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.18038344473400073  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.07460179104697706  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.09678797076981349  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.12568220116789222  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.17064787375311696  - accuracy: 0.75\n",
      "At: 1856 [==========>] Loss 0.11800718046968817  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.1740008760516376  - accuracy: 0.78125\n",
      "At: 1858 [==========>] Loss 0.12435163837332096  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.16680406394929279  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.13533858233226465  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.09241841307739493  - accuracy: 0.875\n",
      "At: 1862 [==========>] Loss 0.17311725374354692  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.13729927323314117  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.1496693026708418  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.10263279262236764  - accuracy: 0.84375\n",
      "At: 1866 [==========>] Loss 0.20210179405775386  - accuracy: 0.625\n",
      "At: 1867 [==========>] Loss 0.10248554105800281  - accuracy: 0.90625\n",
      "At: 1868 [==========>] Loss 0.16468422845997668  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.15895514822607795  - accuracy: 0.84375\n",
      "At: 1870 [==========>] Loss 0.09036989962738852  - accuracy: 0.90625\n",
      "At: 1871 [==========>] Loss 0.13713499789645273  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.1580963780253396  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.09275813104070119  - accuracy: 0.875\n",
      "At: 1874 [==========>] Loss 0.145958624788792  - accuracy: 0.78125\n",
      "At: 1875 [==========>] Loss 0.11248631798489023  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.16813925725108803  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.10908548977698834  - accuracy: 0.875\n",
      "At: 1878 [==========>] Loss 0.10766741097648476  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.12844408613734565  - accuracy: 0.875\n",
      "At: 1880 [==========>] Loss 0.09645141619761535  - accuracy: 0.84375\n",
      "At: 1881 [==========>] Loss 0.12855396527991125  - accuracy: 0.75\n",
      "At: 1882 [==========>] Loss 0.12766940839421062  - accuracy: 0.84375\n",
      "At: 1883 [==========>] Loss 0.14079107454771614  - accuracy: 0.84375\n",
      "At: 1884 [==========>] Loss 0.08663617592582779  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.12144594324219136  - accuracy: 0.75\n",
      "At: 1886 [==========>] Loss 0.16271480121281992  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.0721565854059153  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.1394887859995562  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.12678043220902974  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.15168188863251522  - accuracy: 0.8125\n",
      "At: 1891 [==========>] Loss 0.06656758565473692  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.07700749146470441  - accuracy: 0.96875\n",
      "At: 1893 [==========>] Loss 0.11098672034423329  - accuracy: 0.84375\n",
      "At: 1894 [==========>] Loss 0.09673479914672323  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.07083350313407533  - accuracy: 0.90625\n",
      "At: 1896 [==========>] Loss 0.11806617539922828  - accuracy: 0.84375\n",
      "At: 1897 [==========>] Loss 0.0731506911549473  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.10458597433280829  - accuracy: 0.8125\n",
      "At: 1899 [==========>] Loss 0.09116541124753325  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.1374598480049368  - accuracy: 0.8125\n",
      "At: 1901 [==========>] Loss 0.11107061637600886  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.1742611462275943  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.11584543533828182  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.05079352672603143  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.14768645556478655  - accuracy: 0.78125\n",
      "At: 1906 [==========>] Loss 0.11226654318887655  - accuracy: 0.875\n",
      "At: 1907 [==========>] Loss 0.08516020234929793  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.11399270358987673  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.11824756512389796  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.04322046238030127  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.13319030457925293  - accuracy: 0.78125\n",
      "At: 1912 [==========>] Loss 0.12172513346119027  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.15332551924071514  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.08740067183535069  - accuracy: 0.875\n",
      "At: 1915 [==========>] Loss 0.10846408394111323  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.1458125642253761  - accuracy: 0.84375\n",
      "At: 1917 [==========>] Loss 0.15503717925120944  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.12460588745709303  - accuracy: 0.78125\n",
      "At: 1919 [==========>] Loss 0.10077619394491684  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10737899091198023  - accuracy: 0.90625\n",
      "At: 1921 [==========>] Loss 0.15521689870043676  - accuracy: 0.78125\n",
      "At: 1922 [==========>] Loss 0.12684664183563565  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.18926888456214164  - accuracy: 0.65625\n",
      "At: 1924 [==========>] Loss 0.1471212872531416  - accuracy: 0.71875\n",
      "At: 1925 [==========>] Loss 0.1655066688882653  - accuracy: 0.6875\n",
      "At: 1926 [==========>] Loss 0.0928387588171204  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.09864856725188643  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.1419617850123656  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.180336208117965  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.17678608463820403  - accuracy: 0.625\n",
      "At: 1931 [==========>] Loss 0.113455019102137  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.1474405200108181  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.09143707985724837  - accuracy: 0.875\n",
      "At: 1934 [==========>] Loss 0.14393356117374628  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.14533276842167137  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.14318638057013724  - accuracy: 0.75\n",
      "At: 1937 [==========>] Loss 0.13597222715646923  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.17282415092828407  - accuracy: 0.78125\n",
      "At: 1939 [==========>] Loss 0.087034524590798  - accuracy: 0.9375\n",
      "At: 1940 [==========>] Loss 0.13329190773957403  - accuracy: 0.78125\n",
      "At: 1941 [==========>] Loss 0.1450245884919319  - accuracy: 0.78125\n",
      "At: 1942 [==========>] Loss 0.15230140018418348  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.13411576007544165  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.11986925167235848  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.1627548215266308  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.11207200066215042  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.12483962729873485  - accuracy: 0.8125\n",
      "At: 1948 [==========>] Loss 0.12997975530825187  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.08054775061140056  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.11463411395383419  - accuracy: 0.84375\n",
      "At: 1951 [==========>] Loss 0.15203554196016456  - accuracy: 0.78125\n",
      "At: 1952 [==========>] Loss 0.09150423967274579  - accuracy: 0.84375\n",
      "At: 1953 [==========>] Loss 0.0604895928687182  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.16685334175592476  - accuracy: 0.84375\n",
      "At: 1955 [==========>] Loss 0.06729468608260314  - accuracy: 0.9375\n",
      "At: 1956 [==========>] Loss 0.11765226845851098  - accuracy: 0.8125\n",
      "At: 1957 [==========>] Loss 0.060398568133731456  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.07908933906539473  - accuracy: 0.9375\n",
      "At: 1959 [==========>] Loss 0.11958985610445558  - accuracy: 0.78125\n",
      "At: 1960 [==========>] Loss 0.07609797808207808  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.1806912930241062  - accuracy: 0.71875\n",
      "At: 1962 [==========>] Loss 0.18472273991031793  - accuracy: 0.78125\n",
      "At: 1963 [==========>] Loss 0.10005845068506586  - accuracy: 0.84375\n",
      "At: 1964 [==========>] Loss 0.19509608464363246  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.1529887792363019  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.11323250226006949  - accuracy: 0.875\n",
      "At: 1967 [==========>] Loss 0.10804209853156063  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.19765760823610815  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.16267834558745115  - accuracy: 0.75\n",
      "At: 1970 [==========>] Loss 0.12507081280949944  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.21662177132659233  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.07266925538089992  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.13229351508578238  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.10816322276769022  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.16199242818680767  - accuracy: 0.8125\n",
      "At: 1976 [==========>] Loss 0.06972095641078811  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.08567772476107026  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.13261730900705682  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.13410227667606178  - accuracy: 0.8125\n",
      "At: 1980 [==========>] Loss 0.1221463293318151  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.14929266399611607  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.07102557593600821  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.14068142406812661  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.09383764511393677  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.1421861573029277  - accuracy: 0.8125\n",
      "At: 1986 [==========>] Loss 0.1822972091566935  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.1028272570799311  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.10134395277726829  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.1018264479333268  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.12506213390645654  - accuracy: 0.90625\n",
      "At: 1991 [==========>] Loss 0.13380225443708377  - accuracy: 0.84375\n",
      "At: 1992 [==========>] Loss 0.11446698999938545  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.1519033185139544  - accuracy: 0.75\n",
      "At: 1994 [==========>] Loss 0.1508251536418315  - accuracy: 0.84375\n",
      "At: 1995 [==========>] Loss 0.19052879033693243  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.09613906007051824  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.202465400148265  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.16412894087519664  - accuracy: 0.65625\n",
      "At: 1999 [==========>] Loss 0.09511333129500624  - accuracy: 0.84375\n",
      "At: 2000 [==========>] Loss 0.12367297576796085  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.09226697285210575  - accuracy: 0.84375\n",
      "At: 2002 [==========>] Loss 0.08670165554494978  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.11926678564997317  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.1592141766845814  - accuracy: 0.75\n",
      "At: 2005 [==========>] Loss 0.11978250649598765  - accuracy: 0.8125\n",
      "At: 2006 [==========>] Loss 0.12973175940502904  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.11197404438920255  - accuracy: 0.8125\n",
      "At: 2008 [==========>] Loss 0.11900909231422044  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13213623935616486  - accuracy: 0.84375\n",
      "At: 2010 [==========>] Loss 0.13611134233582667  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.1136162426546719  - accuracy: 0.90625\n",
      "At: 2012 [==========>] Loss 0.10541892915905052  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09343325023221062  - accuracy: 0.875\n",
      "At: 2014 [==========>] Loss 0.206197507683412  - accuracy: 0.59375\n",
      "At: 2015 [==========>] Loss 0.07219578589797943  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.1016993774241266  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.07884486263410762  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.08124247436376474  - accuracy: 0.90625\n",
      "At: 2019 [==========>] Loss 0.14413050834251023  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.09710797153797154  - accuracy: 0.84375\n",
      "At: 2021 [==========>] Loss 0.1173694059098403  - accuracy: 0.84375\n",
      "At: 2022 [==========>] Loss 0.15074600868077345  - accuracy: 0.75\n",
      "At: 2023 [==========>] Loss 0.08976654361036607  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.09792916414324605  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.15312090698298728  - accuracy: 0.75\n",
      "At: 2026 [==========>] Loss 0.1019008094122408  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.12274506882135469  - accuracy: 0.8125\n",
      "At: 2028 [==========>] Loss 0.13121119178379742  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.12267910249968504  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.15472779736562103  - accuracy: 0.78125\n",
      "At: 2031 [==========>] Loss 0.16675419990878576  - accuracy: 0.78125\n",
      "At: 2032 [==========>] Loss 0.1476091321744191  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.1542567687471168  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.2065046784535408  - accuracy: 0.71875\n",
      "At: 2035 [==========>] Loss 0.111603625473074  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09846340969132417  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.1124975544433913  - accuracy: 0.875\n",
      "At: 2038 [==========>] Loss 0.11591836743392948  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.08254576083928603  - accuracy: 0.9375\n",
      "At: 2040 [==========>] Loss 0.10271321138572759  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.06397811514109815  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.11981752230993854  - accuracy: 0.8125\n",
      "At: 2043 [==========>] Loss 0.11898203750720511  - accuracy: 0.84375\n",
      "At: 2044 [==========>] Loss 0.10493227673581808  - accuracy: 0.8125\n",
      "At: 2045 [==========>] Loss 0.22368936142404133  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.06342286861904695  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08203396033221369  - accuracy: 0.875\n",
      "At: 2048 [==========>] Loss 0.10788122281314175  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.12561515627916595  - accuracy: 0.8125\n",
      "At: 2050 [==========>] Loss 0.13728268168215074  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1712160093973992  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.06342698025681326  - accuracy: 0.96875\n",
      "At: 2053 [==========>] Loss 0.09495810469540494  - accuracy: 0.8125\n",
      "At: 2054 [==========>] Loss 0.10062508809141327  - accuracy: 0.875\n",
      "At: 2055 [==========>] Loss 0.055872075198203655  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.11154484636811178  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.13972637097885432  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.15046502402176018  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.1981524042266194  - accuracy: 0.75\n",
      "At: 2060 [==========>] Loss 0.13406033332110645  - accuracy: 0.78125\n",
      "At: 2061 [==========>] Loss 0.1428151579221324  - accuracy: 0.78125\n",
      "At: 2062 [==========>] Loss 0.13129159459292644  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.08123086248716789  - accuracy: 0.9375\n",
      "At: 2064 [==========>] Loss 0.15461159615016012  - accuracy: 0.84375\n",
      "At: 2065 [==========>] Loss 0.04723684509971755  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.16246580280113382  - accuracy: 0.78125\n",
      "At: 2067 [==========>] Loss 0.08115284716423114  - accuracy: 0.8125\n",
      "At: 2068 [==========>] Loss 0.09545911935072596  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.08407824824857124  - accuracy: 0.90625\n",
      "At: 2070 [==========>] Loss 0.1530170976667168  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.14749825144323564  - accuracy: 0.78125\n",
      "At: 2072 [==========>] Loss 0.06335635181873395  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.11091307030347375  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.0898401543305988  - accuracy: 0.9375\n",
      "At: 2075 [==========>] Loss 0.1092908677639079  - accuracy: 0.875\n",
      "At: 2076 [==========>] Loss 0.11929217126638311  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.1452648758923039  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.09434364579802851  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07189340804625036  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.10603868909913636  - accuracy: 0.8125\n",
      "At: 2081 [==========>] Loss 0.15511560993470835  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.11225048786994953  - accuracy: 0.875\n",
      "At: 2083 [==========>] Loss 0.19119312877784805  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.10128461956883614  - accuracy: 0.84375\n",
      "At: 2085 [==========>] Loss 0.08916791800844115  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.08021298953331783  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.17614168037329564  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.08494132863536351  - accuracy: 0.90625\n",
      "At: 2089 [==========>] Loss 0.11703219729863362  - accuracy: 0.875\n",
      "At: 2090 [==========>] Loss 0.08161314727831215  - accuracy: 0.875\n",
      "At: 2091 [==========>] Loss 0.1207916159078638  - accuracy: 0.84375\n",
      "At: 2092 [==========>] Loss 0.0818319382456364  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.13464737178726688  - accuracy: 0.875\n",
      "At: 2094 [==========>] Loss 0.12366641792988725  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.10390776922825834  - accuracy: 0.90625\n",
      "At: 2096 [==========>] Loss 0.12147426122554913  - accuracy: 0.8125\n",
      "At: 2097 [==========>] Loss 0.1193274606551733  - accuracy: 0.875\n",
      "At: 2098 [==========>] Loss 0.14248031073438552  - accuracy: 0.75\n",
      "At: 2099 [==========>] Loss 0.10248765802359133  - accuracy: 0.84375\n",
      "At: 2100 [==========>] Loss 0.05753626894694898  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.15624984989143997  - accuracy: 0.75\n",
      "At: 2102 [==========>] Loss 0.09585036891811549  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.15322313005240396  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.11563354464969147  - accuracy: 0.875\n",
      "At: 2105 [==========>] Loss 0.15936901669198222  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.15001890251191324  - accuracy: 0.8125\n",
      "At: 2107 [==========>] Loss 0.09265868149833595  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.13103402368093015  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.11607306006514209  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.052370587955891626  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.11060292315665889  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.1231052100244067  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.1231516157378219  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.14584559604696223  - accuracy: 0.8125\n",
      "At: 2115 [==========>] Loss 0.12436563790204379  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.12530109552782848  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.15747283807512574  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.13544586607591885  - accuracy: 0.8125\n",
      "At: 2119 [==========>] Loss 0.06489626966758771  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.13212657781212414  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.13120488210111  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.13858837634469212  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.17378112932993128  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.14184121116112586  - accuracy: 0.78125\n",
      "At: 2125 [==========>] Loss 0.08466462521366967  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.054086499578000935  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.0959685107271073  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.11873966688041392  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.1226097318062263  - accuracy: 0.84375\n",
      "At: 2130 [==========>] Loss 0.05920281528455251  - accuracy: 0.9375\n",
      "At: 2131 [==========>] Loss 0.11569564334159416  - accuracy: 0.84375\n",
      "At: 2132 [==========>] Loss 0.20780109041501615  - accuracy: 0.78125\n",
      "At: 2133 [==========>] Loss 0.174378016674828  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.12579419459295688  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.09176717182897623  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.11695421660743072  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.1204223175290985  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.11926814296879037  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.17131521545352435  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.11109694079930356  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.10056522001026783  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.12198672997935882  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.09657131154847687  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.0645952058918845  - accuracy: 0.9375\n",
      "At: 2145 [==========>] Loss 0.11312128506400508  - accuracy: 0.8125\n",
      "At: 2146 [==========>] Loss 0.1843589829307399  - accuracy: 0.75\n",
      "At: 2147 [==========>] Loss 0.13947288539873512  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.17277602498092  - accuracy: 0.71875\n",
      "At: 2149 [==========>] Loss 0.1201020587669234  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.11766018759266968  - accuracy: 0.90625\n",
      "At: 2151 [==========>] Loss 0.11202572343879996  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.21161004977855497  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.17113843656500266  - accuracy: 0.71875\n",
      "At: 2154 [==========>] Loss 0.15524013738666817  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.10961359893800826  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.11022632595041454  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.1245928832599481  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.16556985890351572  - accuracy: 0.71875\n",
      "At: 2159 [==========>] Loss 0.07801792410416304  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.12397619872334412  - accuracy: 0.875\n",
      "At: 2161 [==========>] Loss 0.09786562158557735  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.0973291627913316  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.13727563802716702  - accuracy: 0.8125\n",
      "At: 2164 [==========>] Loss 0.1611588628412728  - accuracy: 0.71875\n",
      "At: 2165 [==========>] Loss 0.09413365836410531  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.09696465606578256  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.09870990472609523  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.08953289940888681  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.1128900266672142  - accuracy: 0.84375\n",
      "At: 2170 [==========>] Loss 0.10515524206028931  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.1707941829128896  - accuracy: 0.71875\n",
      "At: 2172 [==========>] Loss 0.09388561687329161  - accuracy: 0.90625\n",
      "At: 2173 [==========>] Loss 0.11391380049300127  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.12549491507014704  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.13346802683644154  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.11610653250283  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.15003231325791727  - accuracy: 0.78125\n",
      "At: 2178 [==========>] Loss 0.09361496483571188  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.12586304876477308  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.12489759370324893  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.14710076545827616  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.08350867198974901  - accuracy: 0.875\n",
      "At: 2183 [==========>] Loss 0.21214535314383356  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.08014292376342945  - accuracy: 0.875\n",
      "At: 2185 [==========>] Loss 0.1261623192987668  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.15033632131211777  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.1895227672897491  - accuracy: 0.75\n",
      "At: 2188 [==========>] Loss 0.08424406257153791  - accuracy: 0.875\n",
      "At: 2189 [==========>] Loss 0.05663767885720634  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.1316251385325704  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.11397171974928204  - accuracy: 0.84375\n",
      "At: 2192 [==========>] Loss 0.10998003649620143  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.1678403172879832  - accuracy: 0.75\n",
      "At: 2194 [==========>] Loss 0.10997604993501955  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.1760942943763149  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.1331139480683217  - accuracy: 0.84375\n",
      "At: 2197 [==========>] Loss 0.10326006492850556  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.08208867947433769  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.05829026873906493  - accuracy: 0.90625\n",
      "At: 2200 [==========>] Loss 0.054064152422857  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.11660159599850499  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.08739172433582201  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.07920064862172332  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.1364655536978641  - accuracy: 0.78125\n",
      "At: 2205 [==========>] Loss 0.10846473531158417  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.09095589511129024  - accuracy: 0.84375\n",
      "At: 2207 [==========>] Loss 0.10006226346517741  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.1510578238145111  - accuracy: 0.78125\n",
      "At: 2209 [==========>] Loss 0.18430645143784108  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.11127058815772743  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.1335705814096123  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.09886560484431228  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.11655814980818198  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.11741633992282759  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.14087594595703662  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.11862836539945895  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.11584689762853775  - accuracy: 0.84375\n",
      "At: 2218 [==========>] Loss 0.15251233571769873  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.08496511716350849  - accuracy: 0.875\n",
      "At: 2220 [==========>] Loss 0.1289324383856711  - accuracy: 0.78125\n",
      "At: 2221 [==========>] Loss 0.16764559362174392  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.11623644181200188  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.14646702129068234  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.12807729495113135  - accuracy: 0.84375\n",
      "At: 2225 [==========>] Loss 0.12654991872202648  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.11715352959276791  - accuracy: 0.84375\n",
      "At: 2227 [==========>] Loss 0.16983877146632087  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.06609179335323989  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.17260890945492338  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.1347717595291743  - accuracy: 0.75\n",
      "At: 2231 [==========>] Loss 0.15017841855219094  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.17681981358549123  - accuracy: 0.75\n",
      "At: 2233 [==========>] Loss 0.1498553220129853  - accuracy: 0.8125\n",
      "At: 2234 [==========>] Loss 0.1611903992728764  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.11928251807443721  - accuracy: 0.90625\n",
      "At: 2236 [==========>] Loss 0.07236002766422539  - accuracy: 0.96875\n",
      "At: 2237 [==========>] Loss 0.11534291646470159  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.12331535257283817  - accuracy: 0.875\n",
      "At: 2239 [==========>] Loss 0.18399176847440069  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.1754818132442348  - accuracy: 0.71875\n",
      "At: 2241 [==========>] Loss 0.13156900971596636  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.17826146055228434  - accuracy: 0.78125\n",
      "At: 2243 [==========>] Loss 0.0736386270754556  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.09333185435277667  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.07441409085699896  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.13834480184111297  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.1147572930595722  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.1719004536135847  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.09273460156600788  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.08024844537637839  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.07121149201841684  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.14106411501396873  - accuracy: 0.75\n",
      "At: 2253 [==========>] Loss 0.11687597494834845  - accuracy: 0.84375\n",
      "At: 2254 [==========>] Loss 0.11166931841585329  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.12536523686254095  - accuracy: 0.875\n",
      "At: 2256 [==========>] Loss 0.14476412186930826  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.09068021508776324  - accuracy: 0.875\n",
      "At: 2258 [==========>] Loss 0.12654995263632104  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.141985494924593  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.16077818290565898  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.11132899788196414  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.15191073642196293  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.13353915674851838  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.09612089323998231  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.07946477213590493  - accuracy: 0.90625\n",
      "At: 2266 [==========>] Loss 0.11661887223610459  - accuracy: 0.875\n",
      "At: 2267 [==========>] Loss 0.07059772358886124  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.08704543088671221  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.038546186128939536  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.09266355685651817  - accuracy: 0.90625\n",
      "At: 2271 [==========>] Loss 0.1574650165853807  - accuracy: 0.78125\n",
      "At: 2272 [==========>] Loss 0.07286569952350991  - accuracy: 0.90625\n",
      "At: 2273 [==========>] Loss 0.13258893089602075  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.07636891419971702  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.08543621842094046  - accuracy: 0.84375\n",
      "At: 2276 [==========>] Loss 0.09031015585998896  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.1291979194101704  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.08359295683369208  - accuracy: 0.875\n",
      "At: 2279 [==========>] Loss 0.12365506131852708  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.1390793828207537  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.12435639236461556  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.06258793514194584  - accuracy: 0.96875\n",
      "At: 2283 [==========>] Loss 0.1432887290210741  - accuracy: 0.71875\n",
      "At: 2284 [==========>] Loss 0.09237708011427855  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.10854509358958925  - accuracy: 0.78125\n",
      "At: 2286 [==========>] Loss 0.12286314828554881  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.15192392334027202  - accuracy: 0.78125\n",
      "At: 2288 [==========>] Loss 0.10470840879212745  - accuracy: 0.875\n",
      "At: 2289 [==========>] Loss 0.14844848943307848  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.07321328229795618  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.12009741399186369  - accuracy: 0.875\n",
      "At: 2292 [==========>] Loss 0.06638136144988155  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.06561669128240738  - accuracy: 0.90625\n",
      "At: 2294 [==========>] Loss 0.055990255790920394  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.1578364865910399  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.18135083937171853  - accuracy: 0.75\n",
      "At: 2297 [==========>] Loss 0.07829910264107634  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.11509391471362312  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.10470368653110075  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.1271842243630635  - accuracy: 0.75\n",
      "At: 2301 [==========>] Loss 0.21081214822620567  - accuracy: 0.6875\n",
      "At: 2302 [==========>] Loss 0.1627127064946787  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.059748077044939105  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.0801637059529681  - accuracy: 0.90625\n",
      "At: 2305 [==========>] Loss 0.08922729437318475  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.1277144286799034  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.1374088581538545  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.17913052466161852  - accuracy: 0.8125\n",
      "At: 2309 [==========>] Loss 0.14150416829427723  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.14133316922692465  - accuracy: 0.78125\n",
      "At: 2311 [==========>] Loss 0.12594639607205438  - accuracy: 0.8125\n",
      "At: 2312 [==========>] Loss 0.11025174085912527  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.0727064131754785  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.11462416000466097  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.1186997640584233  - accuracy: 0.875\n",
      "At: 2316 [==========>] Loss 0.13916503175938066  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.18060153801357381  - accuracy: 0.71875\n",
      "At: 2318 [==========>] Loss 0.1818628873956903  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.10991787557576269  - accuracy: 0.8125\n",
      "At: 2320 [==========>] Loss 0.09725661609996034  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.1584751305930414  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.19215131892041523  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.132489032411232  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.1530618317116555  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.12582896085662473  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.07366714449633367  - accuracy: 0.90625\n",
      "At: 2327 [==========>] Loss 0.06980961694113061  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.1172422921929589  - accuracy: 0.8125\n",
      "At: 2329 [==========>] Loss 0.09290554498824978  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.16711555164544395  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.10418026360119423  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.09312151397858211  - accuracy: 0.90625\n",
      "At: 2333 [==========>] Loss 0.09777921024386099  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.18193288581288206  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.10149445952875011  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.11198897649548249  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.13993912948405163  - accuracy: 0.84375\n",
      "At: 2338 [==========>] Loss 0.10354241868118749  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.10452266539923363  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.1588038943029777  - accuracy: 0.75\n",
      "At: 2341 [==========>] Loss 0.12172131464299492  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.18672004664733927  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.06937858641166819  - accuracy: 0.90625\n",
      "At: 2344 [==========>] Loss 0.2148703629654552  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.13734456957747382  - accuracy: 0.84375\n",
      "At: 2346 [==========>] Loss 0.09074391583505717  - accuracy: 0.875\n",
      "At: 2347 [==========>] Loss 0.12484328987027533  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.06889523010284795  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.0802882353223155  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.11296444945822323  - accuracy: 0.90625\n",
      "At: 2351 [==========>] Loss 0.07809419778885116  - accuracy: 0.90625\n",
      "At: 2352 [==========>] Loss 0.10560631581813637  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.0640321475183077  - accuracy: 0.9375\n",
      "At: 2354 [==========>] Loss 0.15850926069570076  - accuracy: 0.8125\n",
      "At: 2355 [==========>] Loss 0.051990381737084056  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.11615655385033277  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.1048341184676491  - accuracy: 0.90625\n",
      "At: 2358 [==========>] Loss 0.17065194613523943  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.2070795343042325  - accuracy: 0.625\n",
      "At: 2360 [==========>] Loss 0.13358045023110482  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.1671394699176107  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.07419838498866788  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.17927426355323367  - accuracy: 0.65625\n",
      "At: 2364 [==========>] Loss 0.09008414149047894  - accuracy: 0.875\n",
      "At: 2365 [==========>] Loss 0.0770276969400858  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.1542671700722982  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.11420514791088532  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.0811391183843646  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.09078021426703906  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.0893953017150087  - accuracy: 0.875\n",
      "At: 2371 [==========>] Loss 0.10979067550177557  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.14247941814051485  - accuracy: 0.71875\n",
      "At: 2373 [==========>] Loss 0.10577790872229316  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.0936872511693158  - accuracy: 0.875\n",
      "At: 2375 [==========>] Loss 0.06552093898365173  - accuracy: 0.875\n",
      "At: 2376 [==========>] Loss 0.11453960954041648  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.08363818516237101  - accuracy: 0.875\n",
      "At: 2378 [==========>] Loss 0.11322444614374795  - accuracy: 0.875\n",
      "At: 2379 [==========>] Loss 0.1835185238035516  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.09180068241491626  - accuracy: 0.875\n",
      "At: 2381 [==========>] Loss 0.09564352470353543  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.0698620675358833  - accuracy: 0.90625\n",
      "At: 2383 [==========>] Loss 0.12309465238731221  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.10886775590942124  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.1469338666120923  - accuracy: 0.78125\n",
      "At: 2386 [==========>] Loss 0.10741121821189115  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.07653042326357831  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.11963182935560329  - accuracy: 0.875\n",
      "At: 2389 [==========>] Loss 0.052009838851944476  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.0717049013408442  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.19691960950334308  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.14827568005578157  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.08450835776164684  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.07311483487673712  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.09001795879383905  - accuracy: 0.90625\n",
      "At: 2396 [==========>] Loss 0.07980771625860798  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.08836361406537856  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.10973418691078393  - accuracy: 0.8125\n",
      "At: 2399 [==========>] Loss 0.14286744620066225  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.08988554925091098  - accuracy: 0.9375\n",
      "At: 2401 [==========>] Loss 0.10119710480883673  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.09370772284949076  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.1937592328470156  - accuracy: 0.71875\n",
      "At: 2404 [==========>] Loss 0.1416053565329825  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.04963327671588011  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.1436697662497543  - accuracy: 0.875\n",
      "At: 2407 [==========>] Loss 0.1147926515803338  - accuracy: 0.8125\n",
      "At: 2408 [==========>] Loss 0.0916050274514137  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.15063340356346477  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.15041125084108833  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.10023266871310477  - accuracy: 0.84375\n",
      "At: 2412 [==========>] Loss 0.09921996574634911  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.08489851714842  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.06245880526444757  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.10774246618945404  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.09855613921922746  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.1505986159838204  - accuracy: 0.71875\n",
      "At: 2418 [==========>] Loss 0.1509231779860013  - accuracy: 0.8125\n",
      "At: 2419 [==========>] Loss 0.14399273424919884  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.13803635646164808  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.08539764417818661  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.14720470250626827  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.13018685624202753  - accuracy: 0.84375\n",
      "At: 2424 [==========>] Loss 0.08489592642796533  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.09200966597869677  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.18712379304987675  - accuracy: 0.75\n",
      "At: 2427 [==========>] Loss 0.1270341424794075  - accuracy: 0.78125\n",
      "At: 2428 [==========>] Loss 0.09778525506057006  - accuracy: 0.84375\n",
      "At: 2429 [==========>] Loss 0.10378872566324968  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.1196418460632738  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.14111917669411222  - accuracy: 0.8125\n",
      "At: 2432 [==========>] Loss 0.05664890544829235  - accuracy: 1.0\n",
      "At: 2433 [==========>] Loss 0.06330626632938022  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.04543371386022404  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.14785161456444618  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.08343178232556869  - accuracy: 0.90625\n",
      "At: 2437 [==========>] Loss 0.18964447255034236  - accuracy: 0.78125\n",
      "At: 2438 [==========>] Loss 0.0854321633772496  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.17287094205260714  - accuracy: 0.71875\n",
      "At: 2440 [==========>] Loss 0.1226681619568211  - accuracy: 0.78125\n",
      "At: 2441 [==========>] Loss 0.11777445609948083  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.12574294736825894  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.1257808211207443  - accuracy: 0.8125\n",
      "At: 2444 [==========>] Loss 0.08717070455361534  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.0691357683078533  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.1649341605199523  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.16437333143846275  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.14802034331628325  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.078700356033552  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.08118798859388072  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.06114857352175095  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.14370139897494588  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.12190171626032044  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.1725015867815181  - accuracy: 0.75\n",
      "At: 2455 [==========>] Loss 0.1285304065631357  - accuracy: 0.875\n",
      "At: 2456 [==========>] Loss 0.1323266733644633  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.15502168378519846  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.07430535667227876  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.13355394052978187  - accuracy: 0.75\n",
      "At: 2460 [==========>] Loss 0.06561943566029832  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.056757499763324115  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.18925117289899365  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.09935045373012608  - accuracy: 0.9375\n",
      "At: 2464 [==========>] Loss 0.15918744686782255  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.12687260177476006  - accuracy: 0.78125\n",
      "At: 2466 [==========>] Loss 0.0938011610554476  - accuracy: 0.9375\n",
      "At: 2467 [==========>] Loss 0.11793608038826822  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.07829652058679444  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.14235388280375072  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.12669382580456734  - accuracy: 0.875\n",
      "At: 2471 [==========>] Loss 0.11896800564375608  - accuracy: 0.8125\n",
      "At: 2472 [==========>] Loss 0.10971163660054715  - accuracy: 0.90625\n",
      "At: 2473 [==========>] Loss 0.13045825232473798  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.0804456396233692  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.09024874983960085  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.07295251151460683  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.15883888673100532  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.10735664456988919  - accuracy: 0.875\n",
      "At: 2479 [==========>] Loss 0.07428738914616773  - accuracy: 0.9375\n",
      "At: 2480 [==========>] Loss 0.13765245840871873  - accuracy: 0.75\n",
      "At: 2481 [==========>] Loss 0.062470950445048155  - accuracy: 0.96875\n",
      "At: 2482 [==========>] Loss 0.1632652757824475  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.10147871010419372  - accuracy: 0.875\n",
      "At: 2484 [==========>] Loss 0.08792059712862313  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.10483838401506637  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.11082757037579899  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.12659998568142788  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.17144393221831752  - accuracy: 0.75\n",
      "At: 2489 [==========>] Loss 0.2176935988017732  - accuracy: 0.6875\n",
      "At: 2490 [==========>] Loss 0.08923358434004874  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.12402370716913244  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.12099667980779241  - accuracy: 0.8125\n",
      "At: 2493 [==========>] Loss 0.12433580277759465  - accuracy: 0.8125\n",
      "At: 2494 [==========>] Loss 0.11059473014179241  - accuracy: 0.84375\n",
      "At: 2495 [==========>] Loss 0.07487688949316822  - accuracy: 0.90625\n",
      "At: 2496 [==========>] Loss 0.05487236627741805  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.2027849978623277  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.1295969026345919  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.054580070829719926  - accuracy: 0.96875\n",
      "At: 2500 [==========>] Loss 0.1829544806274096  - accuracy: 0.71875\n",
      "At: 2501 [==========>] Loss 0.16773401308580227  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.12437094790193781  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.1431112770678048  - accuracy: 0.78125\n",
      "At: 2504 [==========>] Loss 0.1690443052734656  - accuracy: 0.71875\n",
      "At: 2505 [==========>] Loss 0.11682678683178419  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.10030009289137705  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.1296907919341905  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.1103961810374472  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.149010964839847  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.12201076055616236  - accuracy: 0.875\n",
      "At: 2511 [==========>] Loss 0.13351807352108655  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.1120665621281432  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.12581378467173338  - accuracy: 0.84375\n",
      "At: 2514 [==========>] Loss 0.153241174417534  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.1994394492650794  - accuracy: 0.6875\n",
      "At: 2516 [==========>] Loss 0.19991759657682082  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.1301792916857557  - accuracy: 0.75\n",
      "At: 2518 [==========>] Loss 0.1380748201357303  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.12034248634111025  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.1610409568196431  - accuracy: 0.78125\n",
      "At: 2521 [==========>] Loss 0.1024509144283324  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.20322121822163397  - accuracy: 0.6875\n",
      "At: 2523 [==========>] Loss 0.1330404744953869  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.16915796407774292  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.08057963797476522  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.11231791671204021  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.15085523725210043  - accuracy: 0.75\n",
      "At: 2528 [==========>] Loss 0.08258651689438427  - accuracy: 0.90625\n",
      "At: 2529 [==========>] Loss 0.1579139215087846  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.15154703654195584  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.05352924012231851  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.13711002303693784  - accuracy: 0.75\n",
      "At: 2533 [==========>] Loss 0.10194068463649791  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.06437198913759265  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.07875868905010827  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.1565461283108369  - accuracy: 0.8125\n",
      "At: 2537 [==========>] Loss 0.10268187992911491  - accuracy: 0.90625\n",
      "At: 2538 [==========>] Loss 0.17925449785763745  - accuracy: 0.65625\n",
      "At: 2539 [==========>] Loss 0.08896043727785456  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.13330123914323222  - accuracy: 0.84375\n",
      "At: 2541 [==========>] Loss 0.06496095797781987  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.06404842398467042  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16271269867665872  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.1337189312653523  - accuracy: 0.8125\n",
      "At: 2545 [==========>] Loss 0.054425761720835124  - accuracy: 0.96875\n",
      "At: 2546 [==========>] Loss 0.14351341509000193  - accuracy: 0.78125\n",
      "At: 2547 [==========>] Loss 0.10769381577096093  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.07797046833287136  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.10184530901088276  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.13371334818321703  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.11914218844015982  - accuracy: 0.78125\n",
      "At: 2552 [==========>] Loss 0.09998737778697978  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.0937650518312014  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.1644378195870945  - accuracy: 0.6875\n",
      "At: 2555 [==========>] Loss 0.16543160947392715  - accuracy: 0.71875\n",
      "At: 2556 [==========>] Loss 0.091417393443706  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.13381988593373967  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.07699581136553793  - accuracy: 0.90625\n",
      "At: 2559 [==========>] Loss 0.11953659542165906  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.20094878028096166  - accuracy: 0.65625\n",
      "At: 2561 [==========>] Loss 0.10252474319278779  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.08018393998234016  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.11645002589053897  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.07801934779833118  - accuracy: 0.9375\n",
      "At: 2565 [==========>] Loss 0.12173537605973583  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1795655590221748  - accuracy: 0.78125\n",
      "At: 2567 [==========>] Loss 0.11183113134167073  - accuracy: 0.8125\n",
      "At: 2568 [==========>] Loss 0.09205443180910697  - accuracy: 0.90625\n",
      "At: 2569 [==========>] Loss 0.07861880354187907  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.200513112289872  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.0856302839203362  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.18376200488195804  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.1689570193592183  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.10731695185068357  - accuracy: 0.875\n",
      "At: 2575 [==========>] Loss 0.04488526487362716  - accuracy: 1.0\n",
      "At: 2576 [==========>] Loss 0.1124709980719612  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.19500250109747075  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.18556325124265546  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.19135872482845598  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.14658549745792465  - accuracy: 0.75\n",
      "At: 2581 [==========>] Loss 0.08117779707362813  - accuracy: 0.875\n",
      "At: 2582 [==========>] Loss 0.19508768237932034  - accuracy: 0.71875\n",
      "At: 2583 [==========>] Loss 0.0780421858583814  - accuracy: 0.90625\n",
      "At: 2584 [==========>] Loss 0.19224038375961822  - accuracy: 0.71875\n",
      "At: 2585 [==========>] Loss 0.08798040793590883  - accuracy: 0.9375\n",
      "At: 2586 [==========>] Loss 0.07662492708755472  - accuracy: 0.9375\n",
      "At: 2587 [==========>] Loss 0.19540752375534806  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.1448007694508687  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.11279366331418567  - accuracy: 0.78125\n",
      "At: 2590 [==========>] Loss 0.22041894339224882  - accuracy: 0.65625\n",
      "At: 2591 [==========>] Loss 0.11844516274927151  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.09449576625888116  - accuracy: 0.84375\n",
      "At: 2593 [==========>] Loss 0.08700911345359139  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.08842173660910502  - accuracy: 0.9375\n",
      "At: 2595 [==========>] Loss 0.07503670078743742  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.09774829790744038  - accuracy: 0.90625\n",
      "At: 2597 [==========>] Loss 0.08973179442725474  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.1132884993900258  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.1386359570556878  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.1346414014340372  - accuracy: 0.84375\n",
      "At: 2601 [==========>] Loss 0.11785985425653647  - accuracy: 0.875\n",
      "At: 2602 [==========>] Loss 0.06169488270106923  - accuracy: 0.9375\n",
      "At: 2603 [==========>] Loss 0.13356719813501228  - accuracy: 0.84375\n",
      "At: 2604 [==========>] Loss 0.12119839912845251  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.1390412400923357  - accuracy: 0.75\n",
      "At: 2606 [==========>] Loss 0.10883396356118641  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.15199476837403883  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.06404099114502718  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.10044535810459845  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.161791753297532  - accuracy: 0.6875\n",
      "At: 2611 [==========>] Loss 0.1602576432521857  - accuracy: 0.71875\n",
      "At: 2612 [==========>] Loss 0.08045919524872092  - accuracy: 0.90625\n",
      "At: 2613 [==========>] Loss 0.08002916131843597  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.09964922901352231  - accuracy: 0.90625\n",
      "At: 2615 [==========>] Loss 0.08761059170907505  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.06657802420743497  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.11074295447841179  - accuracy: 0.78125\n",
      "At: 2618 [==========>] Loss 0.07026040088860239  - accuracy: 0.9375\n",
      "At: 2619 [==========>] Loss 0.09395057426061887  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.11737644381299314  - accuracy: 0.875\n",
      "At: 2621 [==========>] Loss 0.12320330624778661  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.1367564674057929  - accuracy: 0.8125\n",
      "At: 2623 [==========>] Loss 0.09639029090837385  - accuracy: 0.875\n",
      "At: 2624 [==========>] Loss 0.13403070151331012  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.05763052501199145  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.03368377836359844  - accuracy: 1.0\n",
      "At: 2627 [==========>] Loss 0.14202652259493487  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.07474782369112776  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.08391354981434933  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.09851275750056285  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.12438421264307814  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.12134666013099871  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.10484243976562559  - accuracy: 0.8125\n",
      "At: 2634 [==========>] Loss 0.06887798980243652  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.2041200446441092  - accuracy: 0.71875\n",
      "At: 2636 [==========>] Loss 0.0996983203255935  - accuracy: 0.8125\n",
      "At: 2637 [==========>] Loss 0.1486796916239475  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.07295895914897313  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.0892907962631693  - accuracy: 0.84375\n",
      "At: 2640 [==========>] Loss 0.12839309341325655  - accuracy: 0.8125\n",
      "At: 2641 [==========>] Loss 0.120536784836075  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.17395999149275237  - accuracy: 0.6875\n",
      "At: 2643 [==========>] Loss 0.09182416466476442  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.16199700404674447  - accuracy: 0.84375\n",
      "At: 2645 [==========>] Loss 0.08280111613541709  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.14047191619484747  - accuracy: 0.84375\n",
      "At: 2647 [==========>] Loss 0.09566092393994126  - accuracy: 0.875\n",
      "At: 2648 [==========>] Loss 0.1258110152220847  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.12503410754991756  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.11249886270863936  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.16702484363239645  - accuracy: 0.75\n",
      "At: 2652 [==========>] Loss 0.09178806872485526  - accuracy: 0.8125\n",
      "At: 2653 [==========>] Loss 0.1283687334064176  - accuracy: 0.8125\n",
      "At: 2654 [==========>] Loss 0.11781806905176695  - accuracy: 0.8125\n",
      "At: 2655 [==========>] Loss 0.21726819998310443  - accuracy: 0.71875\n",
      "At: 2656 [==========>] Loss 0.028468201553332864  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.09937251786658624  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.08749022074000895  - accuracy: 0.90625\n",
      "At: 2659 [==========>] Loss 0.053560617522224865  - accuracy: 0.90625\n",
      "At: 2660 [==========>] Loss 0.09153432154906885  - accuracy: 0.90625\n",
      "At: 2661 [==========>] Loss 0.07395967956394989  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.07405989483459295  - accuracy: 0.96875\n",
      "At: 2663 [==========>] Loss 0.08179897255533194  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.07713824679994868  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.09968082903769579  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.1327614904161531  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.14006480822896827  - accuracy: 0.8125\n",
      "At: 2668 [==========>] Loss 0.11162037535000843  - accuracy: 0.84375\n",
      "At: 2669 [==========>] Loss 0.1398023196069725  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.10057716330024416  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.06543180451967885  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.0955564816343486  - accuracy: 0.875\n",
      "At: 2673 [==========>] Loss 0.12236702177667459  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.09104327744773534  - accuracy: 0.875\n",
      "At: 2675 [==========>] Loss 0.10726165196739273  - accuracy: 0.875\n",
      "At: 2676 [==========>] Loss 0.15799498485525323  - accuracy: 0.71875\n",
      "At: 2677 [==========>] Loss 0.11443299901651482  - accuracy: 0.84375\n",
      "At: 2678 [==========>] Loss 0.05447613653004434  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.089752081367615  - accuracy: 0.875\n",
      "At: 2680 [==========>] Loss 0.08169793189052957  - accuracy: 0.90625\n",
      "At: 2681 [==========>] Loss 0.08599854622263678  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.15090301671945983  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.19113851938643545  - accuracy: 0.71875\n",
      "At: 2684 [==========>] Loss 0.08253086146092475  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.09795491667461911  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.057763455788474305  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.13907168867893174  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.1492736385519976  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.11505174621309046  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.11297947128557684  - accuracy: 0.78125\n",
      "Epochs  9 / 10\n",
      "At: 1 [==========>] Loss 0.10510152207900568  - accuracy: 0.8125\n",
      "At: 2 [==========>] Loss 0.2419958368462362  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.13535762117315472  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.18817894884038594  - accuracy: 0.75\n",
      "At: 5 [==========>] Loss 0.09899673098560312  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.10852377197812804  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.20981599491747094  - accuracy: 0.75\n",
      "At: 8 [==========>] Loss 0.2604971163793362  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.3609203260460275  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.24200951636067664  - accuracy: 0.71875\n",
      "At: 11 [==========>] Loss 0.2101774280003023  - accuracy: 0.71875\n",
      "At: 12 [==========>] Loss 0.18111362046676166  - accuracy: 0.75\n",
      "At: 13 [==========>] Loss 0.14840218237139513  - accuracy: 0.84375\n",
      "At: 14 [==========>] Loss 0.12342940409986866  - accuracy: 0.8125\n",
      "At: 15 [==========>] Loss 0.14980411208562477  - accuracy: 0.8125\n",
      "At: 16 [==========>] Loss 0.19032332644742692  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.19073210271419508  - accuracy: 0.78125\n",
      "At: 18 [==========>] Loss 0.254004764998631  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.20690749842816492  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.13667168989832115  - accuracy: 0.84375\n",
      "At: 21 [==========>] Loss 0.24101365866520738  - accuracy: 0.71875\n",
      "At: 22 [==========>] Loss 0.1801276209089177  - accuracy: 0.75\n",
      "At: 23 [==========>] Loss 0.09453217303570655  - accuracy: 0.90625\n",
      "At: 24 [==========>] Loss 0.2726065427315631  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.20056188792526058  - accuracy: 0.75\n",
      "At: 26 [==========>] Loss 0.2847266034850552  - accuracy: 0.625\n",
      "At: 27 [==========>] Loss 0.20538090026879363  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.18481864679305188  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.19464180269877562  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.22057724513301985  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.24767536944917573  - accuracy: 0.6875\n",
      "At: 32 [==========>] Loss 0.22281212831617656  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.12007110996222531  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.17724168938394225  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.1799561500780936  - accuracy: 0.8125\n",
      "At: 36 [==========>] Loss 0.20579038906871794  - accuracy: 0.6875\n",
      "At: 37 [==========>] Loss 0.25073561752264806  - accuracy: 0.6875\n",
      "At: 38 [==========>] Loss 0.22739100385241295  - accuracy: 0.71875\n",
      "At: 39 [==========>] Loss 0.18419022208140043  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.270722529714858  - accuracy: 0.65625\n",
      "At: 41 [==========>] Loss 0.07151480996129571  - accuracy: 0.9375\n",
      "At: 42 [==========>] Loss 0.1691324524331219  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.17200758715614262  - accuracy: 0.78125\n",
      "At: 44 [==========>] Loss 0.18115529202362657  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.12123768974904044  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.17668093136706153  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.22122081804538385  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.1457207937007081  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.1335518699016754  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.21995036669916462  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.16152320516250182  - accuracy: 0.78125\n",
      "At: 52 [==========>] Loss 0.27303381008494165  - accuracy: 0.625\n",
      "At: 53 [==========>] Loss 0.1979046562286236  - accuracy: 0.75\n",
      "At: 54 [==========>] Loss 0.14352387843500086  - accuracy: 0.78125\n",
      "At: 55 [==========>] Loss 0.22602660102976502  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.19456272591706425  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.16150815437971913  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.19112331224744122  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.17937475538792602  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.19949942121376837  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.2831452191981093  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.1714957606067968  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.1782431827514926  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.18818793540870352  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.30696714235469125  - accuracy: 0.625\n",
      "At: 66 [==========>] Loss 0.2249086330122565  - accuracy: 0.75\n",
      "At: 67 [==========>] Loss 0.2273627535017153  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.12176674512603355  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.14020724928000067  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.23640054580558023  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.17951891670552125  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.1728188691159115  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.1724900868085577  - accuracy: 0.78125\n",
      "At: 74 [==========>] Loss 0.20383646310216877  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.23471479344107132  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.2738076575114269  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.2372245609580116  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.1274211116463432  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.1736631439607343  - accuracy: 0.84375\n",
      "At: 80 [==========>] Loss 0.21393811012860903  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.17174968061224707  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.21173213055924806  - accuracy: 0.75\n",
      "At: 83 [==========>] Loss 0.14003631547083473  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.2293391070520691  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.1769005701101355  - accuracy: 0.78125\n",
      "At: 86 [==========>] Loss 0.19611529060135996  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.18499603937271986  - accuracy: 0.78125\n",
      "At: 88 [==========>] Loss 0.3054280667723538  - accuracy: 0.59375\n",
      "At: 89 [==========>] Loss 0.2058658197152323  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.2049426017643231  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.18008485002129185  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.06841935903606314  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.12642591589717914  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.141882982175938  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.17661070231079157  - accuracy: 0.75\n",
      "At: 96 [==========>] Loss 0.13598640187715977  - accuracy: 0.84375\n",
      "At: 97 [==========>] Loss 0.09703207140499631  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.16762899401005837  - accuracy: 0.78125\n",
      "At: 99 [==========>] Loss 0.13356805287660672  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.14896764136513507  - accuracy: 0.8125\n",
      "At: 101 [==========>] Loss 0.16256472255073784  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.1527585471567173  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.17716755828646108  - accuracy: 0.78125\n",
      "At: 104 [==========>] Loss 0.12389793278705899  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.17528710738201073  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.22180997911948144  - accuracy: 0.6875\n",
      "At: 107 [==========>] Loss 0.19133091473727187  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.19177762807806642  - accuracy: 0.71875\n",
      "At: 109 [==========>] Loss 0.1118782400203233  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.23908508665196937  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.10923381454389003  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.13516181352583972  - accuracy: 0.875\n",
      "At: 113 [==========>] Loss 0.17294518954046187  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.1455927571179328  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.17431216330844373  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.18230887715474792  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.1807556985631656  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.2744971688543566  - accuracy: 0.59375\n",
      "At: 119 [==========>] Loss 0.11583365598222888  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.15534976920407217  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.1438817046370871  - accuracy: 0.8125\n",
      "At: 122 [==========>] Loss 0.19109131590478293  - accuracy: 0.71875\n",
      "At: 123 [==========>] Loss 0.20547340302672717  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.21992155766421967  - accuracy: 0.71875\n",
      "At: 125 [==========>] Loss 0.18090223399111932  - accuracy: 0.71875\n",
      "At: 126 [==========>] Loss 0.18740047990215808  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.1711252020763665  - accuracy: 0.78125\n",
      "At: 128 [==========>] Loss 0.2714774191075886  - accuracy: 0.6875\n",
      "At: 129 [==========>] Loss 0.10755309428987495  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.2285080219577711  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.14217131242920567  - accuracy: 0.8125\n",
      "At: 132 [==========>] Loss 0.22485463770461003  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.20543414464679868  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.15501975433467985  - accuracy: 0.75\n",
      "At: 135 [==========>] Loss 0.20543189005429227  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.16411831581862013  - accuracy: 0.75\n",
      "At: 137 [==========>] Loss 0.08743691326901207  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.17984235516132618  - accuracy: 0.71875\n",
      "At: 139 [==========>] Loss 0.0826159871035199  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.15472389809828402  - accuracy: 0.78125\n",
      "At: 141 [==========>] Loss 0.2346460436534143  - accuracy: 0.65625\n",
      "At: 142 [==========>] Loss 0.18457508324939675  - accuracy: 0.75\n",
      "At: 143 [==========>] Loss 0.1813029502065061  - accuracy: 0.75\n",
      "At: 144 [==========>] Loss 0.1270493389355754  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.11677224015860725  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.12008200136175547  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.19762427079455056  - accuracy: 0.78125\n",
      "At: 148 [==========>] Loss 0.12952693284889494  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.2232310173417141  - accuracy: 0.75\n",
      "At: 150 [==========>] Loss 0.18272427541926628  - accuracy: 0.78125\n",
      "At: 151 [==========>] Loss 0.18241761315579572  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.17806911556795707  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.15059886762027763  - accuracy: 0.8125\n",
      "At: 154 [==========>] Loss 0.2102391761909918  - accuracy: 0.75\n",
      "At: 155 [==========>] Loss 0.1726136961609584  - accuracy: 0.78125\n",
      "At: 156 [==========>] Loss 0.14958660318979913  - accuracy: 0.8125\n",
      "At: 157 [==========>] Loss 0.2560666807415133  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.17294547198232185  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.10914338247570413  - accuracy: 0.875\n",
      "At: 160 [==========>] Loss 0.1383129603323376  - accuracy: 0.8125\n",
      "At: 161 [==========>] Loss 0.09204111379574634  - accuracy: 0.9375\n",
      "At: 162 [==========>] Loss 0.1779027969886418  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.16856012361326916  - accuracy: 0.8125\n",
      "At: 164 [==========>] Loss 0.14720043058182436  - accuracy: 0.84375\n",
      "At: 165 [==========>] Loss 0.1970667768722732  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.17861820279670954  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.13963689650251482  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.17773684941666668  - accuracy: 0.71875\n",
      "At: 169 [==========>] Loss 0.1611179084373284  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.18262521151472605  - accuracy: 0.78125\n",
      "At: 171 [==========>] Loss 0.22732982283439684  - accuracy: 0.71875\n",
      "At: 172 [==========>] Loss 0.1322880602415195  - accuracy: 0.78125\n",
      "At: 173 [==========>] Loss 0.2753302876138766  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.20532590373941917  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.14509344585699185  - accuracy: 0.84375\n",
      "At: 176 [==========>] Loss 0.21078542900993769  - accuracy: 0.71875\n",
      "At: 177 [==========>] Loss 0.12792693177845582  - accuracy: 0.84375\n",
      "At: 178 [==========>] Loss 0.18016671888280408  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.11284255884961582  - accuracy: 0.875\n",
      "At: 180 [==========>] Loss 0.12658168217753013  - accuracy: 0.78125\n",
      "At: 181 [==========>] Loss 0.05955667559238709  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.17591722167132431  - accuracy: 0.8125\n",
      "At: 183 [==========>] Loss 0.19395881051276767  - accuracy: 0.78125\n",
      "At: 184 [==========>] Loss 0.1559786380478469  - accuracy: 0.8125\n",
      "At: 185 [==========>] Loss 0.13483710099017795  - accuracy: 0.8125\n",
      "At: 186 [==========>] Loss 0.1323043020536488  - accuracy: 0.8125\n",
      "At: 187 [==========>] Loss 0.1406257274903589  - accuracy: 0.84375\n",
      "At: 188 [==========>] Loss 0.17738231057200377  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.12841687961978743  - accuracy: 0.8125\n",
      "At: 190 [==========>] Loss 0.12231270766125336  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.3016445667154741  - accuracy: 0.625\n",
      "At: 192 [==========>] Loss 0.1516866462781375  - accuracy: 0.75\n",
      "At: 193 [==========>] Loss 0.18938363446938541  - accuracy: 0.75\n",
      "At: 194 [==========>] Loss 0.17919021073607067  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.13495789531342173  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.20339322844066243  - accuracy: 0.75\n",
      "At: 197 [==========>] Loss 0.15502583230581213  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.12443061245319594  - accuracy: 0.90625\n",
      "At: 199 [==========>] Loss 0.09879691043538623  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.23441872904067276  - accuracy: 0.65625\n",
      "At: 201 [==========>] Loss 0.16284655842906354  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.12412751486383873  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.1378357154403932  - accuracy: 0.84375\n",
      "At: 204 [==========>] Loss 0.13709832818594225  - accuracy: 0.8125\n",
      "At: 205 [==========>] Loss 0.14097692540822318  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.13057698450654168  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.10542731145893111  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.25326962292269317  - accuracy: 0.625\n",
      "At: 209 [==========>] Loss 0.2146001355219626  - accuracy: 0.6875\n",
      "At: 210 [==========>] Loss 0.10203341245629993  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.13246906900011976  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.20472746022062305  - accuracy: 0.71875\n",
      "At: 213 [==========>] Loss 0.1682420543594245  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.2337831933907593  - accuracy: 0.65625\n",
      "At: 215 [==========>] Loss 0.08342143046097436  - accuracy: 0.90625\n",
      "At: 216 [==========>] Loss 0.16077981504070066  - accuracy: 0.8125\n",
      "At: 217 [==========>] Loss 0.20145141031584196  - accuracy: 0.65625\n",
      "At: 218 [==========>] Loss 0.14615226064219583  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.17822361830939842  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.21510244736479822  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.1686198195737827  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.11128839637929325  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.22629781550637795  - accuracy: 0.75\n",
      "At: 224 [==========>] Loss 0.15254445408250591  - accuracy: 0.84375\n",
      "At: 225 [==========>] Loss 0.14808994686876317  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.1477654497574001  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.22133928402047315  - accuracy: 0.6875\n",
      "At: 228 [==========>] Loss 0.1699705165421134  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.1668202181935322  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.1475417936161687  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.20869838140751643  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.2351275716406901  - accuracy: 0.625\n",
      "At: 233 [==========>] Loss 0.20892485332503435  - accuracy: 0.71875\n",
      "At: 234 [==========>] Loss 0.12427572892983162  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.20458239871178702  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.2380338238287376  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.10857882949251874  - accuracy: 0.8125\n",
      "At: 238 [==========>] Loss 0.1427450013733716  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.1484291599953178  - accuracy: 0.78125\n",
      "At: 240 [==========>] Loss 0.21525243787714987  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.13573938205696837  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.16723965703263816  - accuracy: 0.75\n",
      "At: 243 [==========>] Loss 0.14644429209067902  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.17203675948222039  - accuracy: 0.75\n",
      "At: 245 [==========>] Loss 0.17340637013888904  - accuracy: 0.78125\n",
      "At: 246 [==========>] Loss 0.16756809316037075  - accuracy: 0.75\n",
      "At: 247 [==========>] Loss 0.18764620203866905  - accuracy: 0.75\n",
      "At: 248 [==========>] Loss 0.10085529636416886  - accuracy: 0.8125\n",
      "At: 249 [==========>] Loss 0.12986685453199198  - accuracy: 0.875\n",
      "At: 250 [==========>] Loss 0.2192053458938647  - accuracy: 0.75\n",
      "At: 251 [==========>] Loss 0.1722567684116287  - accuracy: 0.78125\n",
      "At: 252 [==========>] Loss 0.10746618464685004  - accuracy: 0.90625\n",
      "At: 253 [==========>] Loss 0.1501074567728144  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.09523585414787943  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.15808333467585584  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.21629851097110925  - accuracy: 0.75\n",
      "At: 257 [==========>] Loss 0.11945877821487388  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.10369849899063262  - accuracy: 0.84375\n",
      "At: 259 [==========>] Loss 0.18347920010793042  - accuracy: 0.75\n",
      "At: 260 [==========>] Loss 0.13395363860321594  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.07614665925376213  - accuracy: 0.9375\n",
      "At: 262 [==========>] Loss 0.09999732235446805  - accuracy: 0.90625\n",
      "At: 263 [==========>] Loss 0.11229712508206205  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.11826133147704498  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.1977101521949065  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.26492101948069247  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1611049415539817  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.20164306712481778  - accuracy: 0.65625\n",
      "At: 269 [==========>] Loss 0.08375900189333042  - accuracy: 0.875\n",
      "At: 270 [==========>] Loss 0.2392804163661526  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.18731389209734683  - accuracy: 0.6875\n",
      "At: 272 [==========>] Loss 0.09635756081196922  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.16448084131410212  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.1759063403356415  - accuracy: 0.71875\n",
      "At: 275 [==========>] Loss 0.08012171286415065  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.20680840189020114  - accuracy: 0.6875\n",
      "At: 277 [==========>] Loss 0.10227742295092035  - accuracy: 0.90625\n",
      "At: 278 [==========>] Loss 0.12305732358464017  - accuracy: 0.8125\n",
      "At: 279 [==========>] Loss 0.159140636651341  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.13532398746948507  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.12373505486393742  - accuracy: 0.875\n",
      "At: 282 [==========>] Loss 0.21528643326164773  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.11676932715806498  - accuracy: 0.84375\n",
      "At: 284 [==========>] Loss 0.08057418138151187  - accuracy: 0.9375\n",
      "At: 285 [==========>] Loss 0.15700360540391745  - accuracy: 0.78125\n",
      "At: 286 [==========>] Loss 0.07481103579348314  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.13987866528345538  - accuracy: 0.78125\n",
      "At: 288 [==========>] Loss 0.12944730222721135  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.07625071013619643  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.12035813604779835  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.1137970271587954  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.10940917364809005  - accuracy: 0.84375\n",
      "At: 293 [==========>] Loss 0.17388618879637768  - accuracy: 0.75\n",
      "At: 294 [==========>] Loss 0.19949313589979534  - accuracy: 0.6875\n",
      "At: 295 [==========>] Loss 0.12235353972357597  - accuracy: 0.75\n",
      "At: 296 [==========>] Loss 0.14320208951963811  - accuracy: 0.8125\n",
      "At: 297 [==========>] Loss 0.12923854428812992  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.10637821355355573  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.19812719194758593  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.17907708812969944  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.1837597498540037  - accuracy: 0.71875\n",
      "At: 302 [==========>] Loss 0.1355328508750585  - accuracy: 0.78125\n",
      "At: 303 [==========>] Loss 0.10293100872671912  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.16179490807867142  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.25269527553466664  - accuracy: 0.65625\n",
      "At: 306 [==========>] Loss 0.156551812824672  - accuracy: 0.78125\n",
      "At: 307 [==========>] Loss 0.23794476632031097  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.15336489020104765  - accuracy: 0.78125\n",
      "At: 309 [==========>] Loss 0.10558068133832053  - accuracy: 0.84375\n",
      "At: 310 [==========>] Loss 0.21542885980029894  - accuracy: 0.625\n",
      "At: 311 [==========>] Loss 0.06934139886154034  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.12924414146581714  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.10770549032218751  - accuracy: 0.875\n",
      "At: 314 [==========>] Loss 0.22153235250242564  - accuracy: 0.625\n",
      "At: 315 [==========>] Loss 0.11092912868655941  - accuracy: 0.875\n",
      "At: 316 [==========>] Loss 0.185607770142739  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.28278432200456327  - accuracy: 0.5625\n",
      "At: 318 [==========>] Loss 0.1748566600068414  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.09042717774287007  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.1798289473255922  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.19860387684110242  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.09859815517189124  - accuracy: 0.8125\n",
      "At: 323 [==========>] Loss 0.10004119992484714  - accuracy: 0.875\n",
      "At: 324 [==========>] Loss 0.1622524078421545  - accuracy: 0.71875\n",
      "At: 325 [==========>] Loss 0.09192763832861697  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.17406076006477345  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.10195942914704309  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.1403013203602702  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.13151837560035534  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.18784961445390805  - accuracy: 0.75\n",
      "At: 331 [==========>] Loss 0.19395215414104866  - accuracy: 0.6875\n",
      "At: 332 [==========>] Loss 0.21724224149328603  - accuracy: 0.6875\n",
      "At: 333 [==========>] Loss 0.16608669195487208  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.11716400640756311  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.16442152982128833  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.1201345809331872  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.19366863610131618  - accuracy: 0.6875\n",
      "At: 338 [==========>] Loss 0.13169426836513884  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.15752919619271932  - accuracy: 0.75\n",
      "At: 340 [==========>] Loss 0.16381419247902435  - accuracy: 0.8125\n",
      "At: 341 [==========>] Loss 0.1436153978051897  - accuracy: 0.75\n",
      "At: 342 [==========>] Loss 0.14377429297413524  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.20333551417992526  - accuracy: 0.71875\n",
      "At: 344 [==========>] Loss 0.2029257549159824  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.1861981416074598  - accuracy: 0.6875\n",
      "At: 346 [==========>] Loss 0.16578896755373213  - accuracy: 0.75\n",
      "At: 347 [==========>] Loss 0.0639911991716129  - accuracy: 0.9375\n",
      "At: 348 [==========>] Loss 0.0979424711160341  - accuracy: 0.84375\n",
      "At: 349 [==========>] Loss 0.13548753901611837  - accuracy: 0.78125\n",
      "At: 350 [==========>] Loss 0.15251775080659513  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.22956624208305246  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.121027348258271  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.11676025872618051  - accuracy: 0.875\n",
      "At: 354 [==========>] Loss 0.17820078905559597  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.09031836537949861  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.1810527346126323  - accuracy: 0.6875\n",
      "At: 357 [==========>] Loss 0.16368977988144173  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.1393863826921155  - accuracy: 0.78125\n",
      "At: 359 [==========>] Loss 0.0972824758100014  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.1661919774673416  - accuracy: 0.75\n",
      "At: 361 [==========>] Loss 0.08795539453476954  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.1505108244291683  - accuracy: 0.8125\n",
      "At: 363 [==========>] Loss 0.09595892492450936  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.1717167309821121  - accuracy: 0.75\n",
      "At: 365 [==========>] Loss 0.14299742132710586  - accuracy: 0.78125\n",
      "At: 366 [==========>] Loss 0.1543580877749921  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.18795622360610897  - accuracy: 0.6875\n",
      "At: 368 [==========>] Loss 0.1642899733241895  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.15942066380069952  - accuracy: 0.75\n",
      "At: 370 [==========>] Loss 0.1842233385080324  - accuracy: 0.71875\n",
      "At: 371 [==========>] Loss 0.07878750515475691  - accuracy: 0.9375\n",
      "At: 372 [==========>] Loss 0.08025139369061701  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.224955213166742  - accuracy: 0.65625\n",
      "At: 374 [==========>] Loss 0.08882789995078866  - accuracy: 0.84375\n",
      "At: 375 [==========>] Loss 0.13895090224563078  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.10797991161496498  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.20754653491892738  - accuracy: 0.65625\n",
      "At: 378 [==========>] Loss 0.1582497832555535  - accuracy: 0.71875\n",
      "At: 379 [==========>] Loss 0.1293449647286679  - accuracy: 0.8125\n",
      "At: 380 [==========>] Loss 0.1561766613909505  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.14175142358474896  - accuracy: 0.78125\n",
      "At: 382 [==========>] Loss 0.10068721406037034  - accuracy: 0.84375\n",
      "At: 383 [==========>] Loss 0.1907839960670499  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.22098547623484294  - accuracy: 0.6875\n",
      "At: 385 [==========>] Loss 0.1355195158897634  - accuracy: 0.78125\n",
      "At: 386 [==========>] Loss 0.18916463003662287  - accuracy: 0.75\n",
      "At: 387 [==========>] Loss 0.08953984494807221  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.20385556475744543  - accuracy: 0.6875\n",
      "At: 389 [==========>] Loss 0.15816693548159647  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.13334391684440594  - accuracy: 0.8125\n",
      "At: 391 [==========>] Loss 0.0854246677662426  - accuracy: 0.9375\n",
      "At: 392 [==========>] Loss 0.10719083121518093  - accuracy: 0.84375\n",
      "At: 393 [==========>] Loss 0.18566848502540959  - accuracy: 0.71875\n",
      "At: 394 [==========>] Loss 0.08937983983801573  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.15634613383047535  - accuracy: 0.8125\n",
      "At: 396 [==========>] Loss 0.15548374076107313  - accuracy: 0.84375\n",
      "At: 397 [==========>] Loss 0.09789673418562297  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.23233684131607796  - accuracy: 0.65625\n",
      "At: 399 [==========>] Loss 0.11275527071550612  - accuracy: 0.8125\n",
      "At: 400 [==========>] Loss 0.18127028998946235  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.12015901383137637  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.1469858733029787  - accuracy: 0.75\n",
      "At: 403 [==========>] Loss 0.05744913046929583  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.12545159583396895  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.18051497841745223  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.09603354441142836  - accuracy: 0.875\n",
      "At: 407 [==========>] Loss 0.2098641836356185  - accuracy: 0.71875\n",
      "At: 408 [==========>] Loss 0.20680600483915862  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.18812776521887753  - accuracy: 0.71875\n",
      "At: 410 [==========>] Loss 0.13194295753710267  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10275307887735285  - accuracy: 0.90625\n",
      "At: 412 [==========>] Loss 0.13065279112361022  - accuracy: 0.78125\n",
      "At: 413 [==========>] Loss 0.1064683641303592  - accuracy: 0.875\n",
      "At: 414 [==========>] Loss 0.1654630713124331  - accuracy: 0.71875\n",
      "At: 415 [==========>] Loss 0.14528725367575773  - accuracy: 0.84375\n",
      "At: 416 [==========>] Loss 0.19463608493053414  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.11366390733512768  - accuracy: 0.84375\n",
      "At: 418 [==========>] Loss 0.1427503667933404  - accuracy: 0.75\n",
      "At: 419 [==========>] Loss 0.13507743416446966  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.14882196108237067  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.1392506989030609  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.13222477763982612  - accuracy: 0.8125\n",
      "At: 423 [==========>] Loss 0.1153271335897107  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.17015805802720288  - accuracy: 0.75\n",
      "At: 425 [==========>] Loss 0.18927217843236738  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.11842762403963736  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.17479984052558412  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.22067391276042775  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.17201038747156883  - accuracy: 0.75\n",
      "At: 430 [==========>] Loss 0.11525914692019365  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.12016206400399952  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.13391837937606213  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.08391750840548182  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.10077577902673955  - accuracy: 0.90625\n",
      "At: 435 [==========>] Loss 0.16824657648924912  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.10617021569069877  - accuracy: 0.875\n",
      "At: 437 [==========>] Loss 0.11810234667731102  - accuracy: 0.84375\n",
      "At: 438 [==========>] Loss 0.10186684683923988  - accuracy: 0.84375\n",
      "At: 439 [==========>] Loss 0.10171129143047242  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.10198674588774523  - accuracy: 0.84375\n",
      "At: 441 [==========>] Loss 0.18344938141770284  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.1297973471945002  - accuracy: 0.84375\n",
      "At: 443 [==========>] Loss 0.14010262965275838  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.1408454792294279  - accuracy: 0.78125\n",
      "At: 445 [==========>] Loss 0.12032351913723552  - accuracy: 0.84375\n",
      "At: 446 [==========>] Loss 0.20253264503396706  - accuracy: 0.6875\n",
      "At: 447 [==========>] Loss 0.15081143492556673  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.1767800291557597  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.1247164949644523  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.1330405685281864  - accuracy: 0.8125\n",
      "At: 451 [==========>] Loss 0.1607163378726016  - accuracy: 0.78125\n",
      "At: 452 [==========>] Loss 0.12699208704988607  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.15380822563764265  - accuracy: 0.6875\n",
      "At: 454 [==========>] Loss 0.20564078649666484  - accuracy: 0.71875\n",
      "At: 455 [==========>] Loss 0.22725252394034473  - accuracy: 0.6875\n",
      "At: 456 [==========>] Loss 0.1442960134220374  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.15859740269874328  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.08078058098403695  - accuracy: 0.90625\n",
      "At: 459 [==========>] Loss 0.1879756973120109  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.11756797608186864  - accuracy: 0.875\n",
      "At: 461 [==========>] Loss 0.20932969847926708  - accuracy: 0.65625\n",
      "At: 462 [==========>] Loss 0.17375075564040987  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.11797298916714526  - accuracy: 0.84375\n",
      "At: 464 [==========>] Loss 0.16554976965352494  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.20116927269659685  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.1184757024162929  - accuracy: 0.8125\n",
      "At: 467 [==========>] Loss 0.19281245234030847  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.10773088139351336  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.1380143141598918  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.11602263914590874  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.15971971525583856  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.09544362239025898  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.16603210000326626  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.18112186304607575  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.1564081880774384  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.14777925984804496  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.13867172340284017  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.13172891039353857  - accuracy: 0.78125\n",
      "At: 479 [==========>] Loss 0.1579699966948774  - accuracy: 0.78125\n",
      "At: 480 [==========>] Loss 0.17474051705630142  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.10661799744922162  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.08704490064539497  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.13956702021610135  - accuracy: 0.78125\n",
      "At: 484 [==========>] Loss 0.09695008697165874  - accuracy: 0.90625\n",
      "At: 485 [==========>] Loss 0.11709089086623713  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.15754594961439747  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.13272251891232434  - accuracy: 0.84375\n",
      "At: 488 [==========>] Loss 0.15745332768890832  - accuracy: 0.75\n",
      "At: 489 [==========>] Loss 0.14659972489475648  - accuracy: 0.8125\n",
      "At: 490 [==========>] Loss 0.1608303689227713  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.16189867046588763  - accuracy: 0.8125\n",
      "At: 492 [==========>] Loss 0.18138821959146045  - accuracy: 0.75\n",
      "At: 493 [==========>] Loss 0.13809584715159018  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.1596302246994262  - accuracy: 0.75\n",
      "At: 495 [==========>] Loss 0.11257300852742212  - accuracy: 0.875\n",
      "At: 496 [==========>] Loss 0.1848366952309327  - accuracy: 0.75\n",
      "At: 497 [==========>] Loss 0.1719793913211786  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.09233976970698766  - accuracy: 0.8125\n",
      "At: 499 [==========>] Loss 0.15175212686259398  - accuracy: 0.8125\n",
      "At: 500 [==========>] Loss 0.10389891330745454  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.14627027057946007  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.12225402046443543  - accuracy: 0.875\n",
      "At: 503 [==========>] Loss 0.10984088763745001  - accuracy: 0.875\n",
      "At: 504 [==========>] Loss 0.07235817867042287  - accuracy: 0.96875\n",
      "At: 505 [==========>] Loss 0.17918758608411245  - accuracy: 0.78125\n",
      "At: 506 [==========>] Loss 0.24635239503895867  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.09522306676888216  - accuracy: 0.875\n",
      "At: 508 [==========>] Loss 0.08046565612352129  - accuracy: 0.9375\n",
      "At: 509 [==========>] Loss 0.15272368549136417  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.1861783349607681  - accuracy: 0.6875\n",
      "At: 511 [==========>] Loss 0.12467894630890511  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.1987175837176567  - accuracy: 0.71875\n",
      "At: 513 [==========>] Loss 0.24319745700656212  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.15825660321661006  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.14605761990473426  - accuracy: 0.75\n",
      "At: 516 [==========>] Loss 0.13419204736767143  - accuracy: 0.8125\n",
      "At: 517 [==========>] Loss 0.11325020821271535  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.1512571630612377  - accuracy: 0.78125\n",
      "At: 519 [==========>] Loss 0.14557288028615384  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.1377926481128674  - accuracy: 0.84375\n",
      "At: 521 [==========>] Loss 0.10922289872972962  - accuracy: 0.84375\n",
      "At: 522 [==========>] Loss 0.1574533770602027  - accuracy: 0.71875\n",
      "At: 523 [==========>] Loss 0.16438249640809038  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.08526079186242837  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.1296187379011171  - accuracy: 0.8125\n",
      "At: 526 [==========>] Loss 0.18352628571496626  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.24600186483055864  - accuracy: 0.59375\n",
      "At: 528 [==========>] Loss 0.1582064667735329  - accuracy: 0.8125\n",
      "At: 529 [==========>] Loss 0.10269426354210322  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.17373784145564064  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.17192383921039223  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.12863283143220186  - accuracy: 0.8125\n",
      "At: 533 [==========>] Loss 0.09320697406771551  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.16351754051692946  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.17701453343476048  - accuracy: 0.75\n",
      "At: 536 [==========>] Loss 0.20274526981775448  - accuracy: 0.6875\n",
      "At: 537 [==========>] Loss 0.11872241015329654  - accuracy: 0.75\n",
      "At: 538 [==========>] Loss 0.1243408638558568  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.10895918571236653  - accuracy: 0.875\n",
      "At: 540 [==========>] Loss 0.2171040432171466  - accuracy: 0.65625\n",
      "At: 541 [==========>] Loss 0.20056600576852507  - accuracy: 0.6875\n",
      "At: 542 [==========>] Loss 0.14099099561298267  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.16331320440271352  - accuracy: 0.75\n",
      "At: 544 [==========>] Loss 0.1965190224075725  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.05739318000973105  - accuracy: 0.96875\n",
      "At: 546 [==========>] Loss 0.1510786981380251  - accuracy: 0.8125\n",
      "At: 547 [==========>] Loss 0.13173074194742193  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.10645860071934485  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.10713902493918856  - accuracy: 0.8125\n",
      "At: 550 [==========>] Loss 0.1079928104221784  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.14867312144771064  - accuracy: 0.78125\n",
      "At: 552 [==========>] Loss 0.13710504517016447  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.09247520121415055  - accuracy: 0.84375\n",
      "At: 554 [==========>] Loss 0.1235101220649854  - accuracy: 0.78125\n",
      "At: 555 [==========>] Loss 0.1461892796068305  - accuracy: 0.84375\n",
      "At: 556 [==========>] Loss 0.1723849435820048  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.1326305127292095  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.13855963296921317  - accuracy: 0.75\n",
      "At: 559 [==========>] Loss 0.2015357715996106  - accuracy: 0.71875\n",
      "At: 560 [==========>] Loss 0.12350743811727263  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.1362904865470314  - accuracy: 0.78125\n",
      "At: 562 [==========>] Loss 0.07290552341349671  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.12514193060066608  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.13784770559027915  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.09293142535815407  - accuracy: 0.90625\n",
      "At: 566 [==========>] Loss 0.13486543132833728  - accuracy: 0.84375\n",
      "At: 567 [==========>] Loss 0.1617674844840511  - accuracy: 0.78125\n",
      "At: 568 [==========>] Loss 0.21708163675125866  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.1904787149397676  - accuracy: 0.71875\n",
      "At: 570 [==========>] Loss 0.1019942592856061  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.13458195557699654  - accuracy: 0.84375\n",
      "At: 572 [==========>] Loss 0.1437118821264891  - accuracy: 0.78125\n",
      "At: 573 [==========>] Loss 0.10810197078485398  - accuracy: 0.8125\n",
      "At: 574 [==========>] Loss 0.14371842956920344  - accuracy: 0.8125\n",
      "At: 575 [==========>] Loss 0.11243509970170337  - accuracy: 0.875\n",
      "At: 576 [==========>] Loss 0.08550102271169371  - accuracy: 0.90625\n",
      "At: 577 [==========>] Loss 0.18444986748685982  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.16923801374821784  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.1034828488726379  - accuracy: 0.84375\n",
      "At: 580 [==========>] Loss 0.12417786966503114  - accuracy: 0.78125\n",
      "At: 581 [==========>] Loss 0.14153167107914102  - accuracy: 0.8125\n",
      "At: 582 [==========>] Loss 0.12073280499180816  - accuracy: 0.84375\n",
      "At: 583 [==========>] Loss 0.1808830675791706  - accuracy: 0.8125\n",
      "At: 584 [==========>] Loss 0.07873130233900548  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.1802804718321206  - accuracy: 0.71875\n",
      "At: 586 [==========>] Loss 0.08933753605341874  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.08837292169976926  - accuracy: 0.90625\n",
      "At: 588 [==========>] Loss 0.14892547474728857  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.15558763425130956  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.04816019938692201  - accuracy: 0.96875\n",
      "At: 591 [==========>] Loss 0.14520474158033442  - accuracy: 0.78125\n",
      "At: 592 [==========>] Loss 0.07974760394456518  - accuracy: 0.90625\n",
      "At: 593 [==========>] Loss 0.191153354115368  - accuracy: 0.75\n",
      "At: 594 [==========>] Loss 0.12107266736245828  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.13248764671616808  - accuracy: 0.84375\n",
      "At: 596 [==========>] Loss 0.10905698887672548  - accuracy: 0.9375\n",
      "At: 597 [==========>] Loss 0.18938889222368904  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.14682283724909176  - accuracy: 0.78125\n",
      "At: 599 [==========>] Loss 0.12545378218817815  - accuracy: 0.8125\n",
      "At: 600 [==========>] Loss 0.08091884150006723  - accuracy: 0.90625\n",
      "At: 601 [==========>] Loss 0.13287471679426532  - accuracy: 0.78125\n",
      "At: 602 [==========>] Loss 0.11192702994573467  - accuracy: 0.875\n",
      "At: 603 [==========>] Loss 0.09785979500943537  - accuracy: 0.84375\n",
      "At: 604 [==========>] Loss 0.20638954856083877  - accuracy: 0.65625\n",
      "At: 605 [==========>] Loss 0.11842720764212845  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.15323550257933977  - accuracy: 0.8125\n",
      "At: 607 [==========>] Loss 0.17574024446053518  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.12968235213549945  - accuracy: 0.875\n",
      "At: 609 [==========>] Loss 0.11899460858150962  - accuracy: 0.8125\n",
      "At: 610 [==========>] Loss 0.12704101773606852  - accuracy: 0.84375\n",
      "At: 611 [==========>] Loss 0.09707728836174104  - accuracy: 0.875\n",
      "At: 612 [==========>] Loss 0.13192175422666197  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.16706593099541  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.12324244700684206  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.15712801594062598  - accuracy: 0.84375\n",
      "At: 616 [==========>] Loss 0.17497744610470972  - accuracy: 0.75\n",
      "At: 617 [==========>] Loss 0.12364485361782523  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.2196444842380325  - accuracy: 0.6875\n",
      "At: 619 [==========>] Loss 0.15042731499051745  - accuracy: 0.78125\n",
      "At: 620 [==========>] Loss 0.16682194555637075  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.04484557931045085  - accuracy: 0.96875\n",
      "At: 622 [==========>] Loss 0.19072512872456593  - accuracy: 0.71875\n",
      "At: 623 [==========>] Loss 0.13208002479387096  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.11487465248160283  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.12982195077500697  - accuracy: 0.875\n",
      "At: 626 [==========>] Loss 0.1310201677134239  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.13044497921940912  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.11531544786670203  - accuracy: 0.875\n",
      "At: 629 [==========>] Loss 0.16921455702552296  - accuracy: 0.78125\n",
      "At: 630 [==========>] Loss 0.21445278221557607  - accuracy: 0.625\n",
      "At: 631 [==========>] Loss 0.17765893681867045  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.15543179002749083  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.18544460262372653  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.15345066410209163  - accuracy: 0.75\n",
      "At: 635 [==========>] Loss 0.1087120825814064  - accuracy: 0.90625\n",
      "At: 636 [==========>] Loss 0.11815709477564487  - accuracy: 0.90625\n",
      "At: 637 [==========>] Loss 0.14447338138018187  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.13566052984645335  - accuracy: 0.78125\n",
      "At: 639 [==========>] Loss 0.12623856053588278  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.1949201921186965  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.16653587630564254  - accuracy: 0.75\n",
      "At: 642 [==========>] Loss 0.19418140043107615  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.11697242837380648  - accuracy: 0.8125\n",
      "At: 644 [==========>] Loss 0.05905575275837061  - accuracy: 0.96875\n",
      "At: 645 [==========>] Loss 0.1303729392842526  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.12195241134158691  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.17609079788546173  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.190945858747214  - accuracy: 0.75\n",
      "At: 649 [==========>] Loss 0.1626045142825955  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.08933941347137767  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.1858946490774902  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.10506199218314892  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.11007179579686412  - accuracy: 0.875\n",
      "At: 654 [==========>] Loss 0.09680699299357054  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.14303380924870868  - accuracy: 0.75\n",
      "At: 656 [==========>] Loss 0.09253981972025521  - accuracy: 0.875\n",
      "At: 657 [==========>] Loss 0.13964188358377944  - accuracy: 0.84375\n",
      "At: 658 [==========>] Loss 0.12157077178963113  - accuracy: 0.875\n",
      "At: 659 [==========>] Loss 0.13011656689773254  - accuracy: 0.875\n",
      "At: 660 [==========>] Loss 0.12329453478298295  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.12954943797938334  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.09454934539766027  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.08114558777950556  - accuracy: 0.9375\n",
      "At: 664 [==========>] Loss 0.10786377835960749  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.18293140402055036  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.2088279985678051  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.10019297433905999  - accuracy: 0.875\n",
      "At: 668 [==========>] Loss 0.14354597869370866  - accuracy: 0.84375\n",
      "At: 669 [==========>] Loss 0.14376176542626623  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.18922454013873632  - accuracy: 0.71875\n",
      "At: 671 [==========>] Loss 0.13661601180498553  - accuracy: 0.8125\n",
      "At: 672 [==========>] Loss 0.13624025002704676  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.07300408442415834  - accuracy: 0.875\n",
      "At: 674 [==========>] Loss 0.13464545770861638  - accuracy: 0.75\n",
      "At: 675 [==========>] Loss 0.09260859785266541  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.1852821012052133  - accuracy: 0.75\n",
      "At: 677 [==========>] Loss 0.15352689770565361  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.1468590802474802  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.12243923182247621  - accuracy: 0.875\n",
      "At: 680 [==========>] Loss 0.10985371912075924  - accuracy: 0.8125\n",
      "At: 681 [==========>] Loss 0.14167449276228666  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.15951648278271302  - accuracy: 0.71875\n",
      "At: 683 [==========>] Loss 0.16750093177566822  - accuracy: 0.75\n",
      "At: 684 [==========>] Loss 0.0656374582649769  - accuracy: 0.96875\n",
      "At: 685 [==========>] Loss 0.15502274191437543  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.135813288436141  - accuracy: 0.78125\n",
      "At: 687 [==========>] Loss 0.09253947309329076  - accuracy: 0.84375\n",
      "At: 688 [==========>] Loss 0.0811327888450017  - accuracy: 0.90625\n",
      "At: 689 [==========>] Loss 0.1624669090549173  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.11618915023135126  - accuracy: 0.84375\n",
      "At: 691 [==========>] Loss 0.12676819594812844  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.12819383888874403  - accuracy: 0.8125\n",
      "At: 693 [==========>] Loss 0.15515035551436362  - accuracy: 0.84375\n",
      "At: 694 [==========>] Loss 0.18429124194799304  - accuracy: 0.75\n",
      "At: 695 [==========>] Loss 0.14696257678057545  - accuracy: 0.8125\n",
      "At: 696 [==========>] Loss 0.18880867961176762  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.16281292488397792  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.09976717997773643  - accuracy: 0.90625\n",
      "At: 699 [==========>] Loss 0.1363104550034423  - accuracy: 0.8125\n",
      "At: 700 [==========>] Loss 0.08649031783186228  - accuracy: 0.9375\n",
      "At: 701 [==========>] Loss 0.1451116281298376  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.11142158586075836  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.19286943120617406  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.18354331795421622  - accuracy: 0.71875\n",
      "At: 705 [==========>] Loss 0.22057887566577827  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.18713642833848892  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.1049980159636402  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.15826096923085387  - accuracy: 0.84375\n",
      "At: 709 [==========>] Loss 0.21673138275487852  - accuracy: 0.6875\n",
      "At: 710 [==========>] Loss 0.1453143056546856  - accuracy: 0.8125\n",
      "At: 711 [==========>] Loss 0.22695121142379582  - accuracy: 0.6875\n",
      "At: 712 [==========>] Loss 0.14240727599025696  - accuracy: 0.84375\n",
      "At: 713 [==========>] Loss 0.18421263152021944  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.2352272186783642  - accuracy: 0.5625\n",
      "At: 715 [==========>] Loss 0.10194550754104005  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.13501226897988064  - accuracy: 0.8125\n",
      "At: 717 [==========>] Loss 0.07511816452528208  - accuracy: 0.875\n",
      "At: 718 [==========>] Loss 0.18846079618045924  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.09332992102629784  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.13192630106306547  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.09772819460620238  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.1746048039088242  - accuracy: 0.78125\n",
      "At: 723 [==========>] Loss 0.16641290834958844  - accuracy: 0.78125\n",
      "At: 724 [==========>] Loss 0.11152580374671432  - accuracy: 0.8125\n",
      "At: 725 [==========>] Loss 0.155199491590126  - accuracy: 0.875\n",
      "At: 726 [==========>] Loss 0.2019776524040168  - accuracy: 0.65625\n",
      "At: 727 [==========>] Loss 0.18643374775023666  - accuracy: 0.75\n",
      "At: 728 [==========>] Loss 0.14320934058206453  - accuracy: 0.8125\n",
      "At: 729 [==========>] Loss 0.2005352195575292  - accuracy: 0.625\n",
      "At: 730 [==========>] Loss 0.17673231749604976  - accuracy: 0.78125\n",
      "At: 731 [==========>] Loss 0.1523040192900437  - accuracy: 0.78125\n",
      "At: 732 [==========>] Loss 0.13668930675797653  - accuracy: 0.84375\n",
      "At: 733 [==========>] Loss 0.10818796492328871  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.18529071318045148  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.11759456396705391  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.10514532802328022  - accuracy: 0.84375\n",
      "At: 737 [==========>] Loss 0.15233698174293486  - accuracy: 0.8125\n",
      "At: 738 [==========>] Loss 0.1121164975597338  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.08979919086488186  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.16491785635768016  - accuracy: 0.75\n",
      "At: 741 [==========>] Loss 0.08827970830263483  - accuracy: 0.90625\n",
      "At: 742 [==========>] Loss 0.14597693737292775  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.1513938741086515  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.13673560970206255  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.19202992534231506  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.1036104712159911  - accuracy: 0.875\n",
      "At: 747 [==========>] Loss 0.11847619336307733  - accuracy: 0.90625\n",
      "At: 748 [==========>] Loss 0.15239446814600263  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.15858948789987876  - accuracy: 0.75\n",
      "At: 750 [==========>] Loss 0.12495547636456947  - accuracy: 0.8125\n",
      "At: 751 [==========>] Loss 0.15278750134990382  - accuracy: 0.8125\n",
      "At: 752 [==========>] Loss 0.08298770200451064  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.18285002858715788  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.14112882085423328  - accuracy: 0.78125\n",
      "At: 755 [==========>] Loss 0.11616169874302926  - accuracy: 0.84375\n",
      "At: 756 [==========>] Loss 0.22987177595621844  - accuracy: 0.625\n",
      "At: 757 [==========>] Loss 0.09389624189972057  - accuracy: 0.9375\n",
      "At: 758 [==========>] Loss 0.14141988678219158  - accuracy: 0.75\n",
      "At: 759 [==========>] Loss 0.0978739419124845  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.20351802039457284  - accuracy: 0.71875\n",
      "At: 761 [==========>] Loss 0.1635429840317041  - accuracy: 0.71875\n",
      "At: 762 [==========>] Loss 0.13686680099339732  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.1430867029547553  - accuracy: 0.75\n",
      "At: 764 [==========>] Loss 0.10472703489540416  - accuracy: 0.875\n",
      "At: 765 [==========>] Loss 0.14362454410418304  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.14780520476045955  - accuracy: 0.78125\n",
      "At: 767 [==========>] Loss 0.11109115581038323  - accuracy: 0.875\n",
      "At: 768 [==========>] Loss 0.17142906236313893  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.162566853120192  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.10335754630477245  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.2028859682272024  - accuracy: 0.625\n",
      "At: 772 [==========>] Loss 0.14610337654342212  - accuracy: 0.78125\n",
      "At: 773 [==========>] Loss 0.07825368626351531  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.1325303722109201  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.16721932664999464  - accuracy: 0.75\n",
      "At: 776 [==========>] Loss 0.15790178962047324  - accuracy: 0.8125\n",
      "At: 777 [==========>] Loss 0.06853881107572823  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.18131329523611184  - accuracy: 0.71875\n",
      "At: 779 [==========>] Loss 0.10627531186614074  - accuracy: 0.84375\n",
      "At: 780 [==========>] Loss 0.07778366759416282  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.14302559932537984  - accuracy: 0.84375\n",
      "At: 782 [==========>] Loss 0.15677844637556437  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.17077286366968875  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.16316910865038695  - accuracy: 0.71875\n",
      "At: 785 [==========>] Loss 0.19172611760231872  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.12288584651593104  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.16449603709341049  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.0839688873848714  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.131267831039385  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.15351480006156226  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.19044872909125246  - accuracy: 0.71875\n",
      "At: 792 [==========>] Loss 0.16996661533077811  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.12512556764712207  - accuracy: 0.8125\n",
      "At: 794 [==========>] Loss 0.1342359741448485  - accuracy: 0.8125\n",
      "At: 795 [==========>] Loss 0.11910018820368243  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.14281172967553504  - accuracy: 0.78125\n",
      "At: 797 [==========>] Loss 0.16795687131563897  - accuracy: 0.78125\n",
      "At: 798 [==========>] Loss 0.12485418499457308  - accuracy: 0.84375\n",
      "At: 799 [==========>] Loss 0.06615171680512794  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.1596432720677578  - accuracy: 0.6875\n",
      "At: 801 [==========>] Loss 0.08798407354267104  - accuracy: 0.90625\n",
      "At: 802 [==========>] Loss 0.1801475852246743  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.17036097793483013  - accuracy: 0.71875\n",
      "At: 804 [==========>] Loss 0.16234214874551955  - accuracy: 0.75\n",
      "At: 805 [==========>] Loss 0.18836635768567248  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.11840322850785168  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.12610524680682164  - accuracy: 0.78125\n",
      "At: 808 [==========>] Loss 0.1415208685739377  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.10154614332402276  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.21151141007669128  - accuracy: 0.75\n",
      "At: 811 [==========>] Loss 0.13073304051444812  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.14359894138454493  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.1533912371750187  - accuracy: 0.78125\n",
      "At: 814 [==========>] Loss 0.17708058858098003  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.14708577709524448  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.1458505107198213  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.07848472296982316  - accuracy: 0.875\n",
      "At: 818 [==========>] Loss 0.10318368540074607  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.09315505457972317  - accuracy: 0.875\n",
      "At: 820 [==========>] Loss 0.1336430650640497  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.16239777831096383  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.1651822212644547  - accuracy: 0.78125\n",
      "At: 823 [==========>] Loss 0.16350174441766946  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.17301720728139172  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.20933206318594572  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.1483547991148333  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.0854312186165476  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.07798177007880469  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.07901036669598795  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.10750959699618859  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.09988504677397865  - accuracy: 0.875\n",
      "At: 832 [==========>] Loss 0.11576423782367234  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.15872690986515076  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.14819551322495186  - accuracy: 0.75\n",
      "At: 835 [==========>] Loss 0.07340249330237901  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.10804274953965423  - accuracy: 0.8125\n",
      "At: 837 [==========>] Loss 0.1251329583112744  - accuracy: 0.78125\n",
      "At: 838 [==========>] Loss 0.12223516698369061  - accuracy: 0.75\n",
      "At: 839 [==========>] Loss 0.13093438655392092  - accuracy: 0.78125\n",
      "At: 840 [==========>] Loss 0.08459385591504617  - accuracy: 0.875\n",
      "At: 841 [==========>] Loss 0.06310303276631926  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.08229894340338216  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.19480493629969667  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.11118883295989158  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.14652175475947712  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.14108414405815983  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.08644469180200808  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.20067495917894188  - accuracy: 0.6875\n",
      "At: 849 [==========>] Loss 0.1928107264888685  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.09345698262671569  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.09068145206586176  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.1646891902984841  - accuracy: 0.75\n",
      "At: 853 [==========>] Loss 0.1809917461318477  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.22393512811472352  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.07862160916968186  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.10161315448968994  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.09174062948706216  - accuracy: 0.875\n",
      "At: 858 [==========>] Loss 0.25472600432773296  - accuracy: 0.59375\n",
      "At: 859 [==========>] Loss 0.0955156132690823  - accuracy: 0.875\n",
      "At: 860 [==========>] Loss 0.1526105393680631  - accuracy: 0.8125\n",
      "At: 861 [==========>] Loss 0.11840775604264017  - accuracy: 0.8125\n",
      "At: 862 [==========>] Loss 0.13150662604367055  - accuracy: 0.8125\n",
      "At: 863 [==========>] Loss 0.1383861371872876  - accuracy: 0.8125\n",
      "At: 864 [==========>] Loss 0.18369652330757102  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.1743791337098943  - accuracy: 0.75\n",
      "At: 866 [==========>] Loss 0.1490853983035561  - accuracy: 0.8125\n",
      "At: 867 [==========>] Loss 0.09628897778777701  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.19515190321714646  - accuracy: 0.6875\n",
      "At: 869 [==========>] Loss 0.17132925175866726  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.14006319464056824  - accuracy: 0.78125\n",
      "At: 871 [==========>] Loss 0.07163832328785738  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.09379068407804182  - accuracy: 0.84375\n",
      "At: 873 [==========>] Loss 0.19701128493862774  - accuracy: 0.71875\n",
      "At: 874 [==========>] Loss 0.16857093716787425  - accuracy: 0.75\n",
      "At: 875 [==========>] Loss 0.1137752181754474  - accuracy: 0.90625\n",
      "At: 876 [==========>] Loss 0.12033640514824129  - accuracy: 0.8125\n",
      "At: 877 [==========>] Loss 0.18730158285303022  - accuracy: 0.75\n",
      "At: 878 [==========>] Loss 0.056972098124841566  - accuracy: 0.9375\n",
      "At: 879 [==========>] Loss 0.15933708286833675  - accuracy: 0.78125\n",
      "At: 880 [==========>] Loss 0.10841285192452084  - accuracy: 0.875\n",
      "At: 881 [==========>] Loss 0.16431564332678328  - accuracy: 0.75\n",
      "At: 882 [==========>] Loss 0.11328695655387558  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.12774624251598413  - accuracy: 0.8125\n",
      "At: 884 [==========>] Loss 0.13047186158767177  - accuracy: 0.8125\n",
      "At: 885 [==========>] Loss 0.11706467084851539  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.10065906552134149  - accuracy: 0.8125\n",
      "At: 887 [==========>] Loss 0.14680490621124387  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.20299841238576657  - accuracy: 0.6875\n",
      "At: 889 [==========>] Loss 0.1404626578719991  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.15975920162732277  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.08590101889093904  - accuracy: 0.9375\n",
      "At: 892 [==========>] Loss 0.1164394156936557  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.1318134452092143  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.1308505742693964  - accuracy: 0.875\n",
      "At: 895 [==========>] Loss 0.10713729909097869  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.11810162054432648  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.15839612883450685  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.16629000839790772  - accuracy: 0.75\n",
      "At: 899 [==========>] Loss 0.10998655967392058  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.14468573092629294  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.18693189376640876  - accuracy: 0.65625\n",
      "At: 902 [==========>] Loss 0.0940359031561031  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.1307741205398363  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.10099756065423393  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.09808276137297352  - accuracy: 0.90625\n",
      "At: 906 [==========>] Loss 0.06903097980229819  - accuracy: 0.9375\n",
      "At: 907 [==========>] Loss 0.15490520685641035  - accuracy: 0.71875\n",
      "At: 908 [==========>] Loss 0.11707301388338263  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.08557166851966355  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.1420829045308603  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.16009527205194807  - accuracy: 0.84375\n",
      "At: 912 [==========>] Loss 0.15777145861263542  - accuracy: 0.75\n",
      "At: 913 [==========>] Loss 0.1066498736885367  - accuracy: 0.84375\n",
      "At: 914 [==========>] Loss 0.09281194090465697  - accuracy: 0.875\n",
      "At: 915 [==========>] Loss 0.15252093597453154  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.14082804741580945  - accuracy: 0.78125\n",
      "At: 917 [==========>] Loss 0.15272794406997492  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.21102165644223214  - accuracy: 0.65625\n",
      "At: 919 [==========>] Loss 0.13488394692599698  - accuracy: 0.78125\n",
      "At: 920 [==========>] Loss 0.14262120540264658  - accuracy: 0.8125\n",
      "At: 921 [==========>] Loss 0.1277169832083789  - accuracy: 0.8125\n",
      "At: 922 [==========>] Loss 0.14176330175066376  - accuracy: 0.75\n",
      "At: 923 [==========>] Loss 0.09137467454193476  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.19534524765418107  - accuracy: 0.78125\n",
      "At: 925 [==========>] Loss 0.15019717029388846  - accuracy: 0.78125\n",
      "At: 926 [==========>] Loss 0.14303834672773563  - accuracy: 0.8125\n",
      "At: 927 [==========>] Loss 0.13009790349899786  - accuracy: 0.78125\n",
      "At: 928 [==========>] Loss 0.13011369435508388  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.13278952855984077  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.09503380982482701  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.18119642987412982  - accuracy: 0.8125\n",
      "At: 932 [==========>] Loss 0.10762667072404601  - accuracy: 0.84375\n",
      "At: 933 [==========>] Loss 0.11575168041239493  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.1401085274595823  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.054603648969098265  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.16490649090667098  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.1538416695249012  - accuracy: 0.8125\n",
      "At: 938 [==========>] Loss 0.11812831309022682  - accuracy: 0.84375\n",
      "At: 939 [==========>] Loss 0.09562160622952486  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.2290327970465772  - accuracy: 0.71875\n",
      "At: 941 [==========>] Loss 0.12049445543324781  - accuracy: 0.8125\n",
      "At: 942 [==========>] Loss 0.16623738057499315  - accuracy: 0.75\n",
      "At: 943 [==========>] Loss 0.1482552225329235  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.08671654514340527  - accuracy: 0.875\n",
      "At: 945 [==========>] Loss 0.10847182822872223  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.13539743894889938  - accuracy: 0.875\n",
      "At: 947 [==========>] Loss 0.1378132071778238  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.18047955903449905  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.08391588283149762  - accuracy: 0.84375\n",
      "At: 950 [==========>] Loss 0.09660830538933246  - accuracy: 0.875\n",
      "At: 951 [==========>] Loss 0.110301795614518  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.07618943565450216  - accuracy: 0.875\n",
      "At: 953 [==========>] Loss 0.05693178378606639  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.12288231732165111  - accuracy: 0.8125\n",
      "At: 955 [==========>] Loss 0.15126375781353837  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.07357128014930883  - accuracy: 0.9375\n",
      "At: 957 [==========>] Loss 0.14162833239522526  - accuracy: 0.8125\n",
      "At: 958 [==========>] Loss 0.08539641521250893  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.1072319194296185  - accuracy: 0.875\n",
      "At: 960 [==========>] Loss 0.13781295775172064  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.13716807380889104  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.09449719776999847  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.08626733394217244  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.1842611777409769  - accuracy: 0.78125\n",
      "At: 965 [==========>] Loss 0.10449615780699324  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.17876679109585042  - accuracy: 0.6875\n",
      "At: 967 [==========>] Loss 0.11246985758952302  - accuracy: 0.875\n",
      "At: 968 [==========>] Loss 0.13755688745985167  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.11175376960440737  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.14985852011332335  - accuracy: 0.78125\n",
      "At: 971 [==========>] Loss 0.09782048308429392  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.06907741072677623  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.11281233285621053  - accuracy: 0.875\n",
      "At: 974 [==========>] Loss 0.08146157205991682  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.13083138345708784  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.12975008370833396  - accuracy: 0.78125\n",
      "At: 977 [==========>] Loss 0.11047698204091363  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.1578723535217076  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.08718364143629286  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.11936734553902453  - accuracy: 0.84375\n",
      "At: 981 [==========>] Loss 0.1692930255859556  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.09229621341611258  - accuracy: 0.84375\n",
      "At: 983 [==========>] Loss 0.13982745267531085  - accuracy: 0.78125\n",
      "At: 984 [==========>] Loss 0.09294744110600406  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.1700248928749558  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.11808091084721832  - accuracy: 0.84375\n",
      "At: 987 [==========>] Loss 0.15047055560889808  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.1005791173514834  - accuracy: 0.90625\n",
      "At: 989 [==========>] Loss 0.13609787644081558  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.1244513755445418  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.1711451908719136  - accuracy: 0.6875\n",
      "At: 992 [==========>] Loss 0.21806619251285345  - accuracy: 0.625\n",
      "At: 993 [==========>] Loss 0.15563521618214216  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.16350324326727544  - accuracy: 0.75\n",
      "At: 995 [==========>] Loss 0.16607820748498342  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.08569190261491247  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.18058042656548676  - accuracy: 0.6875\n",
      "At: 998 [==========>] Loss 0.11757253841817003  - accuracy: 0.8125\n",
      "At: 999 [==========>] Loss 0.1522245025135941  - accuracy: 0.75\n",
      "At: 1000 [==========>] Loss 0.21116073157887855  - accuracy: 0.6875\n",
      "At: 1001 [==========>] Loss 0.16152700361018907  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.22773992582384112  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.1295455109341945  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.12653448339864942  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.11290928148459026  - accuracy: 0.84375\n",
      "At: 1006 [==========>] Loss 0.12145884716321122  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.12184399615940825  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.19407153263462462  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.09553849590088995  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.14670180332414406  - accuracy: 0.75\n",
      "At: 1011 [==========>] Loss 0.1463166928240941  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.1101573201821045  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.10330800037448225  - accuracy: 0.875\n",
      "At: 1014 [==========>] Loss 0.08197338199325155  - accuracy: 0.90625\n",
      "At: 1015 [==========>] Loss 0.22997483475685374  - accuracy: 0.6875\n",
      "At: 1016 [==========>] Loss 0.15546317939804755  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.17340570942708333  - accuracy: 0.6875\n",
      "At: 1018 [==========>] Loss 0.184786790122869  - accuracy: 0.71875\n",
      "At: 1019 [==========>] Loss 0.20000810545995668  - accuracy: 0.71875\n",
      "At: 1020 [==========>] Loss 0.1569257745616283  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.12655813845270192  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.12544149345796496  - accuracy: 0.84375\n",
      "At: 1023 [==========>] Loss 0.16167837447084088  - accuracy: 0.75\n",
      "At: 1024 [==========>] Loss 0.17042913954120195  - accuracy: 0.78125\n",
      "At: 1025 [==========>] Loss 0.204952852228099  - accuracy: 0.6875\n",
      "At: 1026 [==========>] Loss 0.10427136202752009  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.11485864402967118  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.2367376396119089  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.09015256711341867  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.12188760259407164  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.16289721014980507  - accuracy: 0.8125\n",
      "At: 1032 [==========>] Loss 0.14087326808651895  - accuracy: 0.8125\n",
      "At: 1033 [==========>] Loss 0.13042914420579152  - accuracy: 0.875\n",
      "At: 1034 [==========>] Loss 0.07597754248072419  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.10967307968228326  - accuracy: 0.84375\n",
      "At: 1036 [==========>] Loss 0.1987112950153813  - accuracy: 0.6875\n",
      "At: 1037 [==========>] Loss 0.1399071814142797  - accuracy: 0.8125\n",
      "At: 1038 [==========>] Loss 0.10369641237139877  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.10032248254216432  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.11903428608588185  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.13807816917940566  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.11243767782277106  - accuracy: 0.84375\n",
      "At: 1043 [==========>] Loss 0.20351042084243537  - accuracy: 0.65625\n",
      "At: 1044 [==========>] Loss 0.11822335052858321  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.15249385498333445  - accuracy: 0.78125\n",
      "At: 1046 [==========>] Loss 0.17038946598949434  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.12848015204367205  - accuracy: 0.8125\n",
      "At: 1048 [==========>] Loss 0.1548788417812585  - accuracy: 0.84375\n",
      "At: 1049 [==========>] Loss 0.13897387222100255  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.19442768824491466  - accuracy: 0.71875\n",
      "At: 1051 [==========>] Loss 0.08762902512867307  - accuracy: 0.90625\n",
      "At: 1052 [==========>] Loss 0.11498091167227875  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.09948422967474632  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.10955800811701032  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.1921030511661917  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.1393573728018302  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.15015570826662666  - accuracy: 0.75\n",
      "At: 1058 [==========>] Loss 0.046263451618974445  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.10175281012930149  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.07886847601078852  - accuracy: 0.875\n",
      "At: 1061 [==========>] Loss 0.11130849828886913  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.14394841498911182  - accuracy: 0.71875\n",
      "At: 1063 [==========>] Loss 0.14321274244603385  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.11923333809617681  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.07945427663040636  - accuracy: 0.9375\n",
      "At: 1066 [==========>] Loss 0.12102730111106967  - accuracy: 0.8125\n",
      "At: 1067 [==========>] Loss 0.13083521574632334  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.11016830483368101  - accuracy: 0.90625\n",
      "At: 1069 [==========>] Loss 0.13547714306076575  - accuracy: 0.84375\n",
      "At: 1070 [==========>] Loss 0.10902994846378322  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.10630690860503653  - accuracy: 0.8125\n",
      "At: 1072 [==========>] Loss 0.15188177193431945  - accuracy: 0.78125\n",
      "At: 1073 [==========>] Loss 0.16750693752724416  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.2006095128588905  - accuracy: 0.71875\n",
      "At: 1075 [==========>] Loss 0.07991917347291942  - accuracy: 0.90625\n",
      "At: 1076 [==========>] Loss 0.15064456815026528  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.07489737283360856  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.09635429655106109  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.13862290029938934  - accuracy: 0.875\n",
      "At: 1080 [==========>] Loss 0.15416297853748506  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.11798659719169147  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.11374161959210329  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.12009238714900357  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.08441715892098879  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.12270065077022405  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.15197521540432007  - accuracy: 0.78125\n",
      "At: 1087 [==========>] Loss 0.13461516455018213  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.1614930684649103  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.07967296555634339  - accuracy: 0.90625\n",
      "At: 1090 [==========>] Loss 0.0941468460389995  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.21178716354903007  - accuracy: 0.6875\n",
      "At: 1092 [==========>] Loss 0.12257660940380788  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.14902207322255376  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.142385225809399  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.13408343440290804  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.13462737493059718  - accuracy: 0.78125\n",
      "At: 1097 [==========>] Loss 0.08394396456986022  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.16714029631460064  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.1445039556393174  - accuracy: 0.75\n",
      "At: 1100 [==========>] Loss 0.07727276411802413  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.10012952112214481  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.1317853131432735  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.09296959534960905  - accuracy: 0.8125\n",
      "At: 1104 [==========>] Loss 0.079380827798925  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.0970904351754655  - accuracy: 0.875\n",
      "At: 1106 [==========>] Loss 0.10283769403356849  - accuracy: 0.8125\n",
      "At: 1107 [==========>] Loss 0.18825745937616334  - accuracy: 0.71875\n",
      "At: 1108 [==========>] Loss 0.13430312977355927  - accuracy: 0.8125\n",
      "At: 1109 [==========>] Loss 0.07224889629371645  - accuracy: 0.84375\n",
      "At: 1110 [==========>] Loss 0.11483899283626728  - accuracy: 0.875\n",
      "At: 1111 [==========>] Loss 0.16229104538790495  - accuracy: 0.8125\n",
      "At: 1112 [==========>] Loss 0.16102524854369327  - accuracy: 0.75\n",
      "At: 1113 [==========>] Loss 0.14491341222548298  - accuracy: 0.75\n",
      "At: 1114 [==========>] Loss 0.09842690042893726  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.12347212795860385  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.13284068383882308  - accuracy: 0.8125\n",
      "At: 1117 [==========>] Loss 0.0672147752772777  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.12620407109114104  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.15400117830381183  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.07768319816245017  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.13815370612578595  - accuracy: 0.78125\n",
      "At: 1122 [==========>] Loss 0.10325871745147874  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.14809757692648412  - accuracy: 0.75\n",
      "At: 1124 [==========>] Loss 0.14824565073831636  - accuracy: 0.75\n",
      "At: 1125 [==========>] Loss 0.16120359243583177  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.10456639495029153  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.1373760140684171  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.0584394951552874  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.1541031043214552  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.11104256427818032  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.09804304784964252  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.12013073016110765  - accuracy: 0.75\n",
      "At: 1133 [==========>] Loss 0.1440371011512368  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.08623528206036143  - accuracy: 0.90625\n",
      "At: 1135 [==========>] Loss 0.12447613550941183  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.16865319953293303  - accuracy: 0.75\n",
      "At: 1137 [==========>] Loss 0.1201585126660499  - accuracy: 0.78125\n",
      "At: 1138 [==========>] Loss 0.10033168684406421  - accuracy: 0.8125\n",
      "At: 1139 [==========>] Loss 0.07134249575065482  - accuracy: 0.9375\n",
      "At: 1140 [==========>] Loss 0.19598339726588104  - accuracy: 0.65625\n",
      "At: 1141 [==========>] Loss 0.1325482941564593  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.13858858782291938  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.05320851774369665  - accuracy: 0.96875\n",
      "At: 1144 [==========>] Loss 0.13667509273808676  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.15594062547278587  - accuracy: 0.75\n",
      "At: 1146 [==========>] Loss 0.1278395457199576  - accuracy: 0.875\n",
      "At: 1147 [==========>] Loss 0.22313444657523523  - accuracy: 0.65625\n",
      "At: 1148 [==========>] Loss 0.08128262672326553  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.10791879649517637  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.15706675667268924  - accuracy: 0.78125\n",
      "At: 1151 [==========>] Loss 0.17058924401404935  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.1185321033715641  - accuracy: 0.84375\n",
      "At: 1153 [==========>] Loss 0.18572598376414723  - accuracy: 0.6875\n",
      "At: 1154 [==========>] Loss 0.11082519107664106  - accuracy: 0.90625\n",
      "At: 1155 [==========>] Loss 0.1063790722311318  - accuracy: 0.875\n",
      "At: 1156 [==========>] Loss 0.15463390566296054  - accuracy: 0.71875\n",
      "At: 1157 [==========>] Loss 0.1317876999319912  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.15765809880953804  - accuracy: 0.78125\n",
      "At: 1159 [==========>] Loss 0.11194426829784793  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.09984265171789192  - accuracy: 0.84375\n",
      "At: 1161 [==========>] Loss 0.11595763173082888  - accuracy: 0.84375\n",
      "At: 1162 [==========>] Loss 0.13774599118474534  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.16170489220038425  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.07836628920814107  - accuracy: 0.9375\n",
      "At: 1165 [==========>] Loss 0.17578208039466475  - accuracy: 0.8125\n",
      "At: 1166 [==========>] Loss 0.07335994965244245  - accuracy: 0.875\n",
      "At: 1167 [==========>] Loss 0.14347655613640126  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.11381302634049151  - accuracy: 0.8125\n",
      "At: 1169 [==========>] Loss 0.11352740112572426  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.18039507402142546  - accuracy: 0.71875\n",
      "At: 1171 [==========>] Loss 0.07040381915725256  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.1449797337214808  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.13682386335590074  - accuracy: 0.84375\n",
      "At: 1174 [==========>] Loss 0.18670906983388907  - accuracy: 0.71875\n",
      "At: 1175 [==========>] Loss 0.11765126209932371  - accuracy: 0.84375\n",
      "At: 1176 [==========>] Loss 0.12920277624581494  - accuracy: 0.8125\n",
      "At: 1177 [==========>] Loss 0.06608765522320292  - accuracy: 0.96875\n",
      "At: 1178 [==========>] Loss 0.1358258934974495  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.1455581153824272  - accuracy: 0.71875\n",
      "At: 1180 [==========>] Loss 0.25797777477782835  - accuracy: 0.65625\n",
      "At: 1181 [==========>] Loss 0.0988620266724013  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.10559710680532297  - accuracy: 0.84375\n",
      "At: 1183 [==========>] Loss 0.187818259800261  - accuracy: 0.78125\n",
      "At: 1184 [==========>] Loss 0.1704999888672265  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.09239553733911918  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.12268048467308301  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.13487201434008977  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.07387711635473149  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.14482024679079197  - accuracy: 0.78125\n",
      "At: 1190 [==========>] Loss 0.12061504036930482  - accuracy: 0.8125\n",
      "At: 1191 [==========>] Loss 0.20390927032205228  - accuracy: 0.6875\n",
      "At: 1192 [==========>] Loss 0.08318337678810636  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.1584800058370291  - accuracy: 0.75\n",
      "At: 1194 [==========>] Loss 0.1578643119282796  - accuracy: 0.78125\n",
      "At: 1195 [==========>] Loss 0.14044413148138785  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.13605128157607813  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.1016031362851065  - accuracy: 0.8125\n",
      "At: 1198 [==========>] Loss 0.08624507246776678  - accuracy: 0.90625\n",
      "At: 1199 [==========>] Loss 0.19945007888157493  - accuracy: 0.65625\n",
      "At: 1200 [==========>] Loss 0.077771411960362  - accuracy: 0.9375\n",
      "At: 1201 [==========>] Loss 0.11733592488159661  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.17057641625952547  - accuracy: 0.71875\n",
      "At: 1203 [==========>] Loss 0.14364416134363367  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.07268885002820845  - accuracy: 0.9375\n",
      "At: 1205 [==========>] Loss 0.06829493119599125  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.09487496313617484  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.14206160815558258  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.10075152675662079  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.11937952572592908  - accuracy: 0.78125\n",
      "At: 1210 [==========>] Loss 0.13350999849241507  - accuracy: 0.75\n",
      "At: 1211 [==========>] Loss 0.15171683454754545  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.09594938881871597  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.24715756049050464  - accuracy: 0.53125\n",
      "At: 1214 [==========>] Loss 0.14528580632372784  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.16994687590319685  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.13030424160605947  - accuracy: 0.78125\n",
      "At: 1217 [==========>] Loss 0.09152071999569614  - accuracy: 0.8125\n",
      "At: 1218 [==========>] Loss 0.09653748961329107  - accuracy: 0.875\n",
      "At: 1219 [==========>] Loss 0.1201121298960253  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.11363968845654437  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.0905516038255211  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.19349520936845072  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.12171971468950685  - accuracy: 0.8125\n",
      "At: 1224 [==========>] Loss 0.07247604682289306  - accuracy: 0.9375\n",
      "At: 1225 [==========>] Loss 0.08310083059380421  - accuracy: 0.90625\n",
      "At: 1226 [==========>] Loss 0.09541217138277915  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.1548437954643638  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.14228271540718912  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.14695356959388442  - accuracy: 0.71875\n",
      "At: 1230 [==========>] Loss 0.18292088271204499  - accuracy: 0.75\n",
      "At: 1231 [==========>] Loss 0.12726598996516494  - accuracy: 0.75\n",
      "At: 1232 [==========>] Loss 0.0787094356668536  - accuracy: 0.90625\n",
      "At: 1233 [==========>] Loss 0.11590215887930747  - accuracy: 0.8125\n",
      "At: 1234 [==========>] Loss 0.14901336635557091  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.06910810364403297  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.15578855420391102  - accuracy: 0.8125\n",
      "At: 1237 [==========>] Loss 0.07097138780990005  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.10190693267223261  - accuracy: 0.84375\n",
      "At: 1239 [==========>] Loss 0.20172420641790412  - accuracy: 0.71875\n",
      "At: 1240 [==========>] Loss 0.09448489412347776  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.12134632770827188  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.14297891632877718  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.13853870530436968  - accuracy: 0.8125\n",
      "At: 1244 [==========>] Loss 0.14956261420018382  - accuracy: 0.8125\n",
      "At: 1245 [==========>] Loss 0.11106922001059519  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.07961711907524271  - accuracy: 0.875\n",
      "At: 1247 [==========>] Loss 0.1634166461190326  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.12151585008089594  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.13175255942734765  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.12282724086737967  - accuracy: 0.78125\n",
      "At: 1251 [==========>] Loss 0.1028379240884158  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.07240766284970862  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.1250925273174573  - accuracy: 0.8125\n",
      "At: 1254 [==========>] Loss 0.2078523606094242  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.09159403588148404  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.10842598950175254  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.11878440510284367  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.06788117100605177  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.13698504760209348  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.11937634675049427  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.13510506500381095  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.12233421308196862  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.10726075694495432  - accuracy: 0.875\n",
      "At: 1264 [==========>] Loss 0.08734559685287244  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.09964548145625424  - accuracy: 0.90625\n",
      "At: 1266 [==========>] Loss 0.11360424857338207  - accuracy: 0.78125\n",
      "At: 1267 [==========>] Loss 0.13060593271461177  - accuracy: 0.84375\n",
      "At: 1268 [==========>] Loss 0.1574915051441002  - accuracy: 0.71875\n",
      "At: 1269 [==========>] Loss 0.1529311218275718  - accuracy: 0.78125\n",
      "At: 1270 [==========>] Loss 0.1340228710853222  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.16203148392721456  - accuracy: 0.75\n",
      "At: 1272 [==========>] Loss 0.0597323244726946  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.207132055484685  - accuracy: 0.6875\n",
      "At: 1274 [==========>] Loss 0.13098829295481257  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.08643751054495866  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.09468167300157633  - accuracy: 0.90625\n",
      "At: 1277 [==========>] Loss 0.10838285301930906  - accuracy: 0.78125\n",
      "At: 1278 [==========>] Loss 0.15226708164310435  - accuracy: 0.75\n",
      "At: 1279 [==========>] Loss 0.11154932971596929  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.1078754177446602  - accuracy: 0.875\n",
      "At: 1281 [==========>] Loss 0.1731382103620075  - accuracy: 0.75\n",
      "At: 1282 [==========>] Loss 0.14740095512200735  - accuracy: 0.78125\n",
      "At: 1283 [==========>] Loss 0.12023696862662164  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.1566117379309967  - accuracy: 0.84375\n",
      "At: 1285 [==========>] Loss 0.06471634299845247  - accuracy: 0.875\n",
      "At: 1286 [==========>] Loss 0.11579238880287579  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.11773524886654435  - accuracy: 0.8125\n",
      "At: 1288 [==========>] Loss 0.15640383520080597  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.10808356538770844  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.13866110638683804  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.1826139042793924  - accuracy: 0.8125\n",
      "At: 1292 [==========>] Loss 0.11022236745367434  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.14659648872882255  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.13898001529925355  - accuracy: 0.75\n",
      "At: 1295 [==========>] Loss 0.18495251312651903  - accuracy: 0.6875\n",
      "At: 1296 [==========>] Loss 0.16694108211247904  - accuracy: 0.75\n",
      "At: 1297 [==========>] Loss 0.14975653344724163  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.11677414922602204  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.15090041585848618  - accuracy: 0.75\n",
      "At: 1300 [==========>] Loss 0.11709395281845947  - accuracy: 0.8125\n",
      "At: 1301 [==========>] Loss 0.12616943915935697  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.067052052688876  - accuracy: 0.96875\n",
      "At: 1303 [==========>] Loss 0.10198208582125166  - accuracy: 0.90625\n",
      "At: 1304 [==========>] Loss 0.11146519731428843  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.13619769008796156  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.07733395463778678  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.1530474162196327  - accuracy: 0.8125\n",
      "At: 1308 [==========>] Loss 0.05933123124068552  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.19160498344605598  - accuracy: 0.71875\n",
      "At: 1310 [==========>] Loss 0.1399640055886376  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.13456387473000633  - accuracy: 0.8125\n",
      "At: 1312 [==========>] Loss 0.08711561259072337  - accuracy: 0.9375\n",
      "At: 1313 [==========>] Loss 0.13088968489209213  - accuracy: 0.8125\n",
      "At: 1314 [==========>] Loss 0.06946707938587143  - accuracy: 0.875\n",
      "At: 1315 [==========>] Loss 0.15193244921629087  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.13884120224765434  - accuracy: 0.8125\n",
      "At: 1317 [==========>] Loss 0.08999639789398203  - accuracy: 0.875\n",
      "At: 1318 [==========>] Loss 0.14441541453726742  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.15161834796874357  - accuracy: 0.8125\n",
      "At: 1320 [==========>] Loss 0.13726745753972572  - accuracy: 0.78125\n",
      "At: 1321 [==========>] Loss 0.08623581818847563  - accuracy: 0.875\n",
      "At: 1322 [==========>] Loss 0.13683849551840502  - accuracy: 0.84375\n",
      "At: 1323 [==========>] Loss 0.102941946041258  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.18042676449330666  - accuracy: 0.78125\n",
      "At: 1325 [==========>] Loss 0.07871566085923556  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.1040119966521948  - accuracy: 0.8125\n",
      "At: 1327 [==========>] Loss 0.12677391124704207  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.10747845378560994  - accuracy: 0.8125\n",
      "At: 1329 [==========>] Loss 0.07331907106968502  - accuracy: 0.90625\n",
      "At: 1330 [==========>] Loss 0.13523387147108346  - accuracy: 0.875\n",
      "At: 1331 [==========>] Loss 0.14228179554374176  - accuracy: 0.875\n",
      "At: 1332 [==========>] Loss 0.13692062673877767  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.14632655208978862  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.09493571418637595  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.13792179752423397  - accuracy: 0.78125\n",
      "At: 1336 [==========>] Loss 0.10079549292575618  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.16023187445247175  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.15919889027799344  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.10696275290140217  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.14458154236733742  - accuracy: 0.78125\n",
      "At: 1341 [==========>] Loss 0.11843914174029037  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.11317278146801535  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.20314724386213084  - accuracy: 0.6875\n",
      "At: 1344 [==========>] Loss 0.17173523199425678  - accuracy: 0.71875\n",
      "At: 1345 [==========>] Loss 0.0885003100623363  - accuracy: 0.875\n",
      "At: 1346 [==========>] Loss 0.0834246201329653  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.09347847110694141  - accuracy: 0.90625\n",
      "At: 1348 [==========>] Loss 0.1111220268686944  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.13993576720162393  - accuracy: 0.84375\n",
      "At: 1350 [==========>] Loss 0.12985920646895482  - accuracy: 0.875\n",
      "At: 1351 [==========>] Loss 0.11598303586606995  - accuracy: 0.78125\n",
      "At: 1352 [==========>] Loss 0.08166811684168776  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.16070757956070433  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.20072085778619536  - accuracy: 0.71875\n",
      "At: 1355 [==========>] Loss 0.0768328705880162  - accuracy: 0.875\n",
      "At: 1356 [==========>] Loss 0.12983148895647792  - accuracy: 0.8125\n",
      "At: 1357 [==========>] Loss 0.137032393203252  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.12647110871004374  - accuracy: 0.8125\n",
      "At: 1359 [==========>] Loss 0.06494359906225664  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.17799854756797262  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.0977875507358037  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.12415490454567762  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.12040373796591895  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.14630414829426092  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.11977259980992422  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.0996724063415381  - accuracy: 0.84375\n",
      "At: 1367 [==========>] Loss 0.12029844349165597  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.17246118053881127  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.0803990196768215  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.14686346152355811  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.19181567459347745  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.12055137125148493  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.1522890185600684  - accuracy: 0.75\n",
      "At: 1374 [==========>] Loss 0.14299982327202893  - accuracy: 0.78125\n",
      "At: 1375 [==========>] Loss 0.10877909249934334  - accuracy: 0.875\n",
      "At: 1376 [==========>] Loss 0.11907219948344734  - accuracy: 0.78125\n",
      "At: 1377 [==========>] Loss 0.17348799015660987  - accuracy: 0.71875\n",
      "At: 1378 [==========>] Loss 0.12259710355531726  - accuracy: 0.8125\n",
      "At: 1379 [==========>] Loss 0.15032400991764305  - accuracy: 0.78125\n",
      "At: 1380 [==========>] Loss 0.15631502892284815  - accuracy: 0.71875\n",
      "At: 1381 [==========>] Loss 0.09694147762596185  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.13666028589163742  - accuracy: 0.75\n",
      "At: 1383 [==========>] Loss 0.08211802097734773  - accuracy: 0.90625\n",
      "At: 1384 [==========>] Loss 0.11119624548688913  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.190237252599194  - accuracy: 0.6875\n",
      "At: 1386 [==========>] Loss 0.1838283465255628  - accuracy: 0.75\n",
      "At: 1387 [==========>] Loss 0.05371986141727977  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.15688786297614632  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.12728367816287756  - accuracy: 0.8125\n",
      "At: 1390 [==========>] Loss 0.16334905530974855  - accuracy: 0.75\n",
      "At: 1391 [==========>] Loss 0.1010785374809163  - accuracy: 0.875\n",
      "At: 1392 [==========>] Loss 0.08721955642366187  - accuracy: 0.84375\n",
      "At: 1393 [==========>] Loss 0.1596934335511659  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.11014138340104661  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.18543918840484672  - accuracy: 0.71875\n",
      "At: 1396 [==========>] Loss 0.05217134111103988  - accuracy: 0.90625\n",
      "At: 1397 [==========>] Loss 0.1281991220423065  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.11576726839323918  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.10710345615853248  - accuracy: 0.90625\n",
      "At: 1400 [==========>] Loss 0.13843721315234792  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.0979854953762622  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.1714810976036511  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.11536428355018051  - accuracy: 0.875\n",
      "At: 1404 [==========>] Loss 0.12705706504662173  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.094405864082546  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.15198531014821537  - accuracy: 0.8125\n",
      "At: 1407 [==========>] Loss 0.13076202967666042  - accuracy: 0.78125\n",
      "At: 1408 [==========>] Loss 0.13212279602489396  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.022997946694892555  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.14203882583004504  - accuracy: 0.8125\n",
      "At: 1411 [==========>] Loss 0.14793426454316413  - accuracy: 0.78125\n",
      "At: 1412 [==========>] Loss 0.12897587103459437  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.10063910851317649  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.164476693664205  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.05887103087314813  - accuracy: 0.96875\n",
      "At: 1416 [==========>] Loss 0.16160437170707548  - accuracy: 0.75\n",
      "At: 1417 [==========>] Loss 0.14437130581543423  - accuracy: 0.8125\n",
      "At: 1418 [==========>] Loss 0.12999667253070218  - accuracy: 0.78125\n",
      "At: 1419 [==========>] Loss 0.0802400954368254  - accuracy: 0.90625\n",
      "At: 1420 [==========>] Loss 0.0897699780478291  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.08075733591441511  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.12232308012682433  - accuracy: 0.84375\n",
      "At: 1423 [==========>] Loss 0.14337976178269596  - accuracy: 0.75\n",
      "At: 1424 [==========>] Loss 0.14752505706371566  - accuracy: 0.78125\n",
      "At: 1425 [==========>] Loss 0.07845038251550579  - accuracy: 0.9375\n",
      "At: 1426 [==========>] Loss 0.09224106443424013  - accuracy: 0.90625\n",
      "At: 1427 [==========>] Loss 0.12490809465179709  - accuracy: 0.8125\n",
      "At: 1428 [==========>] Loss 0.08789151604802296  - accuracy: 0.90625\n",
      "At: 1429 [==========>] Loss 0.17303515432038502  - accuracy: 0.71875\n",
      "At: 1430 [==========>] Loss 0.07552637071860213  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.11394002533851505  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.0876046480069049  - accuracy: 0.875\n",
      "At: 1433 [==========>] Loss 0.10590037119284262  - accuracy: 0.8125\n",
      "At: 1434 [==========>] Loss 0.14635197975537645  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.12372471590404654  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.07432656353073809  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.11550435045641043  - accuracy: 0.84375\n",
      "At: 1438 [==========>] Loss 0.17305383932587987  - accuracy: 0.75\n",
      "At: 1439 [==========>] Loss 0.10126679809521727  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.11710619305592683  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.09761961038313433  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.09372668727608562  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.12875242827111902  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.10822637026567299  - accuracy: 0.84375\n",
      "At: 1445 [==========>] Loss 0.19921173837503905  - accuracy: 0.71875\n",
      "At: 1446 [==========>] Loss 0.2055312761102401  - accuracy: 0.625\n",
      "At: 1447 [==========>] Loss 0.14610573412732786  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.08668510734080963  - accuracy: 0.84375\n",
      "At: 1449 [==========>] Loss 0.13255648332534567  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.1167905336241424  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.10751424056044538  - accuracy: 0.875\n",
      "At: 1452 [==========>] Loss 0.07576987507347262  - accuracy: 0.9375\n",
      "At: 1453 [==========>] Loss 0.07065439246101408  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.1439756122864751  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.11232431242189682  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.1188205881836591  - accuracy: 0.84375\n",
      "At: 1457 [==========>] Loss 0.09100689777929083  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.18205507088099773  - accuracy: 0.75\n",
      "At: 1459 [==========>] Loss 0.11241698025692454  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.18092493809394847  - accuracy: 0.71875\n",
      "At: 1461 [==========>] Loss 0.10006309285421136  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.17547204201897088  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.09857474460026698  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.168301426536444  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.12953086474969655  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.09565677619346942  - accuracy: 0.8125\n",
      "At: 1467 [==========>] Loss 0.20924447322006576  - accuracy: 0.6875\n",
      "At: 1468 [==========>] Loss 0.1863986344499473  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.17808344127534892  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.13786842633449486  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.15812808572818238  - accuracy: 0.75\n",
      "At: 1472 [==========>] Loss 0.07772039523417405  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.11686630684939059  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.18824863709731637  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.1705313499226337  - accuracy: 0.8125\n",
      "At: 1476 [==========>] Loss 0.14486193072714038  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.09268755169365553  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.0874077381218519  - accuracy: 0.9375\n",
      "At: 1479 [==========>] Loss 0.13074776895551465  - accuracy: 0.78125\n",
      "At: 1480 [==========>] Loss 0.10254864753569606  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.14203139124092418  - accuracy: 0.75\n",
      "At: 1482 [==========>] Loss 0.11801796165092403  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.19622046342089414  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.169240621010667  - accuracy: 0.75\n",
      "At: 1485 [==========>] Loss 0.15920662773726785  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.07220449025467761  - accuracy: 0.9375\n",
      "At: 1487 [==========>] Loss 0.07459961839626908  - accuracy: 0.875\n",
      "At: 1488 [==========>] Loss 0.12687349104830412  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.20492502973522053  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.11682547496941686  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.15589700701378775  - accuracy: 0.8125\n",
      "At: 1492 [==========>] Loss 0.14918999410663641  - accuracy: 0.75\n",
      "At: 1493 [==========>] Loss 0.1825841490260499  - accuracy: 0.78125\n",
      "At: 1494 [==========>] Loss 0.1428019295153956  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.14725557882823606  - accuracy: 0.78125\n",
      "At: 1496 [==========>] Loss 0.0837773823433354  - accuracy: 0.90625\n",
      "At: 1497 [==========>] Loss 0.1771177478917612  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.16588986469655592  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.11973718049255727  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.08983052915212804  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.09391965573640448  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.14090130423098407  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.12570616837374676  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.1409076356460524  - accuracy: 0.8125\n",
      "At: 1505 [==========>] Loss 0.1238164987504211  - accuracy: 0.78125\n",
      "At: 1506 [==========>] Loss 0.14907368025708817  - accuracy: 0.78125\n",
      "At: 1507 [==========>] Loss 0.15371779760995308  - accuracy: 0.75\n",
      "At: 1508 [==========>] Loss 0.22180759756599  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.10384462499410485  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.11886639811814717  - accuracy: 0.84375\n",
      "At: 1511 [==========>] Loss 0.12463585634205332  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.08175271130311534  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.15752967046206445  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.1321993112069871  - accuracy: 0.8125\n",
      "At: 1515 [==========>] Loss 0.12452332496045258  - accuracy: 0.84375\n",
      "At: 1516 [==========>] Loss 0.10417069613988511  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.17094712223314673  - accuracy: 0.75\n",
      "At: 1518 [==========>] Loss 0.09616864606640693  - accuracy: 0.875\n",
      "At: 1519 [==========>] Loss 0.14233014955608475  - accuracy: 0.84375\n",
      "At: 1520 [==========>] Loss 0.08148811344644355  - accuracy: 0.9375\n",
      "At: 1521 [==========>] Loss 0.08617011246403464  - accuracy: 0.78125\n",
      "At: 1522 [==========>] Loss 0.17468629354485493  - accuracy: 0.6875\n",
      "At: 1523 [==========>] Loss 0.14533721068324668  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.15228820624772116  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.12602862271512325  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.11496609475709951  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.14124447115038635  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.15380167713333326  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.09435702109398869  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.04504568385479782  - accuracy: 1.0\n",
      "At: 1531 [==========>] Loss 0.11400377324017798  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.16851060429532436  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.12331673223678633  - accuracy: 0.84375\n",
      "At: 1534 [==========>] Loss 0.09643421439927766  - accuracy: 0.875\n",
      "At: 1535 [==========>] Loss 0.1466798198060613  - accuracy: 0.8125\n",
      "At: 1536 [==========>] Loss 0.16388189396637273  - accuracy: 0.75\n",
      "At: 1537 [==========>] Loss 0.14530069665026796  - accuracy: 0.75\n",
      "At: 1538 [==========>] Loss 0.14270745614199515  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.07165193030207409  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1453320317446041  - accuracy: 0.8125\n",
      "At: 1541 [==========>] Loss 0.11628186884070935  - accuracy: 0.8125\n",
      "At: 1542 [==========>] Loss 0.0818672027419839  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.16769322285652888  - accuracy: 0.75\n",
      "At: 1544 [==========>] Loss 0.160715681308509  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.21400939373868455  - accuracy: 0.6875\n",
      "At: 1546 [==========>] Loss 0.13743194560988392  - accuracy: 0.84375\n",
      "At: 1547 [==========>] Loss 0.16704809167441692  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.1273135314601103  - accuracy: 0.875\n",
      "At: 1549 [==========>] Loss 0.1527885147316308  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.08150056448658091  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.19066793210684443  - accuracy: 0.75\n",
      "At: 1552 [==========>] Loss 0.1114721688909222  - accuracy: 0.8125\n",
      "At: 1553 [==========>] Loss 0.07638148544585914  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.1455628174753298  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.11515487960500446  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.17269548379562857  - accuracy: 0.71875\n",
      "At: 1557 [==========>] Loss 0.0849305131341867  - accuracy: 0.90625\n",
      "At: 1558 [==========>] Loss 0.15721422280583608  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.07749830157597233  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.0981568719227211  - accuracy: 0.9375\n",
      "At: 1561 [==========>] Loss 0.13721089411138682  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.11364316764276833  - accuracy: 0.8125\n",
      "At: 1563 [==========>] Loss 0.1102398974383227  - accuracy: 0.84375\n",
      "At: 1564 [==========>] Loss 0.12594711957254662  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.12160168450666489  - accuracy: 0.875\n",
      "At: 1566 [==========>] Loss 0.14404450829010917  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.1535578992189771  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.09530643666084479  - accuracy: 0.84375\n",
      "At: 1569 [==========>] Loss 0.09934442988139289  - accuracy: 0.90625\n",
      "At: 1570 [==========>] Loss 0.11706058117585802  - accuracy: 0.875\n",
      "At: 1571 [==========>] Loss 0.14959454341969045  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.14050160892332247  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.06817205086149679  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.13738719355233353  - accuracy: 0.8125\n",
      "At: 1575 [==========>] Loss 0.09337510496537306  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.14373954243456694  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.08225247684495514  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.07687510196828459  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.08789148695385644  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.08656312380289616  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.11450998645500331  - accuracy: 0.8125\n",
      "At: 1582 [==========>] Loss 0.18736579225298888  - accuracy: 0.71875\n",
      "At: 1583 [==========>] Loss 0.10178334486316609  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.12713722562246182  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.10553973506543747  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.1261587372447748  - accuracy: 0.84375\n",
      "At: 1587 [==========>] Loss 0.09975843256997247  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.15291299325849414  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.14906613000117191  - accuracy: 0.75\n",
      "At: 1590 [==========>] Loss 0.1414960515500232  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.10922020265961863  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.07639616011996792  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.14886353128220553  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.11663624566536299  - accuracy: 0.84375\n",
      "At: 1595 [==========>] Loss 0.14815454536572453  - accuracy: 0.875\n",
      "At: 1596 [==========>] Loss 0.18744007108677666  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.1543500262262279  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.1686818946650136  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.2515153859349861  - accuracy: 0.65625\n",
      "At: 1600 [==========>] Loss 0.14859300492773114  - accuracy: 0.78125\n",
      "At: 1601 [==========>] Loss 0.07656905519635826  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.1551463733663127  - accuracy: 0.78125\n",
      "At: 1603 [==========>] Loss 0.1923321955403034  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.2531664013638173  - accuracy: 0.625\n",
      "At: 1605 [==========>] Loss 0.07685736361579606  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.12397800001660624  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.17221404438669347  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.14470176177478516  - accuracy: 0.75\n",
      "At: 1609 [==========>] Loss 0.17075238688420996  - accuracy: 0.6875\n",
      "At: 1610 [==========>] Loss 0.1665164235588869  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.060964138237968854  - accuracy: 0.96875\n",
      "At: 1612 [==========>] Loss 0.06515723247384551  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.1516193788230112  - accuracy: 0.78125\n",
      "At: 1614 [==========>] Loss 0.1577040704786901  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.08108869914569056  - accuracy: 0.9375\n",
      "At: 1616 [==========>] Loss 0.17202113562918794  - accuracy: 0.71875\n",
      "At: 1617 [==========>] Loss 0.10122091190711428  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11774144563288494  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.19588716450339969  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.10124320696510714  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.13594523920058546  - accuracy: 0.78125\n",
      "At: 1622 [==========>] Loss 0.15685581770983792  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.09258424840656188  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.15236795581654283  - accuracy: 0.71875\n",
      "At: 1625 [==========>] Loss 0.12372087093898514  - accuracy: 0.875\n",
      "At: 1626 [==========>] Loss 0.08955192976953441  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.11562472518901964  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.14677179353559633  - accuracy: 0.78125\n",
      "At: 1629 [==========>] Loss 0.14094573725758522  - accuracy: 0.78125\n",
      "At: 1630 [==========>] Loss 0.09043715368232227  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.11878276963089543  - accuracy: 0.84375\n",
      "At: 1632 [==========>] Loss 0.09677717033280399  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08722800665487038  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.1440838417816562  - accuracy: 0.78125\n",
      "At: 1635 [==========>] Loss 0.22742982900706116  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.11237944611050016  - accuracy: 0.875\n",
      "At: 1637 [==========>] Loss 0.07413306923978506  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.17018934269604302  - accuracy: 0.71875\n",
      "At: 1639 [==========>] Loss 0.1689645149546541  - accuracy: 0.71875\n",
      "At: 1640 [==========>] Loss 0.1455354065901666  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.10543006297504251  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.09873230140035995  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.10719690717157089  - accuracy: 0.84375\n",
      "At: 1644 [==========>] Loss 0.10084102962985542  - accuracy: 0.90625\n",
      "At: 1645 [==========>] Loss 0.07512099191601598  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.14315988300152166  - accuracy: 0.75\n",
      "At: 1647 [==========>] Loss 0.1718773619781172  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1256942062251634  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.10963752960211315  - accuracy: 0.8125\n",
      "At: 1650 [==========>] Loss 0.10702356092450281  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.09297937611358198  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.07398270969980424  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.10274138245034906  - accuracy: 0.78125\n",
      "At: 1654 [==========>] Loss 0.04865638116623168  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.18612386974031953  - accuracy: 0.75\n",
      "At: 1656 [==========>] Loss 0.11784981815722413  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.12971496294756651  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.1896776962163995  - accuracy: 0.78125\n",
      "At: 1659 [==========>] Loss 0.07058583417072091  - accuracy: 0.9375\n",
      "At: 1660 [==========>] Loss 0.045468665019428034  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.0817363688287627  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.11186595759722284  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16325547813888674  - accuracy: 0.8125\n",
      "At: 1664 [==========>] Loss 0.11855344799261551  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.11292262847924836  - accuracy: 0.84375\n",
      "At: 1666 [==========>] Loss 0.07802631306790614  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.15086969042677484  - accuracy: 0.71875\n",
      "At: 1668 [==========>] Loss 0.14833597194585493  - accuracy: 0.78125\n",
      "At: 1669 [==========>] Loss 0.13632090549213483  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.048411743108496746  - accuracy: 0.9375\n",
      "At: 1671 [==========>] Loss 0.10933321296906415  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.12626665309783963  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.10183183586815972  - accuracy: 0.84375\n",
      "At: 1674 [==========>] Loss 0.18867227570435777  - accuracy: 0.6875\n",
      "At: 1675 [==========>] Loss 0.18528218970523377  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.23491937178185537  - accuracy: 0.5625\n",
      "At: 1677 [==========>] Loss 0.09387226618795201  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.15887341126382332  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.1253326240515413  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.16862657238923007  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.11705664389478636  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.08452519425415982  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.1820033867956648  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.1303734800563391  - accuracy: 0.8125\n",
      "At: 1685 [==========>] Loss 0.1135795224440804  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.12148218293691765  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.161732927476569  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.05418395349185937  - accuracy: 0.90625\n",
      "At: 1689 [==========>] Loss 0.12224586227212166  - accuracy: 0.8125\n",
      "At: 1690 [==========>] Loss 0.116874137169407  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.10988419354281673  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.18336444568634663  - accuracy: 0.75\n",
      "At: 1693 [==========>] Loss 0.10727553334743324  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.08751464063319774  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.1445451876271078  - accuracy: 0.71875\n",
      "At: 1696 [==========>] Loss 0.1593828018451905  - accuracy: 0.71875\n",
      "At: 1697 [==========>] Loss 0.09849713732559423  - accuracy: 0.90625\n",
      "At: 1698 [==========>] Loss 0.10037402948886154  - accuracy: 0.84375\n",
      "At: 1699 [==========>] Loss 0.11617528467202035  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.12930195216912524  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.09925084769625174  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.11427311751299862  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.19768088965044134  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.09720701554322966  - accuracy: 0.84375\n",
      "At: 1705 [==========>] Loss 0.12064094033829303  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.13609508724322777  - accuracy: 0.84375\n",
      "At: 1707 [==========>] Loss 0.19365289910274236  - accuracy: 0.6875\n",
      "At: 1708 [==========>] Loss 0.09871980995291116  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.17608383645416514  - accuracy: 0.71875\n",
      "At: 1710 [==========>] Loss 0.15799821367671166  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.06184117693442383  - accuracy: 0.96875\n",
      "At: 1712 [==========>] Loss 0.13400415658947146  - accuracy: 0.8125\n",
      "At: 1713 [==========>] Loss 0.10555794396445425  - accuracy: 0.78125\n",
      "At: 1714 [==========>] Loss 0.15411023734606835  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.11332064729712377  - accuracy: 0.84375\n",
      "At: 1716 [==========>] Loss 0.05443911702934966  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.12160890535592558  - accuracy: 0.84375\n",
      "At: 1718 [==========>] Loss 0.14285280804132433  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.1449690690124943  - accuracy: 0.78125\n",
      "At: 1720 [==========>] Loss 0.04662299138398797  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.15046983500560684  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.05217837319936842  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.2097977906748475  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.0765486031286624  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.16405444052938048  - accuracy: 0.78125\n",
      "At: 1726 [==========>] Loss 0.1281632706634915  - accuracy: 0.84375\n",
      "At: 1727 [==========>] Loss 0.14793421084492228  - accuracy: 0.78125\n",
      "At: 1728 [==========>] Loss 0.12215923024582011  - accuracy: 0.84375\n",
      "At: 1729 [==========>] Loss 0.19110394661909919  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.13869664028802842  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.09974552851982349  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.08820899072439331  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.19082024025677144  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.10094955442830067  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.16989414707993583  - accuracy: 0.75\n",
      "At: 1736 [==========>] Loss 0.12856468584496092  - accuracy: 0.8125\n",
      "At: 1737 [==========>] Loss 0.1495298884169573  - accuracy: 0.84375\n",
      "At: 1738 [==========>] Loss 0.13405410066037762  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.10615743939421306  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.14279231045828314  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.1609497796437961  - accuracy: 0.75\n",
      "At: 1742 [==========>] Loss 0.0601166786134629  - accuracy: 0.90625\n",
      "At: 1743 [==========>] Loss 0.14277256112680814  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.07877530235914415  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.13825633365963727  - accuracy: 0.75\n",
      "At: 1746 [==========>] Loss 0.15457203867049246  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.11750838643680407  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.1173792312411827  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.11287584965017546  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.1218345503013343  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.16089630619779827  - accuracy: 0.75\n",
      "At: 1752 [==========>] Loss 0.11449956220865581  - accuracy: 0.84375\n",
      "At: 1753 [==========>] Loss 0.10408601528671932  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.11824185443906073  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.06310730908347323  - accuracy: 0.9375\n",
      "At: 1756 [==========>] Loss 0.183599422651622  - accuracy: 0.71875\n",
      "At: 1757 [==========>] Loss 0.1819235972393265  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.07780574100124461  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.10841350663753561  - accuracy: 0.875\n",
      "At: 1760 [==========>] Loss 0.07618927765079118  - accuracy: 0.875\n",
      "At: 1761 [==========>] Loss 0.11125927733461513  - accuracy: 0.84375\n",
      "At: 1762 [==========>] Loss 0.15549157992773724  - accuracy: 0.75\n",
      "At: 1763 [==========>] Loss 0.13497627011581925  - accuracy: 0.78125\n",
      "At: 1764 [==========>] Loss 0.14208454704988607  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.15117305224658942  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.076273974126086  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.10427309718024644  - accuracy: 0.84375\n",
      "At: 1768 [==========>] Loss 0.11155521512772562  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.07788808189873252  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.0784821410899427  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.13125042793068287  - accuracy: 0.84375\n",
      "At: 1772 [==========>] Loss 0.13337999625886177  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.09680410708768766  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.13197345282183573  - accuracy: 0.84375\n",
      "At: 1775 [==========>] Loss 0.09185312132909486  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.10601952139143413  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.10181781744485319  - accuracy: 0.90625\n",
      "At: 1778 [==========>] Loss 0.11758214170514344  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.08349084392731736  - accuracy: 0.9375\n",
      "At: 1780 [==========>] Loss 0.11315298350039557  - accuracy: 0.8125\n",
      "At: 1781 [==========>] Loss 0.20992580604328626  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.09619256793931957  - accuracy: 0.875\n",
      "At: 1783 [==========>] Loss 0.13651092875004558  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.09562629080083734  - accuracy: 0.84375\n",
      "At: 1785 [==========>] Loss 0.11279142078730095  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.14357396764584418  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.12951158592736892  - accuracy: 0.84375\n",
      "At: 1788 [==========>] Loss 0.1054254678978839  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.11664385684445285  - accuracy: 0.8125\n",
      "At: 1790 [==========>] Loss 0.18511860285631898  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.05936891244068153  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.14578633779900546  - accuracy: 0.8125\n",
      "At: 1793 [==========>] Loss 0.09828891245821222  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.16632933447065462  - accuracy: 0.78125\n",
      "At: 1795 [==========>] Loss 0.08717365091031877  - accuracy: 0.875\n",
      "At: 1796 [==========>] Loss 0.1563660604269803  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.11023987232956364  - accuracy: 0.90625\n",
      "At: 1798 [==========>] Loss 0.14523438778197506  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.08557728757193855  - accuracy: 0.875\n",
      "At: 1800 [==========>] Loss 0.10972120914047051  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.19691658301826687  - accuracy: 0.6875\n",
      "At: 1802 [==========>] Loss 0.1243023444069094  - accuracy: 0.84375\n",
      "At: 1803 [==========>] Loss 0.17747802807084978  - accuracy: 0.75\n",
      "At: 1804 [==========>] Loss 0.13824193992082207  - accuracy: 0.84375\n",
      "At: 1805 [==========>] Loss 0.034167639747891096  - accuracy: 0.9375\n",
      "At: 1806 [==========>] Loss 0.149571775152933  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.20128931159865182  - accuracy: 0.71875\n",
      "At: 1808 [==========>] Loss 0.1707422874334005  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.0837944733768111  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.14874724019378796  - accuracy: 0.71875\n",
      "At: 1811 [==========>] Loss 0.12892685481224647  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.09918859902935376  - accuracy: 0.875\n",
      "At: 1813 [==========>] Loss 0.14457378815390287  - accuracy: 0.78125\n",
      "At: 1814 [==========>] Loss 0.12903519274642605  - accuracy: 0.8125\n",
      "At: 1815 [==========>] Loss 0.16739066614190806  - accuracy: 0.6875\n",
      "At: 1816 [==========>] Loss 0.0447034529133115  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.13843046704043932  - accuracy: 0.71875\n",
      "At: 1818 [==========>] Loss 0.1298093507275983  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.1603251681177751  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.10730973897926122  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.1187412833178012  - accuracy: 0.8125\n",
      "At: 1822 [==========>] Loss 0.15972029157074658  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.1903528560826354  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.16632162561323569  - accuracy: 0.8125\n",
      "At: 1825 [==========>] Loss 0.12309646320606067  - accuracy: 0.875\n",
      "At: 1826 [==========>] Loss 0.07847180060037637  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.125121557484583  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.15384472843240207  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.14038895467689022  - accuracy: 0.84375\n",
      "At: 1830 [==========>] Loss 0.15573348337543175  - accuracy: 0.71875\n",
      "At: 1831 [==========>] Loss 0.13890965491724078  - accuracy: 0.78125\n",
      "At: 1832 [==========>] Loss 0.1262517747343017  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.11528952791434649  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.07648116431969418  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.14435993846636602  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.09603426787840191  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.04812023799770487  - accuracy: 0.90625\n",
      "At: 1838 [==========>] Loss 0.08475715079610477  - accuracy: 0.875\n",
      "At: 1839 [==========>] Loss 0.09231849220701442  - accuracy: 0.875\n",
      "At: 1840 [==========>] Loss 0.09957271159132575  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.10368096300883872  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.1319359997594184  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.11199147806670323  - accuracy: 0.84375\n",
      "At: 1844 [==========>] Loss 0.08741023635673184  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.17967265314689385  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.13012920287231028  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05704951501528577  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.0628179415186775  - accuracy: 0.90625\n",
      "At: 1849 [==========>] Loss 0.16740554707886196  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.029473535024327868  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.18594845803459176  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.08327534054908695  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.10885534568208526  - accuracy: 0.84375\n",
      "At: 1854 [==========>] Loss 0.12373563774881009  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.1843865134161102  - accuracy: 0.8125\n",
      "At: 1856 [==========>] Loss 0.13298711211651165  - accuracy: 0.8125\n",
      "At: 1857 [==========>] Loss 0.15809909435023733  - accuracy: 0.78125\n",
      "At: 1858 [==========>] Loss 0.09945694028110136  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.13743482109925725  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.14827646506892608  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.10505338397676109  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.20707907895922079  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.15226130223014941  - accuracy: 0.78125\n",
      "At: 1864 [==========>] Loss 0.1530127751721852  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.10740594807964449  - accuracy: 0.8125\n",
      "At: 1866 [==========>] Loss 0.19709461332396966  - accuracy: 0.65625\n",
      "At: 1867 [==========>] Loss 0.0956352922904097  - accuracy: 0.84375\n",
      "At: 1868 [==========>] Loss 0.15661827954797994  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.18890894190570948  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.0923863383817023  - accuracy: 0.875\n",
      "At: 1871 [==========>] Loss 0.14941378060763016  - accuracy: 0.78125\n",
      "At: 1872 [==========>] Loss 0.15255847355053623  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.09195737645595689  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.1660178382371838  - accuracy: 0.78125\n",
      "At: 1875 [==========>] Loss 0.10135756369509173  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.1848569720212125  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.11011496852710968  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.1079524799870354  - accuracy: 0.90625\n",
      "At: 1879 [==========>] Loss 0.12315538634759302  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.09518676918788285  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.08306605181266817  - accuracy: 0.90625\n",
      "At: 1882 [==========>] Loss 0.11996379556263706  - accuracy: 0.78125\n",
      "At: 1883 [==========>] Loss 0.1604091047974071  - accuracy: 0.78125\n",
      "At: 1884 [==========>] Loss 0.12099219185307455  - accuracy: 0.8125\n",
      "At: 1885 [==========>] Loss 0.1407175418338176  - accuracy: 0.8125\n",
      "At: 1886 [==========>] Loss 0.1367038152742499  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.07013575040649067  - accuracy: 0.96875\n",
      "At: 1888 [==========>] Loss 0.13892942851060333  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.10098806934749371  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.1390336003454915  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.07437649801152474  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.0577980049550279  - accuracy: 0.96875\n",
      "At: 1893 [==========>] Loss 0.08456153614089489  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.08688531880979157  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.07122992469346334  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.13263867229130571  - accuracy: 0.78125\n",
      "At: 1897 [==========>] Loss 0.06450957763638443  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.12879882814830937  - accuracy: 0.78125\n",
      "At: 1899 [==========>] Loss 0.08970681592256484  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.10196457742173781  - accuracy: 0.875\n",
      "At: 1901 [==========>] Loss 0.09881040771262221  - accuracy: 0.90625\n",
      "At: 1902 [==========>] Loss 0.14883070914039573  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.11271965927340752  - accuracy: 0.84375\n",
      "At: 1904 [==========>] Loss 0.04695247948238341  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.12410992402972193  - accuracy: 0.84375\n",
      "At: 1906 [==========>] Loss 0.1143357124277355  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.08115128080986378  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.1003676851401209  - accuracy: 0.8125\n",
      "At: 1909 [==========>] Loss 0.10019461949145096  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.0635496602844294  - accuracy: 0.875\n",
      "At: 1911 [==========>] Loss 0.12022832311407124  - accuracy: 0.78125\n",
      "At: 1912 [==========>] Loss 0.10633122990085797  - accuracy: 0.875\n",
      "At: 1913 [==========>] Loss 0.15387479406020405  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.06811435784545006  - accuracy: 0.9375\n",
      "At: 1915 [==========>] Loss 0.13216152151905797  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.13726194192168323  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.1683977729378391  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.15619749050594275  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.08345749126722041  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10239454832667003  - accuracy: 0.8125\n",
      "At: 1921 [==========>] Loss 0.14850711752707005  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.13430329432610513  - accuracy: 0.8125\n",
      "At: 1923 [==========>] Loss 0.17624648269619236  - accuracy: 0.6875\n",
      "At: 1924 [==========>] Loss 0.12754425732781516  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.1627543726573909  - accuracy: 0.8125\n",
      "At: 1926 [==========>] Loss 0.08736531735764837  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.1095709466295946  - accuracy: 0.8125\n",
      "At: 1928 [==========>] Loss 0.1317145274595836  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.16547715060278667  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.17940889202567564  - accuracy: 0.6875\n",
      "At: 1931 [==========>] Loss 0.09561576302432129  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.1533447624442958  - accuracy: 0.8125\n",
      "At: 1933 [==========>] Loss 0.09136629327068194  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.15491534502024898  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.12910271343797844  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.08406253692032692  - accuracy: 0.90625\n",
      "At: 1937 [==========>] Loss 0.1358411480545345  - accuracy: 0.71875\n",
      "At: 1938 [==========>] Loss 0.19326230575677494  - accuracy: 0.6875\n",
      "At: 1939 [==========>] Loss 0.0840196242673857  - accuracy: 0.90625\n",
      "At: 1940 [==========>] Loss 0.11570229339001319  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.12386283998026858  - accuracy: 0.8125\n",
      "At: 1942 [==========>] Loss 0.17249672342062633  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.14074679813825533  - accuracy: 0.8125\n",
      "At: 1944 [==========>] Loss 0.1016949491466968  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.1511923438812376  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.12249629102033559  - accuracy: 0.8125\n",
      "At: 1947 [==========>] Loss 0.1254184253543257  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.1237775693045576  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.07091613429609077  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.14322816491777415  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.14633502757821432  - accuracy: 0.78125\n",
      "At: 1952 [==========>] Loss 0.06747064969678337  - accuracy: 0.9375\n",
      "At: 1953 [==========>] Loss 0.07700088859352217  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.19265699368049777  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.06152902803415757  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.13559024669721137  - accuracy: 0.8125\n",
      "At: 1957 [==========>] Loss 0.08932843938202677  - accuracy: 0.875\n",
      "At: 1958 [==========>] Loss 0.08477396033756449  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.10986048735551913  - accuracy: 0.875\n",
      "At: 1960 [==========>] Loss 0.04394873774861431  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.17738587660624994  - accuracy: 0.71875\n",
      "At: 1962 [==========>] Loss 0.19697924328839767  - accuracy: 0.75\n",
      "At: 1963 [==========>] Loss 0.07641831824127496  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.18449256785677406  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.15209274245649762  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.11765254593894968  - accuracy: 0.84375\n",
      "At: 1967 [==========>] Loss 0.1330491079680834  - accuracy: 0.875\n",
      "At: 1968 [==========>] Loss 0.18204100911378066  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.15111628248963255  - accuracy: 0.8125\n",
      "At: 1970 [==========>] Loss 0.1046098204447943  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.2201941198943152  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.10123237249791857  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.12372804929239942  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.11327311009038991  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.1757155702314305  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.06225836698604362  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.09826425452699879  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.12339045410121172  - accuracy: 0.875\n",
      "At: 1979 [==========>] Loss 0.11703262781099258  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.1181380625777187  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.15852312054605236  - accuracy: 0.71875\n",
      "At: 1982 [==========>] Loss 0.060357919714406336  - accuracy: 0.96875\n",
      "At: 1983 [==========>] Loss 0.13640923700215296  - accuracy: 0.84375\n",
      "At: 1984 [==========>] Loss 0.10362954179292888  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.1356740308440559  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.1968978680452161  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.11708295279755328  - accuracy: 0.78125\n",
      "At: 1988 [==========>] Loss 0.10769789283645982  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.09673548139702828  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.1284255865338489  - accuracy: 0.875\n",
      "At: 1991 [==========>] Loss 0.11846812441031783  - accuracy: 0.90625\n",
      "At: 1992 [==========>] Loss 0.09778892282059934  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.111612106679741  - accuracy: 0.84375\n",
      "At: 1994 [==========>] Loss 0.11212712541499266  - accuracy: 0.84375\n",
      "At: 1995 [==========>] Loss 0.17936204707158038  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.12521307181955824  - accuracy: 0.8125\n",
      "At: 1997 [==========>] Loss 0.1876166996571628  - accuracy: 0.78125\n",
      "At: 1998 [==========>] Loss 0.16463261794897838  - accuracy: 0.71875\n",
      "At: 1999 [==========>] Loss 0.08795853272336576  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.1270171602496572  - accuracy: 0.78125\n",
      "At: 2001 [==========>] Loss 0.09243015028201484  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.08050924886097331  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.11091345294388212  - accuracy: 0.84375\n",
      "At: 2004 [==========>] Loss 0.13817210233795216  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.1094110853586565  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.10861396818081773  - accuracy: 0.875\n",
      "At: 2007 [==========>] Loss 0.1049501432022864  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.12394760189860016  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13821268038227064  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.13288223757618875  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.09550719366605534  - accuracy: 0.90625\n",
      "At: 2012 [==========>] Loss 0.11881505142408144  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.11837649797753505  - accuracy: 0.8125\n",
      "At: 2014 [==========>] Loss 0.2139616885802799  - accuracy: 0.71875\n",
      "At: 2015 [==========>] Loss 0.07168276874874245  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.11181701165268923  - accuracy: 0.875\n",
      "At: 2017 [==========>] Loss 0.08331021022481261  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.0775858316553212  - accuracy: 0.90625\n",
      "At: 2019 [==========>] Loss 0.1387941627015573  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.09933875924223391  - accuracy: 0.84375\n",
      "At: 2021 [==========>] Loss 0.13070154218045976  - accuracy: 0.8125\n",
      "At: 2022 [==========>] Loss 0.14526534250223805  - accuracy: 0.75\n",
      "At: 2023 [==========>] Loss 0.08931658944321227  - accuracy: 0.84375\n",
      "At: 2024 [==========>] Loss 0.10418941192492809  - accuracy: 0.875\n",
      "At: 2025 [==========>] Loss 0.15508441625842878  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.0931097865659529  - accuracy: 0.90625\n",
      "At: 2027 [==========>] Loss 0.1563692310501731  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.11591114129935273  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.12878951892865054  - accuracy: 0.75\n",
      "At: 2030 [==========>] Loss 0.18853406740989276  - accuracy: 0.75\n",
      "At: 2031 [==========>] Loss 0.15061412518295425  - accuracy: 0.71875\n",
      "At: 2032 [==========>] Loss 0.16403789045756434  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.13601387799854525  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.1720359024326959  - accuracy: 0.71875\n",
      "At: 2035 [==========>] Loss 0.1136578009675056  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.08093358776852505  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.10745205108825871  - accuracy: 0.875\n",
      "At: 2038 [==========>] Loss 0.09261409262683412  - accuracy: 0.84375\n",
      "At: 2039 [==========>] Loss 0.07262449075258727  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.10404837881451867  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.0764755529095679  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.1255301853382902  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.10618410315025982  - accuracy: 0.84375\n",
      "At: 2044 [==========>] Loss 0.09350947043473724  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.23841468136705493  - accuracy: 0.625\n",
      "At: 2046 [==========>] Loss 0.06254645828120015  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08130898808300585  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.11737938303529882  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.14979772462127913  - accuracy: 0.84375\n",
      "At: 2050 [==========>] Loss 0.1543925947501614  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1793991800015315  - accuracy: 0.8125\n",
      "At: 2052 [==========>] Loss 0.07715318780864623  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.10093197911607346  - accuracy: 0.875\n",
      "At: 2054 [==========>] Loss 0.1514233563156696  - accuracy: 0.71875\n",
      "At: 2055 [==========>] Loss 0.07905667425019125  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.12835583195698877  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.1278769686975351  - accuracy: 0.78125\n",
      "At: 2058 [==========>] Loss 0.14185926113828742  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.20240770155429327  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.1354049237640576  - accuracy: 0.8125\n",
      "At: 2061 [==========>] Loss 0.13966844182733568  - accuracy: 0.84375\n",
      "At: 2062 [==========>] Loss 0.15031657976661048  - accuracy: 0.75\n",
      "At: 2063 [==========>] Loss 0.09209149802711741  - accuracy: 0.90625\n",
      "At: 2064 [==========>] Loss 0.16694104176325453  - accuracy: 0.71875\n",
      "At: 2065 [==========>] Loss 0.04555152514537211  - accuracy: 0.9375\n",
      "At: 2066 [==========>] Loss 0.18111236552532195  - accuracy: 0.75\n",
      "At: 2067 [==========>] Loss 0.0926763883827205  - accuracy: 0.8125\n",
      "At: 2068 [==========>] Loss 0.10215639095314623  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.0935181840889803  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.138963643994216  - accuracy: 0.84375\n",
      "At: 2071 [==========>] Loss 0.11962792734924109  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.05881596739085863  - accuracy: 0.96875\n",
      "At: 2073 [==========>] Loss 0.09851169454173439  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.10341845695800986  - accuracy: 0.84375\n",
      "At: 2075 [==========>] Loss 0.12518666130695114  - accuracy: 0.875\n",
      "At: 2076 [==========>] Loss 0.130456682096538  - accuracy: 0.84375\n",
      "At: 2077 [==========>] Loss 0.14928669157149266  - accuracy: 0.75\n",
      "At: 2078 [==========>] Loss 0.11665362896337161  - accuracy: 0.8125\n",
      "At: 2079 [==========>] Loss 0.07277895683311154  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.09098453304850317  - accuracy: 0.90625\n",
      "At: 2081 [==========>] Loss 0.15699757459101465  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.1304510956005489  - accuracy: 0.78125\n",
      "At: 2083 [==========>] Loss 0.21879360056259256  - accuracy: 0.65625\n",
      "At: 2084 [==========>] Loss 0.07877685828815031  - accuracy: 0.9375\n",
      "At: 2085 [==========>] Loss 0.08596070989291708  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.08991533363696345  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.15208362233808054  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.09948974700628681  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.11083173516190226  - accuracy: 0.90625\n",
      "At: 2090 [==========>] Loss 0.08761198552186808  - accuracy: 0.9375\n",
      "At: 2091 [==========>] Loss 0.13106363074150662  - accuracy: 0.84375\n",
      "At: 2092 [==========>] Loss 0.10627196875562421  - accuracy: 0.8125\n",
      "At: 2093 [==========>] Loss 0.16231966366858414  - accuracy: 0.8125\n",
      "At: 2094 [==========>] Loss 0.11085259435658915  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.10125900831199613  - accuracy: 0.90625\n",
      "At: 2096 [==========>] Loss 0.13806151969498318  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.12754899537345252  - accuracy: 0.84375\n",
      "At: 2098 [==========>] Loss 0.12008814090467862  - accuracy: 0.78125\n",
      "At: 2099 [==========>] Loss 0.10002148772105446  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.055562859516367684  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.1446052211185135  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.09384242927680367  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.1446197393831301  - accuracy: 0.84375\n",
      "At: 2104 [==========>] Loss 0.10744943998893554  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.17620287859380004  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.13931149356480454  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.08548345431706508  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.118086256107433  - accuracy: 0.84375\n",
      "At: 2109 [==========>] Loss 0.12315010731398265  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.07146635668393496  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.12422919470224018  - accuracy: 0.8125\n",
      "At: 2112 [==========>] Loss 0.11698428907231997  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.10470755491809569  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.14360942471067006  - accuracy: 0.78125\n",
      "At: 2115 [==========>] Loss 0.1114732761221002  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.10035690604634626  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.1236245870577564  - accuracy: 0.8125\n",
      "At: 2118 [==========>] Loss 0.14167899012704271  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.08191858055477491  - accuracy: 0.90625\n",
      "At: 2120 [==========>] Loss 0.1661383407986307  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.14890279147329472  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.1291634275041691  - accuracy: 0.84375\n",
      "At: 2123 [==========>] Loss 0.17114533478742733  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.1376134599949414  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10228659152385933  - accuracy: 0.84375\n",
      "At: 2126 [==========>] Loss 0.05957174855528817  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.10231817350076912  - accuracy: 0.90625\n",
      "At: 2128 [==========>] Loss 0.12297654168539107  - accuracy: 0.78125\n",
      "At: 2129 [==========>] Loss 0.17448892986085163  - accuracy: 0.78125\n",
      "At: 2130 [==========>] Loss 0.06903676854635701  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.10323766244231919  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.19940395159097335  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.18100560427144513  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.10549526187185825  - accuracy: 0.84375\n",
      "At: 2135 [==========>] Loss 0.09372237051852178  - accuracy: 0.84375\n",
      "At: 2136 [==========>] Loss 0.14257386648218212  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.11032282717293741  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.10569236121163578  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.1554341559754521  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.12322018683938302  - accuracy: 0.8125\n",
      "At: 2141 [==========>] Loss 0.1295678568477749  - accuracy: 0.8125\n",
      "At: 2142 [==========>] Loss 0.13221774344780943  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.07861747928970397  - accuracy: 0.9375\n",
      "At: 2144 [==========>] Loss 0.07096752193763362  - accuracy: 0.9375\n",
      "At: 2145 [==========>] Loss 0.10577862831503965  - accuracy: 0.875\n",
      "At: 2146 [==========>] Loss 0.1670629023513043  - accuracy: 0.6875\n",
      "At: 2147 [==========>] Loss 0.12389359743551023  - accuracy: 0.8125\n",
      "At: 2148 [==========>] Loss 0.17842974655379196  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.1154896254210919  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.0932354055844703  - accuracy: 0.84375\n",
      "At: 2151 [==========>] Loss 0.10387287596898827  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.21496180516167215  - accuracy: 0.71875\n",
      "At: 2153 [==========>] Loss 0.16705617562516728  - accuracy: 0.65625\n",
      "At: 2154 [==========>] Loss 0.14608152780826314  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.1213870621187684  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.11456588483939495  - accuracy: 0.90625\n",
      "At: 2157 [==========>] Loss 0.11886803268409407  - accuracy: 0.875\n",
      "At: 2158 [==========>] Loss 0.14549756711084116  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.07301094246296849  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.1541418368960499  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.08863699506527428  - accuracy: 0.90625\n",
      "At: 2162 [==========>] Loss 0.07568386483574122  - accuracy: 0.90625\n",
      "At: 2163 [==========>] Loss 0.118180443452674  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.17224152633186005  - accuracy: 0.71875\n",
      "At: 2165 [==========>] Loss 0.101935947258503  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.1276533779768952  - accuracy: 0.8125\n",
      "At: 2167 [==========>] Loss 0.10157865145500829  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.1134422127172994  - accuracy: 0.8125\n",
      "At: 2169 [==========>] Loss 0.14679686706744452  - accuracy: 0.78125\n",
      "At: 2170 [==========>] Loss 0.11532281913208466  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.17061795486860898  - accuracy: 0.75\n",
      "At: 2172 [==========>] Loss 0.10366825004631537  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.1259783956809856  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.11815730551587636  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.12367707340041972  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.1456899339345896  - accuracy: 0.8125\n",
      "At: 2177 [==========>] Loss 0.14740889569044452  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.0936161142658013  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.11944700923231824  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.11617023573291294  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.15291722803232766  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.11259358258711011  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.23032696869481667  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.10450983922569854  - accuracy: 0.84375\n",
      "At: 2185 [==========>] Loss 0.1254031378788702  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.1605219411477863  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.1630266255135167  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.08688907211376087  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.06513809830146694  - accuracy: 0.9375\n",
      "At: 2190 [==========>] Loss 0.11328613782310544  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.12572061352387962  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.10189910946557058  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.17505687793421013  - accuracy: 0.75\n",
      "At: 2194 [==========>] Loss 0.11008894922330233  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.18852672293224004  - accuracy: 0.71875\n",
      "At: 2196 [==========>] Loss 0.14970848571451817  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.07210843659981904  - accuracy: 0.9375\n",
      "At: 2198 [==========>] Loss 0.09220364245783969  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.0461613878215425  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.061358506719776794  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.10145313194279203  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.090736019678745  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.09698635461651048  - accuracy: 0.84375\n",
      "At: 2204 [==========>] Loss 0.1388242991542626  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.10876977280615635  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.11159153071145747  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.09510560389020144  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.14186942201190206  - accuracy: 0.8125\n",
      "At: 2209 [==========>] Loss 0.180523391647819  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.10844982785643752  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.1680138424387918  - accuracy: 0.71875\n",
      "At: 2212 [==========>] Loss 0.10057821824368698  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.11270652456128846  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.10277527589509611  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.14529565598494953  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.12958029723827932  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.12598081471765032  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.1807626072883553  - accuracy: 0.71875\n",
      "At: 2219 [==========>] Loss 0.0902526764964729  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.10139465447563963  - accuracy: 0.875\n",
      "At: 2221 [==========>] Loss 0.17845733328594174  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.1135855837552544  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.15172541066359777  - accuracy: 0.71875\n",
      "At: 2224 [==========>] Loss 0.13588565416116982  - accuracy: 0.84375\n",
      "At: 2225 [==========>] Loss 0.1259723336808541  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.1625597404851999  - accuracy: 0.78125\n",
      "At: 2227 [==========>] Loss 0.20419649151528635  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.0864984457945723  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.19555650731955013  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.10929784904828102  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.16704267845717624  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.15427648691051443  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.1574458977062986  - accuracy: 0.8125\n",
      "At: 2234 [==========>] Loss 0.1709451737885077  - accuracy: 0.6875\n",
      "At: 2235 [==========>] Loss 0.13671235283938032  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.06793302835778833  - accuracy: 0.9375\n",
      "At: 2237 [==========>] Loss 0.12428478562483397  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.1737614342470427  - accuracy: 0.75\n",
      "At: 2239 [==========>] Loss 0.1772317032614868  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.1327637301218839  - accuracy: 0.84375\n",
      "At: 2241 [==========>] Loss 0.1482337761295282  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.14980562032665493  - accuracy: 0.84375\n",
      "At: 2243 [==========>] Loss 0.06168110561860641  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.09759463852915569  - accuracy: 0.8125\n",
      "At: 2245 [==========>] Loss 0.08026330846427465  - accuracy: 0.9375\n",
      "At: 2246 [==========>] Loss 0.14136839819370162  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.12128834050911586  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.17486954553854056  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.10246309674515897  - accuracy: 0.84375\n",
      "At: 2250 [==========>] Loss 0.08089313475817025  - accuracy: 0.875\n",
      "At: 2251 [==========>] Loss 0.07682656719588216  - accuracy: 0.9375\n",
      "At: 2252 [==========>] Loss 0.1188585526285307  - accuracy: 0.84375\n",
      "At: 2253 [==========>] Loss 0.12540348237750337  - accuracy: 0.78125\n",
      "At: 2254 [==========>] Loss 0.13774112821634255  - accuracy: 0.78125\n",
      "At: 2255 [==========>] Loss 0.14153333460540943  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.14177966541350961  - accuracy: 0.84375\n",
      "At: 2257 [==========>] Loss 0.11055919021430413  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.11679932022153246  - accuracy: 0.84375\n",
      "At: 2259 [==========>] Loss 0.13188710356184255  - accuracy: 0.84375\n",
      "At: 2260 [==========>] Loss 0.1832238997560781  - accuracy: 0.65625\n",
      "At: 2261 [==========>] Loss 0.10480797047808812  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.17505946378350123  - accuracy: 0.75\n",
      "At: 2263 [==========>] Loss 0.14029730038049865  - accuracy: 0.78125\n",
      "At: 2264 [==========>] Loss 0.09467087170586737  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.09067624024887772  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.1298419584956157  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.07118866892989174  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.08488226223361002  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.042946276689457774  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.10416687948387791  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.12330673623704319  - accuracy: 0.8125\n",
      "At: 2272 [==========>] Loss 0.09978514767566193  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.11001310570115493  - accuracy: 0.875\n",
      "At: 2274 [==========>] Loss 0.08341975567670815  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.11105416046254417  - accuracy: 0.8125\n",
      "At: 2276 [==========>] Loss 0.10597462504979799  - accuracy: 0.84375\n",
      "At: 2277 [==========>] Loss 0.14114670499735293  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.08614928059421448  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.14901968407065347  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.15590744014973235  - accuracy: 0.75\n",
      "At: 2281 [==========>] Loss 0.12326883184630785  - accuracy: 0.78125\n",
      "At: 2282 [==========>] Loss 0.08147423163230894  - accuracy: 0.90625\n",
      "At: 2283 [==========>] Loss 0.14322624844990606  - accuracy: 0.8125\n",
      "At: 2284 [==========>] Loss 0.11554992488358586  - accuracy: 0.84375\n",
      "At: 2285 [==========>] Loss 0.13911051458607543  - accuracy: 0.71875\n",
      "At: 2286 [==========>] Loss 0.1258169275365127  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.11523544659160614  - accuracy: 0.84375\n",
      "At: 2288 [==========>] Loss 0.09078318638649105  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.13888447563860162  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.0827121954729908  - accuracy: 0.84375\n",
      "At: 2291 [==========>] Loss 0.12610280604003968  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.07543242715401426  - accuracy: 0.9375\n",
      "At: 2293 [==========>] Loss 0.05126825659069323  - accuracy: 0.9375\n",
      "At: 2294 [==========>] Loss 0.05076072259715242  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.15244240926190172  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.16441512308081888  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.0836746074911999  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.08603835216565218  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.1250654663793216  - accuracy: 0.78125\n",
      "At: 2300 [==========>] Loss 0.13188704744260688  - accuracy: 0.75\n",
      "At: 2301 [==========>] Loss 0.19535853440211096  - accuracy: 0.71875\n",
      "At: 2302 [==========>] Loss 0.16727192817587394  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.07002583162271528  - accuracy: 0.84375\n",
      "At: 2304 [==========>] Loss 0.08339564864170994  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10470707204110166  - accuracy: 0.84375\n",
      "At: 2306 [==========>] Loss 0.1407004105492542  - accuracy: 0.78125\n",
      "At: 2307 [==========>] Loss 0.16991705490362652  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.18877569245907871  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.09512038698815095  - accuracy: 0.84375\n",
      "At: 2310 [==========>] Loss 0.16038347017161025  - accuracy: 0.75\n",
      "At: 2311 [==========>] Loss 0.15551042820963576  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.11394432785986074  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.09851846411744233  - accuracy: 0.875\n",
      "At: 2314 [==========>] Loss 0.12323527081093623  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.11246954342309176  - accuracy: 0.84375\n",
      "At: 2316 [==========>] Loss 0.15171958559353704  - accuracy: 0.71875\n",
      "At: 2317 [==========>] Loss 0.19948381012451516  - accuracy: 0.71875\n",
      "At: 2318 [==========>] Loss 0.18812693227778582  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.13011124630154702  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.10024037660346784  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.13625401692666272  - accuracy: 0.84375\n",
      "At: 2322 [==========>] Loss 0.21903431558874648  - accuracy: 0.6875\n",
      "At: 2323 [==========>] Loss 0.15344510112183266  - accuracy: 0.75\n",
      "At: 2324 [==========>] Loss 0.13143085246730193  - accuracy: 0.8125\n",
      "At: 2325 [==========>] Loss 0.12872270089148574  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.08837718521490665  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.0668442145537201  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12696292470744056  - accuracy: 0.78125\n",
      "At: 2329 [==========>] Loss 0.11499151568126445  - accuracy: 0.90625\n",
      "At: 2330 [==========>] Loss 0.11807802985029073  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.09265921135075687  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.12072442433247729  - accuracy: 0.8125\n",
      "At: 2333 [==========>] Loss 0.10956184918119258  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.16420763683839254  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.12041246750333608  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.091659536209016  - accuracy: 0.90625\n",
      "At: 2337 [==========>] Loss 0.15611878739903132  - accuracy: 0.78125\n",
      "At: 2338 [==========>] Loss 0.1008957022290419  - accuracy: 0.84375\n",
      "At: 2339 [==========>] Loss 0.08359126238452225  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.15792515566577556  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.153691254921557  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.17939788805234624  - accuracy: 0.71875\n",
      "At: 2343 [==========>] Loss 0.08894574393029023  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.1891264703857113  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.1432770219939781  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.09734588637283424  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.14323302508636526  - accuracy: 0.78125\n",
      "At: 2348 [==========>] Loss 0.05511097830643742  - accuracy: 0.90625\n",
      "At: 2349 [==========>] Loss 0.09167229289443773  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.12772217858355095  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.09420791907040786  - accuracy: 0.90625\n",
      "At: 2352 [==========>] Loss 0.11331153783129129  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.0959376966333719  - accuracy: 0.90625\n",
      "At: 2354 [==========>] Loss 0.14414269238261757  - accuracy: 0.8125\n",
      "At: 2355 [==========>] Loss 0.05020630994896761  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.1338242974010379  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.129003136960662  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.13085444752935493  - accuracy: 0.84375\n",
      "At: 2359 [==========>] Loss 0.19916313244666306  - accuracy: 0.59375\n",
      "At: 2360 [==========>] Loss 0.13126719495892736  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.15350938173547035  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.06552727057580467  - accuracy: 0.9375\n",
      "At: 2363 [==========>] Loss 0.17714331632487496  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.07907984300018077  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.09986652135853985  - accuracy: 0.84375\n",
      "At: 2366 [==========>] Loss 0.1759116524613245  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.0997536085189107  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.10205801413436075  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.13142355476461615  - accuracy: 0.8125\n",
      "At: 2370 [==========>] Loss 0.0955368620047081  - accuracy: 0.84375\n",
      "At: 2371 [==========>] Loss 0.0901377863119774  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.15248955148854607  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.09293966317359684  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.09964482323345863  - accuracy: 0.875\n",
      "At: 2375 [==========>] Loss 0.05194055106541103  - accuracy: 0.96875\n",
      "At: 2376 [==========>] Loss 0.10993684143020435  - accuracy: 0.8125\n",
      "At: 2377 [==========>] Loss 0.09411368965830005  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.12324478733503341  - accuracy: 0.78125\n",
      "At: 2379 [==========>] Loss 0.17979011324097344  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.07542050294172602  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.1010254017181802  - accuracy: 0.875\n",
      "At: 2382 [==========>] Loss 0.09526551830471477  - accuracy: 0.875\n",
      "At: 2383 [==========>] Loss 0.1520295187927971  - accuracy: 0.78125\n",
      "At: 2384 [==========>] Loss 0.08735483216945214  - accuracy: 0.875\n",
      "At: 2385 [==========>] Loss 0.13020908114287727  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.06962477193794891  - accuracy: 0.90625\n",
      "At: 2387 [==========>] Loss 0.08831926888639173  - accuracy: 0.875\n",
      "At: 2388 [==========>] Loss 0.13401629843368465  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.049284216449856984  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.07918831617119844  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.1850788136081698  - accuracy: 0.6875\n",
      "At: 2392 [==========>] Loss 0.13824641355505907  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.09258966807622501  - accuracy: 0.84375\n",
      "At: 2394 [==========>] Loss 0.06610557560282866  - accuracy: 0.875\n",
      "At: 2395 [==========>] Loss 0.08639621319807272  - accuracy: 0.9375\n",
      "At: 2396 [==========>] Loss 0.05776576418256557  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.0843263245436101  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.1284026803301376  - accuracy: 0.8125\n",
      "At: 2399 [==========>] Loss 0.13297111244780518  - accuracy: 0.8125\n",
      "At: 2400 [==========>] Loss 0.09147874931127388  - accuracy: 0.90625\n",
      "At: 2401 [==========>] Loss 0.10546784092046788  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.09669948481433466  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.16445589140498806  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.14374329903316094  - accuracy: 0.78125\n",
      "At: 2405 [==========>] Loss 0.06748226150751757  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.1307859046264495  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.1031772330062059  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.08783291855254238  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.15070598567263793  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.15516084809873337  - accuracy: 0.84375\n",
      "At: 2411 [==========>] Loss 0.09783907057856935  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.09464064940944641  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.07347599902805067  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.07912116451322952  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.09895943196747199  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.10148386853610375  - accuracy: 0.84375\n",
      "At: 2417 [==========>] Loss 0.18602550574924595  - accuracy: 0.65625\n",
      "At: 2418 [==========>] Loss 0.12660783549254184  - accuracy: 0.84375\n",
      "At: 2419 [==========>] Loss 0.13436136209859892  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.1319389421229886  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.09398017303915927  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.16015184467345062  - accuracy: 0.75\n",
      "At: 2423 [==========>] Loss 0.11724496857744426  - accuracy: 0.84375\n",
      "At: 2424 [==========>] Loss 0.10148825215927232  - accuracy: 0.84375\n",
      "At: 2425 [==========>] Loss 0.07526448036760501  - accuracy: 0.9375\n",
      "At: 2426 [==========>] Loss 0.23002002943724548  - accuracy: 0.65625\n",
      "At: 2427 [==========>] Loss 0.12472710986266862  - accuracy: 0.84375\n",
      "At: 2428 [==========>] Loss 0.08809869611467189  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.13532104368459696  - accuracy: 0.8125\n",
      "At: 2430 [==========>] Loss 0.11770143089423833  - accuracy: 0.84375\n",
      "At: 2431 [==========>] Loss 0.151631553825263  - accuracy: 0.71875\n",
      "At: 2432 [==========>] Loss 0.06085235578224851  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.06547774945830481  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.03772580807107079  - accuracy: 1.0\n",
      "At: 2435 [==========>] Loss 0.154090079569661  - accuracy: 0.8125\n",
      "At: 2436 [==========>] Loss 0.10164196712229148  - accuracy: 0.875\n",
      "At: 2437 [==========>] Loss 0.17974867257984142  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.09809246371522341  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.15433742320675342  - accuracy: 0.75\n",
      "At: 2440 [==========>] Loss 0.12990885803337693  - accuracy: 0.8125\n",
      "At: 2441 [==========>] Loss 0.11131301865125062  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.11437422659444625  - accuracy: 0.875\n",
      "At: 2443 [==========>] Loss 0.11556958186509764  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.06823933796634116  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.05646034652934318  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.17425858475688305  - accuracy: 0.75\n",
      "At: 2447 [==========>] Loss 0.17905338818990024  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.13253622757844996  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.0825355938804975  - accuracy: 0.9375\n",
      "At: 2450 [==========>] Loss 0.052011820587827516  - accuracy: 0.9375\n",
      "At: 2451 [==========>] Loss 0.05710927599866163  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.13789597605453136  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.15261074904599226  - accuracy: 0.75\n",
      "At: 2454 [==========>] Loss 0.15045369415547843  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.13834343418668726  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.15750562499422563  - accuracy: 0.75\n",
      "At: 2457 [==========>] Loss 0.17657901139650278  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.08741239258980855  - accuracy: 0.84375\n",
      "At: 2459 [==========>] Loss 0.130763720599203  - accuracy: 0.78125\n",
      "At: 2460 [==========>] Loss 0.09005066199306228  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.06275376612482422  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.16925207617796684  - accuracy: 0.71875\n",
      "At: 2463 [==========>] Loss 0.10460810832607595  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.15953605577848284  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.14304900444012525  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.09883424092656684  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.09839018564765117  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.08524687536105895  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.1390877806585446  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.14078090485157566  - accuracy: 0.875\n",
      "At: 2471 [==========>] Loss 0.1095381057642284  - accuracy: 0.90625\n",
      "At: 2472 [==========>] Loss 0.09658616775092646  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.1159516993741922  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.06878229322328469  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.09764965839151135  - accuracy: 0.90625\n",
      "At: 2476 [==========>] Loss 0.0660995249065057  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.1718521291693542  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.12962368172164215  - accuracy: 0.78125\n",
      "At: 2479 [==========>] Loss 0.09350639593476877  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.10973577226377954  - accuracy: 0.90625\n",
      "At: 2481 [==========>] Loss 0.05307110830955538  - accuracy: 0.96875\n",
      "At: 2482 [==========>] Loss 0.1597866478248526  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.11644043798210484  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.09008284534670734  - accuracy: 0.90625\n",
      "At: 2485 [==========>] Loss 0.09658534378698143  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.13457895624764482  - accuracy: 0.8125\n",
      "At: 2487 [==========>] Loss 0.10368165009561513  - accuracy: 0.875\n",
      "At: 2488 [==========>] Loss 0.1548778362126722  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.18902725909911486  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.09627632650460685  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.13673907228437587  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.14676910889052472  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.09310716611028981  - accuracy: 0.8125\n",
      "At: 2494 [==========>] Loss 0.10579479627423038  - accuracy: 0.84375\n",
      "At: 2495 [==========>] Loss 0.09042141531817438  - accuracy: 0.90625\n",
      "At: 2496 [==========>] Loss 0.08231086434527715  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.22752743409487763  - accuracy: 0.65625\n",
      "At: 2498 [==========>] Loss 0.1055984713158122  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.07319865560597916  - accuracy: 0.875\n",
      "At: 2500 [==========>] Loss 0.15990080522527247  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.19602115248968643  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.11048965845142888  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.13474603220877146  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.15289121462666705  - accuracy: 0.78125\n",
      "At: 2505 [==========>] Loss 0.12682050597634648  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.087696217289252  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.14741002061141117  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.10130895361674465  - accuracy: 0.875\n",
      "At: 2509 [==========>] Loss 0.1336621052206579  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.11922331250014755  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.15671106129759985  - accuracy: 0.78125\n",
      "At: 2512 [==========>] Loss 0.1071445038902804  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.154357217369301  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.15274165499618042  - accuracy: 0.75\n",
      "At: 2515 [==========>] Loss 0.20343986645607523  - accuracy: 0.6875\n",
      "At: 2516 [==========>] Loss 0.19142116941698414  - accuracy: 0.6875\n",
      "At: 2517 [==========>] Loss 0.11949934180015098  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.12370438136776019  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.12573364256795666  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.15375853001801104  - accuracy: 0.78125\n",
      "At: 2521 [==========>] Loss 0.11825771496001336  - accuracy: 0.8125\n",
      "At: 2522 [==========>] Loss 0.22216887229395912  - accuracy: 0.6875\n",
      "At: 2523 [==========>] Loss 0.12610703065889675  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.15748062642808286  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.06453817314650542  - accuracy: 0.96875\n",
      "At: 2526 [==========>] Loss 0.11314013250087569  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.13040799626368033  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.08484414716424674  - accuracy: 0.90625\n",
      "At: 2529 [==========>] Loss 0.1423508242654593  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.1235273155775998  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.04993994106595642  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.15427822967333713  - accuracy: 0.75\n",
      "At: 2533 [==========>] Loss 0.11633795349468633  - accuracy: 0.84375\n",
      "At: 2534 [==========>] Loss 0.05333256511701885  - accuracy: 0.90625\n",
      "At: 2535 [==========>] Loss 0.06688513893839336  - accuracy: 0.90625\n",
      "At: 2536 [==========>] Loss 0.16429135938551986  - accuracy: 0.71875\n",
      "At: 2537 [==========>] Loss 0.09171346586591972  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.15227192921529753  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.0902453215953276  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.1307347869209049  - accuracy: 0.75\n",
      "At: 2541 [==========>] Loss 0.05928033796215298  - accuracy: 0.9375\n",
      "At: 2542 [==========>] Loss 0.06096929502301769  - accuracy: 0.90625\n",
      "At: 2543 [==========>] Loss 0.16770468634133956  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.17532240950051586  - accuracy: 0.75\n",
      "At: 2545 [==========>] Loss 0.058870773265130266  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.154906030829237  - accuracy: 0.8125\n",
      "At: 2547 [==========>] Loss 0.12142730747158163  - accuracy: 0.84375\n",
      "At: 2548 [==========>] Loss 0.09392510520703934  - accuracy: 0.84375\n",
      "At: 2549 [==========>] Loss 0.11136171918025005  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.11716043070947177  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.14256676929256515  - accuracy: 0.78125\n",
      "At: 2552 [==========>] Loss 0.09625904620108011  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.09136352065774164  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.16108106077582296  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.1649180054897878  - accuracy: 0.8125\n",
      "At: 2556 [==========>] Loss 0.09449770575426719  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.11850682016671975  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.10222676447640447  - accuracy: 0.875\n",
      "At: 2559 [==========>] Loss 0.1037940529445116  - accuracy: 0.90625\n",
      "At: 2560 [==========>] Loss 0.1978505683063688  - accuracy: 0.71875\n",
      "At: 2561 [==========>] Loss 0.09730200139368098  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.11703680904868866  - accuracy: 0.78125\n",
      "At: 2563 [==========>] Loss 0.11658347100634901  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09920144583112966  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.11966134152898013  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1719421698213564  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.09880584460660964  - accuracy: 0.875\n",
      "At: 2568 [==========>] Loss 0.10524712440643241  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.0681012010239436  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.1977067386684717  - accuracy: 0.75\n",
      "At: 2571 [==========>] Loss 0.09351377383905479  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.19787662038205756  - accuracy: 0.71875\n",
      "At: 2573 [==========>] Loss 0.20204564861905006  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.1458500825751982  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.06127249589123975  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.10292223052135682  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.21096277624236737  - accuracy: 0.6875\n",
      "At: 2578 [==========>] Loss 0.18803278574619525  - accuracy: 0.65625\n",
      "At: 2579 [==========>] Loss 0.19016134127137993  - accuracy: 0.65625\n",
      "At: 2580 [==========>] Loss 0.13196795582498352  - accuracy: 0.8125\n",
      "At: 2581 [==========>] Loss 0.0575944935510096  - accuracy: 0.96875\n",
      "At: 2582 [==========>] Loss 0.17777176340575102  - accuracy: 0.6875\n",
      "At: 2583 [==========>] Loss 0.08080795889318657  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.19268958616092008  - accuracy: 0.65625\n",
      "At: 2585 [==========>] Loss 0.10634125448916315  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.09992429331105608  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.17110092437418803  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.14351598509194313  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.11447185958817802  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.21473953083586872  - accuracy: 0.625\n",
      "At: 2591 [==========>] Loss 0.1281412310573672  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.12052701045159381  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.09634859401743576  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.09388841667522352  - accuracy: 0.84375\n",
      "At: 2595 [==========>] Loss 0.07268141462242478  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.10741328153296266  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.07946222778710184  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.12536496084486268  - accuracy: 0.875\n",
      "At: 2599 [==========>] Loss 0.14603126188886767  - accuracy: 0.8125\n",
      "At: 2600 [==========>] Loss 0.13509422644442837  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.1537531179629706  - accuracy: 0.78125\n",
      "At: 2602 [==========>] Loss 0.08311719417686801  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.176663045075008  - accuracy: 0.75\n",
      "At: 2604 [==========>] Loss 0.10442601202226678  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.1255238790872149  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.12216249122038733  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.14578483714440127  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.07646625252100281  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.08709683785493239  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.12795120627383816  - accuracy: 0.8125\n",
      "At: 2611 [==========>] Loss 0.15003933054405397  - accuracy: 0.75\n",
      "At: 2612 [==========>] Loss 0.09120756790806704  - accuracy: 0.8125\n",
      "At: 2613 [==========>] Loss 0.1104972927019019  - accuracy: 0.8125\n",
      "At: 2614 [==========>] Loss 0.1191815712307564  - accuracy: 0.84375\n",
      "At: 2615 [==========>] Loss 0.09661857947659033  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06408973231368165  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.10715921888159843  - accuracy: 0.8125\n",
      "At: 2618 [==========>] Loss 0.05985472476180092  - accuracy: 0.9375\n",
      "At: 2619 [==========>] Loss 0.11342904236825924  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.10166165338985494  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.10994923082449361  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.117442170663148  - accuracy: 0.78125\n",
      "At: 2623 [==========>] Loss 0.10874928544264562  - accuracy: 0.875\n",
      "At: 2624 [==========>] Loss 0.1316857427477548  - accuracy: 0.875\n",
      "At: 2625 [==========>] Loss 0.0728954060319944  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.06774711898468228  - accuracy: 0.90625\n",
      "At: 2627 [==========>] Loss 0.1415172111670653  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.07757571711608155  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.10750570409549601  - accuracy: 0.84375\n",
      "At: 2630 [==========>] Loss 0.09847042818276874  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.11747182576974571  - accuracy: 0.84375\n",
      "At: 2632 [==========>] Loss 0.13686702326894606  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.12088914195513666  - accuracy: 0.84375\n",
      "At: 2634 [==========>] Loss 0.0610172423280264  - accuracy: 0.875\n",
      "At: 2635 [==========>] Loss 0.20149482885032177  - accuracy: 0.65625\n",
      "At: 2636 [==========>] Loss 0.13374113359150108  - accuracy: 0.8125\n",
      "At: 2637 [==========>] Loss 0.1445020484695132  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.09233939659119274  - accuracy: 0.875\n",
      "At: 2639 [==========>] Loss 0.09554097403870274  - accuracy: 0.90625\n",
      "At: 2640 [==========>] Loss 0.11497803410501667  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.13989048202536775  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.17254263788239968  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.09007927628905978  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.1524455210402062  - accuracy: 0.71875\n",
      "At: 2645 [==========>] Loss 0.07766728215380053  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.1595618618619237  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.12478310947548016  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.16413331598230735  - accuracy: 0.75\n",
      "At: 2649 [==========>] Loss 0.13690794068301626  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.09882735503431031  - accuracy: 0.90625\n",
      "At: 2651 [==========>] Loss 0.17074168977457488  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.11268830972711781  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.12280077373209561  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11109117144588344  - accuracy: 0.90625\n",
      "At: 2655 [==========>] Loss 0.2517278268212819  - accuracy: 0.625\n",
      "At: 2656 [==========>] Loss 0.02592354222998186  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.0991045121348895  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.09754317921076441  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.0815537978640198  - accuracy: 0.90625\n",
      "At: 2660 [==========>] Loss 0.09558545804591363  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.08123003044515913  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.07831047114990639  - accuracy: 0.90625\n",
      "At: 2663 [==========>] Loss 0.08649305331063564  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.0708516440462077  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.08577368217162819  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.1312666299872865  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.12382861359888162  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.11833994916244152  - accuracy: 0.84375\n",
      "At: 2669 [==========>] Loss 0.12463890607144464  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.10292540295647193  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.08818200582795158  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.10309578381219213  - accuracy: 0.875\n",
      "At: 2673 [==========>] Loss 0.11726335059975165  - accuracy: 0.875\n",
      "At: 2674 [==========>] Loss 0.10437827340001662  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.1417803742297078  - accuracy: 0.78125\n",
      "At: 2676 [==========>] Loss 0.13752698188008258  - accuracy: 0.78125\n",
      "At: 2677 [==========>] Loss 0.10053408946204725  - accuracy: 0.8125\n",
      "At: 2678 [==========>] Loss 0.04969037361239209  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.07780698030364959  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.08484574770137383  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.07630803507845488  - accuracy: 0.9375\n",
      "At: 2682 [==========>] Loss 0.14354857054601433  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.19399845556355214  - accuracy: 0.71875\n",
      "At: 2684 [==========>] Loss 0.0985601082122145  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.10736653674808921  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.06711771524531474  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.14382525674659544  - accuracy: 0.8125\n",
      "At: 2688 [==========>] Loss 0.1512291547799727  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.1169213088757867  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.1405566218942524  - accuracy: 0.78125\n",
      "Epochs  10 / 10\n",
      "At: 1 [==========>] Loss 0.13746999082659409  - accuracy: 0.84375\n",
      "At: 2 [==========>] Loss 0.19287813326272918  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.14182990161137995  - accuracy: 0.8125\n",
      "At: 4 [==========>] Loss 0.19149869683291043  - accuracy: 0.75\n",
      "At: 5 [==========>] Loss 0.11069332873637558  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.13898285851739645  - accuracy: 0.875\n",
      "At: 7 [==========>] Loss 0.1931024659615183  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.26859587200758805  - accuracy: 0.6875\n",
      "At: 9 [==========>] Loss 0.3612736168797573  - accuracy: 0.5625\n",
      "At: 10 [==========>] Loss 0.23455830935638014  - accuracy: 0.71875\n",
      "At: 11 [==========>] Loss 0.21258486353983197  - accuracy: 0.78125\n",
      "At: 12 [==========>] Loss 0.21242071373734173  - accuracy: 0.78125\n",
      "At: 13 [==========>] Loss 0.18611264831602337  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.09078326214026242  - accuracy: 0.90625\n",
      "At: 15 [==========>] Loss 0.17296148907395284  - accuracy: 0.8125\n",
      "At: 16 [==========>] Loss 0.1921926534995227  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.18174692645702462  - accuracy: 0.78125\n",
      "At: 18 [==========>] Loss 0.2083167604627915  - accuracy: 0.71875\n",
      "At: 19 [==========>] Loss 0.1812586346969346  - accuracy: 0.78125\n",
      "At: 20 [==========>] Loss 0.12310090127109954  - accuracy: 0.875\n",
      "At: 21 [==========>] Loss 0.2248050804305755  - accuracy: 0.78125\n",
      "At: 22 [==========>] Loss 0.16413830479550984  - accuracy: 0.8125\n",
      "At: 23 [==========>] Loss 0.10154285766349727  - accuracy: 0.875\n",
      "At: 24 [==========>] Loss 0.2455823597334526  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.19058489650329613  - accuracy: 0.75\n",
      "At: 26 [==========>] Loss 0.2524051467541698  - accuracy: 0.6875\n",
      "At: 27 [==========>] Loss 0.19776643451787257  - accuracy: 0.75\n",
      "At: 28 [==========>] Loss 0.1558810970241609  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.15504695219799947  - accuracy: 0.84375\n",
      "At: 30 [==========>] Loss 0.20843807461692404  - accuracy: 0.75\n",
      "At: 31 [==========>] Loss 0.25301353924425846  - accuracy: 0.6875\n",
      "At: 32 [==========>] Loss 0.2073463593678212  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.1022733101970457  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.14262006257097964  - accuracy: 0.875\n",
      "At: 35 [==========>] Loss 0.19361939294227404  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.20140606915213133  - accuracy: 0.78125\n",
      "At: 37 [==========>] Loss 0.23628491728506035  - accuracy: 0.71875\n",
      "At: 38 [==========>] Loss 0.26410699959964035  - accuracy: 0.6875\n",
      "At: 39 [==========>] Loss 0.17983462598000505  - accuracy: 0.8125\n",
      "At: 40 [==========>] Loss 0.25402789355994826  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.0880224365602231  - accuracy: 0.9375\n",
      "At: 42 [==========>] Loss 0.1604719033151508  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.16174952264761785  - accuracy: 0.84375\n",
      "At: 44 [==========>] Loss 0.15277880581232375  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.1072366653428342  - accuracy: 0.9375\n",
      "At: 46 [==========>] Loss 0.17964162315127113  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.22306521734588064  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.13523562951738208  - accuracy: 0.84375\n",
      "At: 49 [==========>] Loss 0.14225797268299567  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.17935487801122418  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.20481515982883652  - accuracy: 0.71875\n",
      "At: 52 [==========>] Loss 0.27420235764885137  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.16165580070530314  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.13386454284144594  - accuracy: 0.875\n",
      "At: 55 [==========>] Loss 0.23749103398333962  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.1601781641275521  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.1220554063649244  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.2075245329682379  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.19526383966498495  - accuracy: 0.78125\n",
      "At: 60 [==========>] Loss 0.1999376806651096  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.2423599113508292  - accuracy: 0.6875\n",
      "At: 62 [==========>] Loss 0.20079825143681085  - accuracy: 0.71875\n",
      "At: 63 [==========>] Loss 0.16007314370001247  - accuracy: 0.84375\n",
      "At: 64 [==========>] Loss 0.22337727946242394  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.2672356882907496  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.21270281616754888  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.22965638484248155  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.14176310095262698  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.16017672171421993  - accuracy: 0.75\n",
      "At: 70 [==========>] Loss 0.20684945512343694  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.1880332621650796  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.1365490602680823  - accuracy: 0.875\n",
      "At: 73 [==========>] Loss 0.1549816607814826  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.20011215464871257  - accuracy: 0.75\n",
      "At: 75 [==========>] Loss 0.20119056843241978  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.24553006632550212  - accuracy: 0.6875\n",
      "At: 77 [==========>] Loss 0.24360758382474273  - accuracy: 0.6875\n",
      "At: 78 [==========>] Loss 0.13307742258696145  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.17544562848771023  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.21748925408801595  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.13106827185158654  - accuracy: 0.8125\n",
      "At: 82 [==========>] Loss 0.20842454513171405  - accuracy: 0.75\n",
      "At: 83 [==========>] Loss 0.15400502871624666  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.15664910958382133  - accuracy: 0.78125\n",
      "At: 85 [==========>] Loss 0.173099060848683  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.15489297116309475  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.16379718582151245  - accuracy: 0.75\n",
      "At: 88 [==========>] Loss 0.32688746696509363  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.1796865036569572  - accuracy: 0.71875\n",
      "At: 90 [==========>] Loss 0.21628515058447867  - accuracy: 0.65625\n",
      "At: 91 [==========>] Loss 0.16271365939511548  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.07784549972778687  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.12928241925694223  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.10539957192843133  - accuracy: 0.875\n",
      "At: 95 [==========>] Loss 0.20016732511213803  - accuracy: 0.8125\n",
      "At: 96 [==========>] Loss 0.14781601687534313  - accuracy: 0.875\n",
      "At: 97 [==========>] Loss 0.08578159363899365  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.18571627933324222  - accuracy: 0.71875\n",
      "At: 99 [==========>] Loss 0.12586802369181443  - accuracy: 0.875\n",
      "At: 100 [==========>] Loss 0.12050582782820402  - accuracy: 0.84375\n",
      "At: 101 [==========>] Loss 0.14684064935040766  - accuracy: 0.8125\n",
      "At: 102 [==========>] Loss 0.168503225654841  - accuracy: 0.8125\n",
      "At: 103 [==========>] Loss 0.135639470170405  - accuracy: 0.84375\n",
      "At: 104 [==========>] Loss 0.11805947307373046  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.2061513907809016  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.17138660386545335  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.22780421975111276  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.21255319039971793  - accuracy: 0.75\n",
      "At: 109 [==========>] Loss 0.09025992399616758  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.2657144417069207  - accuracy: 0.65625\n",
      "At: 111 [==========>] Loss 0.09813214418407698  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.14455600642354982  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.20751499252026456  - accuracy: 0.71875\n",
      "At: 114 [==========>] Loss 0.14998246432396645  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.20477521579951752  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.16011438602511346  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.17247698459338862  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.2451261160117766  - accuracy: 0.625\n",
      "At: 119 [==========>] Loss 0.12584217556889227  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.1698018330774531  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.11692889805398732  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.20633797298844053  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.2062679978485149  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.24901458244176372  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.1894071737173872  - accuracy: 0.6875\n",
      "At: 126 [==========>] Loss 0.1921731614048983  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.15933955774851755  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.22475587338953718  - accuracy: 0.6875\n",
      "At: 129 [==========>] Loss 0.16206033900500588  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.21754224590450905  - accuracy: 0.6875\n",
      "At: 131 [==========>] Loss 0.19221771060786677  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.17605923693468906  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.22510691809932962  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.12094299612663639  - accuracy: 0.875\n",
      "At: 135 [==========>] Loss 0.17371872954883008  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.16349137629068083  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.09164358772390763  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.19543263575347947  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.11541401071186616  - accuracy: 0.84375\n",
      "At: 140 [==========>] Loss 0.1114347512159063  - accuracy: 0.875\n",
      "At: 141 [==========>] Loss 0.27660445497963254  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.17519507821238095  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.17743050186962184  - accuracy: 0.75\n",
      "At: 144 [==========>] Loss 0.1303717870547671  - accuracy: 0.84375\n",
      "At: 145 [==========>] Loss 0.11372400840165228  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.10973993217961857  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.17984503503457933  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.11142561076740783  - accuracy: 0.875\n",
      "At: 149 [==========>] Loss 0.19849757114038774  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.20074848721447536  - accuracy: 0.71875\n",
      "At: 151 [==========>] Loss 0.18476699396737895  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.17789507423400924  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.13623947863186048  - accuracy: 0.875\n",
      "At: 154 [==========>] Loss 0.18620326106868823  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.16868488904201806  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.14366683163191274  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.18771555762365447  - accuracy: 0.78125\n",
      "At: 158 [==========>] Loss 0.15933775387039517  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.1124653136138485  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.13407818661474968  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.10668714797652235  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.21766964100211506  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.19088895391502397  - accuracy: 0.78125\n",
      "At: 164 [==========>] Loss 0.16164429910380546  - accuracy: 0.8125\n",
      "At: 165 [==========>] Loss 0.20331988234698822  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.17842141276709772  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.10834437189452105  - accuracy: 0.875\n",
      "At: 168 [==========>] Loss 0.20082004738743228  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.14633302938482637  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.14587006514631423  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.21373443252437296  - accuracy: 0.65625\n",
      "At: 172 [==========>] Loss 0.16642364250988725  - accuracy: 0.71875\n",
      "At: 173 [==========>] Loss 0.2597312352430192  - accuracy: 0.59375\n",
      "At: 174 [==========>] Loss 0.19771259933816276  - accuracy: 0.71875\n",
      "At: 175 [==========>] Loss 0.14807843520862474  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.17254184571437714  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.1130860341294134  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.16698465437818968  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.14714741437003429  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.13568399703149137  - accuracy: 0.84375\n",
      "At: 181 [==========>] Loss 0.05662004870765815  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.17770019680541457  - accuracy: 0.75\n",
      "At: 183 [==========>] Loss 0.13905928291164787  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.14771105410387458  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.10715943651329407  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.15527374821393103  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.14877494267111202  - accuracy: 0.84375\n",
      "At: 188 [==========>] Loss 0.1895904073706189  - accuracy: 0.75\n",
      "At: 189 [==========>] Loss 0.17245314716980206  - accuracy: 0.75\n",
      "At: 190 [==========>] Loss 0.13696309479437868  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.28032066311029186  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.11254520135937524  - accuracy: 0.84375\n",
      "At: 193 [==========>] Loss 0.18458678912544882  - accuracy: 0.75\n",
      "At: 194 [==========>] Loss 0.1730414239413276  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.18066729008123805  - accuracy: 0.71875\n",
      "At: 196 [==========>] Loss 0.16039361429844007  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.13035043064193574  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.12845409265625962  - accuracy: 0.875\n",
      "At: 199 [==========>] Loss 0.1331679270312333  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.21819572924845504  - accuracy: 0.6875\n",
      "At: 201 [==========>] Loss 0.14519204448955597  - accuracy: 0.78125\n",
      "At: 202 [==========>] Loss 0.1030722373667555  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.12605892799267931  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.17464448050161155  - accuracy: 0.78125\n",
      "At: 205 [==========>] Loss 0.12030838999842514  - accuracy: 0.84375\n",
      "At: 206 [==========>] Loss 0.1255033902680708  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.11146272328127636  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.18079414655365691  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.1737957688459001  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.08652602579600192  - accuracy: 0.90625\n",
      "At: 211 [==========>] Loss 0.14271050829199422  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.18346676256912425  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.18443362913328581  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.16175691721186497  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.11714383153494962  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.1645371382458169  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.20808086243083856  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.1732261031786192  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.20284284523118085  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.19574897891230703  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.1625530521155205  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.10387235779344561  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.24878323591638135  - accuracy: 0.625\n",
      "At: 224 [==========>] Loss 0.16749718394566607  - accuracy: 0.75\n",
      "At: 225 [==========>] Loss 0.13142859492113493  - accuracy: 0.8125\n",
      "At: 226 [==========>] Loss 0.1391448443374382  - accuracy: 0.84375\n",
      "At: 227 [==========>] Loss 0.20847440238610018  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.18732735125863512  - accuracy: 0.71875\n",
      "At: 229 [==========>] Loss 0.20601547968272707  - accuracy: 0.71875\n",
      "At: 230 [==========>] Loss 0.1523862440682649  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.21264365803682989  - accuracy: 0.71875\n",
      "At: 232 [==========>] Loss 0.24640734464471103  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.19084859281886468  - accuracy: 0.71875\n",
      "At: 234 [==========>] Loss 0.09745194298769161  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.18756445279043363  - accuracy: 0.75\n",
      "At: 236 [==========>] Loss 0.20447384665693127  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.144683366434935  - accuracy: 0.875\n",
      "At: 238 [==========>] Loss 0.13148205425328208  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.10012992570523722  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.23110432301567552  - accuracy: 0.71875\n",
      "At: 241 [==========>] Loss 0.11920038584202082  - accuracy: 0.875\n",
      "At: 242 [==========>] Loss 0.13454438974479044  - accuracy: 0.84375\n",
      "At: 243 [==========>] Loss 0.10155486090692496  - accuracy: 0.90625\n",
      "At: 244 [==========>] Loss 0.1134957182165165  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.15493869969670906  - accuracy: 0.75\n",
      "At: 246 [==========>] Loss 0.13240970892351714  - accuracy: 0.78125\n",
      "At: 247 [==========>] Loss 0.1312250654405067  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.12485399488028975  - accuracy: 0.8125\n",
      "At: 249 [==========>] Loss 0.11536802752994242  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.1923812633975595  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.20979137684592325  - accuracy: 0.65625\n",
      "At: 252 [==========>] Loss 0.07556595622556385  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.16194781070419778  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.07696112302376878  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.12139961489051347  - accuracy: 0.84375\n",
      "At: 256 [==========>] Loss 0.18874443631782228  - accuracy: 0.6875\n",
      "At: 257 [==========>] Loss 0.12675351986244995  - accuracy: 0.8125\n",
      "At: 258 [==========>] Loss 0.1539900926315823  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.1695116504578857  - accuracy: 0.78125\n",
      "At: 260 [==========>] Loss 0.16284879149173206  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.08438999387120623  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.15510187662856637  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.09373045607812235  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.10546821167886468  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.20918930538673092  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.257615449646596  - accuracy: 0.53125\n",
      "At: 267 [==========>] Loss 0.13431086039899318  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.20207452514057378  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.07839394978813079  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.23329904012704195  - accuracy: 0.6875\n",
      "At: 271 [==========>] Loss 0.17425003617321838  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.10441093940302769  - accuracy: 0.84375\n",
      "At: 273 [==========>] Loss 0.14778212665143753  - accuracy: 0.6875\n",
      "At: 274 [==========>] Loss 0.20809734038529548  - accuracy: 0.71875\n",
      "At: 275 [==========>] Loss 0.06024695024994482  - accuracy: 0.96875\n",
      "At: 276 [==========>] Loss 0.24248895480636434  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.14429963115240518  - accuracy: 0.8125\n",
      "At: 278 [==========>] Loss 0.09377595907714706  - accuracy: 0.875\n",
      "At: 279 [==========>] Loss 0.15875269206433165  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.15598929500282466  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.1369763652035073  - accuracy: 0.78125\n",
      "At: 282 [==========>] Loss 0.17183167733674182  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.13400050937067062  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.0946423858678665  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.16014221018703195  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.07827838462036954  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.15219951226441739  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.09852141247563388  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.08064484220631266  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.11588212035471786  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.11980381176728247  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.1496041788427717  - accuracy: 0.84375\n",
      "At: 293 [==========>] Loss 0.18158352191449745  - accuracy: 0.71875\n",
      "At: 294 [==========>] Loss 0.15083972435565074  - accuracy: 0.71875\n",
      "At: 295 [==========>] Loss 0.15933958985276922  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.11982117321775515  - accuracy: 0.8125\n",
      "At: 297 [==========>] Loss 0.10865536765764343  - accuracy: 0.90625\n",
      "At: 298 [==========>] Loss 0.11048810782281512  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.1495985449930323  - accuracy: 0.75\n",
      "At: 300 [==========>] Loss 0.1487074300154114  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.1642511744063691  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.12326867960657509  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.07352235830054762  - accuracy: 0.9375\n",
      "At: 304 [==========>] Loss 0.15955561066709126  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.23460560566817829  - accuracy: 0.71875\n",
      "At: 306 [==========>] Loss 0.11990673631149346  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.21444806630359078  - accuracy: 0.71875\n",
      "At: 308 [==========>] Loss 0.13354474849901138  - accuracy: 0.84375\n",
      "At: 309 [==========>] Loss 0.05379720016359828  - accuracy: 0.9375\n",
      "At: 310 [==========>] Loss 0.17440988075750388  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.06404102291523843  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.13883087142868766  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.08276807282015639  - accuracy: 0.9375\n",
      "At: 314 [==========>] Loss 0.15388498281804128  - accuracy: 0.8125\n",
      "At: 315 [==========>] Loss 0.11962054479295378  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.2212672136275427  - accuracy: 0.65625\n",
      "At: 317 [==========>] Loss 0.21837541311142006  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.1418204711258914  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.12271074653202683  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.18224720346962042  - accuracy: 0.75\n",
      "At: 321 [==========>] Loss 0.16701790635188332  - accuracy: 0.71875\n",
      "At: 322 [==========>] Loss 0.08308867038091784  - accuracy: 0.9375\n",
      "At: 323 [==========>] Loss 0.09759071908576498  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.17012306435318153  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.109855766866474  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.1806392691811927  - accuracy: 0.6875\n",
      "At: 327 [==========>] Loss 0.10157458460468326  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.09945666069366359  - accuracy: 0.90625\n",
      "At: 329 [==========>] Loss 0.13730208710867303  - accuracy: 0.8125\n",
      "At: 330 [==========>] Loss 0.1650007649452775  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.15116783006015522  - accuracy: 0.78125\n",
      "At: 332 [==========>] Loss 0.2577633844002565  - accuracy: 0.5625\n",
      "At: 333 [==========>] Loss 0.15738766964073417  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.09356716862660505  - accuracy: 0.90625\n",
      "At: 335 [==========>] Loss 0.091831815927341  - accuracy: 0.875\n",
      "At: 336 [==========>] Loss 0.10775069470104724  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.18932671258565553  - accuracy: 0.71875\n",
      "At: 338 [==========>] Loss 0.09534580128737058  - accuracy: 0.84375\n",
      "At: 339 [==========>] Loss 0.17512508981786873  - accuracy: 0.71875\n",
      "At: 340 [==========>] Loss 0.13436451731623178  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.12645836277422504  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.10064282873708669  - accuracy: 0.84375\n",
      "At: 343 [==========>] Loss 0.20330529230143277  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.18919652892317604  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.14358855894172845  - accuracy: 0.78125\n",
      "At: 346 [==========>] Loss 0.1257253057857354  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.09095913243479878  - accuracy: 0.90625\n",
      "At: 348 [==========>] Loss 0.12180306698721989  - accuracy: 0.875\n",
      "At: 349 [==========>] Loss 0.12396433055626471  - accuracy: 0.8125\n",
      "At: 350 [==========>] Loss 0.10234117847439672  - accuracy: 0.90625\n",
      "At: 351 [==========>] Loss 0.2446721910709111  - accuracy: 0.65625\n",
      "At: 352 [==========>] Loss 0.10000017245167847  - accuracy: 0.90625\n",
      "At: 353 [==========>] Loss 0.13744450022979549  - accuracy: 0.84375\n",
      "At: 354 [==========>] Loss 0.15781482586552284  - accuracy: 0.78125\n",
      "At: 355 [==========>] Loss 0.1258070762160352  - accuracy: 0.8125\n",
      "At: 356 [==========>] Loss 0.13978211611790603  - accuracy: 0.78125\n",
      "At: 357 [==========>] Loss 0.13056633968342482  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.12434966880783993  - accuracy: 0.8125\n",
      "At: 359 [==========>] Loss 0.08492796732790286  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.12684167500591884  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.09049415312268835  - accuracy: 0.875\n",
      "At: 362 [==========>] Loss 0.13420113235679632  - accuracy: 0.84375\n",
      "At: 363 [==========>] Loss 0.12452867567530264  - accuracy: 0.8125\n",
      "At: 364 [==========>] Loss 0.1767826312327489  - accuracy: 0.6875\n",
      "At: 365 [==========>] Loss 0.1259117141843109  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.19969891743400012  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.15606332680304785  - accuracy: 0.8125\n",
      "At: 368 [==========>] Loss 0.1565042189217566  - accuracy: 0.8125\n",
      "At: 369 [==========>] Loss 0.1632798551744112  - accuracy: 0.78125\n",
      "At: 370 [==========>] Loss 0.15491593787706892  - accuracy: 0.78125\n",
      "At: 371 [==========>] Loss 0.09553281847805706  - accuracy: 0.9375\n",
      "At: 372 [==========>] Loss 0.08766004638796798  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.1499001389973503  - accuracy: 0.75\n",
      "At: 374 [==========>] Loss 0.08396065765691493  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.14681743609050013  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.08999268492075625  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.17269462037525707  - accuracy: 0.78125\n",
      "At: 378 [==========>] Loss 0.1450984041305047  - accuracy: 0.71875\n",
      "At: 379 [==========>] Loss 0.13482460627811169  - accuracy: 0.84375\n",
      "At: 380 [==========>] Loss 0.17923383316645852  - accuracy: 0.78125\n",
      "At: 381 [==========>] Loss 0.15424305255764514  - accuracy: 0.78125\n",
      "At: 382 [==========>] Loss 0.06952930682975958  - accuracy: 0.9375\n",
      "At: 383 [==========>] Loss 0.21758455768686197  - accuracy: 0.6875\n",
      "At: 384 [==========>] Loss 0.14268909680153072  - accuracy: 0.84375\n",
      "At: 385 [==========>] Loss 0.11344969400353253  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.19898808547424884  - accuracy: 0.65625\n",
      "At: 387 [==========>] Loss 0.07968117050205287  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.19160191872779425  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.1473282733957185  - accuracy: 0.8125\n",
      "At: 390 [==========>] Loss 0.10244599395051875  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.09361321573817258  - accuracy: 0.875\n",
      "At: 392 [==========>] Loss 0.135901088310127  - accuracy: 0.78125\n",
      "At: 393 [==========>] Loss 0.17941502500160325  - accuracy: 0.71875\n",
      "At: 394 [==========>] Loss 0.09167163993914836  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.15460334412919405  - accuracy: 0.8125\n",
      "At: 396 [==========>] Loss 0.12853744287112834  - accuracy: 0.875\n",
      "At: 397 [==========>] Loss 0.11100127200029661  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.19441428871134825  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.15329044016253116  - accuracy: 0.75\n",
      "At: 400 [==========>] Loss 0.13583872339117414  - accuracy: 0.8125\n",
      "At: 401 [==========>] Loss 0.13727116435085007  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.13459812064017673  - accuracy: 0.78125\n",
      "At: 403 [==========>] Loss 0.06190761703627354  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.12135880783233327  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.16512895202108752  - accuracy: 0.78125\n",
      "At: 406 [==========>] Loss 0.10821609213524253  - accuracy: 0.84375\n",
      "At: 407 [==========>] Loss 0.15418133028643635  - accuracy: 0.71875\n",
      "At: 408 [==========>] Loss 0.22411003161248194  - accuracy: 0.625\n",
      "At: 409 [==========>] Loss 0.1965676740275934  - accuracy: 0.65625\n",
      "At: 410 [==========>] Loss 0.1221179733424545  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.0892749761889255  - accuracy: 0.9375\n",
      "At: 412 [==========>] Loss 0.16033524379502878  - accuracy: 0.65625\n",
      "At: 413 [==========>] Loss 0.07306959151938355  - accuracy: 0.90625\n",
      "At: 414 [==========>] Loss 0.1436028320184678  - accuracy: 0.78125\n",
      "At: 415 [==========>] Loss 0.10463132155012156  - accuracy: 0.875\n",
      "At: 416 [==========>] Loss 0.20261315845890937  - accuracy: 0.6875\n",
      "At: 417 [==========>] Loss 0.1643158724242326  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.13352249728451596  - accuracy: 0.84375\n",
      "At: 419 [==========>] Loss 0.10136941581142864  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.15699615065197287  - accuracy: 0.8125\n",
      "At: 421 [==========>] Loss 0.11093589418556911  - accuracy: 0.875\n",
      "At: 422 [==========>] Loss 0.160577212798423  - accuracy: 0.8125\n",
      "At: 423 [==========>] Loss 0.14298431367525583  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.18123636940778837  - accuracy: 0.71875\n",
      "At: 425 [==========>] Loss 0.19254154041240762  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.12206911677153176  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.1906326277087495  - accuracy: 0.78125\n",
      "At: 428 [==========>] Loss 0.23728569037941866  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.20042994073709514  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.1141174462383424  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.12947866253352808  - accuracy: 0.78125\n",
      "At: 432 [==========>] Loss 0.10898398327332934  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.11885436218735429  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.10769888593608334  - accuracy: 0.875\n",
      "At: 435 [==========>] Loss 0.18318525199427219  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.15770711096407158  - accuracy: 0.8125\n",
      "At: 437 [==========>] Loss 0.10134271069364466  - accuracy: 0.875\n",
      "At: 438 [==========>] Loss 0.12023994485365251  - accuracy: 0.875\n",
      "At: 439 [==========>] Loss 0.10819305498304842  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.06122670214650368  - accuracy: 0.96875\n",
      "At: 441 [==========>] Loss 0.17471825659030796  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.13683385592702327  - accuracy: 0.84375\n",
      "At: 443 [==========>] Loss 0.14503957306527765  - accuracy: 0.78125\n",
      "At: 444 [==========>] Loss 0.16599447239862675  - accuracy: 0.78125\n",
      "At: 445 [==========>] Loss 0.16169869381329702  - accuracy: 0.71875\n",
      "At: 446 [==========>] Loss 0.21287034645516267  - accuracy: 0.71875\n",
      "At: 447 [==========>] Loss 0.1395712291289723  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.1529184338529849  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.14572177072019926  - accuracy: 0.78125\n",
      "At: 450 [==========>] Loss 0.08266511301267178  - accuracy: 0.9375\n",
      "At: 451 [==========>] Loss 0.12375305349816643  - accuracy: 0.8125\n",
      "At: 452 [==========>] Loss 0.12928410522135989  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.15155196838076995  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.17438822641360002  - accuracy: 0.75\n",
      "At: 455 [==========>] Loss 0.17846518839067038  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.1480985889082405  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.11843651494355137  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.06005890096707242  - accuracy: 0.9375\n",
      "At: 459 [==========>] Loss 0.16909872780626498  - accuracy: 0.75\n",
      "At: 460 [==========>] Loss 0.10245767455950666  - accuracy: 0.90625\n",
      "At: 461 [==========>] Loss 0.16750943280716885  - accuracy: 0.75\n",
      "At: 462 [==========>] Loss 0.15512584148301115  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.159070363622569  - accuracy: 0.78125\n",
      "At: 464 [==========>] Loss 0.16694816924615036  - accuracy: 0.78125\n",
      "At: 465 [==========>] Loss 0.1400675166424396  - accuracy: 0.78125\n",
      "At: 466 [==========>] Loss 0.0916848033701329  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.16621803819597947  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.07434792378938682  - accuracy: 0.90625\n",
      "At: 469 [==========>] Loss 0.12705031664699543  - accuracy: 0.84375\n",
      "At: 470 [==========>] Loss 0.1209062160796304  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.1824053457151925  - accuracy: 0.71875\n",
      "At: 472 [==========>] Loss 0.12571888716196705  - accuracy: 0.84375\n",
      "At: 473 [==========>] Loss 0.14201464115904516  - accuracy: 0.84375\n",
      "At: 474 [==========>] Loss 0.17892794088195285  - accuracy: 0.75\n",
      "At: 475 [==========>] Loss 0.1485905127869259  - accuracy: 0.78125\n",
      "At: 476 [==========>] Loss 0.15053549240193326  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.09366360440708457  - accuracy: 0.90625\n",
      "At: 478 [==========>] Loss 0.11231538665237796  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.13149079715066814  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.18888842480016124  - accuracy: 0.78125\n",
      "At: 481 [==========>] Loss 0.10947485846395305  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.08035025706371486  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.1057065560826078  - accuracy: 0.84375\n",
      "At: 484 [==========>] Loss 0.10790969218779371  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.12280785561118324  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.17099296588299717  - accuracy: 0.8125\n",
      "At: 487 [==========>] Loss 0.13337413577566148  - accuracy: 0.875\n",
      "At: 488 [==========>] Loss 0.12228316627797772  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.1514152026298311  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.11237596761155982  - accuracy: 0.84375\n",
      "At: 491 [==========>] Loss 0.13683504739126082  - accuracy: 0.8125\n",
      "At: 492 [==========>] Loss 0.18364596299443264  - accuracy: 0.75\n",
      "At: 493 [==========>] Loss 0.12940529837854692  - accuracy: 0.8125\n",
      "At: 494 [==========>] Loss 0.14055376621825924  - accuracy: 0.8125\n",
      "At: 495 [==========>] Loss 0.1196256386559715  - accuracy: 0.875\n",
      "At: 496 [==========>] Loss 0.14329617791607846  - accuracy: 0.8125\n",
      "At: 497 [==========>] Loss 0.14222484973484817  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.07255238922505639  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.18656777128586138  - accuracy: 0.75\n",
      "At: 500 [==========>] Loss 0.08558014992553731  - accuracy: 0.9375\n",
      "At: 501 [==========>] Loss 0.15629766033027695  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.1254827569070658  - accuracy: 0.8125\n",
      "At: 503 [==========>] Loss 0.11958009167901365  - accuracy: 0.8125\n",
      "At: 504 [==========>] Loss 0.11172092230457394  - accuracy: 0.84375\n",
      "At: 505 [==========>] Loss 0.18021035248188066  - accuracy: 0.71875\n",
      "At: 506 [==========>] Loss 0.2359880775561009  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.11598355428289994  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.09455098618246124  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.13167851261386748  - accuracy: 0.78125\n",
      "At: 510 [==========>] Loss 0.18562555555682936  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.1268231803845073  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.18364163706335168  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.18768338888059902  - accuracy: 0.71875\n",
      "At: 514 [==========>] Loss 0.1352078845735718  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.10358929984102896  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.1982176871284611  - accuracy: 0.71875\n",
      "At: 517 [==========>] Loss 0.11016714504219569  - accuracy: 0.875\n",
      "At: 518 [==========>] Loss 0.12950182898206428  - accuracy: 0.90625\n",
      "At: 519 [==========>] Loss 0.12402353301110026  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.14580218013005802  - accuracy: 0.78125\n",
      "At: 521 [==========>] Loss 0.13639717186824213  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.12251670424511871  - accuracy: 0.875\n",
      "At: 523 [==========>] Loss 0.11429045580603299  - accuracy: 0.84375\n",
      "At: 524 [==========>] Loss 0.07043284046828885  - accuracy: 0.9375\n",
      "At: 525 [==========>] Loss 0.11835258046989579  - accuracy: 0.875\n",
      "At: 526 [==========>] Loss 0.17675587186733416  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.18906806375730612  - accuracy: 0.71875\n",
      "At: 528 [==========>] Loss 0.17725491508576297  - accuracy: 0.78125\n",
      "At: 529 [==========>] Loss 0.1150392627925248  - accuracy: 0.8125\n",
      "At: 530 [==========>] Loss 0.18789666818368567  - accuracy: 0.71875\n",
      "At: 531 [==========>] Loss 0.15069211723735448  - accuracy: 0.8125\n",
      "At: 532 [==========>] Loss 0.1383989415615822  - accuracy: 0.75\n",
      "At: 533 [==========>] Loss 0.08227714532249475  - accuracy: 0.90625\n",
      "At: 534 [==========>] Loss 0.14887265458861987  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.13090862542782472  - accuracy: 0.75\n",
      "At: 536 [==========>] Loss 0.14688051113534967  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.08311807534532843  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.16881301658100556  - accuracy: 0.8125\n",
      "At: 539 [==========>] Loss 0.107671639303064  - accuracy: 0.78125\n",
      "At: 540 [==========>] Loss 0.24157330770267815  - accuracy: 0.6875\n",
      "At: 541 [==========>] Loss 0.14537387258088724  - accuracy: 0.78125\n",
      "At: 542 [==========>] Loss 0.11984083441229895  - accuracy: 0.84375\n",
      "At: 543 [==========>] Loss 0.14461358637732089  - accuracy: 0.8125\n",
      "At: 544 [==========>] Loss 0.18279918271711443  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.10152268867604403  - accuracy: 0.84375\n",
      "At: 546 [==========>] Loss 0.15423326363320028  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.10540827144102953  - accuracy: 0.84375\n",
      "At: 548 [==========>] Loss 0.09388904937112381  - accuracy: 0.90625\n",
      "At: 549 [==========>] Loss 0.1349492079414426  - accuracy: 0.78125\n",
      "At: 550 [==========>] Loss 0.11092947897188396  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.12664838202671688  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.13212604691674307  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.09824595591701443  - accuracy: 0.90625\n",
      "At: 554 [==========>] Loss 0.11868926369791984  - accuracy: 0.84375\n",
      "At: 555 [==========>] Loss 0.11752199741535943  - accuracy: 0.875\n",
      "At: 556 [==========>] Loss 0.15679810754086  - accuracy: 0.75\n",
      "At: 557 [==========>] Loss 0.1156924498654417  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.12968700761490942  - accuracy: 0.84375\n",
      "At: 559 [==========>] Loss 0.18435704205500392  - accuracy: 0.75\n",
      "At: 560 [==========>] Loss 0.11555638042882438  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.13124045622363698  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.06756299044196945  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.10141601585353767  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.14037294237911768  - accuracy: 0.78125\n",
      "At: 565 [==========>] Loss 0.1333034943369932  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.13418041357624697  - accuracy: 0.8125\n",
      "At: 567 [==========>] Loss 0.156196719017715  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.21830822300682717  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.18661700518459862  - accuracy: 0.78125\n",
      "At: 570 [==========>] Loss 0.08723155987308254  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.13302294581410887  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.1215936576328828  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.10244244052633389  - accuracy: 0.84375\n",
      "At: 574 [==========>] Loss 0.13337927444440895  - accuracy: 0.78125\n",
      "At: 575 [==========>] Loss 0.10366642654196356  - accuracy: 0.875\n",
      "At: 576 [==========>] Loss 0.09129124995911686  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.1739824718649403  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.17452264051537297  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.10749055822863886  - accuracy: 0.90625\n",
      "At: 580 [==========>] Loss 0.12169948226504734  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.10820392508323688  - accuracy: 0.875\n",
      "At: 582 [==========>] Loss 0.13266588025306375  - accuracy: 0.84375\n",
      "At: 583 [==========>] Loss 0.18324624905933193  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.08361387715800701  - accuracy: 0.9375\n",
      "At: 585 [==========>] Loss 0.12485437848041771  - accuracy: 0.78125\n",
      "At: 586 [==========>] Loss 0.09094344329931123  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.12190380673852877  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.13991396282805646  - accuracy: 0.75\n",
      "At: 589 [==========>] Loss 0.17515838345530663  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.06813472556070986  - accuracy: 0.9375\n",
      "At: 591 [==========>] Loss 0.1525261707060237  - accuracy: 0.78125\n",
      "At: 592 [==========>] Loss 0.1148726617300299  - accuracy: 0.875\n",
      "At: 593 [==========>] Loss 0.14091354588790075  - accuracy: 0.84375\n",
      "At: 594 [==========>] Loss 0.1426491993441855  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.13130014027051318  - accuracy: 0.875\n",
      "At: 596 [==========>] Loss 0.13814431908394137  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.22030045712347454  - accuracy: 0.6875\n",
      "At: 598 [==========>] Loss 0.1530152831214266  - accuracy: 0.78125\n",
      "At: 599 [==========>] Loss 0.12980683989801575  - accuracy: 0.8125\n",
      "At: 600 [==========>] Loss 0.10784057153991523  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.11102690871295035  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.10923412111791189  - accuracy: 0.84375\n",
      "At: 603 [==========>] Loss 0.1183337609560001  - accuracy: 0.84375\n",
      "At: 604 [==========>] Loss 0.17845848398854539  - accuracy: 0.78125\n",
      "At: 605 [==========>] Loss 0.12147321036406843  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.15735241476872042  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.1492121679833535  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.11298514565431204  - accuracy: 0.875\n",
      "At: 609 [==========>] Loss 0.1246120019850169  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.16021541008763973  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.10728270994296521  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.14724614504606076  - accuracy: 0.875\n",
      "At: 613 [==========>] Loss 0.13717648179036507  - accuracy: 0.8125\n",
      "At: 614 [==========>] Loss 0.11032060617396415  - accuracy: 0.90625\n",
      "At: 615 [==========>] Loss 0.14464187551753066  - accuracy: 0.8125\n",
      "At: 616 [==========>] Loss 0.1565780569755683  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.1374382599118827  - accuracy: 0.75\n",
      "At: 618 [==========>] Loss 0.2055620611220434  - accuracy: 0.75\n",
      "At: 619 [==========>] Loss 0.15306754674048179  - accuracy: 0.78125\n",
      "At: 620 [==========>] Loss 0.1755465767474304  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.07466811008045313  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.19082344169411472  - accuracy: 0.6875\n",
      "At: 623 [==========>] Loss 0.1232732001081876  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.10543558598068363  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.11884018569101389  - accuracy: 0.90625\n",
      "At: 626 [==========>] Loss 0.11183029025281811  - accuracy: 0.875\n",
      "At: 627 [==========>] Loss 0.13260403033745574  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.13763795659772027  - accuracy: 0.78125\n",
      "At: 629 [==========>] Loss 0.16429876257903492  - accuracy: 0.6875\n",
      "At: 630 [==========>] Loss 0.22515321437643487  - accuracy: 0.65625\n",
      "At: 631 [==========>] Loss 0.1903341924904758  - accuracy: 0.71875\n",
      "At: 632 [==========>] Loss 0.1253347130946092  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.1386490281465366  - accuracy: 0.78125\n",
      "At: 634 [==========>] Loss 0.1440099223542756  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.12703851425042176  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.1491845207445726  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.11495147539367831  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.12126723073538798  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.15356110762622538  - accuracy: 0.78125\n",
      "At: 640 [==========>] Loss 0.1661889110488013  - accuracy: 0.75\n",
      "At: 641 [==========>] Loss 0.12334464457865163  - accuracy: 0.84375\n",
      "At: 642 [==========>] Loss 0.167869793689144  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.09941490352887696  - accuracy: 0.8125\n",
      "At: 644 [==========>] Loss 0.07400649856836589  - accuracy: 0.84375\n",
      "At: 645 [==========>] Loss 0.09800584832393483  - accuracy: 0.90625\n",
      "At: 646 [==========>] Loss 0.14524496944331217  - accuracy: 0.78125\n",
      "At: 647 [==========>] Loss 0.18584611488669855  - accuracy: 0.8125\n",
      "At: 648 [==========>] Loss 0.14479292381121694  - accuracy: 0.8125\n",
      "At: 649 [==========>] Loss 0.1651065772411477  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.09580925073688495  - accuracy: 0.90625\n",
      "At: 651 [==========>] Loss 0.18520882859884732  - accuracy: 0.75\n",
      "At: 652 [==========>] Loss 0.09980197645430061  - accuracy: 0.8125\n",
      "At: 653 [==========>] Loss 0.1105156619411877  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.12331053825717875  - accuracy: 0.8125\n",
      "At: 655 [==========>] Loss 0.1035122006549263  - accuracy: 0.90625\n",
      "At: 656 [==========>] Loss 0.12926972574334836  - accuracy: 0.8125\n",
      "At: 657 [==========>] Loss 0.091854552148587  - accuracy: 0.875\n",
      "At: 658 [==========>] Loss 0.12164038340650236  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.12946735818305508  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.10563933094358618  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.15786615673208482  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.11594256659953366  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.08427218084448634  - accuracy: 0.84375\n",
      "At: 664 [==========>] Loss 0.110714332236682  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.13712579109491332  - accuracy: 0.8125\n",
      "At: 666 [==========>] Loss 0.1709385096906601  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.12092965338833166  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.1618414808068251  - accuracy: 0.78125\n",
      "At: 669 [==========>] Loss 0.14682111821116767  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.18551283967349325  - accuracy: 0.71875\n",
      "At: 671 [==========>] Loss 0.1285601487716546  - accuracy: 0.84375\n",
      "At: 672 [==========>] Loss 0.1165896390571978  - accuracy: 0.875\n",
      "At: 673 [==========>] Loss 0.06056921254051891  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.09576503066110714  - accuracy: 0.84375\n",
      "At: 675 [==========>] Loss 0.09535400661247612  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.13116942712568305  - accuracy: 0.84375\n",
      "At: 677 [==========>] Loss 0.15471934211457028  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.12990606079475508  - accuracy: 0.84375\n",
      "At: 679 [==========>] Loss 0.10447412918351652  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.11236590766567497  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.12717864390605982  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.138188280585732  - accuracy: 0.8125\n",
      "At: 683 [==========>] Loss 0.13853783360322797  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.07094800686683445  - accuracy: 0.96875\n",
      "At: 685 [==========>] Loss 0.13393001065438678  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.09868261076756014  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.06382893519192112  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.07834727442422848  - accuracy: 0.9375\n",
      "At: 689 [==========>] Loss 0.1531664034403625  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.11640031199342724  - accuracy: 0.84375\n",
      "At: 691 [==========>] Loss 0.10179715183897346  - accuracy: 0.875\n",
      "At: 692 [==========>] Loss 0.09310256606162512  - accuracy: 0.84375\n",
      "At: 693 [==========>] Loss 0.13996702098850458  - accuracy: 0.84375\n",
      "At: 694 [==========>] Loss 0.16381479735497984  - accuracy: 0.75\n",
      "At: 695 [==========>] Loss 0.13467385117149677  - accuracy: 0.8125\n",
      "At: 696 [==========>] Loss 0.1589398178555283  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.1693199964546716  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.10762924489712623  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.125895644638522  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.09188806376155062  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.13615896811638373  - accuracy: 0.75\n",
      "At: 702 [==========>] Loss 0.09010688485797112  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.19510922076707216  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.1636931338299604  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.20821945362877223  - accuracy: 0.6875\n",
      "At: 706 [==========>] Loss 0.15792642923628453  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.09124905463771486  - accuracy: 0.875\n",
      "At: 708 [==========>] Loss 0.16809604771270076  - accuracy: 0.78125\n",
      "At: 709 [==========>] Loss 0.1614673795183417  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.1586808269808263  - accuracy: 0.75\n",
      "At: 711 [==========>] Loss 0.1864304067257385  - accuracy: 0.71875\n",
      "At: 712 [==========>] Loss 0.15873386070808815  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.13766712624505553  - accuracy: 0.84375\n",
      "At: 714 [==========>] Loss 0.20305361416225795  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.12384832289486164  - accuracy: 0.8125\n",
      "At: 716 [==========>] Loss 0.11276580356667346  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.11252513285923949  - accuracy: 0.875\n",
      "At: 718 [==========>] Loss 0.16810530625209025  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.07051988233965084  - accuracy: 0.90625\n",
      "At: 720 [==========>] Loss 0.12124357229272936  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.11310019894078036  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.14001072671771353  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.1317981854432436  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.0806261959186812  - accuracy: 0.90625\n",
      "At: 725 [==========>] Loss 0.13914184276385408  - accuracy: 0.84375\n",
      "At: 726 [==========>] Loss 0.17870773811369023  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.16029583684616877  - accuracy: 0.78125\n",
      "At: 728 [==========>] Loss 0.14468985760019876  - accuracy: 0.8125\n",
      "At: 729 [==========>] Loss 0.17412093952128083  - accuracy: 0.71875\n",
      "At: 730 [==========>] Loss 0.13604961667578214  - accuracy: 0.78125\n",
      "At: 731 [==========>] Loss 0.1675455042492931  - accuracy: 0.75\n",
      "At: 732 [==========>] Loss 0.18236286810376479  - accuracy: 0.78125\n",
      "At: 733 [==========>] Loss 0.09561497421476228  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.20257236209465798  - accuracy: 0.6875\n",
      "At: 735 [==========>] Loss 0.119891671669731  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.1001799730933715  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.13146336799848918  - accuracy: 0.8125\n",
      "At: 738 [==========>] Loss 0.06552327945942817  - accuracy: 0.9375\n",
      "At: 739 [==========>] Loss 0.05724566095353777  - accuracy: 0.9375\n",
      "At: 740 [==========>] Loss 0.1452502650467613  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.11412026461615518  - accuracy: 0.8125\n",
      "At: 742 [==========>] Loss 0.12727756798654874  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.12330591128678656  - accuracy: 0.875\n",
      "At: 744 [==========>] Loss 0.14134828984774905  - accuracy: 0.78125\n",
      "At: 745 [==========>] Loss 0.15500775104883813  - accuracy: 0.84375\n",
      "At: 746 [==========>] Loss 0.13885492726924267  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.15718526152027917  - accuracy: 0.78125\n",
      "At: 748 [==========>] Loss 0.12470095515526441  - accuracy: 0.84375\n",
      "At: 749 [==========>] Loss 0.17343531826758918  - accuracy: 0.71875\n",
      "At: 750 [==========>] Loss 0.12019392815387164  - accuracy: 0.90625\n",
      "At: 751 [==========>] Loss 0.15063147251173495  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.07097223233488903  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.19599934680041858  - accuracy: 0.65625\n",
      "At: 754 [==========>] Loss 0.16286751309634528  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.09193799946603123  - accuracy: 0.90625\n",
      "At: 756 [==========>] Loss 0.2004547489600753  - accuracy: 0.75\n",
      "At: 757 [==========>] Loss 0.08777771876918425  - accuracy: 0.90625\n",
      "At: 758 [==========>] Loss 0.09406671826441021  - accuracy: 0.875\n",
      "At: 759 [==========>] Loss 0.08144017179892443  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.18803874097538145  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.1458647383067543  - accuracy: 0.84375\n",
      "At: 762 [==========>] Loss 0.11739510388441395  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.1256681366935557  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.09671479846125622  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.11431675954436772  - accuracy: 0.84375\n",
      "At: 766 [==========>] Loss 0.09390055912188085  - accuracy: 0.875\n",
      "At: 767 [==========>] Loss 0.13539602041359589  - accuracy: 0.8125\n",
      "At: 768 [==========>] Loss 0.15799924926169967  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.12304137537276899  - accuracy: 0.875\n",
      "At: 770 [==========>] Loss 0.10544907522759508  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.16558096078915974  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.12082440953247721  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.08812245404082625  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.1181061618299615  - accuracy: 0.84375\n",
      "At: 775 [==========>] Loss 0.1741687981056723  - accuracy: 0.75\n",
      "At: 776 [==========>] Loss 0.16662952869877568  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.0913628634214408  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.15346721622814682  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.11671645721747131  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.07575544907629667  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.15323185947524917  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.15402778547913965  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.1889451554717455  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.17193865309216738  - accuracy: 0.71875\n",
      "At: 785 [==========>] Loss 0.194806015796441  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.12534045955806733  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.14991832673194755  - accuracy: 0.8125\n",
      "At: 788 [==========>] Loss 0.10259787257879815  - accuracy: 0.8125\n",
      "At: 789 [==========>] Loss 0.1466396393638994  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.12578025511356417  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.14458522938527152  - accuracy: 0.84375\n",
      "At: 792 [==========>] Loss 0.16184155420972904  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.10883365648950537  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.10935764367324313  - accuracy: 0.875\n",
      "At: 795 [==========>] Loss 0.07724466243474763  - accuracy: 0.90625\n",
      "At: 796 [==========>] Loss 0.1180069539228715  - accuracy: 0.875\n",
      "At: 797 [==========>] Loss 0.15722070783791336  - accuracy: 0.75\n",
      "At: 798 [==========>] Loss 0.13658827662530665  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.0769051817304525  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.14313989755884118  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.1006925155734288  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.16740515894444352  - accuracy: 0.8125\n",
      "At: 803 [==========>] Loss 0.14753851503636262  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.12434202516798862  - accuracy: 0.875\n",
      "At: 805 [==========>] Loss 0.1347376961814033  - accuracy: 0.84375\n",
      "At: 806 [==========>] Loss 0.10047326865665059  - accuracy: 0.8125\n",
      "At: 807 [==========>] Loss 0.0827542524716327  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.10149398565952311  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.11433405766496996  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.14126704948835864  - accuracy: 0.84375\n",
      "At: 811 [==========>] Loss 0.11580549089792815  - accuracy: 0.875\n",
      "At: 812 [==========>] Loss 0.12289086195835773  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.18739752340562665  - accuracy: 0.6875\n",
      "At: 814 [==========>] Loss 0.16829069660585885  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.14226246717808955  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.12853630453483697  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.05904342683647065  - accuracy: 0.96875\n",
      "At: 818 [==========>] Loss 0.1267058462847085  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.12133796944898803  - accuracy: 0.875\n",
      "At: 820 [==========>] Loss 0.12096732293984681  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.15400152693764702  - accuracy: 0.8125\n",
      "At: 822 [==========>] Loss 0.17332545231336938  - accuracy: 0.8125\n",
      "At: 823 [==========>] Loss 0.15070344716185693  - accuracy: 0.84375\n",
      "At: 824 [==========>] Loss 0.1450568676320244  - accuracy: 0.84375\n",
      "At: 825 [==========>] Loss 0.20507061312494176  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.10321315218417879  - accuracy: 0.84375\n",
      "At: 827 [==========>] Loss 0.09067183826516231  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.05513323156364899  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.07925211648758759  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.09724007311972732  - accuracy: 0.875\n",
      "At: 831 [==========>] Loss 0.07185025801492372  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.08343617191265616  - accuracy: 0.875\n",
      "At: 833 [==========>] Loss 0.1941205174704561  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.11989103718656682  - accuracy: 0.75\n",
      "At: 835 [==========>] Loss 0.06758972374062125  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.11136525707662898  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.11274342759991478  - accuracy: 0.875\n",
      "At: 838 [==========>] Loss 0.10273180706303134  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.07771738672593496  - accuracy: 0.875\n",
      "At: 840 [==========>] Loss 0.10446971923205144  - accuracy: 0.90625\n",
      "At: 841 [==========>] Loss 0.05611660217103914  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.07251245585106465  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.1612538354318711  - accuracy: 0.78125\n",
      "At: 844 [==========>] Loss 0.10694512208755169  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.118953962727249  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.14073430994478192  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.0885081181259411  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.16246605664958022  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.14327231507730143  - accuracy: 0.8125\n",
      "At: 850 [==========>] Loss 0.08615239421392279  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.09216255947331913  - accuracy: 0.90625\n",
      "At: 852 [==========>] Loss 0.11499296504657489  - accuracy: 0.84375\n",
      "At: 853 [==========>] Loss 0.14119517224741882  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.22883906870358853  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.09020647720131963  - accuracy: 0.84375\n",
      "At: 856 [==========>] Loss 0.07662546238498376  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.06279496467988999  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.2696151880529582  - accuracy: 0.59375\n",
      "At: 859 [==========>] Loss 0.11310901766819492  - accuracy: 0.8125\n",
      "At: 860 [==========>] Loss 0.11327905302373177  - accuracy: 0.875\n",
      "At: 861 [==========>] Loss 0.09444019591811247  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.09624844280608565  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.16212273619546358  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.17155790126544807  - accuracy: 0.75\n",
      "At: 865 [==========>] Loss 0.17372072657877702  - accuracy: 0.75\n",
      "At: 866 [==========>] Loss 0.16033770559487298  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.07291020981861976  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.1783735400764701  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.1630732837835243  - accuracy: 0.8125\n",
      "At: 870 [==========>] Loss 0.1407231352022698  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.06160162297513352  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.0879786354009058  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.18529670946245413  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.16039249934421096  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.11932340924799384  - accuracy: 0.90625\n",
      "At: 876 [==========>] Loss 0.12500612703963226  - accuracy: 0.78125\n",
      "At: 877 [==========>] Loss 0.15758174742401054  - accuracy: 0.75\n",
      "At: 878 [==========>] Loss 0.05229125413821407  - accuracy: 1.0\n",
      "At: 879 [==========>] Loss 0.1657586932072821  - accuracy: 0.6875\n",
      "At: 880 [==========>] Loss 0.11002893666706762  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.16340135481707638  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.09310031311489851  - accuracy: 0.90625\n",
      "At: 883 [==========>] Loss 0.11257472281896927  - accuracy: 0.84375\n",
      "At: 884 [==========>] Loss 0.12865795231428032  - accuracy: 0.8125\n",
      "At: 885 [==========>] Loss 0.10980488010568831  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.11402597200074721  - accuracy: 0.8125\n",
      "At: 887 [==========>] Loss 0.1484089156916802  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.15774840675429835  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.09148302786315385  - accuracy: 0.90625\n",
      "At: 890 [==========>] Loss 0.14189197467034412  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.10990255135538325  - accuracy: 0.84375\n",
      "At: 892 [==========>] Loss 0.12898223211866286  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.14843762571437366  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.15115269446226742  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.10817946400673764  - accuracy: 0.90625\n",
      "At: 896 [==========>] Loss 0.11484612610986666  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.15084838124309516  - accuracy: 0.84375\n",
      "At: 898 [==========>] Loss 0.11206658912212762  - accuracy: 0.84375\n",
      "At: 899 [==========>] Loss 0.09209058571787689  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.17339677652461155  - accuracy: 0.75\n",
      "At: 901 [==========>] Loss 0.1556533606090688  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.09663646816387805  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.12920059570951817  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.11441561254795532  - accuracy: 0.84375\n",
      "At: 905 [==========>] Loss 0.04757668223628224  - accuracy: 0.96875\n",
      "At: 906 [==========>] Loss 0.08525032889523039  - accuracy: 0.9375\n",
      "At: 907 [==========>] Loss 0.12220050603780179  - accuracy: 0.84375\n",
      "At: 908 [==========>] Loss 0.108245022459997  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.08973395672208073  - accuracy: 0.90625\n",
      "At: 910 [==========>] Loss 0.11573382293700302  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.1363786887037458  - accuracy: 0.84375\n",
      "At: 912 [==========>] Loss 0.1498512125636763  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.10739709368608995  - accuracy: 0.84375\n",
      "At: 914 [==========>] Loss 0.10167933610838287  - accuracy: 0.90625\n",
      "At: 915 [==========>] Loss 0.13240372106098042  - accuracy: 0.84375\n",
      "At: 916 [==========>] Loss 0.18721989985687343  - accuracy: 0.6875\n",
      "At: 917 [==========>] Loss 0.16245754876854868  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.18760082864443717  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.14086257147431722  - accuracy: 0.71875\n",
      "At: 920 [==========>] Loss 0.12338544202911036  - accuracy: 0.875\n",
      "At: 921 [==========>] Loss 0.1283877846115554  - accuracy: 0.84375\n",
      "At: 922 [==========>] Loss 0.1619175166120908  - accuracy: 0.78125\n",
      "At: 923 [==========>] Loss 0.11617843300968372  - accuracy: 0.84375\n",
      "At: 924 [==========>] Loss 0.17149431144247895  - accuracy: 0.8125\n",
      "At: 925 [==========>] Loss 0.19438487085482903  - accuracy: 0.78125\n",
      "At: 926 [==========>] Loss 0.11003852114177386  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.11203455521174224  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.13250375375117068  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.14591228398051062  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.09525294589980697  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.17962925078657915  - accuracy: 0.75\n",
      "At: 932 [==========>] Loss 0.0880251128463104  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.08779682697214541  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.14415671018233306  - accuracy: 0.8125\n",
      "At: 935 [==========>] Loss 0.0492049451934054  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.1432067742691542  - accuracy: 0.8125\n",
      "At: 937 [==========>] Loss 0.15575545371835064  - accuracy: 0.8125\n",
      "At: 938 [==========>] Loss 0.10814136884925356  - accuracy: 0.84375\n",
      "At: 939 [==========>] Loss 0.10619803903606398  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.1910765244375951  - accuracy: 0.75\n",
      "At: 941 [==========>] Loss 0.1034863473106232  - accuracy: 0.90625\n",
      "At: 942 [==========>] Loss 0.12897320167727344  - accuracy: 0.84375\n",
      "At: 943 [==========>] Loss 0.09918615714646299  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.09964939194183002  - accuracy: 0.90625\n",
      "At: 945 [==========>] Loss 0.09104108350727486  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.12189225663923005  - accuracy: 0.8125\n",
      "At: 947 [==========>] Loss 0.14267698642799156  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.1883258230521852  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.08058906962986413  - accuracy: 0.84375\n",
      "At: 950 [==========>] Loss 0.11650355966638079  - accuracy: 0.90625\n",
      "At: 951 [==========>] Loss 0.0938345575473036  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.0666958374975915  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.0637327048938649  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.10834472452083443  - accuracy: 0.8125\n",
      "At: 955 [==========>] Loss 0.11744886280489042  - accuracy: 0.875\n",
      "At: 956 [==========>] Loss 0.06907842839485398  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.1316599917711125  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.07579436445821632  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.11712376046474149  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.14118610975807683  - accuracy: 0.78125\n",
      "At: 961 [==========>] Loss 0.12030423677237215  - accuracy: 0.875\n",
      "At: 962 [==========>] Loss 0.0891869232336008  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.09486019071170934  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.16184856807448014  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.12311127653356259  - accuracy: 0.875\n",
      "At: 966 [==========>] Loss 0.1300004261329483  - accuracy: 0.84375\n",
      "At: 967 [==========>] Loss 0.13705575343317344  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.16085349108011718  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.12979627868421745  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.0968172979504549  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.12592569331037298  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.06720929669641869  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.13815381394208828  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.06771518010127188  - accuracy: 0.96875\n",
      "At: 975 [==========>] Loss 0.1194721458979128  - accuracy: 0.84375\n",
      "At: 976 [==========>] Loss 0.08678434476878147  - accuracy: 0.875\n",
      "At: 977 [==========>] Loss 0.09642753193531768  - accuracy: 0.8125\n",
      "At: 978 [==========>] Loss 0.16581314496931582  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.10164392693721935  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.16209194508446842  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.16089800203222354  - accuracy: 0.8125\n",
      "At: 982 [==========>] Loss 0.05985218742435869  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.09923241787796397  - accuracy: 0.8125\n",
      "At: 984 [==========>] Loss 0.09657370668869489  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.18963397493280992  - accuracy: 0.71875\n",
      "At: 986 [==========>] Loss 0.10932120711771917  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.13810883243824862  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.09904211871874166  - accuracy: 0.8125\n",
      "At: 989 [==========>] Loss 0.12895661840223738  - accuracy: 0.875\n",
      "At: 990 [==========>] Loss 0.14602201668402026  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.1257920222512638  - accuracy: 0.84375\n",
      "At: 992 [==========>] Loss 0.1789050183358546  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.12101100929663272  - accuracy: 0.875\n",
      "At: 994 [==========>] Loss 0.12564971418735463  - accuracy: 0.78125\n",
      "At: 995 [==========>] Loss 0.141673128689722  - accuracy: 0.8125\n",
      "At: 996 [==========>] Loss 0.07145694899972166  - accuracy: 0.84375\n",
      "At: 997 [==========>] Loss 0.12035577603765364  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.11843097636423972  - accuracy: 0.8125\n",
      "At: 999 [==========>] Loss 0.1326063425869113  - accuracy: 0.84375\n",
      "At: 1000 [==========>] Loss 0.1690998369070223  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.15673164524695513  - accuracy: 0.75\n",
      "At: 1002 [==========>] Loss 0.18419978835486012  - accuracy: 0.8125\n",
      "At: 1003 [==========>] Loss 0.12492441673307772  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.10987346414570387  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.06155039001093002  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.08060428119626113  - accuracy: 0.9375\n",
      "At: 1007 [==========>] Loss 0.13322772753629947  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.1608682195559533  - accuracy: 0.8125\n",
      "At: 1009 [==========>] Loss 0.1000444686590427  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.14017441542951714  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.1442298050111549  - accuracy: 0.75\n",
      "At: 1012 [==========>] Loss 0.0917691332535553  - accuracy: 0.875\n",
      "At: 1013 [==========>] Loss 0.07465795825220856  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.08598389562481006  - accuracy: 0.9375\n",
      "At: 1015 [==========>] Loss 0.16903741164214742  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.12127886754809619  - accuracy: 0.84375\n",
      "At: 1017 [==========>] Loss 0.17350247969963922  - accuracy: 0.71875\n",
      "At: 1018 [==========>] Loss 0.1543184584497669  - accuracy: 0.71875\n",
      "At: 1019 [==========>] Loss 0.20418230164059192  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.13048533497181322  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.1164282150057803  - accuracy: 0.84375\n",
      "At: 1022 [==========>] Loss 0.11474641027460289  - accuracy: 0.84375\n",
      "At: 1023 [==========>] Loss 0.16893801784337031  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.1972077321989506  - accuracy: 0.6875\n",
      "At: 1025 [==========>] Loss 0.18341549132824062  - accuracy: 0.75\n",
      "At: 1026 [==========>] Loss 0.13920749854509235  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.143431932956868  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.21065081868432373  - accuracy: 0.71875\n",
      "At: 1029 [==========>] Loss 0.0961501708508952  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.10602009275371671  - accuracy: 0.875\n",
      "At: 1031 [==========>] Loss 0.14801848409661839  - accuracy: 0.8125\n",
      "At: 1032 [==========>] Loss 0.11647543588156119  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.12508652146379612  - accuracy: 0.84375\n",
      "At: 1034 [==========>] Loss 0.06956458102418724  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.0997142273802861  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.11652713078655738  - accuracy: 0.875\n",
      "At: 1037 [==========>] Loss 0.1363931972576592  - accuracy: 0.8125\n",
      "At: 1038 [==========>] Loss 0.09134137044868013  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.09229552461763474  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.10296260476586372  - accuracy: 0.875\n",
      "At: 1041 [==========>] Loss 0.12406197326157065  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.09596919499155473  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.1677170677199075  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.09752805768152926  - accuracy: 0.90625\n",
      "At: 1045 [==========>] Loss 0.10825344662728059  - accuracy: 0.84375\n",
      "At: 1046 [==========>] Loss 0.15719923566781216  - accuracy: 0.8125\n",
      "At: 1047 [==========>] Loss 0.1607695712834909  - accuracy: 0.78125\n",
      "At: 1048 [==========>] Loss 0.16673362394395813  - accuracy: 0.84375\n",
      "At: 1049 [==========>] Loss 0.13168338879186256  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.1341817859386862  - accuracy: 0.78125\n",
      "At: 1051 [==========>] Loss 0.10349735786871543  - accuracy: 0.84375\n",
      "At: 1052 [==========>] Loss 0.09195330679441865  - accuracy: 0.875\n",
      "At: 1053 [==========>] Loss 0.09586384278616189  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.12092602874450106  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.18376741577802697  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.12779314843438538  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.1324388578829379  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.0522058646998798  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.058864606595523936  - accuracy: 0.9375\n",
      "At: 1060 [==========>] Loss 0.09726578101469072  - accuracy: 0.875\n",
      "At: 1061 [==========>] Loss 0.09546585688027648  - accuracy: 0.84375\n",
      "At: 1062 [==========>] Loss 0.16267908141053372  - accuracy: 0.78125\n",
      "At: 1063 [==========>] Loss 0.10050564210631154  - accuracy: 0.90625\n",
      "At: 1064 [==========>] Loss 0.1281040077721694  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.09931544910863574  - accuracy: 0.8125\n",
      "At: 1066 [==========>] Loss 0.08201551133090876  - accuracy: 0.875\n",
      "At: 1067 [==========>] Loss 0.10959212500733025  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.11232464994563064  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.12448086657531901  - accuracy: 0.78125\n",
      "At: 1070 [==========>] Loss 0.1252935409851974  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.10660103535756577  - accuracy: 0.84375\n",
      "At: 1072 [==========>] Loss 0.1024464716540264  - accuracy: 0.90625\n",
      "At: 1073 [==========>] Loss 0.17712941706491955  - accuracy: 0.78125\n",
      "At: 1074 [==========>] Loss 0.17479710068950596  - accuracy: 0.78125\n",
      "At: 1075 [==========>] Loss 0.10362656921030534  - accuracy: 0.84375\n",
      "At: 1076 [==========>] Loss 0.13710765093176658  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.08020444222890928  - accuracy: 0.9375\n",
      "At: 1078 [==========>] Loss 0.09192323927227188  - accuracy: 0.90625\n",
      "At: 1079 [==========>] Loss 0.10629406148408907  - accuracy: 0.84375\n",
      "At: 1080 [==========>] Loss 0.12862188653329276  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.09259686404211409  - accuracy: 0.875\n",
      "At: 1082 [==========>] Loss 0.09380287117228481  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.10645675528672091  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.08501683813125122  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.13917485709729988  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.1465115958121862  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.1295633909063531  - accuracy: 0.84375\n",
      "At: 1088 [==========>] Loss 0.16144171528521506  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.07312158540842768  - accuracy: 1.0\n",
      "At: 1090 [==========>] Loss 0.08837583203477672  - accuracy: 0.875\n",
      "At: 1091 [==========>] Loss 0.17750017229365558  - accuracy: 0.75\n",
      "At: 1092 [==========>] Loss 0.09819580965434922  - accuracy: 0.90625\n",
      "At: 1093 [==========>] Loss 0.1409773694117769  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.14257170020951904  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.13959188719719845  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.09561351399472881  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.07475177938876648  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.18323592003600372  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.10951360982238385  - accuracy: 0.78125\n",
      "At: 1100 [==========>] Loss 0.09058664055143723  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.10132971837729506  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.10853282833977726  - accuracy: 0.875\n",
      "At: 1103 [==========>] Loss 0.11434386963007298  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.06222223219531845  - accuracy: 0.96875\n",
      "At: 1105 [==========>] Loss 0.06659629189475894  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.08432072871544097  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.20137379345985534  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.08471923032065845  - accuracy: 0.875\n",
      "At: 1109 [==========>] Loss 0.05829604844115495  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.11031246766978423  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.17999501550549804  - accuracy: 0.71875\n",
      "At: 1112 [==========>] Loss 0.15946181831160516  - accuracy: 0.71875\n",
      "At: 1113 [==========>] Loss 0.14054760411643735  - accuracy: 0.84375\n",
      "At: 1114 [==========>] Loss 0.08586070669255869  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.11206341931816725  - accuracy: 0.875\n",
      "At: 1116 [==========>] Loss 0.10635847066444415  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.05487855308109355  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.12161974823425853  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.14095012754416375  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.08744923014324812  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.11411419285416192  - accuracy: 0.84375\n",
      "At: 1122 [==========>] Loss 0.08055235731908095  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.12573950791032035  - accuracy: 0.8125\n",
      "At: 1124 [==========>] Loss 0.14259016313093525  - accuracy: 0.875\n",
      "At: 1125 [==========>] Loss 0.16227158911713224  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.094625501983854  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.09673837966733516  - accuracy: 0.84375\n",
      "At: 1128 [==========>] Loss 0.07248149027340758  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.16707834561635687  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.11417665205584202  - accuracy: 0.8125\n",
      "At: 1131 [==========>] Loss 0.11749589029398275  - accuracy: 0.84375\n",
      "At: 1132 [==========>] Loss 0.07084040310816112  - accuracy: 0.9375\n",
      "At: 1133 [==========>] Loss 0.1495882029848837  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.08920560561600883  - accuracy: 0.8125\n",
      "At: 1135 [==========>] Loss 0.10885576780149425  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.13104782303973833  - accuracy: 0.84375\n",
      "At: 1137 [==========>] Loss 0.11677673310779496  - accuracy: 0.78125\n",
      "At: 1138 [==========>] Loss 0.11678835957850714  - accuracy: 0.875\n",
      "At: 1139 [==========>] Loss 0.058944835404006  - accuracy: 0.9375\n",
      "At: 1140 [==========>] Loss 0.1520736194720076  - accuracy: 0.71875\n",
      "At: 1141 [==========>] Loss 0.12007650185156427  - accuracy: 0.84375\n",
      "At: 1142 [==========>] Loss 0.13972444064051015  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.053152123451188  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.12640090347851707  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.1361722876761829  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.13843946043434313  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.19152422625257387  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.0707417121677856  - accuracy: 0.9375\n",
      "At: 1149 [==========>] Loss 0.09548805702893957  - accuracy: 0.90625\n",
      "At: 1150 [==========>] Loss 0.11025673035994246  - accuracy: 0.78125\n",
      "At: 1151 [==========>] Loss 0.1414042236328044  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.13106481740329495  - accuracy: 0.78125\n",
      "At: 1153 [==========>] Loss 0.15518669114514394  - accuracy: 0.84375\n",
      "At: 1154 [==========>] Loss 0.08867951861405458  - accuracy: 0.90625\n",
      "At: 1155 [==========>] Loss 0.11273760815527059  - accuracy: 0.875\n",
      "At: 1156 [==========>] Loss 0.16740029640467596  - accuracy: 0.75\n",
      "At: 1157 [==========>] Loss 0.11919179703875357  - accuracy: 0.8125\n",
      "At: 1158 [==========>] Loss 0.13275772462176716  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.11282374894526594  - accuracy: 0.8125\n",
      "At: 1160 [==========>] Loss 0.08042952564507827  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.08392484261977981  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.125182549889856  - accuracy: 0.84375\n",
      "At: 1163 [==========>] Loss 0.175516524707317  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.09116211120639614  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.14835425905948957  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.07660591968018989  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.15034985890823807  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.10900797118159995  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.09966155291356288  - accuracy: 0.90625\n",
      "At: 1170 [==========>] Loss 0.18499708796044095  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.06035762657864914  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.10979753290466138  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.12911878027307927  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.16626466694957137  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.1230596326679448  - accuracy: 0.84375\n",
      "At: 1176 [==========>] Loss 0.12468851269742581  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.0937677407214747  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.153398469412104  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.14096725377101388  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.18886567637377116  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.08295510399894963  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.09298744423657329  - accuracy: 0.875\n",
      "At: 1183 [==========>] Loss 0.1353862384855089  - accuracy: 0.8125\n",
      "At: 1184 [==========>] Loss 0.16790924055918954  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.06941612657796484  - accuracy: 0.9375\n",
      "At: 1186 [==========>] Loss 0.1378128507994248  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.11183610182015638  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.07581940762636769  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.16313010409919138  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.10688166363551631  - accuracy: 0.875\n",
      "At: 1191 [==========>] Loss 0.14318299103537752  - accuracy: 0.78125\n",
      "At: 1192 [==========>] Loss 0.06274887684333275  - accuracy: 0.9375\n",
      "At: 1193 [==========>] Loss 0.1406600261632382  - accuracy: 0.8125\n",
      "At: 1194 [==========>] Loss 0.12525909722590708  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.12918022643066643  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.10574099323968163  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.09139523061540106  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.08293026118302031  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.19148727691697395  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.08015077495331241  - accuracy: 0.90625\n",
      "At: 1201 [==========>] Loss 0.13272070567299077  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.1574638816421961  - accuracy: 0.71875\n",
      "At: 1203 [==========>] Loss 0.11651594966130296  - accuracy: 0.875\n",
      "At: 1204 [==========>] Loss 0.09116469691910078  - accuracy: 0.9375\n",
      "At: 1205 [==========>] Loss 0.04422848361505646  - accuracy: 0.96875\n",
      "At: 1206 [==========>] Loss 0.09226228175329282  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.13068381773279733  - accuracy: 0.84375\n",
      "At: 1208 [==========>] Loss 0.08821010154714501  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.10390354870666924  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.13590704853387364  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.15077613226827186  - accuracy: 0.75\n",
      "At: 1212 [==========>] Loss 0.1105747186790644  - accuracy: 0.78125\n",
      "At: 1213 [==========>] Loss 0.18051950330248423  - accuracy: 0.75\n",
      "At: 1214 [==========>] Loss 0.12739731161180473  - accuracy: 0.875\n",
      "At: 1215 [==========>] Loss 0.15311729651850614  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.1047350036126215  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.0748138153356507  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.08315675347620491  - accuracy: 0.9375\n",
      "At: 1219 [==========>] Loss 0.14820038186057458  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.08462703222121021  - accuracy: 0.9375\n",
      "At: 1221 [==========>] Loss 0.07753538126839862  - accuracy: 0.90625\n",
      "At: 1222 [==========>] Loss 0.17204864530653285  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.0914253764957797  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.09621709778624488  - accuracy: 0.84375\n",
      "At: 1225 [==========>] Loss 0.07141646761079735  - accuracy: 0.9375\n",
      "At: 1226 [==========>] Loss 0.09227997125364959  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.1538711327419185  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.13151064106736823  - accuracy: 0.8125\n",
      "At: 1229 [==========>] Loss 0.11858770212402833  - accuracy: 0.8125\n",
      "At: 1230 [==========>] Loss 0.16073245707048167  - accuracy: 0.71875\n",
      "At: 1231 [==========>] Loss 0.12495285848209747  - accuracy: 0.875\n",
      "At: 1232 [==========>] Loss 0.10569311174963147  - accuracy: 0.875\n",
      "At: 1233 [==========>] Loss 0.09694550244691283  - accuracy: 0.90625\n",
      "At: 1234 [==========>] Loss 0.13807770823402066  - accuracy: 0.75\n",
      "At: 1235 [==========>] Loss 0.06952751467196083  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.13515081201953844  - accuracy: 0.8125\n",
      "At: 1237 [==========>] Loss 0.09035855492222228  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.07953120053181144  - accuracy: 0.96875\n",
      "At: 1239 [==========>] Loss 0.14436201342095198  - accuracy: 0.875\n",
      "At: 1240 [==========>] Loss 0.1334175790689468  - accuracy: 0.78125\n",
      "At: 1241 [==========>] Loss 0.11896458388844852  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.14888685720577743  - accuracy: 0.78125\n",
      "At: 1243 [==========>] Loss 0.10593591094471852  - accuracy: 0.875\n",
      "At: 1244 [==========>] Loss 0.16970669387993131  - accuracy: 0.71875\n",
      "At: 1245 [==========>] Loss 0.08934155490878976  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.0785749516813648  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.15056321433562078  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.11442148058096005  - accuracy: 0.84375\n",
      "At: 1249 [==========>] Loss 0.11756080750754874  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.14744142666591137  - accuracy: 0.8125\n",
      "At: 1251 [==========>] Loss 0.12701174333329773  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.0760118650158435  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.075131486173576  - accuracy: 0.90625\n",
      "At: 1254 [==========>] Loss 0.18266119479980183  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.07957924124395493  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.09500588044467359  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.13619808632862562  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.07028687455747926  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.13012467179687648  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.1205877034879209  - accuracy: 0.75\n",
      "At: 1261 [==========>] Loss 0.12159958121387623  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.1452868181947159  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.10191301139152316  - accuracy: 0.90625\n",
      "At: 1264 [==========>] Loss 0.06308877477563873  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.1372980187935978  - accuracy: 0.8125\n",
      "At: 1266 [==========>] Loss 0.10148018212491175  - accuracy: 0.875\n",
      "At: 1267 [==========>] Loss 0.11551142958953227  - accuracy: 0.84375\n",
      "At: 1268 [==========>] Loss 0.13551607700518933  - accuracy: 0.84375\n",
      "At: 1269 [==========>] Loss 0.12789039011800493  - accuracy: 0.8125\n",
      "At: 1270 [==========>] Loss 0.102605008344334  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.1495923952431672  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.062017360785241296  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.18030381325461986  - accuracy: 0.78125\n",
      "At: 1274 [==========>] Loss 0.124734174489802  - accuracy: 0.84375\n",
      "At: 1275 [==========>] Loss 0.0859754707451568  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.09881324649820689  - accuracy: 0.90625\n",
      "At: 1277 [==========>] Loss 0.0790666112435428  - accuracy: 0.875\n",
      "At: 1278 [==========>] Loss 0.14025848699181637  - accuracy: 0.78125\n",
      "At: 1279 [==========>] Loss 0.10914690775924754  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.09728878496042051  - accuracy: 0.8125\n",
      "At: 1281 [==========>] Loss 0.16028904189002355  - accuracy: 0.75\n",
      "At: 1282 [==========>] Loss 0.11684897852702131  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.1415341672721012  - accuracy: 0.8125\n",
      "At: 1284 [==========>] Loss 0.14956453599613245  - accuracy: 0.84375\n",
      "At: 1285 [==========>] Loss 0.07704090623767158  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.1011198249978612  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.10535278763023445  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.17582140299880938  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.12821082058157593  - accuracy: 0.78125\n",
      "At: 1290 [==========>] Loss 0.1317466826648485  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.15857882769957704  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.09936613047657036  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.13153953292762965  - accuracy: 0.8125\n",
      "At: 1294 [==========>] Loss 0.16272094040224316  - accuracy: 0.78125\n",
      "At: 1295 [==========>] Loss 0.1546236020156224  - accuracy: 0.84375\n",
      "At: 1296 [==========>] Loss 0.16112511347558722  - accuracy: 0.75\n",
      "At: 1297 [==========>] Loss 0.11657885098709807  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.07835854919996443  - accuracy: 0.90625\n",
      "At: 1299 [==========>] Loss 0.15349583698561725  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.14982490786697666  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.12240903793304989  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.09949250923184794  - accuracy: 0.84375\n",
      "At: 1303 [==========>] Loss 0.1183301981126049  - accuracy: 0.84375\n",
      "At: 1304 [==========>] Loss 0.1257811786609858  - accuracy: 0.84375\n",
      "At: 1305 [==========>] Loss 0.13055813921361176  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.05832090096536988  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.15842965440287568  - accuracy: 0.71875\n",
      "At: 1308 [==========>] Loss 0.08492863428126228  - accuracy: 0.875\n",
      "At: 1309 [==========>] Loss 0.15226785158490894  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.14161579664247223  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.16210923320740078  - accuracy: 0.78125\n",
      "At: 1312 [==========>] Loss 0.06194949200903091  - accuracy: 0.9375\n",
      "At: 1313 [==========>] Loss 0.18210174022523173  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.0646860340668857  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.13099567136600485  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.11657613579715469  - accuracy: 0.875\n",
      "At: 1317 [==========>] Loss 0.09356510260540644  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.11475434113561037  - accuracy: 0.78125\n",
      "At: 1319 [==========>] Loss 0.10477412520195731  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.12730050567416187  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.08198140302467097  - accuracy: 0.875\n",
      "At: 1322 [==========>] Loss 0.1529549321900069  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.0814996466852166  - accuracy: 0.9375\n",
      "At: 1324 [==========>] Loss 0.1126147075541146  - accuracy: 0.84375\n",
      "At: 1325 [==========>] Loss 0.09847255930092146  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.08463061154491927  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.16899672760318826  - accuracy: 0.6875\n",
      "At: 1328 [==========>] Loss 0.07923354823350894  - accuracy: 0.875\n",
      "At: 1329 [==========>] Loss 0.05155740632159839  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.11094153406115413  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.17707022914594894  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.1008000500490652  - accuracy: 0.84375\n",
      "At: 1333 [==========>] Loss 0.145407527237961  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.12052470612888466  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.10439936735843626  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.10954280166003805  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.1561851720584001  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.14190301729571594  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.1237981672994262  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.13303107107812087  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.07508753491505779  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.12154742261520543  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.1556785695494612  - accuracy: 0.8125\n",
      "At: 1344 [==========>] Loss 0.1917473123249851  - accuracy: 0.6875\n",
      "At: 1345 [==========>] Loss 0.10504567431743911  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.10745540697944002  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.09926823284904658  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09052757234905258  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.13221140865575975  - accuracy: 0.84375\n",
      "At: 1350 [==========>] Loss 0.138479964344593  - accuracy: 0.8125\n",
      "At: 1351 [==========>] Loss 0.08715453093690961  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.10851280515444996  - accuracy: 0.84375\n",
      "At: 1353 [==========>] Loss 0.1620990562568249  - accuracy: 0.75\n",
      "At: 1354 [==========>] Loss 0.16146952055303093  - accuracy: 0.75\n",
      "At: 1355 [==========>] Loss 0.06669062390387451  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.11186719845820221  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.10689087930375465  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.12462632312868918  - accuracy: 0.875\n",
      "At: 1359 [==========>] Loss 0.0700642281313108  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.16054789944190506  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.10217217077234186  - accuracy: 0.8125\n",
      "At: 1362 [==========>] Loss 0.12678266154335743  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.09954461393758099  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.12429619669864836  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.13570630064220132  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.15188437898555784  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.10072108417566511  - accuracy: 0.90625\n",
      "At: 1368 [==========>] Loss 0.19394298962062595  - accuracy: 0.625\n",
      "At: 1369 [==========>] Loss 0.08561780255681387  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.10593288342356325  - accuracy: 0.84375\n",
      "At: 1371 [==========>] Loss 0.18359421280438665  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.11330697776379618  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.1325505302489126  - accuracy: 0.875\n",
      "At: 1374 [==========>] Loss 0.10983133231259803  - accuracy: 0.84375\n",
      "At: 1375 [==========>] Loss 0.1382547924163025  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.08868825024324069  - accuracy: 0.875\n",
      "At: 1377 [==========>] Loss 0.1443012438603174  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.12208986172173428  - accuracy: 0.875\n",
      "At: 1379 [==========>] Loss 0.16560827107677906  - accuracy: 0.6875\n",
      "At: 1380 [==========>] Loss 0.12539817694522154  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.0848324786123315  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.12700833516572346  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.09570171102738446  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.10892080352340011  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.15605863278527216  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.16497416997900438  - accuracy: 0.78125\n",
      "At: 1387 [==========>] Loss 0.05710436603105094  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.15490675490754274  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.14163986934337414  - accuracy: 0.78125\n",
      "At: 1390 [==========>] Loss 0.11462581199943268  - accuracy: 0.875\n",
      "At: 1391 [==========>] Loss 0.11232622964732605  - accuracy: 0.8125\n",
      "At: 1392 [==========>] Loss 0.06372392071447758  - accuracy: 0.90625\n",
      "At: 1393 [==========>] Loss 0.11754583637633409  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.0805062803819911  - accuracy: 0.90625\n",
      "At: 1395 [==========>] Loss 0.20195116134012187  - accuracy: 0.6875\n",
      "At: 1396 [==========>] Loss 0.06054843040510542  - accuracy: 0.96875\n",
      "At: 1397 [==========>] Loss 0.12144720945700008  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.10675624609081692  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.0952994657106587  - accuracy: 0.875\n",
      "At: 1400 [==========>] Loss 0.1225660352246119  - accuracy: 0.84375\n",
      "At: 1401 [==========>] Loss 0.08142047224542834  - accuracy: 0.90625\n",
      "At: 1402 [==========>] Loss 0.14294715921059858  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.11647244313633194  - accuracy: 0.90625\n",
      "At: 1404 [==========>] Loss 0.09924508090444202  - accuracy: 0.90625\n",
      "At: 1405 [==========>] Loss 0.07783085979233367  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.1496467387291609  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.10905508263969294  - accuracy: 0.875\n",
      "At: 1408 [==========>] Loss 0.16008707788459212  - accuracy: 0.75\n",
      "At: 1409 [==========>] Loss 0.03646011651755049  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.12250649043539434  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.11785921827978027  - accuracy: 0.84375\n",
      "At: 1412 [==========>] Loss 0.11122417747844955  - accuracy: 0.875\n",
      "At: 1413 [==========>] Loss 0.08559438692986432  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.16790319245770768  - accuracy: 0.8125\n",
      "At: 1415 [==========>] Loss 0.09020075599737853  - accuracy: 0.875\n",
      "At: 1416 [==========>] Loss 0.14882111685681637  - accuracy: 0.75\n",
      "At: 1417 [==========>] Loss 0.10566145840153839  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.16395112430563147  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.0869545447875489  - accuracy: 0.9375\n",
      "At: 1420 [==========>] Loss 0.09750389453168624  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.10409419733586849  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.14452788616828066  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.13885167847078855  - accuracy: 0.84375\n",
      "At: 1424 [==========>] Loss 0.13937492967871434  - accuracy: 0.8125\n",
      "At: 1425 [==========>] Loss 0.07353285492701331  - accuracy: 0.96875\n",
      "At: 1426 [==========>] Loss 0.10799022931357108  - accuracy: 0.875\n",
      "At: 1427 [==========>] Loss 0.11722198224269802  - accuracy: 0.8125\n",
      "At: 1428 [==========>] Loss 0.09552340398634118  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.14108893920683083  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.08233377196347144  - accuracy: 0.84375\n",
      "At: 1431 [==========>] Loss 0.11661600568862782  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.12200747156169225  - accuracy: 0.78125\n",
      "At: 1433 [==========>] Loss 0.12337681409548215  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.1301451435157522  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.11464041064263908  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.08056200659698781  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.1385235249888163  - accuracy: 0.75\n",
      "At: 1438 [==========>] Loss 0.11885594754185326  - accuracy: 0.84375\n",
      "At: 1439 [==========>] Loss 0.15436540306290153  - accuracy: 0.8125\n",
      "At: 1440 [==========>] Loss 0.11078034513711663  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.08508246785042185  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.12163361276350841  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.1375361364719067  - accuracy: 0.84375\n",
      "At: 1444 [==========>] Loss 0.117314788299173  - accuracy: 0.84375\n",
      "At: 1445 [==========>] Loss 0.18891450230761847  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.19804871740015498  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.16528179966718803  - accuracy: 0.75\n",
      "At: 1448 [==========>] Loss 0.07310501354251941  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.15769581090062545  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.11877163672316152  - accuracy: 0.8125\n",
      "At: 1451 [==========>] Loss 0.12198121463164796  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.10337798337724591  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.06363873742779284  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.1218315374762352  - accuracy: 0.84375\n",
      "At: 1455 [==========>] Loss 0.10878796171080277  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.08696056511612661  - accuracy: 0.9375\n",
      "At: 1457 [==========>] Loss 0.07627012251388515  - accuracy: 0.90625\n",
      "At: 1458 [==========>] Loss 0.12345551028239547  - accuracy: 0.84375\n",
      "At: 1459 [==========>] Loss 0.1219287263845493  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.16253466977056003  - accuracy: 0.78125\n",
      "At: 1461 [==========>] Loss 0.11473322113856958  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.166111673878471  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.1008144002962485  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.1731012174468086  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.11583531217289872  - accuracy: 0.78125\n",
      "At: 1466 [==========>] Loss 0.09520491001291911  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.19879159807647556  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.17043488899276016  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.16808534592534374  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.13574595155834776  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.12227541083347676  - accuracy: 0.84375\n",
      "At: 1472 [==========>] Loss 0.08334264982252632  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.08920028538758143  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.20154064878052924  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.16844694410679778  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.0893421231239572  - accuracy: 0.84375\n",
      "At: 1477 [==========>] Loss 0.10755197390274868  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.10924636868412016  - accuracy: 0.9375\n",
      "At: 1479 [==========>] Loss 0.12253029232807117  - accuracy: 0.875\n",
      "At: 1480 [==========>] Loss 0.09371703020722363  - accuracy: 0.875\n",
      "At: 1481 [==========>] Loss 0.10267785982566976  - accuracy: 0.8125\n",
      "At: 1482 [==========>] Loss 0.10252174574304174  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.23813769986205505  - accuracy: 0.625\n",
      "At: 1484 [==========>] Loss 0.1294170132965895  - accuracy: 0.71875\n",
      "At: 1485 [==========>] Loss 0.17852435629780466  - accuracy: 0.71875\n",
      "At: 1486 [==========>] Loss 0.0784211796170931  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.07749725161744518  - accuracy: 0.90625\n",
      "At: 1488 [==========>] Loss 0.13226877109655652  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.17708751714008997  - accuracy: 0.78125\n",
      "At: 1490 [==========>] Loss 0.0942444834198333  - accuracy: 0.90625\n",
      "At: 1491 [==========>] Loss 0.15556578189215312  - accuracy: 0.78125\n",
      "At: 1492 [==========>] Loss 0.1209609325192521  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.14300872022565503  - accuracy: 0.78125\n",
      "At: 1494 [==========>] Loss 0.13037691299909393  - accuracy: 0.84375\n",
      "At: 1495 [==========>] Loss 0.09744605149396347  - accuracy: 0.84375\n",
      "At: 1496 [==========>] Loss 0.08726520468205388  - accuracy: 0.84375\n",
      "At: 1497 [==========>] Loss 0.16503693576622327  - accuracy: 0.78125\n",
      "At: 1498 [==========>] Loss 0.17397345715197063  - accuracy: 0.75\n",
      "At: 1499 [==========>] Loss 0.10733510139717935  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.09336943965834037  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.1032921830996222  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.1447663346660328  - accuracy: 0.78125\n",
      "At: 1503 [==========>] Loss 0.146318892208576  - accuracy: 0.75\n",
      "At: 1504 [==========>] Loss 0.1113862803146449  - accuracy: 0.875\n",
      "At: 1505 [==========>] Loss 0.1354142056615105  - accuracy: 0.78125\n",
      "At: 1506 [==========>] Loss 0.1686279533688328  - accuracy: 0.75\n",
      "At: 1507 [==========>] Loss 0.12008703488834924  - accuracy: 0.875\n",
      "At: 1508 [==========>] Loss 0.19562299187344864  - accuracy: 0.75\n",
      "At: 1509 [==========>] Loss 0.09457345382636663  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.1427018509671193  - accuracy: 0.75\n",
      "At: 1511 [==========>] Loss 0.10184352773847474  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.07126461096717529  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.15502743192886564  - accuracy: 0.78125\n",
      "At: 1514 [==========>] Loss 0.12281315877604182  - accuracy: 0.84375\n",
      "At: 1515 [==========>] Loss 0.14564843872893352  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.11497516863653515  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.13826209344868934  - accuracy: 0.84375\n",
      "At: 1518 [==========>] Loss 0.12132030036813149  - accuracy: 0.78125\n",
      "At: 1519 [==========>] Loss 0.13144892377215137  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.09878128046052044  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.09659748790959949  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.19105972478686836  - accuracy: 0.6875\n",
      "At: 1523 [==========>] Loss 0.1002590022887776  - accuracy: 0.8125\n",
      "At: 1524 [==========>] Loss 0.15648626884739308  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.12334339113425262  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.08663562852930973  - accuracy: 0.90625\n",
      "At: 1527 [==========>] Loss 0.15664520401750873  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.12551924723924152  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.07743698129730764  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.05174917995498306  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.10980027035743581  - accuracy: 0.875\n",
      "At: 1532 [==========>] Loss 0.16559819303703954  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.18090908647845338  - accuracy: 0.6875\n",
      "At: 1534 [==========>] Loss 0.1071252769616892  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.14266584007845612  - accuracy: 0.8125\n",
      "At: 1536 [==========>] Loss 0.1401820496677523  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10059565807475213  - accuracy: 0.875\n",
      "At: 1538 [==========>] Loss 0.10732265586118347  - accuracy: 0.84375\n",
      "At: 1539 [==========>] Loss 0.061567897825434856  - accuracy: 0.96875\n",
      "At: 1540 [==========>] Loss 0.19709188218419837  - accuracy: 0.6875\n",
      "At: 1541 [==========>] Loss 0.11028470561944545  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.08186281880174112  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.11955152528960844  - accuracy: 0.875\n",
      "At: 1544 [==========>] Loss 0.12733637442732593  - accuracy: 0.84375\n",
      "At: 1545 [==========>] Loss 0.19366484314792168  - accuracy: 0.75\n",
      "At: 1546 [==========>] Loss 0.11889387515385487  - accuracy: 0.84375\n",
      "At: 1547 [==========>] Loss 0.16199439353841472  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.13621257508283116  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.1486410969319491  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.07869173482101709  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.144529496488254  - accuracy: 0.8125\n",
      "At: 1552 [==========>] Loss 0.0865042870124341  - accuracy: 0.90625\n",
      "At: 1553 [==========>] Loss 0.07037826799316822  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.1213522089286607  - accuracy: 0.875\n",
      "At: 1555 [==========>] Loss 0.11728802186823752  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.14721573815716676  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.1039237711817329  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.12390130415234434  - accuracy: 0.875\n",
      "At: 1559 [==========>] Loss 0.0730308815701907  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.153310190299139  - accuracy: 0.75\n",
      "At: 1561 [==========>] Loss 0.13832951306006075  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.0910082006981148  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.0820128355957436  - accuracy: 0.9375\n",
      "At: 1564 [==========>] Loss 0.09624819894762957  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.12991226058574676  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.12149391201485717  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.13017908383737098  - accuracy: 0.84375\n",
      "At: 1568 [==========>] Loss 0.0872562770904241  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.10420791142547144  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.0908995936719043  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.1346426821538875  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.14010293968900478  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.05061240969090332  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.1349211513563441  - accuracy: 0.8125\n",
      "At: 1575 [==========>] Loss 0.10857170006535878  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.13874684399505754  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.060119914261304475  - accuracy: 0.96875\n",
      "At: 1578 [==========>] Loss 0.07543044138438966  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.08450482248522362  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.10637690457485999  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.10053365327904175  - accuracy: 0.8125\n",
      "At: 1582 [==========>] Loss 0.15198274287815466  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.10770114106158125  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.10158608608958776  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.08743154756630889  - accuracy: 0.90625\n",
      "At: 1586 [==========>] Loss 0.1500295145774492  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.09836495558082098  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.12846994759894712  - accuracy: 0.75\n",
      "At: 1589 [==========>] Loss 0.1377109946371199  - accuracy: 0.84375\n",
      "At: 1590 [==========>] Loss 0.121930787670197  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.09977451909556398  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.06451369439312195  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.16473301103569593  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.0858387256806041  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.14228809635411474  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.17837326210949406  - accuracy: 0.6875\n",
      "At: 1597 [==========>] Loss 0.14632752839751956  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.15255092442671825  - accuracy: 0.8125\n",
      "At: 1599 [==========>] Loss 0.19199568919837964  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.1480923104624828  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.06711836675989144  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.10545842035418865  - accuracy: 0.875\n",
      "At: 1603 [==========>] Loss 0.18105204119889018  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.23890037279444848  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.07527186909145477  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.11071354739244815  - accuracy: 0.875\n",
      "At: 1607 [==========>] Loss 0.17863652735792446  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.14316139624958366  - accuracy: 0.84375\n",
      "At: 1609 [==========>] Loss 0.13561392085140034  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.14300408511539492  - accuracy: 0.8125\n",
      "At: 1611 [==========>] Loss 0.08536606238867767  - accuracy: 0.875\n",
      "At: 1612 [==========>] Loss 0.07509151391535816  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.11733035261631874  - accuracy: 0.875\n",
      "At: 1614 [==========>] Loss 0.13547877033186295  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.09338740485600164  - accuracy: 0.875\n",
      "At: 1616 [==========>] Loss 0.10068616091332329  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.09410782314583585  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11341999477063841  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.1889022263382633  - accuracy: 0.75\n",
      "At: 1620 [==========>] Loss 0.10055295865990203  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.12828563299124401  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.16562032291641426  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.08771174516695443  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.1470010025636816  - accuracy: 0.71875\n",
      "At: 1625 [==========>] Loss 0.12628943526150507  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.09897904785889533  - accuracy: 0.875\n",
      "At: 1627 [==========>] Loss 0.08580728070871932  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.13440983956709088  - accuracy: 0.84375\n",
      "At: 1629 [==========>] Loss 0.13407592408907354  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.07731826980513726  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.09130121010562459  - accuracy: 0.90625\n",
      "At: 1632 [==========>] Loss 0.09341428047606323  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08132484802647595  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.11668267924273132  - accuracy: 0.875\n",
      "At: 1635 [==========>] Loss 0.22431220126139018  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.09093934060383063  - accuracy: 0.9375\n",
      "At: 1637 [==========>] Loss 0.07146822381788931  - accuracy: 0.875\n",
      "At: 1638 [==========>] Loss 0.15086753405560915  - accuracy: 0.75\n",
      "At: 1639 [==========>] Loss 0.12380436107404777  - accuracy: 0.8125\n",
      "At: 1640 [==========>] Loss 0.12441917734240236  - accuracy: 0.84375\n",
      "At: 1641 [==========>] Loss 0.11023628408926284  - accuracy: 0.78125\n",
      "At: 1642 [==========>] Loss 0.08583241712211698  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.12002650271724083  - accuracy: 0.84375\n",
      "At: 1644 [==========>] Loss 0.09738514969454676  - accuracy: 0.90625\n",
      "At: 1645 [==========>] Loss 0.044848704770298814  - accuracy: 0.96875\n",
      "At: 1646 [==========>] Loss 0.13119063996802627  - accuracy: 0.78125\n",
      "At: 1647 [==========>] Loss 0.16865740181451266  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.11553105012442617  - accuracy: 0.875\n",
      "At: 1649 [==========>] Loss 0.08978436957875052  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.07354100242315997  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.12377558717304799  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.0782734377410457  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.08847938689708856  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.059715307157145465  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.16688925079872363  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.10653755667299196  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.14491684281802442  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.2030348331916242  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.09748570133212514  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.03849087984835337  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.0693427914648493  - accuracy: 0.9375\n",
      "At: 1662 [==========>] Loss 0.1118245979218778  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16369954246644902  - accuracy: 0.78125\n",
      "At: 1664 [==========>] Loss 0.12276024687994908  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.09995940664852543  - accuracy: 0.90625\n",
      "At: 1666 [==========>] Loss 0.09823801968481545  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.14449974708294572  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.14868675237443746  - accuracy: 0.78125\n",
      "At: 1669 [==========>] Loss 0.11579929566551748  - accuracy: 0.8125\n",
      "At: 1670 [==========>] Loss 0.06317881441745125  - accuracy: 0.875\n",
      "At: 1671 [==========>] Loss 0.12320451073680928  - accuracy: 0.84375\n",
      "At: 1672 [==========>] Loss 0.1380434088100848  - accuracy: 0.75\n",
      "At: 1673 [==========>] Loss 0.09527767497668768  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.16244910170530646  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.17623727722187843  - accuracy: 0.71875\n",
      "At: 1676 [==========>] Loss 0.167506469148092  - accuracy: 0.71875\n",
      "At: 1677 [==========>] Loss 0.08466269433072728  - accuracy: 0.9375\n",
      "At: 1678 [==========>] Loss 0.1747149976001649  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.09410570161740868  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.13640802995554435  - accuracy: 0.78125\n",
      "At: 1681 [==========>] Loss 0.11705406630006744  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.09392740152814769  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.15893830556936306  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.09965509847310018  - accuracy: 0.90625\n",
      "At: 1685 [==========>] Loss 0.09522573555157952  - accuracy: 0.90625\n",
      "At: 1686 [==========>] Loss 0.09983467737288297  - accuracy: 0.90625\n",
      "At: 1687 [==========>] Loss 0.15351227346084523  - accuracy: 0.78125\n",
      "At: 1688 [==========>] Loss 0.04467763160058898  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.117475876211214  - accuracy: 0.875\n",
      "At: 1690 [==========>] Loss 0.10023771718558085  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.09296417921248351  - accuracy: 0.90625\n",
      "At: 1692 [==========>] Loss 0.15647630921350178  - accuracy: 0.78125\n",
      "At: 1693 [==========>] Loss 0.09713896598032164  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.10914118462297868  - accuracy: 0.84375\n",
      "At: 1695 [==========>] Loss 0.10581361873705843  - accuracy: 0.90625\n",
      "At: 1696 [==========>] Loss 0.14597747389941634  - accuracy: 0.84375\n",
      "At: 1697 [==========>] Loss 0.10835740019770551  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.08669100428499343  - accuracy: 0.84375\n",
      "At: 1699 [==========>] Loss 0.11595417241849401  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.13035169903746452  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.10421775479249844  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.08023021179596099  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.18145139396791765  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.06965208094809214  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.09647196940387953  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.15817089786145505  - accuracy: 0.8125\n",
      "At: 1707 [==========>] Loss 0.1749137870620121  - accuracy: 0.78125\n",
      "At: 1708 [==========>] Loss 0.09213192406272008  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.1592830702077918  - accuracy: 0.78125\n",
      "At: 1710 [==========>] Loss 0.17414914228833606  - accuracy: 0.71875\n",
      "At: 1711 [==========>] Loss 0.09094090570618615  - accuracy: 0.875\n",
      "At: 1712 [==========>] Loss 0.10468600254193029  - accuracy: 0.90625\n",
      "At: 1713 [==========>] Loss 0.08927468054527228  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.1815696236381354  - accuracy: 0.71875\n",
      "At: 1715 [==========>] Loss 0.10474830633278912  - accuracy: 0.84375\n",
      "At: 1716 [==========>] Loss 0.04854582476956018  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08340587881799913  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.12333100595094891  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.11890254179096477  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.07644445353710697  - accuracy: 0.90625\n",
      "At: 1721 [==========>] Loss 0.13669802995893887  - accuracy: 0.84375\n",
      "At: 1722 [==========>] Loss 0.06557081428872698  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.17203692230737222  - accuracy: 0.71875\n",
      "At: 1724 [==========>] Loss 0.09610997279683062  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.1563483210676715  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.09631267978444205  - accuracy: 0.875\n",
      "At: 1727 [==========>] Loss 0.10376627877391403  - accuracy: 0.875\n",
      "At: 1728 [==========>] Loss 0.09723180417506132  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.1697639376467599  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.13134175238414914  - accuracy: 0.84375\n",
      "At: 1731 [==========>] Loss 0.07261644620973513  - accuracy: 0.9375\n",
      "At: 1732 [==========>] Loss 0.07090358314960138  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.1783126830012337  - accuracy: 0.65625\n",
      "At: 1734 [==========>] Loss 0.09879712863367889  - accuracy: 0.875\n",
      "At: 1735 [==========>] Loss 0.14619069411770985  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.10890008297615243  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.14918594179311884  - accuracy: 0.8125\n",
      "At: 1738 [==========>] Loss 0.10176183940368397  - accuracy: 0.875\n",
      "At: 1739 [==========>] Loss 0.11605458201324105  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.12516742558607663  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.1462472694169312  - accuracy: 0.8125\n",
      "At: 1742 [==========>] Loss 0.05115425481813228  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.12436892621590423  - accuracy: 0.84375\n",
      "At: 1744 [==========>] Loss 0.09958375997925302  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.14403887100875612  - accuracy: 0.8125\n",
      "At: 1746 [==========>] Loss 0.13412306008991598  - accuracy: 0.875\n",
      "At: 1747 [==========>] Loss 0.12103199714030075  - accuracy: 0.84375\n",
      "At: 1748 [==========>] Loss 0.12489568048058491  - accuracy: 0.8125\n",
      "At: 1749 [==========>] Loss 0.10047781512214796  - accuracy: 0.90625\n",
      "At: 1750 [==========>] Loss 0.10250347695763164  - accuracy: 0.90625\n",
      "At: 1751 [==========>] Loss 0.17469280937230539  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.09451362127465693  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.06276650118022706  - accuracy: 0.9375\n",
      "At: 1754 [==========>] Loss 0.08300664314010944  - accuracy: 0.90625\n",
      "At: 1755 [==========>] Loss 0.08223796543806655  - accuracy: 0.875\n",
      "At: 1756 [==========>] Loss 0.14331766764467013  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.16169277269914176  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.08420673197575201  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.1000552028989432  - accuracy: 0.875\n",
      "At: 1760 [==========>] Loss 0.09417733482920351  - accuracy: 0.84375\n",
      "At: 1761 [==========>] Loss 0.1114980641465107  - accuracy: 0.8125\n",
      "At: 1762 [==========>] Loss 0.13380405954830987  - accuracy: 0.875\n",
      "At: 1763 [==========>] Loss 0.0935526005789366  - accuracy: 0.875\n",
      "At: 1764 [==========>] Loss 0.12761746801047347  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.14457069451602925  - accuracy: 0.78125\n",
      "At: 1766 [==========>] Loss 0.06382822332276727  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.06520618015160182  - accuracy: 0.96875\n",
      "At: 1768 [==========>] Loss 0.11939799558675662  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.0650207281557717  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.07517050945960004  - accuracy: 0.90625\n",
      "At: 1771 [==========>] Loss 0.14722729661213893  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.11540294939446455  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.10560651212401626  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.1602336540840809  - accuracy: 0.8125\n",
      "At: 1775 [==========>] Loss 0.10026599994370455  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.10997407989788771  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.09862754834299281  - accuracy: 0.9375\n",
      "At: 1778 [==========>] Loss 0.1158065988218049  - accuracy: 0.8125\n",
      "At: 1779 [==========>] Loss 0.09786058176343554  - accuracy: 0.875\n",
      "At: 1780 [==========>] Loss 0.11271637681277934  - accuracy: 0.8125\n",
      "At: 1781 [==========>] Loss 0.1969159696300124  - accuracy: 0.6875\n",
      "At: 1782 [==========>] Loss 0.12711134652848619  - accuracy: 0.8125\n",
      "At: 1783 [==========>] Loss 0.13293036144510997  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.06968068011260581  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.08813119113040886  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.12695313698515232  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.14940189640152082  - accuracy: 0.78125\n",
      "At: 1788 [==========>] Loss 0.10219794243626584  - accuracy: 0.875\n",
      "At: 1789 [==========>] Loss 0.10913159871603098  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.1937126988067988  - accuracy: 0.6875\n",
      "At: 1791 [==========>] Loss 0.05696794457152141  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.12745991953452052  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.09742460429727985  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.1585162262980035  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.07763694860150755  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.11015506475763116  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.1311418181088399  - accuracy: 0.8125\n",
      "At: 1798 [==========>] Loss 0.1416731021190214  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.06207762219247604  - accuracy: 0.9375\n",
      "At: 1800 [==========>] Loss 0.1043733877276906  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.1769768662374624  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.13042761930784885  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.16252692248877298  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.1441506377131309  - accuracy: 0.78125\n",
      "At: 1805 [==========>] Loss 0.037699756775335894  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.15167288455622924  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.1734557794338481  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.17081361138060025  - accuracy: 0.75\n",
      "At: 1809 [==========>] Loss 0.07749314941277238  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.09840523116376315  - accuracy: 0.84375\n",
      "At: 1811 [==========>] Loss 0.1559671979954947  - accuracy: 0.78125\n",
      "At: 1812 [==========>] Loss 0.10132042949332179  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.11160451075390435  - accuracy: 0.90625\n",
      "At: 1814 [==========>] Loss 0.09359031779038446  - accuracy: 0.90625\n",
      "At: 1815 [==========>] Loss 0.16226816281365253  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.03856286723240532  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.12809798539247808  - accuracy: 0.75\n",
      "At: 1818 [==========>] Loss 0.11761707169034363  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.15347651696827969  - accuracy: 0.8125\n",
      "At: 1820 [==========>] Loss 0.12441430887555413  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.11574204750811937  - accuracy: 0.8125\n",
      "At: 1822 [==========>] Loss 0.1772035516292838  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.16596911490277988  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.1958584427872858  - accuracy: 0.6875\n",
      "At: 1825 [==========>] Loss 0.13444849611945575  - accuracy: 0.8125\n",
      "At: 1826 [==========>] Loss 0.07350774451865646  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.10007180739775028  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.14797556002624462  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.15354908821254057  - accuracy: 0.78125\n",
      "At: 1830 [==========>] Loss 0.1371926764371993  - accuracy: 0.8125\n",
      "At: 1831 [==========>] Loss 0.14710186367922334  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.14335050430019003  - accuracy: 0.75\n",
      "At: 1833 [==========>] Loss 0.08838390189288729  - accuracy: 0.9375\n",
      "At: 1834 [==========>] Loss 0.07493479375100462  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.14404357985662886  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.1108512793840932  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.05932449007526039  - accuracy: 0.875\n",
      "At: 1838 [==========>] Loss 0.1274250151436673  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.0703096622442857  - accuracy: 0.90625\n",
      "At: 1840 [==========>] Loss 0.1144853320395004  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.09322750309244744  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.12270094410204996  - accuracy: 0.8125\n",
      "At: 1843 [==========>] Loss 0.12433537734994089  - accuracy: 0.78125\n",
      "At: 1844 [==========>] Loss 0.08367475188996366  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.16580029318005784  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.1156909269053224  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.057990695861521994  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.06587350320951262  - accuracy: 0.90625\n",
      "At: 1849 [==========>] Loss 0.18445701640138612  - accuracy: 0.78125\n",
      "At: 1850 [==========>] Loss 0.04739051261282763  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.1629355520816011  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.0920088515628234  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.12364539549196374  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.1239630182725934  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.13547766491329685  - accuracy: 0.8125\n",
      "At: 1856 [==========>] Loss 0.11214153867745293  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.1513316119549981  - accuracy: 0.84375\n",
      "At: 1858 [==========>] Loss 0.12620957657818782  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.1570242141689623  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.15737114645785616  - accuracy: 0.75\n",
      "At: 1861 [==========>] Loss 0.0863116386796133  - accuracy: 0.875\n",
      "At: 1862 [==========>] Loss 0.18167127170038322  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.12414054460336131  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.1536678253376924  - accuracy: 0.75\n",
      "At: 1865 [==========>] Loss 0.0840144543047405  - accuracy: 0.9375\n",
      "At: 1866 [==========>] Loss 0.2098814481675113  - accuracy: 0.65625\n",
      "At: 1867 [==========>] Loss 0.11600790384184989  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.15422208161976497  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.1613981617212909  - accuracy: 0.78125\n",
      "At: 1870 [==========>] Loss 0.09189946057269771  - accuracy: 0.875\n",
      "At: 1871 [==========>] Loss 0.1160294818270732  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.14205183747848948  - accuracy: 0.78125\n",
      "At: 1873 [==========>] Loss 0.07723062796965502  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.14386697821984828  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.12402236730441672  - accuracy: 0.84375\n",
      "At: 1876 [==========>] Loss 0.19759222611918165  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.1009854640241464  - accuracy: 0.8125\n",
      "At: 1878 [==========>] Loss 0.10400093222132462  - accuracy: 0.90625\n",
      "At: 1879 [==========>] Loss 0.1286816336288231  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.09489746341988159  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.11931522278956057  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.11228950879094346  - accuracy: 0.84375\n",
      "At: 1883 [==========>] Loss 0.12323827111743726  - accuracy: 0.78125\n",
      "At: 1884 [==========>] Loss 0.09472019356748518  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.11744038824233545  - accuracy: 0.8125\n",
      "At: 1886 [==========>] Loss 0.12194384801821122  - accuracy: 0.8125\n",
      "At: 1887 [==========>] Loss 0.07831153134101798  - accuracy: 0.84375\n",
      "At: 1888 [==========>] Loss 0.14683563303285888  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.1188917512107742  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.1574351406675568  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.06446001572018464  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.0778946155419696  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.1026933698803816  - accuracy: 0.84375\n",
      "At: 1894 [==========>] Loss 0.08311734875904545  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.07835508120019039  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.12393169171748662  - accuracy: 0.84375\n",
      "At: 1897 [==========>] Loss 0.049486074052266314  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.10849997811643494  - accuracy: 0.84375\n",
      "At: 1899 [==========>] Loss 0.08799297728846846  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.11380931852818793  - accuracy: 0.90625\n",
      "At: 1901 [==========>] Loss 0.10910419051185699  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.13160250918921446  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.12270999711811391  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.04830282823986357  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.14022349458841843  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.10019248378698367  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.08981214243547167  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.08431222497723763  - accuracy: 0.8125\n",
      "At: 1909 [==========>] Loss 0.10229353958841636  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.0571500992926372  - accuracy: 0.875\n",
      "At: 1911 [==========>] Loss 0.12890145577578954  - accuracy: 0.84375\n",
      "At: 1912 [==========>] Loss 0.12729942184715787  - accuracy: 0.875\n",
      "At: 1913 [==========>] Loss 0.17410423449488616  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.10058166794535975  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.1031812572975455  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.15941430065350368  - accuracy: 0.78125\n",
      "At: 1917 [==========>] Loss 0.1845790342021585  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.13344022253776572  - accuracy: 0.8125\n",
      "At: 1919 [==========>] Loss 0.08793276355005031  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.09618249627656228  - accuracy: 0.875\n",
      "At: 1921 [==========>] Loss 0.13970041473694822  - accuracy: 0.71875\n",
      "At: 1922 [==========>] Loss 0.1661106314608023  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.20259387381837357  - accuracy: 0.625\n",
      "At: 1924 [==========>] Loss 0.12876925301287495  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.17535167842977187  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.08275228334493161  - accuracy: 0.875\n",
      "At: 1927 [==========>] Loss 0.07399100891627165  - accuracy: 0.90625\n",
      "At: 1928 [==========>] Loss 0.10833809201794922  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.18520331878016988  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.18186067498347952  - accuracy: 0.75\n",
      "At: 1931 [==========>] Loss 0.08460981088207271  - accuracy: 0.9375\n",
      "At: 1932 [==========>] Loss 0.15756542844281052  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.08405974681687187  - accuracy: 0.84375\n",
      "At: 1934 [==========>] Loss 0.16605763665949677  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.11923442077023809  - accuracy: 0.84375\n",
      "At: 1936 [==========>] Loss 0.10503222021089426  - accuracy: 0.78125\n",
      "At: 1937 [==========>] Loss 0.1367557053940286  - accuracy: 0.8125\n",
      "At: 1938 [==========>] Loss 0.17434784859218977  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.10591116283529022  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.12388084594712351  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.1305089051622355  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.1561041470808358  - accuracy: 0.75\n",
      "At: 1943 [==========>] Loss 0.14372507848451455  - accuracy: 0.8125\n",
      "At: 1944 [==========>] Loss 0.10535241885031334  - accuracy: 0.84375\n",
      "At: 1945 [==========>] Loss 0.16579496942497957  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.09387768487853579  - accuracy: 0.90625\n",
      "At: 1947 [==========>] Loss 0.13590602437173038  - accuracy: 0.8125\n",
      "At: 1948 [==========>] Loss 0.12411253679242618  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.07275913832778659  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.15098803501265445  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.13174235717820038  - accuracy: 0.78125\n",
      "At: 1952 [==========>] Loss 0.09606293320594814  - accuracy: 0.90625\n",
      "At: 1953 [==========>] Loss 0.06684655815924283  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.16860875369069359  - accuracy: 0.78125\n",
      "At: 1955 [==========>] Loss 0.06542685731217146  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.10717241225568311  - accuracy: 0.875\n",
      "At: 1957 [==========>] Loss 0.058631542213771246  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.10128519437414352  - accuracy: 0.8125\n",
      "At: 1959 [==========>] Loss 0.13385623338893463  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.06397600902721072  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.18156904989999384  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.2061398624430558  - accuracy: 0.75\n",
      "At: 1963 [==========>] Loss 0.07381478306452471  - accuracy: 0.96875\n",
      "At: 1964 [==========>] Loss 0.20628088133428035  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.14015389612207116  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.1079562234053853  - accuracy: 0.875\n",
      "At: 1967 [==========>] Loss 0.12268787528360608  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.19467000704686455  - accuracy: 0.6875\n",
      "At: 1969 [==========>] Loss 0.15830282951559838  - accuracy: 0.8125\n",
      "At: 1970 [==========>] Loss 0.12741213204745086  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.18768783998623195  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.08925104807921391  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.14758054666808285  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.139065264233455  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.1415240611058468  - accuracy: 0.75\n",
      "At: 1976 [==========>] Loss 0.07357661905100862  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.09517013273026773  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.11432638229264339  - accuracy: 0.84375\n",
      "At: 1979 [==========>] Loss 0.12142155479386393  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.11970929732474686  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.14172219153080998  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06326142741886334  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.13715705686588026  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.07775016137544372  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.16871787271414643  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.19042940111085804  - accuracy: 0.71875\n",
      "At: 1987 [==========>] Loss 0.1161763279377239  - accuracy: 0.8125\n",
      "At: 1988 [==========>] Loss 0.08733544750242052  - accuracy: 0.9375\n",
      "At: 1989 [==========>] Loss 0.09253819267142835  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.10587971828305792  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.13777070848634343  - accuracy: 0.78125\n",
      "At: 1992 [==========>] Loss 0.09533292822047212  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.1712538633571191  - accuracy: 0.71875\n",
      "At: 1994 [==========>] Loss 0.08859288624047085  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.1661503278850815  - accuracy: 0.8125\n",
      "At: 1996 [==========>] Loss 0.07848978469553622  - accuracy: 0.9375\n",
      "At: 1997 [==========>] Loss 0.198890844116973  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.11946610699758037  - accuracy: 0.84375\n",
      "At: 1999 [==========>] Loss 0.09816944926805447  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.1313628365342795  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.10828043153841398  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.08946317186445368  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.12348977347247787  - accuracy: 0.875\n",
      "At: 2004 [==========>] Loss 0.1303453484220054  - accuracy: 0.875\n",
      "At: 2005 [==========>] Loss 0.11391417342098582  - accuracy: 0.84375\n",
      "At: 2006 [==========>] Loss 0.13215646215180132  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.1187577747717907  - accuracy: 0.78125\n",
      "At: 2008 [==========>] Loss 0.13392061746692518  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.13096871448108527  - accuracy: 0.84375\n",
      "At: 2010 [==========>] Loss 0.11568748787346005  - accuracy: 0.78125\n",
      "At: 2011 [==========>] Loss 0.12357054422245858  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.11265506031380584  - accuracy: 0.8125\n",
      "At: 2013 [==========>] Loss 0.08983555043609738  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.22328346391596296  - accuracy: 0.625\n",
      "At: 2015 [==========>] Loss 0.0629080541474603  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.11501105584032091  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.07026526884226762  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.0917219244047622  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.10746563160957107  - accuracy: 0.875\n",
      "At: 2020 [==========>] Loss 0.058598346181992916  - accuracy: 0.90625\n",
      "At: 2021 [==========>] Loss 0.08964601403160027  - accuracy: 0.90625\n",
      "At: 2022 [==========>] Loss 0.13535292478811928  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.09709040287880505  - accuracy: 0.84375\n",
      "At: 2024 [==========>] Loss 0.07936738428019971  - accuracy: 0.9375\n",
      "At: 2025 [==========>] Loss 0.15638411321510826  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.07856749359787803  - accuracy: 0.90625\n",
      "At: 2027 [==========>] Loss 0.10377292147548872  - accuracy: 0.8125\n",
      "At: 2028 [==========>] Loss 0.1192648754928999  - accuracy: 0.8125\n",
      "At: 2029 [==========>] Loss 0.11594265925328122  - accuracy: 0.84375\n",
      "At: 2030 [==========>] Loss 0.1578698659828821  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.166541180074351  - accuracy: 0.71875\n",
      "At: 2032 [==========>] Loss 0.125124153689623  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.16328132680416235  - accuracy: 0.8125\n",
      "At: 2034 [==========>] Loss 0.2241505782552149  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.10109374253965239  - accuracy: 0.84375\n",
      "At: 2036 [==========>] Loss 0.09572474966817032  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.12155642914619519  - accuracy: 0.8125\n",
      "At: 2038 [==========>] Loss 0.14150298041005577  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.0917853142813069  - accuracy: 0.875\n",
      "At: 2040 [==========>] Loss 0.08158863905137193  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.06535725398704809  - accuracy: 0.875\n",
      "At: 2042 [==========>] Loss 0.14183447771955476  - accuracy: 0.8125\n",
      "At: 2043 [==========>] Loss 0.11303793788098618  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.0868437643081476  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.20503891780593747  - accuracy: 0.71875\n",
      "At: 2046 [==========>] Loss 0.06916685629046535  - accuracy: 0.875\n",
      "At: 2047 [==========>] Loss 0.07425793046980626  - accuracy: 0.875\n",
      "At: 2048 [==========>] Loss 0.10577739975175537  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.14372813032038975  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.1487652250611647  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.16805892783335885  - accuracy: 0.75\n",
      "At: 2052 [==========>] Loss 0.044584385496983535  - accuracy: 0.96875\n",
      "At: 2053 [==========>] Loss 0.10217372997000151  - accuracy: 0.8125\n",
      "At: 2054 [==========>] Loss 0.11428845071980802  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.05872923940647627  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.10942286198366773  - accuracy: 0.875\n",
      "At: 2057 [==========>] Loss 0.13724008574102797  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.14075001131425266  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.2173573438326254  - accuracy: 0.65625\n",
      "At: 2060 [==========>] Loss 0.14529747421704486  - accuracy: 0.78125\n",
      "At: 2061 [==========>] Loss 0.16460941655869635  - accuracy: 0.75\n",
      "At: 2062 [==========>] Loss 0.125140814021166  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.10924328364264256  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.20801661481204592  - accuracy: 0.71875\n",
      "At: 2065 [==========>] Loss 0.031112186221742322  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.20933947424955182  - accuracy: 0.6875\n",
      "At: 2067 [==========>] Loss 0.09547615026094908  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.1040035906970784  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.10273435844150682  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.13056074737772866  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.12742085135196882  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.059131385960782074  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.11060276018360929  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.0698965997105586  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.13650315274920882  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.14442888426845166  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.1362626143275339  - accuracy: 0.84375\n",
      "At: 2078 [==========>] Loss 0.12518934275477  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07528548280517033  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.0847535017889748  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.14237990919810317  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.11547037661724018  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.17851100232712916  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.09295692218521115  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.08020848585763457  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.10502923551601775  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.13036665165328432  - accuracy: 0.84375\n",
      "At: 2088 [==========>] Loss 0.07960655924769183  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.11351318798624374  - accuracy: 0.8125\n",
      "At: 2090 [==========>] Loss 0.0991040882716772  - accuracy: 0.875\n",
      "At: 2091 [==========>] Loss 0.12890337013841358  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.11076579330059302  - accuracy: 0.84375\n",
      "At: 2093 [==========>] Loss 0.13456239053719388  - accuracy: 0.875\n",
      "At: 2094 [==========>] Loss 0.12063003542722563  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.11412047749657367  - accuracy: 0.90625\n",
      "At: 2096 [==========>] Loss 0.14053985449358164  - accuracy: 0.84375\n",
      "At: 2097 [==========>] Loss 0.10053578219830347  - accuracy: 0.90625\n",
      "At: 2098 [==========>] Loss 0.14125318100064854  - accuracy: 0.75\n",
      "At: 2099 [==========>] Loss 0.09744798100041724  - accuracy: 0.84375\n",
      "At: 2100 [==========>] Loss 0.053636726619186115  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.17978060292485587  - accuracy: 0.75\n",
      "At: 2102 [==========>] Loss 0.09178290085824828  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.1622312638975119  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.11874053918615649  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.14626983064431154  - accuracy: 0.8125\n",
      "At: 2106 [==========>] Loss 0.13488669422576094  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.06854455657064981  - accuracy: 0.96875\n",
      "At: 2108 [==========>] Loss 0.135512415902163  - accuracy: 0.875\n",
      "At: 2109 [==========>] Loss 0.11081040932528913  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.05117042925323402  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.095003825444477  - accuracy: 0.90625\n",
      "At: 2112 [==========>] Loss 0.09702084873289235  - accuracy: 0.84375\n",
      "At: 2113 [==========>] Loss 0.12088990728763502  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.12133603864214919  - accuracy: 0.875\n",
      "At: 2115 [==========>] Loss 0.10461070437043056  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.10711044037600677  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.1162070693076782  - accuracy: 0.84375\n",
      "At: 2118 [==========>] Loss 0.14716290868491116  - accuracy: 0.78125\n",
      "At: 2119 [==========>] Loss 0.06128769415283127  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.14436400144272316  - accuracy: 0.8125\n",
      "At: 2121 [==========>] Loss 0.1319539296396104  - accuracy: 0.84375\n",
      "At: 2122 [==========>] Loss 0.12111033711752597  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.1680620266023123  - accuracy: 0.8125\n",
      "At: 2124 [==========>] Loss 0.13499970106832565  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10807394930371705  - accuracy: 0.84375\n",
      "At: 2126 [==========>] Loss 0.042649868314952696  - accuracy: 1.0\n",
      "At: 2127 [==========>] Loss 0.11453658852488416  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.11516243313766938  - accuracy: 0.875\n",
      "At: 2129 [==========>] Loss 0.1506352394668559  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.06975055541329023  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.10835418247719285  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.19518110420030482  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.1509315258792714  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.11117403781448915  - accuracy: 0.84375\n",
      "At: 2135 [==========>] Loss 0.08269325779669749  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.13234211117189904  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.1028183404208029  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.09626797090248795  - accuracy: 0.90625\n",
      "At: 2139 [==========>] Loss 0.19428270580823687  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.08169145230392788  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.09988921577049525  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.13161907622090363  - accuracy: 0.8125\n",
      "At: 2143 [==========>] Loss 0.07895070007643945  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.08212396817196604  - accuracy: 0.875\n",
      "At: 2145 [==========>] Loss 0.11356287601440468  - accuracy: 0.75\n",
      "At: 2146 [==========>] Loss 0.161659807912644  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.16872015069710888  - accuracy: 0.78125\n",
      "At: 2148 [==========>] Loss 0.17756782104451882  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.12976450760549058  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.08860706087121106  - accuracy: 0.90625\n",
      "At: 2151 [==========>] Loss 0.09424045702636624  - accuracy: 0.875\n",
      "At: 2152 [==========>] Loss 0.21258620235924003  - accuracy: 0.71875\n",
      "At: 2153 [==========>] Loss 0.17270981115798284  - accuracy: 0.75\n",
      "At: 2154 [==========>] Loss 0.1309206816289542  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.1348537645059424  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.10596714093358993  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.17472874774469943  - accuracy: 0.78125\n",
      "At: 2158 [==========>] Loss 0.1323336215387173  - accuracy: 0.8125\n",
      "At: 2159 [==========>] Loss 0.08085975370725358  - accuracy: 0.875\n",
      "At: 2160 [==========>] Loss 0.13788385112418072  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.10186887589909174  - accuracy: 0.84375\n",
      "At: 2162 [==========>] Loss 0.0949747585215702  - accuracy: 0.8125\n",
      "At: 2163 [==========>] Loss 0.12654464419236625  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.1638653713978556  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.10217184984199972  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.09847190011228925  - accuracy: 0.875\n",
      "At: 2167 [==========>] Loss 0.08963120043388859  - accuracy: 0.9375\n",
      "At: 2168 [==========>] Loss 0.07308492474443068  - accuracy: 0.875\n",
      "At: 2169 [==========>] Loss 0.11458147866117309  - accuracy: 0.84375\n",
      "At: 2170 [==========>] Loss 0.09714614615649857  - accuracy: 0.90625\n",
      "At: 2171 [==========>] Loss 0.1474641790968871  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.07299069286527182  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.14903718669719762  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.12111649430921426  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.1118888787522885  - accuracy: 0.90625\n",
      "At: 2176 [==========>] Loss 0.10179537172490982  - accuracy: 0.875\n",
      "At: 2177 [==========>] Loss 0.12379465040437253  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.10995463602552086  - accuracy: 0.84375\n",
      "At: 2179 [==========>] Loss 0.14613837939123214  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.10319255830171647  - accuracy: 0.90625\n",
      "At: 2181 [==========>] Loss 0.17686586101239302  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.09099873842078907  - accuracy: 0.90625\n",
      "At: 2183 [==========>] Loss 0.21990494875641886  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.07093164949106365  - accuracy: 0.875\n",
      "At: 2185 [==========>] Loss 0.10679640269576632  - accuracy: 0.84375\n",
      "At: 2186 [==========>] Loss 0.16766166216318074  - accuracy: 0.75\n",
      "At: 2187 [==========>] Loss 0.1702740987087905  - accuracy: 0.75\n",
      "At: 2188 [==========>] Loss 0.06694146381507445  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.052854651920146045  - accuracy: 0.9375\n",
      "At: 2190 [==========>] Loss 0.12136402401667423  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.11768874407949102  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.0872155164762434  - accuracy: 0.90625\n",
      "At: 2193 [==========>] Loss 0.1298341561019614  - accuracy: 0.8125\n",
      "At: 2194 [==========>] Loss 0.09810990311468377  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.1771605198419514  - accuracy: 0.71875\n",
      "At: 2196 [==========>] Loss 0.1359809704645957  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.09230514463971275  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.0866394216383057  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.04916571128489206  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.03912455502506641  - accuracy: 0.96875\n",
      "At: 2201 [==========>] Loss 0.11776667250759064  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.07681037615450872  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.07396403451048086  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.10586471792168997  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.11280827167657946  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.10018005238729011  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.08010591836950853  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.14542836838423645  - accuracy: 0.8125\n",
      "At: 2209 [==========>] Loss 0.18182581606939108  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.12259778474309023  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.14575500432864408  - accuracy: 0.8125\n",
      "At: 2212 [==========>] Loss 0.079837608701122  - accuracy: 0.9375\n",
      "At: 2213 [==========>] Loss 0.11242502963888312  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.17114898063579503  - accuracy: 0.71875\n",
      "At: 2215 [==========>] Loss 0.13263028535369095  - accuracy: 0.84375\n",
      "At: 2216 [==========>] Loss 0.12221981026435465  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.09945403410220725  - accuracy: 0.875\n",
      "At: 2218 [==========>] Loss 0.14469446146191417  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.06405131927627639  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.1122149987172709  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.1736279446904897  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.10058665631311745  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.16491458027891065  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.12198158499197445  - accuracy: 0.84375\n",
      "At: 2225 [==========>] Loss 0.11382181626162846  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.1338478080774736  - accuracy: 0.8125\n",
      "At: 2227 [==========>] Loss 0.1957582223681772  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.08771269279718452  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.1701578741922281  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.12082117216490658  - accuracy: 0.8125\n",
      "At: 2231 [==========>] Loss 0.15783322680049935  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.16390773689982122  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.16104115154195178  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.12042893745856313  - accuracy: 0.84375\n",
      "At: 2235 [==========>] Loss 0.11646574382054571  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.08796156872324072  - accuracy: 0.875\n",
      "At: 2237 [==========>] Loss 0.1189917434607914  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.133127041615734  - accuracy: 0.84375\n",
      "At: 2239 [==========>] Loss 0.1892223291195076  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.14849674035831475  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.13383908259538257  - accuracy: 0.84375\n",
      "At: 2242 [==========>] Loss 0.17336196880767674  - accuracy: 0.78125\n",
      "At: 2243 [==========>] Loss 0.08151343363528434  - accuracy: 0.9375\n",
      "At: 2244 [==========>] Loss 0.08962731353858139  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.0708615409937626  - accuracy: 0.9375\n",
      "At: 2246 [==========>] Loss 0.13255414120012277  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.1146180089821076  - accuracy: 0.78125\n",
      "At: 2248 [==========>] Loss 0.15627137075135947  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.09626961886583456  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.0994718621553083  - accuracy: 0.875\n",
      "At: 2251 [==========>] Loss 0.058938874754438875  - accuracy: 0.90625\n",
      "At: 2252 [==========>] Loss 0.11929120001170884  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.13364220893843917  - accuracy: 0.71875\n",
      "At: 2254 [==========>] Loss 0.10084506565506551  - accuracy: 0.78125\n",
      "At: 2255 [==========>] Loss 0.12360779325959714  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.1521283866070824  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.10797586315289703  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.12394723079895083  - accuracy: 0.8125\n",
      "At: 2259 [==========>] Loss 0.13509594516897666  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.17640887711744824  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.09634269182032852  - accuracy: 0.84375\n",
      "At: 2262 [==========>] Loss 0.1490286913380924  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.16610799403359028  - accuracy: 0.78125\n",
      "At: 2264 [==========>] Loss 0.09605281105473204  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.1036456520392903  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.12360032044539115  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.059257814678846166  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.0757019559057741  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.038752281946729285  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.08843189698911559  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.1468716136020094  - accuracy: 0.75\n",
      "At: 2272 [==========>] Loss 0.08114279244014766  - accuracy: 0.90625\n",
      "At: 2273 [==========>] Loss 0.10557962675688988  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.0980251205628702  - accuracy: 0.84375\n",
      "At: 2275 [==========>] Loss 0.10762105655666024  - accuracy: 0.8125\n",
      "At: 2276 [==========>] Loss 0.08972708480460553  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.15110681541832566  - accuracy: 0.78125\n",
      "At: 2278 [==========>] Loss 0.09379138948229439  - accuracy: 0.875\n",
      "At: 2279 [==========>] Loss 0.13387396340853322  - accuracy: 0.84375\n",
      "At: 2280 [==========>] Loss 0.127529922059005  - accuracy: 0.8125\n",
      "At: 2281 [==========>] Loss 0.11280788483965178  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.06900356577578565  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.15453249958171605  - accuracy: 0.71875\n",
      "At: 2284 [==========>] Loss 0.10965207282954367  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.11844274921131143  - accuracy: 0.84375\n",
      "At: 2286 [==========>] Loss 0.11708450005457785  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.14169730089438393  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.08846006394647918  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.14165579795898553  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.07106223601896458  - accuracy: 0.875\n",
      "At: 2291 [==========>] Loss 0.13142444778222218  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.09764795975758264  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.04400583854287307  - accuracy: 0.9375\n",
      "At: 2294 [==========>] Loss 0.04184930864454938  - accuracy: 0.96875\n",
      "At: 2295 [==========>] Loss 0.1520359129253624  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.14418905570100615  - accuracy: 0.84375\n",
      "At: 2297 [==========>] Loss 0.06793323428881207  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.0917080681949555  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.1298261956456937  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.1212811078643985  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.1984038394747471  - accuracy: 0.71875\n",
      "At: 2302 [==========>] Loss 0.17456395590097745  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.041485888221389385  - accuracy: 0.96875\n",
      "At: 2304 [==========>] Loss 0.09122803927521173  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.0914573880966294  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.12700933794064634  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.14742824008609134  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.16677696244829987  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.14896654110771287  - accuracy: 0.78125\n",
      "At: 2310 [==========>] Loss 0.13194962650096992  - accuracy: 0.8125\n",
      "At: 2311 [==========>] Loss 0.1552889439239821  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.10363598771996732  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.08780380775570541  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.09243691790626107  - accuracy: 0.90625\n",
      "At: 2315 [==========>] Loss 0.11470736640269771  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.14051698240659405  - accuracy: 0.875\n",
      "At: 2317 [==========>] Loss 0.18313661442765455  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.1792690445979114  - accuracy: 0.75\n",
      "At: 2319 [==========>] Loss 0.09928579974150781  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.08436688448340061  - accuracy: 0.90625\n",
      "At: 2321 [==========>] Loss 0.15167324238767485  - accuracy: 0.75\n",
      "At: 2322 [==========>] Loss 0.18367793316286737  - accuracy: 0.78125\n",
      "At: 2323 [==========>] Loss 0.13606778966845157  - accuracy: 0.84375\n",
      "At: 2324 [==========>] Loss 0.17548449181811157  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.14463528028082057  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.07769558717129393  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.07293693441951184  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.11422313437452207  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.0885180508065854  - accuracy: 0.90625\n",
      "At: 2330 [==========>] Loss 0.14634872656169132  - accuracy: 0.78125\n",
      "At: 2331 [==========>] Loss 0.10495819994465021  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.09506682262195641  - accuracy: 0.90625\n",
      "At: 2333 [==========>] Loss 0.09686924112091502  - accuracy: 0.90625\n",
      "At: 2334 [==========>] Loss 0.1707854230942229  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.10366543249917623  - accuracy: 0.875\n",
      "At: 2336 [==========>] Loss 0.09907870130966431  - accuracy: 0.8125\n",
      "At: 2337 [==========>] Loss 0.13664282799825678  - accuracy: 0.8125\n",
      "At: 2338 [==========>] Loss 0.1181475062231318  - accuracy: 0.78125\n",
      "At: 2339 [==========>] Loss 0.09469243310874847  - accuracy: 0.8125\n",
      "At: 2340 [==========>] Loss 0.1415805874434764  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.13865351158860442  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.143337039014353  - accuracy: 0.84375\n",
      "At: 2343 [==========>] Loss 0.10869483749850264  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.21893128967761802  - accuracy: 0.65625\n",
      "At: 2345 [==========>] Loss 0.1504956146832127  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.07810321133581695  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.12459022573038525  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.07284917209790194  - accuracy: 0.90625\n",
      "At: 2349 [==========>] Loss 0.08775509282753426  - accuracy: 0.84375\n",
      "At: 2350 [==========>] Loss 0.11904781068727263  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.0695742936329926  - accuracy: 0.9375\n",
      "At: 2352 [==========>] Loss 0.09605769083944721  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.07616373698169503  - accuracy: 0.9375\n",
      "At: 2354 [==========>] Loss 0.14112274459408863  - accuracy: 0.75\n",
      "At: 2355 [==========>] Loss 0.051436424837104366  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.10986475705670279  - accuracy: 0.875\n",
      "At: 2357 [==========>] Loss 0.15796890920157475  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.15230113677530974  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.19510611401419486  - accuracy: 0.625\n",
      "At: 2360 [==========>] Loss 0.11906328062785151  - accuracy: 0.78125\n",
      "At: 2361 [==========>] Loss 0.14154159480929032  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.08209497219904736  - accuracy: 0.875\n",
      "At: 2363 [==========>] Loss 0.17610653580634744  - accuracy: 0.8125\n",
      "At: 2364 [==========>] Loss 0.08234541863496836  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.0871463117410336  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.16986801385417413  - accuracy: 0.6875\n",
      "At: 2367 [==========>] Loss 0.0955814355370913  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.09064243041284581  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.09603494716035993  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.08219258631597536  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.09409957951116289  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.13271341021363567  - accuracy: 0.75\n",
      "At: 2373 [==========>] Loss 0.11115190905827049  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.12677293597321262  - accuracy: 0.8125\n",
      "At: 2375 [==========>] Loss 0.04937556297065089  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.11482028970611202  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.09813656632946563  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.14242668276709597  - accuracy: 0.8125\n",
      "At: 2379 [==========>] Loss 0.17937337481181848  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.05827037049257222  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.07607864100580858  - accuracy: 0.9375\n",
      "At: 2382 [==========>] Loss 0.09824183824469525  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1134781131028538  - accuracy: 0.875\n",
      "At: 2384 [==========>] Loss 0.1042305810731508  - accuracy: 0.875\n",
      "At: 2385 [==========>] Loss 0.11974796057023457  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.10607902360116359  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.06978146434058882  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.12933111260447394  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.05169016592692169  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.06334038249179043  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.17775672129165815  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.12577866925360184  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.11336130032955119  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.06715617767908769  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.10116050280648975  - accuracy: 0.875\n",
      "At: 2396 [==========>] Loss 0.08401293229881007  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.07529658633469569  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.10628238446171587  - accuracy: 0.8125\n",
      "At: 2399 [==========>] Loss 0.15275079476641606  - accuracy: 0.75\n",
      "At: 2400 [==========>] Loss 0.09838757564869324  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.10345169958557626  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.08720736220170902  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.18239613727509058  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.13166315684454066  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.05665512350120938  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.13312280936747722  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.10447192485309485  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.08842866327731413  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.15262932465948112  - accuracy: 0.84375\n",
      "At: 2410 [==========>] Loss 0.1612215833214987  - accuracy: 0.78125\n",
      "At: 2411 [==========>] Loss 0.10610982207325059  - accuracy: 0.90625\n",
      "At: 2412 [==========>] Loss 0.0731915563288376  - accuracy: 0.9375\n",
      "At: 2413 [==========>] Loss 0.07501278464071783  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.06836768058135878  - accuracy: 0.9375\n",
      "At: 2415 [==========>] Loss 0.08392914314444  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.10397926287518969  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.168173185970188  - accuracy: 0.71875\n",
      "At: 2418 [==========>] Loss 0.09552968476480653  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.1483199464258853  - accuracy: 0.75\n",
      "At: 2420 [==========>] Loss 0.13482475181678918  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.09251334112467569  - accuracy: 0.84375\n",
      "At: 2422 [==========>] Loss 0.13318920173449933  - accuracy: 0.8125\n",
      "At: 2423 [==========>] Loss 0.11929778374863224  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.0958590754428871  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.09378066929715946  - accuracy: 0.875\n",
      "At: 2426 [==========>] Loss 0.1666758913834081  - accuracy: 0.75\n",
      "At: 2427 [==========>] Loss 0.13354638988715195  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.06949004729493427  - accuracy: 0.9375\n",
      "At: 2429 [==========>] Loss 0.09931176375720152  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.14160688589688927  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.1551248709757349  - accuracy: 0.84375\n",
      "At: 2432 [==========>] Loss 0.06880706715715566  - accuracy: 0.90625\n",
      "At: 2433 [==========>] Loss 0.06184076775913218  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.042455228923169316  - accuracy: 0.90625\n",
      "At: 2435 [==========>] Loss 0.16529315411786358  - accuracy: 0.78125\n",
      "At: 2436 [==========>] Loss 0.09676753272946045  - accuracy: 0.90625\n",
      "At: 2437 [==========>] Loss 0.17377524082214788  - accuracy: 0.75\n",
      "At: 2438 [==========>] Loss 0.11391129393006591  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.16269697723904447  - accuracy: 0.71875\n",
      "At: 2440 [==========>] Loss 0.0905933874957426  - accuracy: 0.9375\n",
      "At: 2441 [==========>] Loss 0.10798065216938856  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.12681119080088119  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.12761555783004777  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.0805035212377945  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.06765302565246932  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.18550734275645758  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.17309467888164112  - accuracy: 0.8125\n",
      "At: 2448 [==========>] Loss 0.11725037213135345  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.06938212935377903  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.05587553952817847  - accuracy: 0.9375\n",
      "At: 2451 [==========>] Loss 0.05172336480008895  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.1395978780677505  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.12181034890770093  - accuracy: 0.84375\n",
      "At: 2454 [==========>] Loss 0.14515762451846498  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.12314744989285376  - accuracy: 0.78125\n",
      "At: 2456 [==========>] Loss 0.12939654649276477  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.1579736227759569  - accuracy: 0.8125\n",
      "At: 2458 [==========>] Loss 0.06947621970010971  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.13164140247412423  - accuracy: 0.78125\n",
      "At: 2460 [==========>] Loss 0.08852147478873396  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.05222116024101512  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.16798931368508746  - accuracy: 0.71875\n",
      "At: 2463 [==========>] Loss 0.11316812183045562  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.17791636651967013  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.12991561700234855  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.0923106004688547  - accuracy: 0.875\n",
      "At: 2467 [==========>] Loss 0.10045693215245907  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.07715891436779822  - accuracy: 0.9375\n",
      "At: 2469 [==========>] Loss 0.10506314430075442  - accuracy: 0.90625\n",
      "At: 2470 [==========>] Loss 0.11843221370389137  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.1100422211898375  - accuracy: 0.90625\n",
      "At: 2472 [==========>] Loss 0.10420327395235074  - accuracy: 0.84375\n",
      "At: 2473 [==========>] Loss 0.10537848408880135  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.08356734626011554  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10215600304880158  - accuracy: 0.84375\n",
      "At: 2476 [==========>] Loss 0.07186343770398804  - accuracy: 0.875\n",
      "At: 2477 [==========>] Loss 0.15375456114553593  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.11422682354248939  - accuracy: 0.8125\n",
      "At: 2479 [==========>] Loss 0.08080100908955173  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.11070472567004461  - accuracy: 0.78125\n",
      "At: 2481 [==========>] Loss 0.0666218153623218  - accuracy: 0.90625\n",
      "At: 2482 [==========>] Loss 0.16274519518656844  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.10070123042010029  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.10064723348280899  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.11146562169482983  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.12892603489954327  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.12090957051940827  - accuracy: 0.84375\n",
      "At: 2488 [==========>] Loss 0.18122164143731945  - accuracy: 0.71875\n",
      "At: 2489 [==========>] Loss 0.16585635789820588  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.11958001215066036  - accuracy: 0.84375\n",
      "At: 2491 [==========>] Loss 0.11407596818455877  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.15024909647628393  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.09051660031966555  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.09714982930738962  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.10009906629034161  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.05554830509211514  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.20663186868501188  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.11844691443294747  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.04680568034864929  - accuracy: 0.96875\n",
      "At: 2500 [==========>] Loss 0.1587514479757203  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.12848902339552448  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.11480915873928921  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.11247826783831995  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.14069313136459516  - accuracy: 0.84375\n",
      "At: 2505 [==========>] Loss 0.09824160498292528  - accuracy: 0.84375\n",
      "At: 2506 [==========>] Loss 0.10907341753436144  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.14016090241370582  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.09252420252177472  - accuracy: 0.90625\n",
      "At: 2509 [==========>] Loss 0.1295188605435492  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.11626499658283523  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14389782598656958  - accuracy: 0.78125\n",
      "At: 2512 [==========>] Loss 0.0939653097989879  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.13734149552663674  - accuracy: 0.84375\n",
      "At: 2514 [==========>] Loss 0.13875571908967665  - accuracy: 0.84375\n",
      "At: 2515 [==========>] Loss 0.1831658145734908  - accuracy: 0.78125\n",
      "At: 2516 [==========>] Loss 0.1942514907251965  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.09398265538804818  - accuracy: 0.84375\n",
      "At: 2518 [==========>] Loss 0.11744088940387891  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.09103110014954677  - accuracy: 0.84375\n",
      "At: 2520 [==========>] Loss 0.13843165396119061  - accuracy: 0.8125\n",
      "At: 2521 [==========>] Loss 0.10830645189192303  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.2287345805908944  - accuracy: 0.625\n",
      "At: 2523 [==========>] Loss 0.12221675659207061  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.17781660297639496  - accuracy: 0.75\n",
      "At: 2525 [==========>] Loss 0.06835022706318727  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.10871698942492752  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.12400648212711937  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.08810364378586005  - accuracy: 0.8125\n",
      "At: 2529 [==========>] Loss 0.14560723489926508  - accuracy: 0.8125\n",
      "At: 2530 [==========>] Loss 0.13207558273201686  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.0429672489462101  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.10044788236387409  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.09574579060741445  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.0669488356976189  - accuracy: 0.90625\n",
      "At: 2535 [==========>] Loss 0.09494192002768224  - accuracy: 0.84375\n",
      "At: 2536 [==========>] Loss 0.1587078252952235  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.09954488651751023  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.1670876039956682  - accuracy: 0.75\n",
      "At: 2539 [==========>] Loss 0.08343572216371017  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.10798857806976778  - accuracy: 0.84375\n",
      "At: 2541 [==========>] Loss 0.07464239869261469  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.09172217458535378  - accuracy: 0.84375\n",
      "At: 2543 [==========>] Loss 0.17439356254594868  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.13286059334737382  - accuracy: 0.8125\n",
      "At: 2545 [==========>] Loss 0.05983376578880696  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.1293495949240493  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.10723634521721166  - accuracy: 0.8125\n",
      "At: 2548 [==========>] Loss 0.08860111113684763  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.08877179838899978  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.13411182923357248  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.12405465876278302  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.1075034518876776  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.0736159384716123  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.14549598507095401  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.159320669795167  - accuracy: 0.78125\n",
      "At: 2556 [==========>] Loss 0.08082261995322493  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.12483207402755286  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.08229803553797801  - accuracy: 0.875\n",
      "At: 2559 [==========>] Loss 0.12125137264481511  - accuracy: 0.78125\n",
      "At: 2560 [==========>] Loss 0.21077352809306454  - accuracy: 0.65625\n",
      "At: 2561 [==========>] Loss 0.09531040394947507  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.10871456917772176  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.11145215483552882  - accuracy: 0.84375\n",
      "At: 2564 [==========>] Loss 0.08526835828562683  - accuracy: 0.90625\n",
      "At: 2565 [==========>] Loss 0.12715816365556198  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.1679978286032043  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.1279802365065116  - accuracy: 0.75\n",
      "At: 2568 [==========>] Loss 0.09321578900581168  - accuracy: 0.875\n",
      "At: 2569 [==========>] Loss 0.08483233899244616  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.20912031718161683  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.10145969006743652  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.17137654755183346  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.1732135613274534  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.11439507730796919  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.07203579635550617  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.12379048945582605  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.21009587569066213  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.18251582520854762  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.18768564339346952  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.1570665646337126  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.07267303816978929  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.19728549027425463  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.0669399380349681  - accuracy: 0.90625\n",
      "At: 2584 [==========>] Loss 0.19494075762462665  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.09350774142039123  - accuracy: 0.84375\n",
      "At: 2586 [==========>] Loss 0.09369879099425116  - accuracy: 0.78125\n",
      "At: 2587 [==========>] Loss 0.16269927599478168  - accuracy: 0.75\n",
      "At: 2588 [==========>] Loss 0.11586695523093486  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.09225922389098658  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.19658683566973  - accuracy: 0.71875\n",
      "At: 2591 [==========>] Loss 0.1268493201227086  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.09005394004230002  - accuracy: 0.875\n",
      "At: 2593 [==========>] Loss 0.07293185609212935  - accuracy: 0.90625\n",
      "At: 2594 [==========>] Loss 0.08248555085798517  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.09233080731089707  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.10021457316015785  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.08367637598062924  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.10917997791481424  - accuracy: 0.875\n",
      "At: 2599 [==========>] Loss 0.10993432440735136  - accuracy: 0.875\n",
      "At: 2600 [==========>] Loss 0.14304817273921966  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.12928495742097526  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.07380419177796212  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.12580294514488102  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.11924570584594285  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.13391370551374726  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.12226523614172581  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.13684448766196278  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.06482565876326724  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.09084501900770615  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.11599790931250273  - accuracy: 0.84375\n",
      "At: 2611 [==========>] Loss 0.1525082290830224  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.0873177844281725  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.08702475681770924  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.09869091336278882  - accuracy: 0.875\n",
      "At: 2615 [==========>] Loss 0.06552987873280341  - accuracy: 0.9375\n",
      "At: 2616 [==========>] Loss 0.04689294126305934  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.08246037421860938  - accuracy: 0.90625\n",
      "At: 2618 [==========>] Loss 0.08249971038577743  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.09473480871311166  - accuracy: 0.90625\n",
      "At: 2620 [==========>] Loss 0.11424911366204481  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.11362877884969597  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.1237081677839206  - accuracy: 0.84375\n",
      "At: 2623 [==========>] Loss 0.08333426974607962  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.13542050448729995  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.053286703036779307  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.05530303664019866  - accuracy: 0.9375\n",
      "At: 2627 [==========>] Loss 0.1362545087757645  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.07670152905274838  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.07864140991682361  - accuracy: 0.90625\n",
      "At: 2630 [==========>] Loss 0.08961708165098856  - accuracy: 0.9375\n",
      "At: 2631 [==========>] Loss 0.1425263895210354  - accuracy: 0.8125\n",
      "At: 2632 [==========>] Loss 0.14431218953537886  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.08658206663098827  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.06449318348113255  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.19105762781477606  - accuracy: 0.71875\n",
      "At: 2636 [==========>] Loss 0.11042174031236487  - accuracy: 0.84375\n",
      "At: 2637 [==========>] Loss 0.12049728442604074  - accuracy: 0.875\n",
      "At: 2638 [==========>] Loss 0.08593824363866401  - accuracy: 0.875\n",
      "At: 2639 [==========>] Loss 0.07003963154079469  - accuracy: 0.90625\n",
      "At: 2640 [==========>] Loss 0.12157777309472913  - accuracy: 0.875\n",
      "At: 2641 [==========>] Loss 0.1279161546627782  - accuracy: 0.84375\n",
      "At: 2642 [==========>] Loss 0.1655072427916  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.07103008327827884  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.1549416395598135  - accuracy: 0.8125\n",
      "At: 2645 [==========>] Loss 0.0675343652116409  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.14799110283077727  - accuracy: 0.75\n",
      "At: 2647 [==========>] Loss 0.10631325906576136  - accuracy: 0.90625\n",
      "At: 2648 [==========>] Loss 0.11958248044735803  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.11976764422521068  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.10522149807398434  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.18761551312433644  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.08766923264336138  - accuracy: 0.875\n",
      "At: 2653 [==========>] Loss 0.10487668537871245  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11146316235185055  - accuracy: 0.84375\n",
      "At: 2655 [==========>] Loss 0.22328338740114956  - accuracy: 0.71875\n",
      "At: 2656 [==========>] Loss 0.02475244851061403  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.09859818117730726  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.08930351003907328  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.052019732986693525  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.11245719797867801  - accuracy: 0.84375\n",
      "At: 2661 [==========>] Loss 0.06783084396874453  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.047763246947448504  - accuracy: 0.96875\n",
      "At: 2663 [==========>] Loss 0.08593879586147107  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.07498120915807456  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.06954068256206053  - accuracy: 0.9375\n",
      "At: 2666 [==========>] Loss 0.14349159682023896  - accuracy: 0.75\n",
      "At: 2667 [==========>] Loss 0.10845320072581317  - accuracy: 0.90625\n",
      "At: 2668 [==========>] Loss 0.14819545847755475  - accuracy: 0.78125\n",
      "At: 2669 [==========>] Loss 0.13138113905991797  - accuracy: 0.75\n",
      "At: 2670 [==========>] Loss 0.09970758649060638  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.06633337057901394  - accuracy: 0.96875\n",
      "At: 2672 [==========>] Loss 0.11856536156048517  - accuracy: 0.8125\n",
      "At: 2673 [==========>] Loss 0.13050631771505566  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.11770986525901747  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.12826742376844386  - accuracy: 0.84375\n",
      "At: 2676 [==========>] Loss 0.13026951606893272  - accuracy: 0.75\n",
      "At: 2677 [==========>] Loss 0.12354898422640052  - accuracy: 0.8125\n",
      "At: 2678 [==========>] Loss 0.0445890027396686  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.06354507501947106  - accuracy: 0.9375\n",
      "At: 2680 [==========>] Loss 0.11194562987241505  - accuracy: 0.8125\n",
      "At: 2681 [==========>] Loss 0.09647926294276409  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.15332636640363795  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.17121092297205706  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09213974834308324  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.11193878121176046  - accuracy: 0.78125\n",
      "At: 2686 [==========>] Loss 0.046768048422114586  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.141355348146157  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.16344840603875266  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.11104455116137432  - accuracy: 0.875\n",
      "At: 2690 [==========>] Loss 0.15331958170616516  - accuracy: 0.78125\n"
     ]
    }
   ],
   "source": [
    "import model.nn as nn\n",
    "import numpy as np\n",
    "from model.optimizer import optimizer as optimizer\n",
    "\n",
    "X = np.array(X_train).T\n",
    "Y = np.array([y_train])\n",
    "\n",
    "# optimize using  ADAM\n",
    "net = nn.nn([26, 32, 32, 1], ['relu', 'relu', 'sigmoid'], epochs=10)\n",
    "net.cost_function = 'mseloss'\n",
    "print('net architecture :')\n",
    "print(net)\n",
    "\n",
    "optim = optimizer.AdamOptimizer\n",
    "optim(X, Y, net, alpha=0.00009, lamb=0.05, print_at=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Adam:\n",
      " accuracy =  74.33800984855523\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(X_test).T\n",
    "prediction = net.forward(X_test)\n",
    "y_actual = np.array([y_test])\n",
    "\n",
    "prediction = 1 * (prediction >= 0.5)\n",
    "accuracy = np.sum(prediction == y_actual[0]) / prediction.shape[1]\n",
    "\n",
    "print('for Adam:\\n accuracy = ', accuracy * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "21526"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = net.forward(X_test)\n",
    "result = pd.DataFrame({'id': [*range(1, prediction[0].size + 1)], 'prediction': prediction[0], 'actual': y_actual[0]})\n",
    "result.to_sql(con=database_connection, name='result', if_exists='replace', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [56], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(\u001B[43mhistory\u001B[49m\u001B[38;5;241m.\u001B[39mhistory)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(history_df\u001B[38;5;241m.\u001B[39mloc[:, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#BDE2E2\u001B[39m\u001B[38;5;124m\"\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(history_df\u001B[38;5;241m.\u001B[39mloc[:, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#C2C4E2\u001B[39m\u001B[38;5;124m\"\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\n",
    "plt.plot(history_df.loc[:, ['val_loss']], \"#C2C4E2\", label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(history_df.loc[:, ['accuracy']], \"#BDE2E2\", label='Training accuracy')\n",
    "plt.plot(history_df.loc[:, ['val_accuracy']], \"#C2C4E2\", label='Validation accuracy')\n",
    "\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [48], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Predicting the test set results\u001B[39;00m\n\u001B[0;32m      2\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m----> 3\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43my_test\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43my_pred\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m output\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredict.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m (y_pred \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\n",
      "File \u001B[1;32md:\\study\\python\\venv\\lib\\site-packages\\pandas\\core\\frame.py:662\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    656\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[0;32m    657\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    658\u001B[0m     )\n\u001B[0;32m    660\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    661\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[1;32m--> 662\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[0;32m    664\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmrecords\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmrecords\u001B[39;00m\n",
      "File \u001B[1;32md:\\study\\python\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[0;32m    491\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[1;32m--> 493\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\study\\python\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_integrity:\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# figure out the index, if necessary\u001B[39;00m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m         index \u001B[38;5;241m=\u001B[39m \u001B[43m_extract_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[1;32md:\\study\\python\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:653\u001B[0m, in \u001B[0;36m_extract_index\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    651\u001B[0m         raw_lengths\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mlen\u001B[39m(val))\n\u001B[0;32m    652\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(val, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m val\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 653\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPer-column arrays must each be 1-dimensional\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    655\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m indexes \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m raw_lengths:\n\u001B[0;32m    656\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf using all scalar values, you must pass an index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "cmap1 = sns.diverging_palette(260, -10, s=50, l=75, n=5, as_cmap=True)\n",
    "plt.subplots(figsize=(12, 8))\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix), cmap=cmap1, annot=True, annot_kws={'size': 15})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     20110\n",
      "           1       0.74      0.46      0.57      5398\n",
      "\n",
      "    accuracy                           0.85     25508\n",
      "   macro avg       0.80      0.71      0.74     25508\n",
      "weighted avg       0.84      0.85      0.84     25508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.07870916],\n       [0.01766342],\n       [0.13494852],\n       ...,\n       [0.05311829],\n       [0.00807065],\n       [0.29090303]], dtype=float32)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the code to save predictions in the format used for competition scoring\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'y_test': y_test, 'y_pred': list(y_pred)})\n",
    "output.to_csv('Predict.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
