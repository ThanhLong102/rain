{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import db.Db as db\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>44</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>22</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8</td>\n",
       "      <td>NA</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NA</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NE</td>\n",
       "      <td>24</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>41</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>33</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall Evaporation Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NA       NA   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NA       NA   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NA       NA   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NA       NA   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NA       NA   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W             44          W  ...          71           22   \n",
       "1         WNW             44        NNW  ...          44           25   \n",
       "2         WSW             46          W  ...          38           30   \n",
       "3          NE             24         SE  ...          45           16   \n",
       "4           W             41        ENE  ...          82           33   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am Temp3pm RainToday  \\\n",
       "0       1007.7       1007.1         8        NA     16.9    21.8        No   \n",
       "1       1010.6       1007.8        NA        NA     17.2    24.3        No   \n",
       "2       1007.6       1008.7        NA         2     21.0    23.2        No   \n",
       "3       1017.6       1012.8        NA        NA     18.1    26.5        No   \n",
       "4       1010.8       1006.0         7         8     17.8    29.7        No   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, column_name = db.getWeather()\n",
    "data = pd.DataFrame(data=result, columns=column_name)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119590 entries, 0 to 119589\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           119590 non-null  object \n",
      " 1   Location       119590 non-null  object \n",
      " 2   MinTemp        119590 non-null  float64\n",
      " 3   MaxTemp        119590 non-null  float64\n",
      " 4   Rainfall       119590 non-null  float64\n",
      " 5   Evaporation    119590 non-null  object \n",
      " 6   Sunshine       119590 non-null  object \n",
      " 7   WindGustDir    119590 non-null  object \n",
      " 8   WindGustSpeed  119590 non-null  int64  \n",
      " 9   WindDir9am     119590 non-null  object \n",
      " 10  WindDir3pm     119590 non-null  object \n",
      " 11  WindSpeed9am   119590 non-null  int64  \n",
      " 12  WindSpeed3pm   119590 non-null  int64  \n",
      " 13  Humidity9am    119590 non-null  int64  \n",
      " 14  Humidity3pm    119590 non-null  int64  \n",
      " 15  Pressure9am    119590 non-null  float64\n",
      " 16  Pressure3pm    119590 non-null  float64\n",
      " 17  Cloud9am       119590 non-null  object \n",
      " 18  Cloud3pm       119590 non-null  object \n",
      " 19  Temp9am        119590 non-null  float64\n",
      " 20  Temp3pm        119590 non-null  float64\n",
      " 21  RainToday      119590 non-null  object \n",
      " 22  RainTomorrow   119590 non-null  object \n",
      "dtypes: float64(7), int64(5), object(11)\n",
      "memory usage: 21.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='RainTomorrow', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#first of all let us evaluate the target and find out if our data is imbalanced or not\n",
    "cols = [\"#C2C4E2\", \"#EED4E5\"]\n",
    "sns.countplot(x=data[\"RainTomorrow\"], palette=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation amongst numeric attributes\n",
    "corrmat = data.corr()\n",
    "cmap = sns.diverging_palette(260, -10, s=50, l=75, n=6, as_cmap=True)\n",
    "plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(corrmat, cmap=cmap, annot=True, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    119590\n",
       "Name: Date, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsing datetime\n",
    "#exploring the length of date objects\n",
    "lengths = data[\"Date\"].str.len()\n",
    "lengths.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>44</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NE</td>\n",
       "      <td>24</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>41</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Location  MinTemp  MaxTemp  Rainfall Evaporation Sunshine  \\\n",
       "0 2008-12-01   Albury     13.4     22.9       0.6          NA       NA   \n",
       "1 2008-12-02   Albury      7.4     25.1       0.0          NA       NA   \n",
       "2 2008-12-03   Albury     12.9     25.7       0.0          NA       NA   \n",
       "3 2008-12-04   Albury      9.2     28.0       0.0          NA       NA   \n",
       "4 2008-12-05   Albury     17.5     32.3       1.0          NA       NA   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Temp3pm  RainToday  RainTomorrow  \\\n",
       "0           W             44          W  ...    21.8         No            No   \n",
       "1         WNW             44        NNW  ...    24.3         No            No   \n",
       "2         WSW             46          W  ...    23.2         No            No   \n",
       "3          NE             24         SE  ...    26.5         No            No   \n",
       "4           W             41        ENE  ...    29.7         No            No   \n",
       "\n",
       "   year  month     month_sin  month_cos day   day_sin   day_cos  \n",
       "0  2008     12 -2.449294e-16        1.0   1  0.201299  0.979530  \n",
       "1  2008     12 -2.449294e-16        1.0   2  0.394356  0.918958  \n",
       "2  2008     12 -2.449294e-16        1.0   3  0.571268  0.820763  \n",
       "3  2008     12 -2.449294e-16        1.0   4  0.724793  0.688967  \n",
       "4  2008     12 -2.449294e-16        1.0   5  0.848644  0.528964  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There don't seem to be any error in dates so parsing values into datetime\n",
    "data['Date'] = pd.to_datetime(data[\"Date\"])\n",
    "#Creating a collumn of year\n",
    "data['year'] = data.Date.dt.year\n",
    "\n",
    "# function to encode datetime into cyclic parameters.\n",
    "#As I am planning to use this data in a neural network I prefer the months and days in a cyclic continuous feature.\n",
    "\n",
    "def encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "data['month'] = data.Date.dt.month\n",
    "data = encode(data, 'month', 12)\n",
    "\n",
    "data['day'] = data.Date.dt.day\n",
    "data = encode(data, 'day', 31)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Days In Year')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# roughly a year's span section\n",
    "section = data[:360]\n",
    "tm = section[\"day\"].plot(color=\"#C2C4E2\")\n",
    "tm.set_title(\"Distribution Of Days Over Year\")\n",
    "tm.set_ylabel(\"Days In month\")\n",
    "tm.set_xlabel(\"Days In Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sine Encoded Months')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cyclic_month = sns.scatterplot(x=\"month_sin\", y=\"month_cos\", data=data, color=\"#C2C4E2\")\n",
    "cyclic_month.set_title(\"Cyclic Encoding of Month\")\n",
    "cyclic_month.set_ylabel(\"Cosine Encoded Months\")\n",
    "cyclic_month.set_xlabel(\"Sine Encoded Months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sine Encoded Day')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABedUlEQVR4nO3deVxU1f8/8NcwAzMgMmAgS5EimCuIK6GVGiSomfaxPtqGmKK5Zmgq3z5qaaVZH1PTMsut0jStbLFIUyktQsMlF1wg1FzAhWVkZ2bO7w9/3E8TMMzgDDMDr+fjMQ+Zc8498z6z3Pv2LufKhBACRERERFQjJ1sHQERERGTPmCwRERERGcFkiYiIiMgIJktERERERjBZIiIiIjKCyRIRERGREUyWiIiIiIxgskRERERkBJMlIiIiIiOYLBGRSVq3bo34+HjpeUpKCmQyGVJSUmwWk63V9B7Ex8ejdevWNovJVB9//DHat28PZ2dneHp62jocIrvGZInIAWVlZWH8+PFo06YNVCoVPDw80KdPHyxbtgylpaW2Ds9sVUlHbY/NmzfbOsRG5dSpU4iPj0dwcDA++OADrF69uta2L7/8ssFn4ebmhrvvvhtDhgzBunXrUF5e3oCRE9mGwtYBEJF5duzYgccffxxKpRJxcXHo3LkzKioqsH//frz44os4ceKE0Y2fpTzwwAMoLS2Fi4uLxfqcOnUqevbsWa08MjLSYq9hbR988AH0er2twzAqJSUFer0ey5YtQ0hIiEnLvPfee3B3d0d5eTkuXbqEH374Ac8++yyWLl2Kb7/9FoGBgVaOmsh2mCwROZDs7GyMHDkSrVq1wp49e+Dv7y/VTZo0CZmZmdixY0eDxOLk5ASVSmXRPu+//3489thjFu2zoTk7O9s6hDpdvXoVAMw6/PbYY4/B29tbej537lxs3LgRcXFxePzxx/Hbb79ZOkwiu8HDcEQOZPHixSgqKsKaNWsMEqUqISEheP755wEAffv2RZcuXWrsp127doiJiZGeV+1lCA0NhUqlgo+PD2JjY/H777/XGktt5yylpaVh0KBB8PLyQrNmzRAWFoZly5bVY7Q1k8lkmDx5MrZv347OnTtDqVSiU6dOSE5Ortb20qVLGDNmDAICAqBUKhEUFIQJEyagoqJCavPnn3/i8ccfR4sWLeDm5oZ77723xoTz4sWLGDZsGJo1a4aWLVvihRdeqPEQ1D/PWTp37hxkMhneeustrF69GsHBwVAqlejZsycOHjxYbfmtW7eiY8eOUKlU6Ny5M7788kuzzoN699130alTJyiVSgQEBGDSpEkoKCiQ6lu3bo158+YBAHx8fCCTyfDyyy+b1Pc/PfXUUxg7dizS0tKwa9cuqXzfvn14/PHHcffdd0OpVCIwMBAvvPCCwSHidevWQSaT4fDhw9X6ff311yGXy3Hp0qV6xUVkadyzRORAvvnmG7Rp0wa9e/eus+0zzzyDhIQEHD9+HJ07d5bKDx48iDNnzuA///mPVDZmzBisX78eAwcOxNixY6HVarFv3z789ttv6NGjh8nx7dq1Cw8//DD8/f3x/PPPw8/PDxkZGfj222+lJM6Ymzdv4vr169XK77jjDshkMun5/v378cUXX2DixIlo3rw5li9fjuHDh+PChQu44447AACXL19Gr169UFBQgHHjxqF9+/a4dOkStm3bhpKSEri4uCA3Nxe9e/dGSUkJpk6dijvuuAMbNmzAI488gm3btuHRRx8FAJSWliIqKgoXLlzA1KlTERAQgI8//hh79uwx+b3ZtGkTbt68ifHjx0Mmk2Hx4sX417/+hT///FPaG7Vjxw6MGDECoaGhWLhwIfLz8zFmzBjceeedJr3Gyy+/jFdeeQXR0dGYMGECTp8+jffeew8HDx7EL7/8AmdnZyxduhQfffQRvvzyS+nQWlhYmMnj+KdnnnkGq1evxs6dO/HQQw8BuJXwlZSUYMKECbjjjjtw4MABvPPOO7h48SK2bt0K4NaeqkmTJmHjxo3o2rWrQZ8bN25Ev379TB43kdUJInIIhYWFAoAYOnSoSe0LCgqESqUSs2bNMiifOnWqaNasmSgqKhJCCLFnzx4BQEydOrVaH3q9Xvq7VatWYtSoUdLzvXv3CgBi7969QgghtFqtCAoKEq1atRL5+fm19lOTqr5qe1y5ckVqC0C4uLiIzMxMqezo0aMCgHjnnXeksri4OOHk5CQOHjxY67imTZsmAIh9+/ZJdTdv3hRBQUGidevWQqfTCSGEWLp0qQAgPvvsM6ldcXGxCAkJMXgPhBBi1KhRolWrVtLz7OxsAUDccccdIi8vTyr/6quvBADxzTffSGWhoaHirrvuEjdv3pTKUlJSBACDPmty9epV4eLiIgYMGCDFLYQQK1asEADE2rVrpbJ58+YJAOLatWtG+zSlbX5+vgAgHn30UamspKSkWruFCxcKmUwmzp8/L5U98cQTIiAgwCDeQ4cOCQBi3bp1dcZG1FB4GI7IQWg0GgBA8+bNTWqvVqsxdOhQfPrppxBCAAB0Oh22bNkiHU4CgM8//xwymUw6NPN3f9+bU5fDhw8jOzsb06ZNq3YujKn9zJ07F7t27ar2aNGihUG76OhoBAcHS8/DwsLg4eGBP//8E8Ctw4rbt2/HkCFDatwzVhXPd999h169euG+++6T6tzd3TFu3DicO3cOJ0+elNr5+/sbnE/l5uaGcePGmTQuABgxYgS8vLyk5/fffz8ASDFfvnwZx44dQ1xcHNzd3aV2ffv2RWhoaJ39//jjj6ioqMC0adPg5PS/VXtCQgI8PDysdi5bVaw3b96UylxdXaW/i4uLcf36dfTu3RtCCIPDbnFxcbh8+TL27t0rlW3cuBGurq4YPny4VeIlqg8mS0QOwsPDA4DhRqkucXFxuHDhAvbt2wfg1gY1NzcXzzzzjNQmKysLAQEB1RISc2VlZQGAwSE/c4WGhiI6Orra459X3N19993VlvXy8kJ+fj4A4Nq1a9BoNHXGcv78ebRr165aeYcOHaT6qn9DQkKqJX01LVubf8ZclThVxVz1WjVdnWbKFWtVy/8zJhcXF7Rp00aqt7SioiIAhkn8hQsXEB8fjxYtWsDd3R0+Pj7o27cvAKCwsFBq99BDD8Hf3x8bN24EcCvJ/fTTTzF06FCT/1NA1BCYLBE5CA8PDwQEBOD48eMmLxMTEwNfX1988sknAIBPPvkEfn5+iI6OtlaYDUIul9dYXrUHzR45YsymqPo+ViV0Op0ODz30EHbs2IFZs2Zh+/bt2LVrF9avXw8ABtMqyOVyPPnkk/j8889RVlaGvXv34vLly3j66acbfBxExjBZInIgDz/8MLKyspCammpS+6qN0bZt25Cfn4/t27fjiSeeMNhwBwcH4/Lly8jLy7ut2KoOi5mTzFmLj48PPDw86oylVatWOH36dLXyU6dOSfVV/2ZlZVVLbGpatr6qXiszM7NaXU1ltS3/z5gqKiqQnZ0t1Vvaxx9/DADS1ZXHjh3DmTNn8N///hezZs3C0KFDER0djYCAgBqXj4uLg0ajwTfffIONGzfCx8fH4EpNInvAZInIgcycORPNmjXD2LFjkZubW60+Kyur2mX6zzzzDPLz8zF+/HgUFRVV+1/78OHDIYTAK6+8Uq0/c/Z6dOvWDUFBQVi6dKnBperm9mMJTk5OGDZsGL755psapz+oimfQoEE4cOCAQfJZXFyM1atXo3Xr1ujYsaPU7vLly9i2bZvUrqSkxKKTfwYEBKBz58746KOPpENbAPDTTz/h2LFjdS5fdbhy+fLlBu/3mjVrUFhYiMGDB1ss1iqbNm3Chx9+iMjISERFRQH43x60v8cghKh1+oiwsDCEhYXhww8/xOeff46RI0dCoeCF2mRf+I0kciDBwcHYtGkTRowYgQ4dOhjM4P3rr79i69atBvdvA4CuXbuic+fO2Lp1Kzp06IBu3boZ1Pfv3x/PPPMMli9fjrNnzyI2NhZ6vR779u1D//79MXnyZJNic3JywnvvvYchQ4YgPDwco0ePhr+/P06dOoUTJ07ghx9+qLOPffv2oaysrFp51QbVHK+//jp27tyJvn37Yty4cejQoQOuXLmCrVu3Yv/+/fD09MTs2bPx6aefYuDAgZg6dSpatGiBDRs2IDs7G59//rl0onRCQgJWrFiBuLg4pKenw9/fHx9//DHc3NzMismUmIcOHYo+ffpg9OjRyM/Px4oVK9C5c2eDBKomPj4+SEpKwiuvvILY2Fg88sgjOH36NN5991307Nnztg9tbdu2De7u7qioqJBm8P7ll1/QpUsXaToAAGjfvj2Cg4MxY8YMXLp0CR4eHvj888+lc7NqEhcXhxkzZgAAD8GRfbLNRXhEdDvOnDkjEhISROvWrYWLi4to3ry56NOnj3jnnXdEWVlZtfaLFy8WAMTrr79eY39arVa8+eabon379sLFxUX4+PiIgQMHivT0dKlNXVMHVNm/f7946KGHRPPmzUWzZs1EWFiYwSX9Nalr6oB58+ZJbQGISZMmVevjn/EJIcT58+dFXFyc8PHxEUqlUrRp00ZMmjRJlJeXS22ysrLEY489Jjw9PYVKpRK9evUS3377bbX+z58/Lx555BHh5uYmvL29xfPPPy+Sk5NNnjrgzTffrNbnP8cmhBCbN28W7du3F0qlUnTu3Fl8/fXXYvjw4aJ9+/ZG38MqK1asEO3btxfOzs7C19dXTJgwodpUDvWZOqDqoVKpxF133SUefvhhsXbt2hq/bydPnhTR0dHC3d1deHt7i4SEBGl6h5qmBLhy5YqQy+XinnvuMWmMRA1NJoSDn11IRHVatmwZXnjhBZw7d67GK8nIvoWHh8PHx8dgluzG5Pr16/D398fcuXMxZ84cW4dDVA3PWSJq5IQQWLNmDfr27ctEyc5VVlZCq9UalKWkpODo0aPo16+fbYJqAOvXr4dOpzOY0oLInvCcJaJGqri4GF9//TX27t2LY8eO4auvvrJ1SFSHS5cuITo6Gk8//TQCAgJw6tQprFq1Cn5+fnjuuedsHZ7F7dmzBydPnsRrr72GYcOGmXz/O6KGxsNwRI3UuXPnEBQUBE9PT0ycOBGvvfaarUOiOhQWFmLcuHH45ZdfcO3aNTRr1gxRUVFYtGiRwYzljUW/fv3w66+/ok+fPvjkk094LziyW0yWiIiIiIzgOUtERERERjBZIiIiIjKCJ3hbgF6vx+XLl9G8eXOz7tJOREREtiOEwM2bNxEQECBNQlsTJksWcPnyZQQGBto6DCIiIqqHv/76C3fddVet9UyWLKB58+YAbr3ZHh4eNo6GiIiITKHRaBAYGChtx2vDZMkCqg69eXh4MFkiIiJyMHWdQsMTvImIiIiMYLJEREREZASTJSIiIiIjmCwRERERGcFkiYiIiMgIJktERERERjBZIiIiIjKCyRIRERGREUyWiIiIiIxgskRERERkhEMlSz///DOGDBmCgIAAyGQybN++vc5lUlJS0K1bNyiVSoSEhGD9+vXV2qxcuRKtW7eGSqVCREQEDhw4YPngicjhlJVVoqi4AgWFZSgqrkBZWaWtQyIiG3CoZKm4uBhdunTBypUrTWqfnZ2NwYMHo3///jhy5AimTZuGsWPH4ocffpDabNmyBYmJiZg3bx4OHTqELl26ICYmBlevXrXWMIjIAZSWVuJsZj4OH8nFsePXcPhILs5m5qO0lAkTUVMjE0IIWwdRHzKZDF9++SWGDRtWa5tZs2Zhx44dOH78uFQ2cuRIFBQUIDk5GQAQERGBnj17YsWKFQAAvV6PwMBATJkyBbNnzzYpFo1GA7VajcLCQt5Il6gRKCu7lSgVFJZXq/NUK9E2xAsqlbMNIiMiSzJ1++1Qe5bMlZqaiujoaIOymJgYpKamAgAqKiqQnp5u0MbJyQnR0dFSm5qUl5dDo9EYPIiodo52OEurEzUmSgBQUFgOrc5+/4/paO81kSNQ2DoAa8rJyYGvr69Bma+vLzQaDUpLS5Gfnw+dTldjm1OnTtXa78KFC/HKK69YJWaixqa0tBKZWYZ7aTzVSoQEe8HV1T73zmi1eqP1ujrqbcUR32siR9Co9yxZS1JSEgoLC6XHX3/9ZeuQiOxSWVn1jTdwa+9MZla+3e71UCiMrxrlddTbgqO+10SOoFHvWfLz80Nubq5BWW5uLjw8PODq6gq5XA65XF5jGz8/v1r7VSqVUCqVVomZqDFx1MNZCrkMnmplrecsKeQyG0RlnKO+10SOwP7+e2RBkZGR2L17t0HZrl27EBkZCQBwcXFB9+7dDdro9Xrs3r1bakNE9eeoh7NUKmeEBHvBU234n6KqQ1r2eHK3o77XRI7AofYsFRUVITMzU3qenZ2NI0eOoEWLFrj77ruRlJSES5cu4aOPPgIAPPfcc1ixYgVmzpyJZ599Fnv27MFnn32GHTt2SH0kJiZi1KhR6NGjB3r16oWlS5eiuLgYo0ePbvDxETU2jng4q4qrqzPahnhBqxPQafWQK5ygkMvsMlECHPu9JrJ3DpUs/f777+jfv7/0PDExEQAwatQorF+/HleuXMGFCxek+qCgIOzYsQMvvPACli1bhrvuugsffvghYmJipDYjRozAtWvXMHfuXOTk5CA8PBzJycnVTvomIvM54uGsv7PXxKgmjv5eE9kzh51nyZ5wniWi2vEKrYbD95rIPKZuvx1qzxIROR5HO5zlyPheE1kHkyUisjpurBsO32siy+MZf0RERERGcM8SkYMqK6uEVieg1eqh4OEWclD8HpMjYLJE5IB4Ii81Bvwek6PgYTgiB8PbWlBjwO8xORImS0QOhre1oMaA32NyJEyWiBwMb2tBjQG/x+RImCwRORje1oIaA36PyZHw20jkYKpua1ET3taCHAW/x+RImCwRORiVyhkhwV7VNjRVVxHxsmtyBPwekyPh1AFEDoi3taDGgN9jchRMlogcFDco1Bjwe0yOgIfhiIiIiIxgskRERERkBJMlIiIiIiOYLBEREREZwWSJiIiIyAgmS0RERERGcOoAolqUlVVCqxPQavVQcP4XoiaJ6wECmCwR1ai0tBKZWfkGd0WvmlnY1ZUrSqKmgOsBqsLDcET/UFZWfQUJAAWF5cjMykdZWaWNIiOihsL1AP0dkyWif9DqRLUVZJWCwnJodaKBIyKihsb1AP0dkyWif9Bq9UbrdXXUE5Hj43qA/o7JEtE/KBTGfxbyOuqJyPFxPUB/x0+b6B8Uchk81coa6zzVSijksgaOiIgaGtcD9HdMloj+QaVyRkiwV7UVZdVVMLxsmKjx43qA/o5TBxDVwNXVGW1DvKDVCei0esg5vwpRk8P1AFVhskRUC64QiYjrAQJ4GI6IiIjIKCZLREREREY4XLK0cuVKtG7dGiqVChEREThw4ECtbfv16weZTFbtMXjwYKlNfHx8tfrY2NiGGAoRERE5AIc6Z2nLli1ITEzEqlWrEBERgaVLlyImJganT59Gy5Ytq7X/4osvUFFRIT2/ceMGunTpgscff9ygXWxsLNatWyc9VyprvlyUiIiImh6H2rO0ZMkSJCQkYPTo0ejYsSNWrVoFNzc3rF27tsb2LVq0gJ+fn/TYtWsX3NzcqiVLSqXSoJ2Xl1dDDIeIiIgcgMMkSxUVFUhPT0d0dLRU5uTkhOjoaKSmpprUx5o1azBy5Eg0a9bMoDwlJQUtW7ZEu3btMGHCBNy4ccNoP+Xl5dBoNAYPIiIiapwcJlm6fv06dDodfH19Dcp9fX2Rk5NT5/IHDhzA8ePHMXbsWIPy2NhYfPTRR9i9ezfeeOMN/PTTTxg4cCB0Ol2tfS1cuBBqtVp6BAYG1m9QREREZPcc6pyl27FmzRqEhoaiV69eBuUjR46U/g4NDUVYWBiCg4ORkpKCqKioGvtKSkpCYmKi9Fyj0TBhIiIiaqQcZs+St7c35HI5cnNzDcpzc3Ph5+dndNni4mJs3rwZY8aMqfN12rRpA29vb2RmZtbaRqlUwsPDw+BBREREjZPDJEsuLi7o3r07du/eLZXp9Xrs3r0bkZGRRpfdunUrysvL8fTTT9f5OhcvXsSNGzfg7+9/2zETERGR43OYZAkAEhMT8cEHH2DDhg3IyMjAhAkTUFxcjNGjRwMA4uLikJSUVG25NWvWYNiwYbjjjjsMyouKivDiiy/it99+w7lz57B7924MHToUISEhiImJaZAxERERkX1zqHOWRowYgWvXrmHu3LnIyclBeHg4kpOTpZO+L1y4ACcnw/zv9OnT2L9/P3bu3FmtP7lcjj/++AMbNmxAQUEBAgICMGDAACxYsIBzLREREREAQCaEELYOwtFpNBqo1WoUFhby/KXbVFZWCa1OQKvVQ8E7fBMRAeC60VpM3X471J4latxKSyuRmZWPgsJyqcxTrURIsBdcXblSIKKmietG23Ooc5ao8Sorq74yAICCwnJkZuWjrKzSRpEREdkO1432gckS2QWtTlRbGVQpKCyHVsejxUTU9HDdaB+YLJFd0Gr1Rut1ddQTETVGXDfaByZLZBcUCuNfRXkd9UREjRHXjfaB7zLZBYVcBk91zdM1eKqVUMhlDRwREZHtcd1oH5gskV1QqZwREuxVbaVQdcUHL5EloqaI60b7wKkDyG64ujqjbYgXtDoBnVYPOecSISLiutEOMFkiu8IfPxFRdVw32hYPwxEREREZwWSJiIiIyAgmS0RERERGMFkiIiIiMoLJEhEREZERTJaIiIiIjGCyRERERGQEkyUiIiIiI5gsERERERnBZImIiIjICCZLREREREYwWSIiIiIygskSERERkRFMloiIiIiMYLJEREREZASTJSIiIiIjmCwRERERGaGwdQBk38rKKqHVCWi1eigUTlDIZVCpnG0dFhERWRHX/YaYLFGtSksrkZmVj4LCcqnMU61ESLAXXF2b7o+GiKgx47q/Oh6GoxqVlVX/sQBAQWE5MrPyUVZWaaPIiIjIWrjurxmTJaqRVieq/ViqFBSWQ6sTDRwRERFZG9f9NWOyRDXSavVG63V11BMRkePhur9mDpcsrVy5Eq1bt4ZKpUJERAQOHDhQa9v169dDJpMZPFQqlUEbIQTmzp0Lf39/uLq6Ijo6GmfPnrX2MOyeQmH8qyGvo56IiBwP1/01c6hRb9myBYmJiZg3bx4OHTqELl26ICYmBlevXq11GQ8PD1y5ckV6nD9/3qB+8eLFWL58OVatWoW0tDQ0a9YMMTExKCsrs/Zw7JpCLoOnWlljnadaCYVc1sARERGRtXHdXzOHSpaWLFmChIQEjB49Gh07dsSqVavg5uaGtWvX1rqMTCaDn5+f9PD19ZXqhBBYunQp/vOf/2Do0KEICwvDRx99hMuXL2P79u0NMCL7pVI5IyTYq9qPpuqKiKZ8CSkRUWPFdX/NHGbqgIqKCqSnpyMpKUkqc3JyQnR0NFJTU2tdrqioCK1atYJer0e3bt3w+uuvo1OnTgCA7Oxs5OTkIDo6WmqvVqsRERGB1NRUjBw5ssY+y8vLUV7+vxPgNBrN7Q7PLrm6OqNtiBe0OgGdVg8559ogImr0uO6vzmH2LF2/fh06nc5gzxAA+Pr6Iicnp8Zl2rVrh7Vr1+Krr77CJ598Ar1ej969e+PixYsAIC1nTp8AsHDhQqjVaukRGBh4O0OzayqVM9ybuUCtVsG9mUuT/rEQETUVXPcbcphkqT4iIyMRFxeH8PBw9O3bF1988QV8fHzw/vvv31a/SUlJKCwslB5//fWXhSImIiIie+MwyZK3tzfkcjlyc3MNynNzc+Hn52dSH87OzujatSsyMzMBQFrO3D6VSiU8PDwMHkRERNQ4OUyy5OLigu7du2P37t1SmV6vx+7duxEZGWlSHzqdDseOHYO/vz8AICgoCH5+fgZ9ajQapKWlmdwnERERNW4Oc4I3ACQmJmLUqFHo0aMHevXqhaVLl6K4uBijR48GAMTFxeHOO+/EwoULAQDz58/Hvffei5CQEBQUFODNN9/E+fPnMXbsWAC3rpSbNm0aXn31VbRt2xZBQUGYM2cOAgICMGzYMFsNk4iIiOyIQyVLI0aMwLVr1zB37lzk5OQgPDwcycnJ0gnaFy5cgJPT/3aW5efnIyEhATk5OfDy8kL37t3x66+/omPHjlKbmTNnori4GOPGjUNBQQHuu+8+JCcnV5u8koiIiJommRCiad7oxYI0Gg3UajUKCwt5/hIREZGDMHX77TDnLBERERHZApMlIiIiIiOYLBEREREZwWSJiIiIyAgmS0RERERGMFkiIiIiMoLJEhEREZERZidLf/75pzXiICIiIrJLZidLISEh6N+/Pz755BOUlZVZIyYiIiIiu2F2snTo0CGEhYUhMTERfn5+GD9+PA4cOGCN2IiIiIhszuxkKTw8HMuWLcPly5exdu1aXLlyBffddx86d+6MJUuW4Nq1a9aIk4iIiMgm6n2Ct0KhwL/+9S9s3boVb7zxBjIzMzFjxgwEBgYiLi4OV65csWScRERERDZR72Tp999/x8SJE+Hv748lS5ZgxowZyMrKwq5du3D58mUMHTrUknESERER2YTC3AWWLFmCdevW4fTp0xg0aBA++ugjDBo0CE5Ot/KuoKAgrF+/Hq1bt7Z0rEREREQNzuxk6b333sOzzz6L+Ph4+Pv719imZcuWWLNmzW0HR0RERGRrMiGEsHUQjk6j0UCtVqOwsBAeHh42jaWsrBJanYBWq4dC4QSFXAaVytmmMRERUdNmr9smU7ffZu9ZqlJSUoILFy6goqLCoDwsLKy+XdJtKi2tRGZWPgoKy6UyT7USIcFecHW1/ZeSiIiansawbTI7Wbp27Rri4+ORnJxcY71Op7vtoMh8ZWXVv4wAUFBYjsysfLQN8bKLLJ6IiJqOxrJtMvtquGnTpqGwsBBpaWlwdXVFcnIyNmzYgLZt2+Lrr7+2RoxkAq1OVPsyVikoLIdWx6OtRETUsBrLtsnsPUt79uzBV199hR49esDJyQmtWrXCQw89BA8PDyxcuBCDBw+2RpxUB61Wb7ReV0c9ERGRpTWWbZPZe5aKi4vRsmVLAICXl5c0Y3doaCgOHTpk2ejIZAqF8Y9SXkc9ERGRpTWWbZPZUbZr1w6nT58GAHTp0gXvv/8+Ll26hFWrVtU6lQBZn0Iug6daWWOdp1oJhVzWwBEREVFT11i2TWYfhnv++eelW5nMmzcPsbGx2LhxI1xcXLB+/XpLx0cmUqmcERLsVesVB45wAh0RETUujWXbdNvzLJWUlODUqVO4++674e3tbam4HIo9zrOk0+oht6O5LIiIqOmy122T1edZquLm5oZu3brdbjdkIfbw5SMiIvo7R982mXXOUnFxMebOnYvOnTvD3d0dzZs3R1hYGObPn4+SkhJrxUhERERkMybvWaqoqEDfvn1x/PhxDBw4EEOGDIEQAhkZGXjttdfw/fff4+eff4azs2Nnj0RERER/Z3Ky9N577+HixYs4evQo2rVrZ1B36tQp9OvXD6tWrcKUKVMsHiQRERGRrZh8GO6LL77AnDlzqiVKANC+fXu89NJL2LZtm0WDIyIiIrI1k5OlkydPol+/frXW9+/fHydPnrRETERERER2w+RkqaCgAHfccUet9XfccQcKCwstEhQRERGRvTA5WdLr9ZDL5bV35OQEnU5nkaCMWblyJVq3bg2VSoWIiAgcOHCg1rYffPAB7r//fnh5ecHLywvR0dHV2sfHx0Mmkxk8YmNjrT0MIiIichAmn+AthEBUVBQUipoX0Wq1FguqNlu2bEFiYiJWrVqFiIgILF26FDExMTh9+rR0v7q/S0lJwRNPPIHevXtDpVLhjTfewIABA3DixAnceeedUrvY2FisW7dOeq5U1jw1OxERETU9Js/g/corr5jU4bx5824rIGMiIiLQs2dPrFixAsCtvV2BgYGYMmUKZs+eXefyOp0OXl5eWLFiBeLi4gDc2rNUUFCA7du31zsue5rBm4iIiExj8Rm8rZkEmaKiogLp6elISkqSypycnBAdHY3U1FST+igpKUFlZSVatGhhUJ6SkoKWLVvCy8sLDz74IF599VWj52eVl5ejvPx/97jRaDRmjoaIiIgchVkzeNvS9evXodPp4Ovra1Du6+uLnJwck/qYNWsWAgICEB0dLZXFxsbio48+wu7du/HGG2/gp59+wsCBA42ef7Vw4UKo1WrpERgYWL9BERERkd277XvDOYpFixZh8+bNSElJgUqlkspHjhwp/R0aGoqwsDAEBwcjJSUFUVFRNfaVlJSExMRE6blGo2HCRERE1Eg5zJ4lb29vyOVy5ObmGpTn5ubCz8/P6LJvvfUWFi1ahJ07dyIsLMxo2zZt2sDb2xuZmZm1tlEqlfDw8DB4EBERUePkMMmSi4sLunfvjt27d0tler0eu3fvRmRkZK3LLV68GAsWLEBycjJ69OhR5+tcvHgRN27cgL+/v0XiJiIiIsfmMMkSACQmJuKDDz7Ahg0bkJGRgQkTJqC4uBijR48GAMTFxRmcAP7GG29gzpw5WLt2LVq3bo2cnBzk5OSgqKgIAFBUVIQXX3wRv/32G86dO4fdu3dj6NChCAkJQUxMjE3GSERERPbFpHOWli9fbnKHU6dOrXcwdRkxYgSuXbuGuXPnIicnB+Hh4UhOTpZO+r5w4QKcnP6X/7333nuoqKjAY489ZtDPvHnz8PLLL0Mul+OPP/7Ahg0bUFBQgICAAAwYMAALFizgXEtEREQEwMR5loKCggyeX7t2DSUlJfD09ARw61Yobm5uaNmyJf7880+rBGrPOM8SERGR4zF1+23SYbjs7Gzp8dprryE8PBwZGRnIy8tDXl4eMjIy0K1bNyxYsMBiAyAiIiKyBybP4F0lODgY27ZtQ9euXQ3K09PT8dhjjyE7O9uiAToC7lkiIiJyPBbds/R3V65cqfE+cDqdrtpl/URERESOzuxkKSoqCuPHj8ehQ4eksvT0dEyYMMFgZmwiIiKixsDsZGnt2rXw8/NDjx49oFQqoVQq0atXL/j6+uLDDz+0RoxERERENmP27U58fHzw3Xff4cyZMzh16hQAoH379rjnnnssHhwRERGRrdX73nCtW7eGEALBwcFQKJrMLeZsoqysElqdgFarh0LhBIVcBpXK2dZhERERWYW9bffMznJKSkowZcoUbNiwAQBw5swZtGnTBlOmTMGdd96J2bNnWzzIpqy0tBKZWfkoKCyXyjzVSoQEe8HVlQkTERE1Lva43TP7nKWkpCQcPXoUKSkpUKlUUnl0dDS2bNli0eCaurKy6l8YACgoLEdmVj7KyiptFBkREZHl2et2z+w9S9u3b8eWLVtw7733QiaTSeWdOnVCVlaWRYNr6rQ6Ue0LU6WgsBxanVlTZBEREdk1e93umb1n6dq1a2jZsmW18uLiYoPkiW6fVqs3Wq+ro56IiMiR2Ot2z+xkqUePHtixY4f0vCpB+vDDDxEZGWm5yAgKhfGPR15HPRERkSOx1+2e2YfhXn/9dQwcOBAnT56EVqvFsmXLcPLkSfz666/46aefrBFjk6WQy+CpVta4S9JTrYRCzj15RETUeNjrds/sFO2+++7DkSNHoNVqERoaip07d6Jly5ZITU1F9+7drRFjk6VSOSMk2AueaqVBedVVAZw+gIiIGhN73e6ZfSNdqs7aN9Ktmm9Cp9VDbgfzTRAREVlTQ233TN1+m3QYTqPRmPzC1kgWmjomRkRE1JTY23bPpGTJ09PT5CvddDrdbQVEREREZE9MSpb27t0r/X3u3DnMnj0b8fHx0tVvqamp2LBhAxYuXGidKImIiIhsxOxzlqKiojB27Fg88cQTBuWbNm3C6tWrkZKSYsn4HIK1z1kiIiIiyzN1+2321XCpqano0aNHtfIePXrgwIED5nZHREREZNfMTpYCAwPxwQcfVCv/8MMPERgYaJGgiIiIiOyF2ZNSvv322xg+fDi+//57REREAAAOHDiAs2fP4vPPP7d4gERERES2ZPaepUGDBuHs2bMYMmQI8vLykJeXhyFDhuDMmTMYNGiQNWIkIiIishlOSmkBPMGbiIjI8Vh0Usp/KigowJo1a5CRkQEA6NSpE5599lmo1er6RUtERERkp8w+DPf7778jODgYb7/9tnQYbsmSJQgODsahQ4esESMRERGRzZh9GO7+++9HSEgIPvjgAygUt3ZMabVajB07Fn/++Sd+/vlnqwRqz3gYjoiIyPGYuv02O1lydXXF4cOH0b59e4PykydPokePHigpKalfxA6MyRIREZHjsdqklB4eHrhw4UK18r/++gvNmzc3tzsiIiIiu2Z2sjRixAiMGTMGW7ZswV9//YW//voLmzdvrvEWKERERESOzuyr4d566y3IZDLExcVBq9UCAJydnTFhwgQsWrTI4gESERER2ZLZe5ZcXFywbNky5Ofn48iRIzhy5Ajy8vLw9ttvQ6lUWiNGAytXrkTr1q2hUqkQERFR5/3otm7divbt20OlUiE0NBTfffedQb0QAnPnzoW/vz9cXV0RHR2Ns2fPWnMIRERE5EDMTpYKCwuRl5cHNzc3hIaGIjQ0FG5ubsjLy4NGo7FGjJItW7YgMTER8+bNw6FDh9ClSxfExMTg6tWrNbb/9ddf8cQTT2DMmDE4fPgwhg0bhmHDhuH48eNSm8WLF2P58uVYtWoV0tLS0KxZM8TExKCsrMyqYyEiIiLHYPbVcAMHDsSQIUMwceJEg/JVq1bh66+/rrbnxpIiIiLQs2dPrFixAgCg1+sRGBiIKVOmYPbs2dXajxgxAsXFxfj222+lsnvvvRfh4eFYtWoVhBAICAjA9OnTMWPGDAC3kkFfX1+sX78eI0eONCkuXg1HRETkeKx2NVxaWhr69+9frbxfv35IS0sztzuTVVRUID09HdHR0VKZk5MToqOjkZqaWuMyqampBu0BICYmRmqfnZ2NnJwcgzZqtRoRERG19gkA5eXl0Gg0Bg8iIiJqnMxOlsrLy6UTu/+usrISpaWlFgmqJtevX4dOp4Ovr69Bua+vL3JycmpcJicnx2j7qn/N6RMAFi5cCLVaLT0CAwPNHg8RERE5BrOTpV69emH16tXVyletWoXu3btbJCh7l5SUhMLCQunx119/2TokIiIishKzpw549dVXER0djaNHjyIqKgoAsHv3bhw8eBA7d+60eIBVvL29IZfLkZuba1Cem5sLPz+/Gpfx8/Mz2r7q39zcXPj7+xu0CQ8PrzUWpVLZIFf+ERERke2ZvWepT58+SE1NRWBgID777DN88803CAkJwR9//IH777/fGjECuDVlQffu3bF7926pTK/XY/fu3YiMjKxxmcjISIP2ALBr1y6pfVBQEPz8/AzaaDQapKWl1donERERNS1m71kCgPDwcGzcuNHSsdQpMTERo0aNQo8ePdCrVy8sXboUxcXFGD16NAAgLi4Od955JxYuXAgAeP7559G3b1/897//xeDBg7F582b8/vvv0mFEmUyGadOm4dVXX0Xbtm0RFBSEOXPmICAgAMOGDWvw8REREZH9qVeypNfrkZmZiatXr0Kv1xvUPfDAAxYJrCYjRozAtWvXMHfuXOTk5CA8PBzJycnSCdoXLlyAk9P/dpb17t0bmzZtwn/+8x/83//9H9q2bYvt27ejc+fOUpuZM2eiuLgY48aNQ0FBAe677z4kJydDpVJZbRymKCurhFYnoNXqoVA4QSGXQaVytmlMREREDcletoVmz7P022+/4cknn8T58+fxz0VlMhl0Op1FA3QElp5nqbS0EplZ+SgoLJfKPNVKhAR7wdWVCRMRETV+DbEttNo8S8899xx69OiB48ePIy8vD/n5+dIjLy/vtoKmW1n0P78cAFBQWI7MrHyUlVXaKDIiIqKGYW/bQrMPw509exbbtm1DSEiINeJp8rQ6Ue3LUaWgsBxanVk7AomIiByOvW0Lzd6zFBERgczMTGvEQgC0Wr3Rel0d9URERI7O3raFZu9ZmjJlCqZPn46cnByEhobC2dnwuGFYWJjFgmuKFArj+au8jnoiIiJHZ2/bQrOTpeHDhwMAnn32WalMJpNBCNFkT/C2JIVcBk+1ssbdj55qJRRymQ2iIiIiajj2ti00O1nKzs62Rhz0/6lUzggJ9qr1CgBOH0BERI2dvW0LzU6WWrVqZY046G9cXZ3RNsQLWp2ATquHnPMsERFRE2NP20KTD/pNnDgRRUVF0vNPP/0UxcXF0vOCggIMGjTIstE1YSqVM9ybuUCtVsG9mQsTJSIianLsZVto8qSUcrkcV65cQcuWLQEAHh4eOHLkCNq0aQPg1s1nAwICmuQ5S5aelJKIiIisz+KTUv4zpzJz4m8iIiIih8Tr0ImIiIiMYLJEREREZIRZV8PNnTsXbm5uAICKigq89tprUKvVAICSkhLLR0dERERkYyaf4N2vXz/IZHVPArV3797bDsrR8ARvIiIix2Pq9tvkPUspKSmWiIuIiIjIofCcJSIiIiIjmCwRERERGcFkiYiIiMgIJktERERERjBZIiIiIjKiXsnSvn378PTTTyMyMhKXLl0CAHz88cfYv3+/RYMjIiIisjWzk6XPP/8cMTExcHV1xeHDh1FeXg4AKCwsxOuvv27xAImIiIhsyexk6dVXX8WqVavwwQcfwNnZWSrv06cPDh06ZNHgiIiIiGzN7GTp9OnTeOCBB6qVq9VqFBQUWCImIiIiIrthdrLk5+eHzMzMauX79+9HmzZtLBIUERERkb0wO1lKSEjA888/j7S0NMhkMly+fBkbN27EjBkzMGHCBGvESERERGQzJt8brsrs2bOh1+sRFRWFkpISPPDAA1AqlZgxYwamTJlijRiJiIiIbEYmhBD1WbCiogKZmZkoKipCx44d4e7ubunYHIapdy0mIiIi+2Hq9tvsPUtVXFxc0LFjx/ouTkREROQQzE6WiouLsWjRIuzevRtXr16FXq83qP/zzz8tFhwRERGRrZmdLI0dOxY//fQTnnnmGfj7+0Mmk1kjLiIiIiK7YHay9P3332PHjh3o06ePNeKpVV5eHqZMmYJvvvkGTk5OGD58OJYtW1bruVJ5eXmYN28edu7ciQsXLsDHxwfDhg3DggULoFarpXY1JXuffvopRo4cabWxEBERkeMwO1ny8vJCixYtrBGLUU899RSuXLmCXbt2obKyEqNHj8a4ceOwadOmGttfvnwZly9fxltvvYWOHTvi/PnzeO6553D58mVs27bNoO26desQGxsrPff09LTmUG5bWVkltDoBrVYPhcIJCrkMKpVz3QsSERHZGUfYppl9Ndwnn3yCr776Chs2bICbm5u14jKQkZGBjh074uDBg+jRowcAIDk5GYMGDcLFixcREBBgUj9bt27F008/jeLiYigUt/JEmUyGL7/8EsOGDat3fA15NVxpaSUys/JRUFgulXmqlQgJ9oKrq319uYiIiIyx9TbN1O232ZNS/ve//8UPP/wAX19fhIaGolu3bgYPa0hNTYWnp6eUKAFAdHQ0nJyckJaWZnI/VW9GVaJUZdKkSfD29kavXr2wdu1a1JU/lpeXQ6PRGDwaQllZ9S8VABQUliMzKx9lZZUNEgcREdHtcqRtmtmH4W5nD0x95eTkoGXLlgZlCoUCLVq0QE5Ojkl9XL9+HQsWLMC4ceMMyufPn48HH3wQbm5u2LlzJyZOnIiioiJMnTq11r4WLlyIV155xfyB3CatTlT7UlUpKCyHVlevKbOIiIganCNt08xOlubNm2exF589ezbeeOMNo20yMjJu+3U0Gg0GDx6Mjh074uWXXzaomzNnjvR3165dUVxcjDfffNNospSUlITExESD/gMDA287zrpotXqj9bo66omIiOyFI23T6j0ppSVMnz4d8fHxRtu0adMGfn5+uHr1qkG5VqtFXl4e/Pz8jC5/8+ZNxMbGonnz5vjyyy/h7Gz8GGhERAQWLFiA8vJyKJXKGtsolcpa66xJoTB+1FReRz0REZG9cKRtmknJUosWLXDmzBl4e3vDy8vL6NxKeXl5Jr+4j48PfHx86mwXGRmJgoICpKeno3v37gCAPXv2QK/XIyIiotblNBoNYmJioFQq8fXXX0OlUtX5WkeOHIGXl5dNkqG6KOQyeKqVNe629FQroZBzzisiInIMjrRNMylZevvtt9G8eXPp74aeiLJDhw6IjY1FQkICVq1ahcrKSkyePBkjR46UroS7dOkSoqKi8NFHH6FXr17QaDQYMGAASkpK8MknnxiciO3j4wO5XI5vvvkGubm5uPfee6FSqbBr1y68/vrrmDFjRoOOz1QqlTNCgr1qvXLA3i61JCIiqo0jbdPqfSPdhpaXl4fJkycbTEq5fPlyaVLKc+fOISgoCHv37kW/fv2QkpKC/v3719hXdnY2WrdujeTkZCQlJSEzMxNCCISEhGDChAlISEiAk5Ppu/8a+ka6VXNS6LR6yO10TgoiIiJT2HKbZur22+xk6dChQ3B2dkZoaCgA4KuvvsK6deukk6ddXFxuL3IH1NDJEhEREd0+q82zNH78eJw5cwbArZvmjhgxAm5ubti6dStmzpxZ/4iJiIiI7JDZydKZM2cQHh4O4NaM2H379sWmTZuwfv16fP7555aOj4iIiMimzE6WhBDQ62/NffDjjz9i0KBBAIDAwEBcv37dstERERER2ZjZyVKPHj3w6quv4uOPP8ZPP/2EwYMHA7h10rSvr6/FAyQiIiKyJbOTpaVLl+LQoUOYPHkyXnrpJYSEhAAAtm3bht69e1s8QCIiIiJbstjUAWVlZZDL5XXOkN0Y8Wo4IiIix2Pq9rvetztJT0+X7tvWsWNHdOvWrb5dEREREdkts5Olq1evYsSIEfjpp5/g6ekJACgoKED//v2xefNmk25fQkREROQozD5nacqUKSgqKsKJEyeQl5eHvLw8HD9+HBqNBlOnTrVGjEREREQ2Y/Y5S2q1Gj/++CN69uxpUH7gwAEMGDAABQUFlozPIfCcJSIiIsdjtRm89Xp9jSdxOzs7S/MvERERETUWZidLDz74IJ5//nlcvnxZKrt06RJeeOEFREVFWTQ4IiIiIlszO1lasWIFNBoNWrdujeDgYAQHByMoKAgajQbvvPOONWIkIiIishmzr4YLDAzEoUOH8OOPP+LUqVMAgA4dOiA6OtriwRERERHZmsUmpWzKeII3ERGR47H4Cd579uxBx44dodFoqtUVFhaiU6dO2LdvX/2iJSIiIrJTJidLS5cuRUJCQo2Zl1qtxvjx47FkyRKLBkdERERkayYnS0ePHkVsbGyt9QMGDEB6erpFgiIiIiKyFyYnS7m5uUZvkqtQKHDt2jWLBEVERERkL0xOlu68804cP3681vo//vgD/v7+FgmKiIiIyF6YnCwNGjQIc+bMQVlZWbW60tJSzJs3Dw8//LBFgyMiIiKyNZOnDsjNzUW3bt0gl8sxefJktGvXDgBw6tQprFy5EjqdDocOHYKvr69VA7ZHnDqAiIjI8Zi6/TZ5UkpfX1/8+uuvmDBhApKSklCVY8lkMsTExGDlypVNMlFyRGVlldDqBLRaPRQKJyjkMqhUtZ+PRkREVJumsE0xawbvVq1a4bvvvkN+fj4yMzMhhEDbtm3h5eVlrfjIwkpLK5GZlY+CwnKpzFOtREiwF1xdG9eXm4iIrKupbFM4g7cFOMphuLKySpzNNPxSV/FUK9E2xKvR/W+AiIisozFsUyw+gzc5Pq1O1PilBoCCwnJodcybiYjINE1pm8JkqQnRavVG63V11BMREVVpStsUJktNiEJh/OOW11FPRERUpSltUxrPSKhOCrkMnmpljXWeaiUUclkDR0RERI6qKW1TmCw1ISqVM0KCvap9uauuXLD3E/GIiMh+NKVtillTB5Djc3V1RtsQL2h1AjqtHvJGOicGERFZX1PZpjjMnqW8vDw89dRT8PDwgKenJ8aMGYOioiKjy/Tr1w8ymczg8dxzzxm0uXDhAgYPHgw3Nze0bNkSL774IrRarTWHYnMqlTPcm7lArVbBvZlLo/tSExFRw2kK2xSH2bP01FNP4cqVK9i1axcqKysxevRojBs3Dps2bTK6XEJCAubPny89d3Nzk/7W6XQYPHgw/Pz88Ouvv+LKlSuIi4uDs7MzXn/9dauNhYiIiByHQ0xKmZGRgY4dO+LgwYPo0aMHACA5ORmDBg3CxYsXERAQUONy/fr1Q3h4OJYuXVpj/ffff4+HH34Yly9flm7VsmrVKsyaNQvXrl2Di4uLSfE5yqSURERE9D+NalLK1NRUeHp6SokSAERHR8PJyQlpaWlGl924cSO8vb3RuXNnJCUloaSkxKDf0NBQg3vaxcTEQKPR4MSJE5YfCBERETkchzgMl5OTg5YtWxqUKRQKtGjRAjk5ObUu9+STT6JVq1YICAjAH3/8gVmzZuH06dP44osvpH7/efPfqufG+i0vL0d5+f9mLdVoNGaPiYiIiByDTZOl2bNn44033jDaJiMjo979jxs3Tvo7NDQU/v7+iIqKQlZWFoKDg+vd78KFC/HKK6/Ue3kiIiJyHDZNlqZPn474+Hijbdq0aQM/Pz9cvXrVoFyr1SIvLw9+fn4mv15ERAQAIDMzE8HBwfDz88OBAwcM2uTm5gKA0X6TkpKQmJgoPddoNAgMDDQ5DiIiInIcNk2WfHx84OPjU2e7yMhIFBQUID09Hd27dwcA7NmzB3q9XkqATHHkyBEAgL+/v9Tva6+9hqtXr0qH+Xbt2gUPDw907Nix1n6USiWUyppnLSUiIqLGxSFO8O7QoQNiY2ORkJCAAwcO4JdffsHkyZMxcuRI6Uq4S5cuoX379tKeoqysLCxYsADp6ek4d+4cvv76a8TFxeGBBx5AWFgYAGDAgAHo2LEjnnnmGRw9ehQ//PAD/vOf/2DSpElMhoiIiAiAgyRLwK2r2tq3b4+oqCgMGjQI9913H1avXi3VV1ZW4vTp09LVbi4uLvjxxx8xYMAAtG/fHtOnT8fw4cPxzTffSMvI5XJ8++23kMvliIyMxNNPP424uDiDeZmIiIioaXOIeZbsHedZIiIicjyNap4lIiIiIlthskRERERkBJMlIiIiIiOYLBEREREZwWSJiIiIyAgmS0RERERGMFkiIiIiMoLJEhEREZERNr03HDVOZWWV0OoEtFo9FAonKOQyqFTOtg6LiKhJ4jr59jFZIosqLa1EZlY+CgrLpTJPtRIhwV5wdeWPk4ioIXGdbBk8DEcWU1ZW/UcJAAWF5cjMykdZWaWNIiMianq4TrYcJktkMVqdqPajrFJQWA6tjrchJCJqKFwnWw6TJbIYrVZvtF5XRz0REVkO18mWw2SJLEahMP51ktdRT0RElsN1suXwnSKLUchl8FQra6zzVCuhkMsaOCIioqaL62TLYbJEFqNSOSMk2Kvaj7PqygteqkpE1HC4TrYcTh1AFuXq6oy2IV7Q6gR0Wj3knNODiMhmuE62DCZLZHH8ERIR2Q+uk28fD8MRERERGcFkiYiIiMgIJktERERERjBZIiIiIjKCyRIRERGREUyWiIiIiIxgskRERERkBJMlIiIiIiOYLBEREREZwWSJiIiIyAgmS0RERERGMFkiIiIiMoLJEhEREZERDpMs5eXl4amnnoKHhwc8PT0xZswYFBUV1dr+3LlzkMlkNT62bt0qtaupfvPmzQ0xJCIiInIAClsHYKqnnnoKV65cwa5du1BZWYnRo0dj3Lhx2LRpU43tAwMDceXKFYOy1atX480338TAgQMNytetW4fY2Fjpuaenp8XjJyIiIsfkEMlSRkYGkpOTcfDgQfTo0QMA8M4772DQoEF46623EBAQUG0ZuVwOPz8/g7Ivv/wS//73v+Hu7m5Q7unpWa0tOa6yskpodQJarR4KhRMUchlUKmdbh0VETRjXS47NIQ7DpaamwtPTU0qUACA6OhpOTk5IS0szqY/09HQcOXIEY8aMqVY3adIkeHt7o1evXli7di2EEBaLnRpWaWklzmbm4/CRXBw7fg2Hj+TibGY+SksrbR0aETVRXC85PofYs5STk4OWLVsalCkUCrRo0QI5OTkm9bFmzRp06NABvXv3NiifP38+HnzwQbi5uWHnzp2YOHEiioqKMHXq1Fr7Ki8vR3l5ufRco9GYMRqylrKySmRm5aOgsNygvKCwHJlZ+Wgb4sX/yRFRg+J6qXGw6Z6l2bNn13oSdtXj1KlTt/06paWl2LRpU417lebMmYM+ffqga9eumDVrFmbOnIk333zTaH8LFy6EWq2WHoGBgbcdI90+rU5UWyFVKSgsh1bHPYZE1LC4XmocbLpnafr06YiPjzfapk2bNvDz88PVq1cNyrVaLfLy8kw612jbtm0oKSlBXFxcnW0jIiKwYMEClJeXQ6lU1tgmKSkJiYmJ0nONRsOEyQ5otXqj9bo66omILI3rpcbBpsmSj48PfHx86mwXGRmJgoICpKeno3v37gCAPXv2QK/XIyIios7l16xZg0ceecSk1zpy5Ai8vLxqTZQAQKlUGq0n21AojO8olddRT0RkaVwvNQ4Occ5Shw4dEBsbi4SEBKxatQqVlZWYPHkyRo4cKV0Jd+nSJURFReGjjz5Cr169pGUzMzPx888/47vvvqvW7zfffIPc3Fzce++9UKlU2LVrF15//XXMmDGjwcZGlqOQy+CpVta4y9tTrYRCLrNBVETUlHG91Dg4TEq7ceNGtG/fHlFRURg0aBDuu+8+rF69WqqvrKzE6dOnUVJSYrDc2rVrcdddd2HAgAHV+nR2dsbKlSsRGRmJ8PBwvP/++1iyZAnmzZtn9fGQ5alUzggJ9oKn2nCvn6daiZBgnkRJRA2P66XGQSZ4nfxt02g0UKvVKCwshIeHh63DafKq5jPRafWQcz4TIrIDXC/ZJ1O33w5xGI7IHFwBEZG94XrJsTnMYTgiIiIiW2CyRERERGQEkyUiIiIiI5gsERERERnBZImIiIjICCZLREREREYwWSIiIiIygskSERERkRFMloiIiIiMYLJEREREZARvd0LUAKruC6XV6qHgfaGIrIa/NbIGJktEVlZaWonMrHwUFJZLZVV3HHd15UqcyFL4WyNr4WE4IisqK6u+8gaAgsJyZGblo6ys0kaRETUu/K2RNTFZIrIirU5UW3lXKSgsh1YnGjgiosaJvzWyJiZLRFak1eqN1uvqqCci0/C3RtbEZInIihQK4z8xeR31RGQa/tbImvjtIbIihVwGT7WyxjpPtRIKuayBIyJqnPhbI2tiskRkRSqVM0KCvaqtxKuu0OElzUSWwd8aWROnDiCyMldXZ7QN8YJWJ6DT6iHn3C9EVsHfGlkLkyWiBsCVNVHD4G+NrIGH4YiIiIiMYLJEREREZASTJSIiIiIjeM4SEUl4E1KqDb8b1JQxWSIiALwJKdWO3w1q6ngYjoh4E1KqFb8bREyWiAi8CSnVjt8NIiZLRATehJRqx+8GEZMlIgJvQkq143eDiMkSEYE3IaXa8btB5EDJ0muvvYbevXvDzc0Nnp6eJi0jhMDcuXPh7+8PV1dXREdH4+zZswZt8vLy8NRTT8HDwwOenp4YM2YMioqKrDACIvvlyDchLSurRFFxBQoKy1BUXOEQJxw7UsyO/N0gshSHmTqgoqICjz/+OCIjI7FmzRqTllm8eDGWL1+ODRs2ICgoCHPmzEFMTAxOnjwJlUoFAHjqqadw5coV7Nq1C5WVlRg9ejTGjRuHTZs2WXM4RHbHEW9C6oiXtDtizI743SCyJJkQwqEuZVi/fj2mTZuGgoICo+2EEAgICMD06dMxY8YMAEBhYSF8fX2xfv16jBw5EhkZGejYsSMOHjyIHj16AACSk5MxaNAgXLx4EQEBASbFpNFooFarUVhYCA8Pj9saHxGZpqysEmczq1/SDtxKPtqG2N9eD0eMmagxM3X77TCH4cyVnZ2NnJwcREdHS2VqtRoRERFITU0FAKSmpsLT01NKlAAgOjoaTk5OSEtLa/CYich0jnhJuyPGTEQOdBjOXDk5OQAAX19fg3JfX1+pLicnBy1btjSoVygUaNGihdSmJuXl5Sgv/98KT6PRWCpsIjKRI17S7ogxE5GN9yzNnj0bMpnM6OPUqVO2DLFGCxcuhFqtlh6BgYG2DomoyXHES9odMWYisvGepenTpyM+Pt5omzZt2tSrbz8/PwBAbm4u/P39pfLc3FyEh4dLba5evWqwnFarRV5enrR8TZKSkpCYmCg912g0TJiIGljVJe21nf9jj5e0O2LMRGTjZMnHxwc+Pj5W6TsoKAh+fn7YvXu3lBxpNBqkpaVhwoQJAIDIyEgUFBQgPT0d3bt3BwDs2bMHer0eERERtfatVCqhVNY87wgRNYyqS9pru7LMHk+UdsSYiciBzlm6cOEC8vLycOHCBeh0Ohw5cgQAEBISAnd3dwBA+/btsXDhQjz66KOQyWSYNm0aXn31VbRt21aaOiAgIADDhg0DAHTo0AGxsbFISEjAqlWrUFlZicmTJ2PkyJEmXwlHRLbjiJe0O2LMRE2dwyRLc+fOxYYNG6TnXbt2BQDs3bsX/fr1AwCcPn0ahYWFUpuZM2eiuLgY48aNQ0FBAe677z4kJydLcywBwMaNGzF58mRERUXByckJw4cPx/LlyxtmUER02xwxyXDEmImaMoebZ8kecZ4lIiIix9Pk51kiIiIisgQmS0RERERGMFkiIiIiMoLJEhEREZERTJaIiIiIjGCyRERERGQEkyUiIiIiI5gsERERERnBZImIiIjICIe53Yk9q5oEXaPR2DgSIiIiMlXVdruum5kwWbKAmzdvAgACAwNtHAkRERGZ6+bNm1Cr1bXW895wFqDX63H58mU0b94cMpnMYv1qNBoEBgbir7/+apT3nGvs4wMa/xgb+/iAxj/Gxj4+oPGPkeOrPyEEbt68iYCAADg51X5mEvcsWYCTkxPuuusuq/Xv4eHRKH8AVRr7+IDGP8bGPj6g8Y+xsY8PaPxj5Pjqx9gepSo8wZuIiIjICCZLREREREYwWbJjSqUS8+bNg1KptHUoVtHYxwc0/jE29vEBjX+MjX18QOMfI8dnfTzBm4iIiMgI7lkiIiIiMoLJEhEREZERTJaIiIiIjGCyRERERGQEkyUbeu2119C7d2+4ubnB09PTpGWEEJg7dy78/f3h6uqK6OhonD171qBNXl4ennrqKXh4eMDT0xNjxoxBUVGRFUZQN3NjOXfuHGQyWY2PrVu3Su1qqt+8eXNDDMlAfd7rfv36VYv9ueeeM2hz4cIFDB48GG5ubmjZsiVefPFFaLVaaw6lVuaOMS8vD1OmTEG7du3g6uqKu+++G1OnTkVhYaFBO1t9hitXrkTr1q2hUqkQERGBAwcOGG2/detWtG/fHiqVCqGhofjuu+8M6k35TTY0c8b4wQcf4P7774eXlxe8vLwQHR1drX18fHy1zyo2Ntbaw6iVOeNbv359tdhVKpVBG0f/DGtap8hkMgwePFhqY0+f4c8//4whQ4YgICAAMpkM27dvr3OZlJQUdOvWDUqlEiEhIVi/fn21Nub+ts0iyGbmzp0rlixZIhITE4VarTZpmUWLFgm1Wi22b98ujh49Kh555BERFBQkSktLpTaxsbGiS5cu4rfffhP79u0TISEh4oknnrDSKIwzNxatViuuXLli8HjllVeEu7u7uHnzptQOgFi3bp1Bu7+/Bw2lPu913759RUJCgkHshYWFUr1WqxWdO3cW0dHR4vDhw+K7774T3t7eIikpydrDqZG5Yzx27Jj417/+Jb7++muRmZkpdu/eLdq2bSuGDx9u0M4Wn+HmzZuFi4uLWLt2rThx4oRISEgQnp6eIjc3t8b2v/zyi5DL5WLx4sXi5MmT4j//+Y9wdnYWx44dk9qY8ptsSOaO8cknnxQrV64Uhw8fFhkZGSI+Pl6o1Wpx8eJFqc2oUaNEbGyswWeVl5fXUEMyYO741q1bJzw8PAxiz8nJMWjj6J/hjRs3DMZ3/PhxIZfLxbp166Q29vQZfvfdd+Kll14SX3zxhQAgvvzyS6Pt//zzT+Hm5iYSExPFyZMnxTvvvCPkcrlITk6W2pj7npmLyZIdWLdunUnJkl6vF35+fuLNN9+UygoKCoRSqRSffvqpEEKIkydPCgDi4MGDUpvvv/9eyGQycenSJYvHboylYgkPDxfPPvusQZkpPzBrq+/4+vbtK55//vla67/77jvh5ORksEJ/7733hIeHhygvL7dI7Kay1Gf42WefCRcXF1FZWSmV2eIz7NWrl5g0aZL0XKfTiYCAALFw4cIa2//73/8WgwcPNiiLiIgQ48ePF0KY9ptsaOaO8Z+0Wq1o3ry52LBhg1Q2atQoMXToUEuHWi/mjq+u9Wtj/Azffvtt0bx5c1FUVCSV2dNn+HemrAdmzpwpOnXqZFA2YsQIERMTIz2/3fesLjwM50Cys7ORk5OD6OhoqUytViMiIgKpqakAgNTUVHh6eqJHjx5Sm+joaDg5OSEtLa1B47VELOnp6Thy5AjGjBlTrW7SpEnw9vZGr169sHbtWogGnjLsdsa3ceNGeHt7o3PnzkhKSkJJSYlBv6GhofD19ZXKYmJioNFocOLECcsPxAhLfZ8KCwvh4eEBhcLwdpQN+RlWVFQgPT3d4Pfj5OSE6Oho6ffzT6mpqQbtgVufRVV7U36TDak+Y/ynkpISVFZWokWLFgblKSkpaNmyJdq1a4cJEybgxo0bFo3dFPUdX1FREVq1aoXAwEAMHTrU4HfUGD/DNWvWYOTIkWjWrJlBuT18hvVR1+/QEu9ZXXgjXQeSk5MDAAYb0arnVXU5OTlo2bKlQb1CoUCLFi2kNg3FErGsWbMGHTp0QO/evQ3K58+fjwcffBBubm7YuXMnJk6ciKKiIkydOtVi8delvuN78skn0apVKwQEBOCPP/7ArFmzcPr0aXzxxRdSvzV9xlV1DckSn+H169exYMECjBs3zqC8oT/D69evQ6fT1fjenjp1qsZlavss/v57qyqrrU1Dqs8Y/2nWrFkICAgw2PDExsbiX//6F4KCgpCVlYX/+7//w8CBA5Gamgq5XG7RMRhTn/G1a9cOa9euRVhYGAoLC/HWW2+hd+/eOHHiBO66665G9xkeOHAAx48fx5o1awzK7eUzrI/afocajQalpaXIz8+/7e99XZgsWdjs2bPxxhtvGG2TkZGB9u3bN1BElmfqGG9XaWkpNm3ahDlz5lSr+3tZ165dUVxcjDfffNMiG1prj+/vSUNoaCj8/f0RFRWFrKwsBAcH17tfczTUZ6jRaDB48GB07NgRL7/8skGdNT9Dqp9FixZh8+bNSElJMTgJeuTIkdLfoaGhCAsLQ3BwMFJSUhAVFWWLUE0WGRmJyMhI6Xnv3r3RoUMHvP/++1iwYIENI7OONWvWIDQ0FL169TIod+TP0B4wWbKw6dOnIz4+3mibNm3a1KtvPz8/AEBubi78/f2l8tzcXISHh0ttrl69arCcVqtFXl6etPztMnWMtxvLtm3bUFJSgri4uDrbRkREYMGCBSgvL7/t+wc11PiqREREAAAyMzMRHBwMPz+/aldx5ObmAoBDfYY3b95EbGwsmjdvji+//BLOzs5G21vyM6yJt7c35HK59F5Wyc3NrXUsfn5+Rtub8ptsSPUZY5W33noLixYtwo8//oiwsDCjbdu0aQNvb29kZmY26Ib2dsZXxdnZGV27dkVmZiaAxvUZFhcXY/PmzZg/f36dr2Orz7A+avsdenh4wNXVFXK5/La/F3WyyJlPdFvMPcH7rbfeksoKCwtrPMH7999/l9r88MMPNj3Bu76x9O3bt9oVVLV59dVXhZeXV71jrQ9Lvdf79+8XAMTRo0eFEP87wfvvV3G8//77wsPDQ5SVlVluACao7xgLCwvFvffeK/r27SuKi4tNeq2G+Ax79eolJk+eLD3X6XTizjvvNHqC98MPP2xQFhkZWe0Eb2O/yYZm7hiFEOKNN94QHh4eIjU11aTX+Ouvv4RMJhNfffXVbcdrrvqM7++0Wq1o166deOGFF4QQjeczFOLWtkSpVIrr16/X+Rq2/Az/Diae4N25c2eDsieeeKLaCd63872oM06L9EL1cv78eXH48GHp0vjDhw+Lw4cPG1wi365dO/HFF19IzxctWiQ8PT3FV199Jf744w8xdOjQGqcO6Nq1q0hLSxP79+8Xbdu2tenUAcZiuXjxomjXrp1IS0szWO7s2bNCJpOJ77//vlqfX3/9tfjggw/EsWPHxNmzZ8W7774r3NzcxNy5c60+nn8yd3yZmZli/vz54vfffxfZ2dniq6++Em3atBEPPPCAtEzV1AEDBgwQR44cEcnJycLHx8emUweYM8bCwkIREREhQkNDRWZmpsGlylqtVghhu89w8+bNQqlUivXr14uTJ0+KcePGCU9PT+nKw2eeeUbMnj1bav/LL78IhUIh3nrrLZGRkSHmzZtX49QBdf0mG5K5Y1y0aJFwcXER27ZtM/isqtZDN2/eFDNmzBCpqakiOztb/Pjjj6Jbt26ibdu2DZ6812d8r7zyivjhhx9EVlaWSE9PFyNHjhQqlUqcOHFCauPon2GV++67T4wYMaJaub19hjdv3pS2dwDEkiVLxOHDh8X58+eFEELMnj1bPPPMM1L7qqkDXnzxRZGRkSFWrlxZ49QBxt6z28VkyYZGjRolAFR77N27V2qD/z8XTRW9Xi/mzJkjfH19hVKpFFFRUeL06dMG/d64cUM88cQTwt3dXXh4eIjRo0cbJGANqa5YsrOzq41ZCCGSkpJEYGCg0Ol01fr8/vvvRXh4uHB3dxfNmjUTXbp0EatWraqxrbWZO74LFy6IBx54QLRo0UIolUoREhIiXnzxRYN5loQQ4ty5c2LgwIHC1dVVeHt7i+nTpxtcdt+QzB3j3r17a/xeAxDZ2dlCCNt+hu+88464++67hYuLi+jVq5f47bffpLq+ffuKUaNGGbT/7LPPxD333CNcXFxEp06dxI4dOwzqTflNNjRzxtiqVasaP6t58+YJIYQoKSkRAwYMED4+PsLZ2Vm0atVKJCQkWGwjVB/mjG/atGlSW19fXzFo0CBx6NAhg/4c/TMUQohTp04JAGLnzp3V+rK3z7C2dUTVmEaNGiX69u1bbZnw8HDh4uIi2rRpY7BdrGLsPbtdMiEa+HprIiIiIgfCeZaIiIiIjGCyRERERGQEkyUiIiIiI5gsERERERnBZImIiIjICCZLREREREYwWSIiIiIygskSEVmUTCbD9u3bbR2GzbRu3RpLly69rT5efvllm9yXjIhqxmSJiEx27do1TJgwAXfffTeUSiX8/PwQExODX375RWpz5coVDBw40KpxrF+/HjKZrNpDpVJZ9XXtxblz5wzG3bx5c3Tq1AmTJk3C2bNnbR0eUaOjsHUAROQ4hg8fjoqKCmzYsAFt2rRBbm4udu/ejRs3bkhtLHaX7zp4eHjg9OnTBmUymaxBXtte/Pjjj+jUqRNKSkpw7NgxLFu2DF26dME333xj93eSJ3Ik3LNERCYpKCjAvn378MYbb6B///5o1aoVevXqhaSkJDzyyCNSu78fhqvaA/LFF1+gf//+cHNzQ5cuXZCammrQ9/79+3H//ffD1dUVgYGBmDp1KoqLi43GI5PJ4OfnZ/Dw9fWV6vv164epU6di5syZaNGiBfz8/PDyyy9XG9P48ePh6+sLlUqFzp0749tvv5XqP//8c3Tq1AlKpRKtW7fGf//7X4Plr169iiFDhsDV1RVBQUHYuHFjje/b2LFj4ePjAw8PDzz44IM4evSoQZtFixbB19cXzZs3x5gxY1BWVmZ07FXuuOMO+Pn5oU2bNhg6dCh+/PFHREREYMyYMdDpdACArKwsDB06FL6+vnB3d0fPnj3x448/Sn3Mnz8fnTt3rtZ3eHg45syZY1IcRI0dkyUiMom7uzvc3d2xfft2lJeXm7XsSy+9hBkzZuDIkSO455578MQTT0Cr1QK4tTGPjY3F8OHD8ccff2DLli3Yv38/Jk+efNsxb9iwAc2aNUNaWhoWL16M+fPnY9euXQAAvV6PgQMH4pdffsEnn3yCkydPYtGiRZDL5QCA9PR0/Pvf/8bIkSNx7NgxvPzyy5gzZw7Wr18v9R8fH4+//voLe/fuxbZt2/Duu+/i6tWrBjE8/vjjuHr1Kr7//nukp6ejW7duiIqKQl5eHgDgs88+w8svv4zXX38dv//+O/z9/fHuu+/Wa7xOTk54/vnncf78eaSnpwMAioqKMGjQIOzevRuHDx9GbGwshgwZggsXLgAAnn32WWRkZODgwYNSP4cPH8Yff/yB0aNH1ysOokbHYrfkJaJGb9u2bcLLy0uoVCrRu3dvkZSUJI4ePWrQBoD48ssvhRBCZGdnCwDiww8/lOpPnDghAIiMjAwhhBBjxowR48aNM+hj3759wsnJSZSWltYYx7p16wQA0axZM4NHbGys1KZv377ivvvuM1iuZ8+eYtasWUIIIX744Qfh5ORU693ln3zySfHQQw8ZlL344ouiY8eOQgghTp8+LQCIAwcOSPUZGRkCgHj77belcXh4eIiysjKDfoKDg8X7778vhBAiMjJSTJw40aA+IiJCdOnSpca4hPjf+3r48OFqdVUxbNmypdblO3XqJN555x3p+cCBA8WECROk51OmTBH9+vWrdXmipoZ7lojIZMOHD8fly5fx9ddfIzY2FikpKejWrZvB3paahIWFSX/7+/sDgLQH5ujRo1i/fr2058rd3R0xMTHQ6/XIzs6utc/mzZvjyJEjBo8PP/yw1teteu2q1z1y5Ajuuusu3HPPPTX2n5GRgT59+hiU9enTB2fPnoVOp0NGRgYUCgW6d+8u1bdv3x6enp7S86NHj6KoqAh33HGHwfiys7ORlZUlvU5ERITB60RGRtY67roIIQD87/ytoqIizJgxAx06dICnpyfc3d2RkZEh7VkCgISEBHz66acoKytDRUUFNm3ahGeffbbeMRA1NjzBm4jMolKp8NBDD+Ghhx7CnDlzMHbsWMybNw/x8fG1LuPs7Cz9XbUR1+v1AG5tzMePH4+pU6dWW+7uu++utU8nJyeEhIQYjfXvr1v12lWv6+rqanRZSygqKoK/vz9SUlKq1f09qbKkjIwMAEBQUBAAYMaMGdi1axfeeusthISEwNXVFY899hgqKiqkZYYMGQKlUokvv/wSLi4uqKysxGOPPWaV+IgcEZMlIrotHTt2vK15lbp164aTJ0/WmfhYWlhYGC5evIgzZ87UuHepQ4cOBlMiAMAvv/yCe+65B3K5HO3bt4dWq0V6ejp69uwJADh9+jQKCgqk9t26dUNOTg4UCgVat25dYxwdOnRAWloa4uLipLLffvutXmPS6/VYvnw5goKC0LVrVynm+Ph4PProowBuJXDnzp0zWE6hUGDUqFFYt24dXFxcMHLkyAZJJokcBZMlIjLJjRs38Pjjj+PZZ59FWFgYmjdvjt9//x2LFy/G0KFD693vrFmzcO+992Ly5MkYO3YsmjVrhpMnT2LXrl1YsWJFrcsJIZCTk1OtvGXLlnByqvsMg759++KBBx7A8OHDsWTJEoSEhODUqVOQyWSIjY3F9OnT0bNnTyxYsAAjRoxAamoqVqxYIZ183a5dO8TGxmL8+PF47733oFAoMG3aNIMkIzo6GpGRkRg2bBgWL16Me+65B5cvX8aOHTvw6KOPokePHnj++ecRHx+PHj16oE+fPti4cSNOnDiBNm3a1DmGGzduICcnByUlJTh+/DiWLl2KAwcOYMeOHdKJ6m3btsUXX3yBIUOGQCaTYc6cOdLetb8bO3YsOnToAADVkkSipo7JEhGZxN3dHREREXj77beRlZWFyspKBAYGIiEhAf/3f/9X737DwsLw008/4aWXXsL9998PIQSCg4MxYsQIo8tpNBrp/Ke/u3LlislzPX3++eeYMWMGnnjiCRQXFyMkJASLFi0CcGuv0GeffYa5c+diwYIF8Pf3x/z58w0ON65btw5jx45F37594evri1dffdXgcnuZTIbvvvsOL730EkaPHo1r167Bz88PDzzwgDTNwYgRI5CVlYWZM2eirKwMw4cPx4QJE/DDDz/UGX90dDQAwM3NDa1atUL//v2xevVqg710S5YswbPPPovevXvD29sbs2bNgkajqdZX27Zt0bt3b+Tl5VU7h4qoqZOJqrMBiYioyRJCoG3btpg4cSISExNtHQ6RXeGeJSKiJu7atWvYvHkzcnJyOLcSUQ2YLBERNXEtW7aEt7c3Vq9eDS8vL1uHQ2R3mCwRETVxPBuDyDhOSklERERkBJMlIiIiIiOYLBEREREZwWSJiIiIyAgmS0RERERGMFkiIiIiMoLJEhEREZERTJaIiIiIjGCyRERERGTE/wOzXp2o8jJR7gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cyclic_day = sns.scatterplot(x='day_sin', y='day_cos', data=data, color=\"#C2C4E2\")\n",
    "cyclic_day.set_title(\"Cyclic Encoding of Day\")\n",
    "cyclic_day.set_ylabel(\"Cosine Encoded Day\")\n",
    "cyclic_day.set_xlabel(\"Sine Encoded Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Location', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Cloud9am', 'Cloud3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (data.dtypes == \"object\")\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location 0\n",
      "Evaporation 0\n",
      "Sunshine 0\n",
      "WindGustDir 0\n",
      "WindDir9am 0\n",
      "WindDir3pm 0\n",
      "Cloud9am 0\n",
      "Cloud3pm 0\n",
      "RainToday 0\n",
      "RainTomorrow 0\n"
     ]
    }
   ],
   "source": [
    "# Missing values in categorical variables\n",
    "\n",
    "for i in object_cols:\n",
    "    print(i, data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filling missing values with mode of the column in value\n",
    "\n",
    "for i in object_cols:\n",
    "    data[i].fillna(data[i].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric variables:\n",
      "['MinTemp', 'MaxTemp', 'Rainfall', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\n"
     ]
    }
   ],
   "source": [
    "# Get list of numeric variables\n",
    "t = (data.dtypes == \"float64\")\n",
    "num_cols = list(t[t].index)\n",
    "\n",
    "print(\"Numeric variables:\")\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp 0\n",
      "MaxTemp 0\n",
      "Rainfall 0\n",
      "Pressure9am 0\n",
      "Pressure3pm 0\n",
      "Temp9am 0\n",
      "Temp3pm 0\n",
      "month_sin 0\n",
      "month_cos 0\n",
      "day_sin 0\n",
      "day_cos 0\n"
     ]
    }
   ],
   "source": [
    "# Missing values in numeric variables\n",
    "\n",
    "for i in num_cols:\n",
    "    print(i, data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>44</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NE</td>\n",
       "      <td>24</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>W</td>\n",
       "      <td>41</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Location  MinTemp  MaxTemp  Rainfall Evaporation Sunshine  \\\n",
       "0 2008-12-01   Albury     13.4     22.9       0.6          NA       NA   \n",
       "1 2008-12-02   Albury      7.4     25.1       0.0          NA       NA   \n",
       "2 2008-12-03   Albury     12.9     25.7       0.0          NA       NA   \n",
       "3 2008-12-04   Albury      9.2     28.0       0.0          NA       NA   \n",
       "4 2008-12-05   Albury     17.5     32.3       1.0          NA       NA   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Temp3pm  RainToday  RainTomorrow  \\\n",
       "0           W             44          W  ...    21.8         No            No   \n",
       "1         WNW             44        NNW  ...    24.3         No            No   \n",
       "2         WSW             46          W  ...    23.2         No            No   \n",
       "3          NE             24         SE  ...    26.5         No            No   \n",
       "4           W             41        ENE  ...    29.7         No            No   \n",
       "\n",
       "   year  month     month_sin  month_cos day   day_sin   day_cos  \n",
       "0  2008     12 -2.449294e-16        1.0   1  0.201299  0.979530  \n",
       "1  2008     12 -2.449294e-16        1.0   2  0.394356  0.918958  \n",
       "2  2008     12 -2.449294e-16        1.0   3  0.571268  0.820763  \n",
       "3  2008     12 -2.449294e-16        1.0   4  0.724793  0.688967  \n",
       "4  2008     12 -2.449294e-16        1.0   5  0.848644  0.528964  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filling missing values with median of the column in value\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i].fillna(data[i].median(), inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119590 entries, 0 to 119589\n",
      "Data columns (total 30 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   Date           119590 non-null  datetime64[ns]\n",
      " 1   Location       119590 non-null  int32         \n",
      " 2   MinTemp        119590 non-null  float64       \n",
      " 3   MaxTemp        119590 non-null  float64       \n",
      " 4   Rainfall       119590 non-null  float64       \n",
      " 5   Evaporation    119590 non-null  int32         \n",
      " 6   Sunshine       119590 non-null  int32         \n",
      " 7   WindGustDir    119590 non-null  int32         \n",
      " 8   WindGustSpeed  119590 non-null  int64         \n",
      " 9   WindDir9am     119590 non-null  int32         \n",
      " 10  WindDir3pm     119590 non-null  int32         \n",
      " 11  WindSpeed9am   119590 non-null  int64         \n",
      " 12  WindSpeed3pm   119590 non-null  int64         \n",
      " 13  Humidity9am    119590 non-null  int64         \n",
      " 14  Humidity3pm    119590 non-null  int64         \n",
      " 15  Pressure9am    119590 non-null  float64       \n",
      " 16  Pressure3pm    119590 non-null  float64       \n",
      " 17  Cloud9am       119590 non-null  int32         \n",
      " 18  Cloud3pm       119590 non-null  int32         \n",
      " 19  Temp9am        119590 non-null  float64       \n",
      " 20  Temp3pm        119590 non-null  float64       \n",
      " 21  RainToday      119590 non-null  int32         \n",
      " 22  RainTomorrow   119590 non-null  int32         \n",
      " 23  year           119590 non-null  int64         \n",
      " 24  month          119590 non-null  int64         \n",
      " 25  month_sin      119590 non-null  float64       \n",
      " 26  month_cos      119590 non-null  float64       \n",
      " 27  day            119590 non-null  int64         \n",
      " 28  day_sin        119590 non-null  float64       \n",
      " 29  day_cos        119590 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int32(10), int64(8)\n",
      "memory usage: 22.8 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>1</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0 2008-12-01         1     13.4     22.9       0.6          349       145   \n",
       "1 2008-12-02         1      7.4     25.1       0.0          349       145   \n",
       "2 2008-12-03         1     12.9     25.7       0.0          349       145   \n",
       "3 2008-12-04         1      9.2     28.0       0.0          349       145   \n",
       "4 2008-12-05         1     17.5     32.3       1.0          349       145   \n",
       "\n",
       "   WindGustDir  WindGustSpeed  WindDir9am  ...  Temp3pm  RainToday  \\\n",
       "0           14             44          14  ...     21.8          0   \n",
       "1           15             44           7  ...     24.3          0   \n",
       "2           16             46          14  ...     23.2          0   \n",
       "3            5             24          10  ...     26.5          0   \n",
       "4           14             41           1  ...     29.7          0   \n",
       "\n",
       "   RainTomorrow  year  month     month_sin  month_cos  day   day_sin   day_cos  \n",
       "0             0  2008     12 -2.449294e-16        1.0    1  0.201299  0.979530  \n",
       "1             0  2008     12 -2.449294e-16        1.0    2  0.394356  0.918958  \n",
       "2             0  2008     12 -2.449294e-16        1.0    3  0.571268  0.820763  \n",
       "3             0  2008     12 -2.449294e-16        1.0    4  0.724793  0.688967  \n",
       "4             0  2008     12 -2.449294e-16        1.0    5  0.848644  0.528964  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for i in object_cols:\n",
    "    data[i] = label_encoder.fit_transform(data[i])\n",
    "\n",
    "data.info()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>6.084085e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.650786</td>\n",
       "      <td>-0.873365</td>\n",
       "      <td>-0.018202</td>\n",
       "      <td>0.836961</td>\n",
       "      <td>1.692124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinTemp</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-4.563064e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-3.233061</td>\n",
       "      <td>-0.736009</td>\n",
       "      <td>-0.045001</td>\n",
       "      <td>0.724530</td>\n",
       "      <td>3.378629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxTemp</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>4.030707e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.989452</td>\n",
       "      <td>-0.770904</td>\n",
       "      <td>-0.083870</td>\n",
       "      <td>0.717670</td>\n",
       "      <td>3.523060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rainfall</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>4.753192e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-0.274179</td>\n",
       "      <td>-0.274179</td>\n",
       "      <td>-0.274179</td>\n",
       "      <td>-0.203300</td>\n",
       "      <td>43.151278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evaporation</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.007677e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.977341</td>\n",
       "      <td>-0.624635</td>\n",
       "      <td>0.459160</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.866602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sunshine</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.635098e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.064135</td>\n",
       "      <td>-1.046486</td>\n",
       "      <td>0.569780</td>\n",
       "      <td>0.829181</td>\n",
       "      <td>0.829181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindGustDir</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.102740e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.685186</td>\n",
       "      <td>-1.087103</td>\n",
       "      <td>0.109064</td>\n",
       "      <td>0.906509</td>\n",
       "      <td>1.504593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.278609e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.530298</td>\n",
       "      <td>-0.675656</td>\n",
       "      <td>-0.082171</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>7.039651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindDir9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-9.886639e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.597101</td>\n",
       "      <td>-0.979023</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.875209</td>\n",
       "      <td>1.699313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindDir3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-7.795234e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.717770</td>\n",
       "      <td>-0.705782</td>\n",
       "      <td>0.103809</td>\n",
       "      <td>0.913399</td>\n",
       "      <td>1.520591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>5.893958e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.632937</td>\n",
       "      <td>-0.837149</td>\n",
       "      <td>-0.155044</td>\n",
       "      <td>0.640744</td>\n",
       "      <td>8.257575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.330894e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.199606</td>\n",
       "      <td>-0.701099</td>\n",
       "      <td>-0.009480</td>\n",
       "      <td>0.566869</td>\n",
       "      <td>7.828868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>2.813889e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-3.569925</td>\n",
       "      <td>-0.642462</td>\n",
       "      <td>0.037127</td>\n",
       "      <td>0.716717</td>\n",
       "      <td>1.657687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>1.045702e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.458287</td>\n",
       "      <td>-0.718459</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.683069</td>\n",
       "      <td>2.374569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pressure9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.505051e-14</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-5.234333</td>\n",
       "      <td>-0.655873</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.668359</td>\n",
       "      <td>3.288647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pressure3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>1.006536e-14</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-5.435375</td>\n",
       "      <td>-0.677120</td>\n",
       "      <td>-0.007545</td>\n",
       "      <td>0.676276</td>\n",
       "      <td>3.468545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloud9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.121753e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.797057</td>\n",
       "      <td>-0.947684</td>\n",
       "      <td>0.184813</td>\n",
       "      <td>1.034186</td>\n",
       "      <td>1.034186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloud3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>1.045702e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.886136</td>\n",
       "      <td>-1.009928</td>\n",
       "      <td>0.158348</td>\n",
       "      <td>1.034556</td>\n",
       "      <td>1.034556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temp9am</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-4.487013e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-3.145385</td>\n",
       "      <td>-0.742058</td>\n",
       "      <td>-0.059824</td>\n",
       "      <td>0.715443</td>\n",
       "      <td>3.568424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temp3pm</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-3.498349e-16</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.967852</td>\n",
       "      <td>-0.757915</td>\n",
       "      <td>-0.084690</td>\n",
       "      <td>0.690983</td>\n",
       "      <td>3.618051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RainToday</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-8.650809e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-0.532718</td>\n",
       "      <td>-0.532718</td>\n",
       "      <td>-0.532718</td>\n",
       "      <td>-0.532718</td>\n",
       "      <td>1.877167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-2.084940e-14</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-2.274744</td>\n",
       "      <td>-0.691781</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.891183</td>\n",
       "      <td>1.682665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_sin</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-1.241771e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.434665</td>\n",
       "      <td>-0.725855</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>1.400575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_cos</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>1.616085e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.390232</td>\n",
       "      <td>-1.201147</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>0.726791</td>\n",
       "      <td>1.432466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_sin</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>-4.901729e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.402132</td>\n",
       "      <td>-1.018026</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>1.014636</td>\n",
       "      <td>1.398742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_cos</th>\n",
       "      <td>119590.0</td>\n",
       "      <td>3.636192e-17</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-1.392944</td>\n",
       "      <td>-1.056010</td>\n",
       "      <td>-0.045528</td>\n",
       "      <td>1.009915</td>\n",
       "      <td>1.453764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count          mean       std       min       25%       50%  \\\n",
       "Location       119590.0  6.084085e-17  1.000004 -1.650786 -0.873365 -0.018202   \n",
       "MinTemp        119590.0 -4.563064e-17  1.000004 -3.233061 -0.736009 -0.045001   \n",
       "MaxTemp        119590.0  4.030707e-16  1.000004 -2.989452 -0.770904 -0.083870   \n",
       "Rainfall       119590.0  4.753192e-17  1.000004 -0.274179 -0.274179 -0.274179   \n",
       "Evaporation    119590.0 -1.007677e-16  1.000004 -1.977341 -0.624635  0.459160   \n",
       "Sunshine       119590.0 -1.635098e-16  1.000004 -2.064135 -1.046486  0.569780   \n",
       "WindGustDir    119590.0 -1.102740e-16  1.000004 -1.685186 -1.087103  0.109064   \n",
       "WindGustSpeed  119590.0 -1.278609e-16  1.000004 -2.530298 -0.675656 -0.082171   \n",
       "WindDir9am     119590.0 -9.886639e-17  1.000004 -1.597101 -0.979023  0.051106   \n",
       "WindDir3pm     119590.0 -7.795234e-17  1.000004 -1.717770 -0.705782  0.103809   \n",
       "WindSpeed9am   119590.0  5.893958e-17  1.000004 -1.632937 -0.837149 -0.155044   \n",
       "WindSpeed3pm   119590.0 -1.330894e-17  1.000004 -2.199606 -0.701099 -0.009480   \n",
       "Humidity9am    119590.0  2.813889e-16  1.000004 -3.569925 -0.642462  0.037127   \n",
       "Humidity3pm    119590.0  1.045702e-16  1.000004 -2.458287 -0.718459  0.006469   \n",
       "Pressure9am    119590.0 -1.505051e-14  1.000004 -5.234333 -0.655873 -0.007845   \n",
       "Pressure3pm    119590.0  1.006536e-14  1.000004 -5.435375 -0.677120 -0.007545   \n",
       "Cloud9am       119590.0 -1.121753e-16  1.000004 -1.797057 -0.947684  0.184813   \n",
       "Cloud3pm       119590.0  1.045702e-16  1.000004 -1.886136 -1.009928  0.158348   \n",
       "Temp9am        119590.0 -4.487013e-16  1.000004 -3.145385 -0.742058 -0.059824   \n",
       "Temp3pm        119590.0 -3.498349e-16  1.000004 -2.967852 -0.757915 -0.084690   \n",
       "RainToday      119590.0 -8.650809e-17  1.000004 -0.532718 -0.532718 -0.532718   \n",
       "year           119590.0 -2.084940e-14  1.000004 -2.274744 -0.691781  0.099701   \n",
       "month_sin      119590.0 -1.241771e-17  1.000004 -1.434665 -0.725855 -0.017045   \n",
       "month_cos      119590.0  1.616085e-17  1.000004 -1.390232 -1.201147  0.021117   \n",
       "day_sin        119590.0 -4.901729e-17  1.000004 -1.402132 -1.018026 -0.001695   \n",
       "day_cos        119590.0  3.636192e-17  1.000004 -1.392944 -1.056010 -0.045528   \n",
       "\n",
       "                    75%        max  \n",
       "Location       0.836961   1.692124  \n",
       "MinTemp        0.724530   3.378629  \n",
       "MaxTemp        0.717670   3.523060  \n",
       "Rainfall      -0.203300  43.151278  \n",
       "Evaporation    0.866602   0.866602  \n",
       "Sunshine       0.829181   0.829181  \n",
       "WindGustDir    0.906509   1.504593  \n",
       "WindGustSpeed  0.585500   7.039651  \n",
       "WindDir9am     0.875209   1.699313  \n",
       "WindDir3pm     0.913399   1.520591  \n",
       "WindSpeed9am   0.640744   8.257575  \n",
       "WindSpeed3pm   0.566869   7.828868  \n",
       "Humidity9am    0.716717   1.657687  \n",
       "Humidity3pm    0.683069   2.374569  \n",
       "Pressure9am    0.668359   3.288647  \n",
       "Pressure3pm    0.676276   3.468545  \n",
       "Cloud9am       1.034186   1.034186  \n",
       "Cloud3pm       1.034556   1.034556  \n",
       "Temp9am        0.715443   3.568424  \n",
       "Temp3pm        0.690983   3.618051  \n",
       "RainToday     -0.532718   1.877167  \n",
       "year           0.891183   1.682665  \n",
       "month_sin      0.691765   1.400575  \n",
       "month_cos      0.726791   1.432466  \n",
       "day_sin        1.014636   1.398742  \n",
       "day_cos        1.009915   1.453764  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing attributes of scale data\n",
    "featuresLR = data.drop(['Date', 'Location','day', 'month'], axis=1)\n",
    "featuresLR.head(5)\n",
    "\n",
    "features = data.drop(['RainTomorrow', 'Date', 'day', 'month'], axis=1)  # dropping target and extra columns\n",
    "\n",
    "target = data['RainTomorrow']\n",
    "\n",
    "#Set up a standard scaler for the features\n",
    "col_names = list(features.columns)\n",
    "s_scaler = preprocessing.StandardScaler()\n",
    "features = s_scaler.fit_transform(features)\n",
    "features = pd.DataFrame(features, columns=col_names)\n",
    "\n",
    "features.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Detecting outliers\n",
    "#looking at the scaled features\n",
    "colours = [\"#D0DBEE\", \"#C2C4E2\", \"#EED4E5\", \"#D1E6DC\", \"#BDE2E2\"]\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxenplot(data=features, palette=colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107630, 27)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full data for\n",
    "features[\"RainTomorrow\"] = target\n",
    "\n",
    "#Dropping with outlier\n",
    "\n",
    "features = features[(features[\"MinTemp\"] < 2.3) & (features[\"MinTemp\"] > -2.3)]\n",
    "features = features[(features[\"MaxTemp\"] < 2.3) & (features[\"MaxTemp\"] > -2)]\n",
    "features = features[(features[\"Rainfall\"] < 4.5)]\n",
    "features = features[(features[\"Evaporation\"] < 2.8)]\n",
    "features = features[(features[\"Sunshine\"] < 2.1)]\n",
    "features = features[(features[\"WindGustSpeed\"] < 4) & (features[\"WindGustSpeed\"] > -4)]\n",
    "features = features[(features[\"WindSpeed9am\"] < 4)]\n",
    "features = features[(features[\"WindSpeed3pm\"] < 2.5)]\n",
    "features = features[(features[\"Humidity9am\"] > -3)]\n",
    "features = features[(features[\"Humidity3pm\"] > -2.2)]\n",
    "features = features[(features[\"Pressure9am\"] < 2) & (features[\"Pressure9am\"] > -2.7)]\n",
    "features = features[(features[\"Pressure3pm\"] < 2) & (features[\"Pressure3pm\"] > -2.7)]\n",
    "features = features[(features[\"Cloud9am\"] < 1.8)]\n",
    "features = features[(features[\"Cloud3pm\"] < 2)]\n",
    "features = features[(features[\"Temp9am\"] < 2.3) & (features[\"Temp9am\"] > -2)]\n",
    "features = features[(features[\"Temp3pm\"] < 2.3) & (features[\"Temp3pm\"] > -2)]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#looking at the scaled features without outliers\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxenplot(data=features, palette=colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107630"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_connection = db.getConnectDataFrame()\n",
    "features.to_sql(con=database_connection, name='weather_processed', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>349</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  WindGustDir  \\\n",
       "0     13.4     22.9       0.6          349       145           14   \n",
       "1      7.4     25.1       0.0          349       145           15   \n",
       "2     12.9     25.7       0.0          349       145           16   \n",
       "3      9.2     28.0       0.0          349       145            5   \n",
       "4     17.5     32.3       1.0          349       145           14   \n",
       "\n",
       "   WindGustSpeed  WindDir9am  WindDir3pm  WindSpeed9am  ...  Cloud3pm  \\\n",
       "0             44          14          15            20  ...        10   \n",
       "1             44           7          16             4  ...        10   \n",
       "2             46          14          16            19  ...         2   \n",
       "3             24          10           0            11  ...        10   \n",
       "4             41           1           8             7  ...         8   \n",
       "\n",
       "   Temp9am  Temp3pm  RainToday  RainTomorrow  year     month_sin  month_cos  \\\n",
       "0     16.9     21.8          0             0  2008 -2.449294e-16        1.0   \n",
       "1     17.2     24.3          0             0  2008 -2.449294e-16        1.0   \n",
       "2     21.0     23.2          0             0  2008 -2.449294e-16        1.0   \n",
       "3     18.1     26.5          0             0  2008 -2.449294e-16        1.0   \n",
       "4     17.8     29.7          0             0  2008 -2.449294e-16        1.0   \n",
       "\n",
       "    day_sin   day_cos  \n",
       "0  0.201299  0.979530  \n",
       "1  0.394356  0.918958  \n",
       "2  0.571268  0.820763  \n",
       "3  0.724793  0.688967  \n",
       "4  0.848644  0.528964  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_LR = featuresLR.RainTomorrow.values\n",
    "x_dataLR = featuresLR\n",
    "x_dataLR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119590"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# In order to scale all the features between 0 and 1:\n",
    "x_LR = (x_dataLR - np.min(x_dataLR)) / (np.max(x_dataLR) - np.min(x_dataLR))\n",
    "x_LR.head(5)\n",
    "x_LR.to_sql(con=database_connection, name='weather_processed_logistic', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sklearn's library for splitting our dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_lR = x_LR.drop('RainTomorrow', axis=1)\n",
    "x_trainLR, x_testLR, y_trainLR, y_testLR = train_test_split(x_LR, y_LR, test_size=0.2, random_state=75)\n",
    "\n",
    "# For our matrix calculations we need to transpose our matrixis:\n",
    "x_trainLR = x_trainLR.T\n",
    "y_trainLR = y_trainLR.T\n",
    "x_testLR = x_testLR.T\n",
    "y_testLR = y_testLR.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.726239\n",
      "Cost after iteration 20: 0.194602\n",
      "Cost after iteration 40: 0.110900\n",
      "Cost after iteration 60: 0.076218\n",
      "Cost after iteration 80: 0.057651\n",
      "Cost after iteration 100: 0.046198\n",
      "Cost after iteration 120: 0.038468\n",
      "Cost after iteration 140: 0.032916\n",
      "Cost after iteration 160: 0.028744\n",
      "Cost after iteration 180: 0.025498\n",
      "Cost after iteration 200: 0.022902\n",
      "Cost after iteration 220: 0.020782\n",
      "Cost after iteration 240: 0.019017\n",
      "Cost after iteration 260: 0.017526\n",
      "Cost after iteration 280: 0.016250\n",
      "Cost after iteration 300: 0.015146\n",
      "Cost after iteration 320: 0.014182\n",
      "Cost after iteration 340: 0.013332\n",
      "Cost after iteration 360: 0.012578\n",
      "Cost after iteration 380: 0.011904\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23918"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model.logisticRegresstion as LR\n",
    "dimension = x_trainLR.shape[0]\n",
    "w, b = LR.initialize_weight_bias(dimension)    # Creating an initial weight matrix of (x_train data[0] x 1)\n",
    "\n",
    "# Updating our w and b by using update method. \n",
    "# Update method contains our forward and backward propagation.\n",
    "parameters, gradients, cost_list = LR.update(w, b, x_trainLR, y_trainLR, learning_rate=1, nu_of_iteration=400)\n",
    "\n",
    "# Lets use x_test for predicting y:\n",
    "y_test_predictions = LR.prediction(parameters['weight'], parameters['bias'], x_testLR) \n",
    "\n",
    "# Investigate the accuracy:\n",
    "print('Test accuracy: {}%'.format(100 - np.mean(np.abs(y_test_predictions - y_testLR))*100))\n",
    "result = pd.DataFrame({'id': [*range(1, y_test_predictions[0].size + 1)], 'prediction': y_test_predictions[0], 'actual': y_testLR})\n",
    "\n",
    "result.to_sql(con=database_connection, name='result_logisticregrestion', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id         labels      corr\n",
      "0    1   RainTomorrow  1.000000\n",
      "1    2    Humidity3pm  0.453435\n",
      "2    3      RainToday  0.294478\n",
      "3    4    Humidity9am  0.269206\n",
      "4    5       Rainfall  0.264366\n",
      "5    6  WindGustSpeed  0.206806\n",
      "6    7       Cloud3pm  0.164293\n",
      "7    8       Cloud9am  0.151110\n",
      "8    9        MinTemp  0.086450\n",
      "9   10   WindSpeed9am  0.063437\n",
      "10  11   WindSpeed3pm  0.056912\n",
      "11  12    WindGustDir  0.042792\n",
      "12  13     WindDir9am  0.025351\n",
      "13  14     WindDir3pm  0.016407\n",
      "14  15       Location  0.006181\n",
      "15  16        day_cos -0.000899\n",
      "16  17        day_sin -0.001142\n",
      "17  18           year -0.010569\n",
      "18  19      month_sin -0.023054\n",
      "19  20        Temp9am -0.025784\n",
      "20  21       Sunshine -0.026128\n",
      "21  22    Evaporation -0.042372\n",
      "22  23      month_cos -0.053897\n",
      "23  24        MaxTemp -0.165297\n",
      "24  25        Temp3pm -0.200424\n",
      "25  26    Pressure3pm -0.207190\n",
      "26  27    Pressure9am -0.224642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_data = features.corr()[\"RainTomorrow\"].sort_values(ascending=False)\n",
    "df_corr = pd.DataFrame({\n",
    "    'id': [*range(1, corr_data.index.size + 1)],\n",
    "    'labels': corr_data.index,\n",
    "    'corr': corr_data.values})\n",
    "df_corr.set_index('id')\n",
    "print(df_corr)\n",
    "df_corr.to_sql(con=database_connection, name='correlation', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107630, 26)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = features.drop([\"RainTomorrow\"], axis=1)\n",
    "y = features[\"RainTomorrow\"]\n",
    "\n",
    "# Splitting test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net architecture :\n",
      " -> Linear(26 , 32) -> relu -> Linear(32 , 32) -> relu -> Linear(32 , 1) -> sigmoid\n",
      "Epochs  1 / 10\n",
      "At: 1 [==========>] Loss 0.19391736664099757  - accuracy: 0.71875\n",
      "At: 2 [==========>] Loss 0.19079821136405134  - accuracy: 0.6875\n",
      "At: 3 [==========>] Loss 0.15927270632717458  - accuracy: 0.78125\n",
      "At: 4 [==========>] Loss 0.2249064715717659  - accuracy: 0.6875\n",
      "At: 5 [==========>] Loss 0.1862593205854006  - accuracy: 0.78125\n",
      "At: 6 [==========>] Loss 0.1978052328487636  - accuracy: 0.8125\n",
      "At: 7 [==========>] Loss 0.19716318009536515  - accuracy: 0.65625\n",
      "At: 8 [==========>] Loss 0.16223449023218092  - accuracy: 0.78125\n",
      "At: 9 [==========>] Loss 0.2635827475307128  - accuracy: 0.5\n",
      "At: 10 [==========>] Loss 0.18678781995460958  - accuracy: 0.6875\n",
      "At: 11 [==========>] Loss 0.18362340681405487  - accuracy: 0.75\n",
      "At: 12 [==========>] Loss 0.1563851811775379  - accuracy: 0.75\n",
      "At: 13 [==========>] Loss 0.15023686492065152  - accuracy: 0.78125\n",
      "At: 14 [==========>] Loss 0.1317084627610636  - accuracy: 0.9375\n",
      "At: 15 [==========>] Loss 0.17444171829014457  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.2073437370887862  - accuracy: 0.6875\n",
      "At: 17 [==========>] Loss 0.16435276412058566  - accuracy: 0.78125\n",
      "At: 18 [==========>] Loss 0.17643973643095517  - accuracy: 0.71875\n",
      "At: 19 [==========>] Loss 0.1820008224210436  - accuracy: 0.78125\n",
      "At: 20 [==========>] Loss 0.1302428222686869  - accuracy: 0.84375\n",
      "At: 21 [==========>] Loss 0.2010334424464777  - accuracy: 0.6875\n",
      "At: 22 [==========>] Loss 0.1520305894825878  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.11102470121090016  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.20678242819665232  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.20096785462931058  - accuracy: 0.75\n",
      "At: 26 [==========>] Loss 0.21779402488638744  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.17701108242067368  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.1811794162020512  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.15464626730474346  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.19357733081933579  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.20748555229122312  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.20220062062116606  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.09692355616117129  - accuracy: 0.9375\n",
      "At: 34 [==========>] Loss 0.13148751129425806  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.14873181070463176  - accuracy: 0.8125\n",
      "At: 36 [==========>] Loss 0.17426058142984668  - accuracy: 0.8125\n",
      "At: 37 [==========>] Loss 0.1928828616309481  - accuracy: 0.71875\n",
      "At: 38 [==========>] Loss 0.21256616526162836  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.15834048012612306  - accuracy: 0.8125\n",
      "At: 40 [==========>] Loss 0.18958240365761966  - accuracy: 0.65625\n",
      "At: 41 [==========>] Loss 0.08699923338441215  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.1763837683968599  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.14911003524176725  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.16675903007046414  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.0970522102981872  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.1667030083903581  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.18259825935431062  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.10529404888873233  - accuracy: 0.875\n",
      "At: 49 [==========>] Loss 0.0851093183712214  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.12450833444956325  - accuracy: 0.8125\n",
      "At: 51 [==========>] Loss 0.17848371761935602  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.23567460303744184  - accuracy: 0.625\n",
      "At: 53 [==========>] Loss 0.14960609636250297  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.10458437051670849  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.16238943111840673  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.13810993047947287  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.1472265004917092  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.18565716705385754  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.18656407827017343  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.21017000768507055  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.19349446247031477  - accuracy: 0.6875\n",
      "At: 62 [==========>] Loss 0.1471036538703395  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.21991943471994255  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.1659954523119881  - accuracy: 0.75\n",
      "At: 65 [==========>] Loss 0.21020915325227663  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.167353596627333  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.18540938203232826  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.08048587497501712  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.14482653274534932  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.16128156486713824  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.12849939533812516  - accuracy: 0.84375\n",
      "At: 72 [==========>] Loss 0.12742700679050845  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.1224571965579213  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.17423745082187192  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.18930494920700797  - accuracy: 0.78125\n",
      "At: 76 [==========>] Loss 0.20158290775420365  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.17465756430257298  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.14040283820998636  - accuracy: 0.78125\n",
      "At: 79 [==========>] Loss 0.18866198166060041  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.16530463113184396  - accuracy: 0.78125\n",
      "At: 81 [==========>] Loss 0.13137710939873587  - accuracy: 0.75\n",
      "At: 82 [==========>] Loss 0.19206900412493047  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.12369592451138234  - accuracy: 0.78125\n",
      "At: 84 [==========>] Loss 0.16727145493889528  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.1649465934005178  - accuracy: 0.78125\n",
      "At: 86 [==========>] Loss 0.1345954105375755  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.12760246281885526  - accuracy: 0.84375\n",
      "At: 88 [==========>] Loss 0.2659367282437362  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.15994421878782195  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.1883965141483097  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.12446713996222405  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.10055691360613742  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.12036118281985309  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.16169891910538137  - accuracy: 0.84375\n",
      "At: 95 [==========>] Loss 0.14122397944198178  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.15918475652786987  - accuracy: 0.78125\n",
      "At: 97 [==========>] Loss 0.09294851606082859  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.2123448810426053  - accuracy: 0.71875\n",
      "At: 99 [==========>] Loss 0.13839453561602733  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.1167807813452946  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.11179835578606494  - accuracy: 0.875\n",
      "At: 102 [==========>] Loss 0.15915839991139352  - accuracy: 0.8125\n",
      "At: 103 [==========>] Loss 0.1410531476103043  - accuracy: 0.84375\n",
      "At: 104 [==========>] Loss 0.10549216285568538  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.13714690837212673  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.162041297359783  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.17506879987254456  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.16904121097624591  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.07716469271318083  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.2086595299064148  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.0914333338086222  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.1204073518804442  - accuracy: 0.84375\n",
      "At: 113 [==========>] Loss 0.17468884038587065  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.1198866938875014  - accuracy: 0.84375\n",
      "At: 115 [==========>] Loss 0.13668063943604797  - accuracy: 0.8125\n",
      "At: 116 [==========>] Loss 0.21108301603801677  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.14170723220713072  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.24012025902344855  - accuracy: 0.625\n",
      "At: 119 [==========>] Loss 0.13314465148887455  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.1299110638851396  - accuracy: 0.8125\n",
      "At: 121 [==========>] Loss 0.17893528371807627  - accuracy: 0.78125\n",
      "At: 122 [==========>] Loss 0.16321128777455107  - accuracy: 0.75\n",
      "At: 123 [==========>] Loss 0.14919129117657653  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.21342987714174  - accuracy: 0.625\n",
      "At: 125 [==========>] Loss 0.1389977686552256  - accuracy: 0.84375\n",
      "At: 126 [==========>] Loss 0.20542358465809246  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.14298456556144018  - accuracy: 0.8125\n",
      "At: 128 [==========>] Loss 0.21290846398026408  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.11655452862225496  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.18546691534467358  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.1436622806959904  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.18043079103446974  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.17853983131660786  - accuracy: 0.78125\n",
      "At: 134 [==========>] Loss 0.14716513284403723  - accuracy: 0.71875\n",
      "At: 135 [==========>] Loss 0.19492362710345912  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.14551085054756557  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.09342023951559009  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.1578050202252057  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.1211225729465404  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.14044400635043364  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.21755264481436354  - accuracy: 0.625\n",
      "At: 142 [==========>] Loss 0.15919589088429614  - accuracy: 0.8125\n",
      "At: 143 [==========>] Loss 0.17408564084443046  - accuracy: 0.8125\n",
      "At: 144 [==========>] Loss 0.1031978776738418  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.11283314165385305  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.09677335154276358  - accuracy: 0.875\n",
      "At: 147 [==========>] Loss 0.15796367050339225  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.14757945685200624  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.14396099517920943  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.14840700558799058  - accuracy: 0.78125\n",
      "At: 151 [==========>] Loss 0.12451198471424686  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.15013298346754614  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.1422263297604154  - accuracy: 0.78125\n",
      "At: 154 [==========>] Loss 0.13576968622644736  - accuracy: 0.8125\n",
      "At: 155 [==========>] Loss 0.18386329964208592  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1431757501300467  - accuracy: 0.78125\n",
      "At: 157 [==========>] Loss 0.2020071296551999  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.1616352735346597  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.12391702485537011  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.1365034669681954  - accuracy: 0.78125\n",
      "At: 161 [==========>] Loss 0.10325532881857832  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.1674191562818999  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.12368895856265112  - accuracy: 0.8125\n",
      "At: 164 [==========>] Loss 0.14644675437228638  - accuracy: 0.8125\n",
      "At: 165 [==========>] Loss 0.156460963762232  - accuracy: 0.75\n",
      "At: 166 [==========>] Loss 0.15191336354232127  - accuracy: 0.8125\n",
      "At: 167 [==========>] Loss 0.09863337281817477  - accuracy: 0.8125\n",
      "At: 168 [==========>] Loss 0.14554691774328143  - accuracy: 0.78125\n",
      "At: 169 [==========>] Loss 0.1357254169811643  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.13659946945163542  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.18500816820714494  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.13917404879310932  - accuracy: 0.8125\n",
      "At: 173 [==========>] Loss 0.22239572113683037  - accuracy: 0.71875\n",
      "At: 174 [==========>] Loss 0.14894406588121406  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.13981579865235944  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.18501063517677235  - accuracy: 0.6875\n",
      "At: 177 [==========>] Loss 0.10974062877576969  - accuracy: 0.84375\n",
      "At: 178 [==========>] Loss 0.1486920104069429  - accuracy: 0.78125\n",
      "At: 179 [==========>] Loss 0.1208219095660601  - accuracy: 0.78125\n",
      "At: 180 [==========>] Loss 0.1289752676877386  - accuracy: 0.84375\n",
      "At: 181 [==========>] Loss 0.061498871412580876  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.14859510235602869  - accuracy: 0.75\n",
      "At: 183 [==========>] Loss 0.11705504063686756  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.10017667437091302  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.11735027022993241  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.11570520389102588  - accuracy: 0.84375\n",
      "At: 187 [==========>] Loss 0.13064477918526185  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.1433187336916062  - accuracy: 0.8125\n",
      "At: 189 [==========>] Loss 0.14795618485154904  - accuracy: 0.8125\n",
      "At: 190 [==========>] Loss 0.10424236438075132  - accuracy: 0.9375\n",
      "At: 191 [==========>] Loss 0.22608614064204752  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.11717786051393092  - accuracy: 0.875\n",
      "At: 193 [==========>] Loss 0.20128444656658487  - accuracy: 0.65625\n",
      "At: 194 [==========>] Loss 0.12934620433450994  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.1124460828492205  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.119741797218408  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.12549087343474705  - accuracy: 0.84375\n",
      "At: 198 [==========>] Loss 0.10227720880360516  - accuracy: 0.90625\n",
      "At: 199 [==========>] Loss 0.08807971947707177  - accuracy: 0.90625\n",
      "At: 200 [==========>] Loss 0.15828252318328484  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.13700634532638317  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.13151833105339822  - accuracy: 0.84375\n",
      "At: 203 [==========>] Loss 0.13358014445104566  - accuracy: 0.78125\n",
      "At: 204 [==========>] Loss 0.1001431587821873  - accuracy: 0.875\n",
      "At: 205 [==========>] Loss 0.12638105482660988  - accuracy: 0.875\n",
      "At: 206 [==========>] Loss 0.09336048388341113  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.10764348454147726  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.16015326265305876  - accuracy: 0.78125\n",
      "At: 209 [==========>] Loss 0.16960092185618597  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.09022899519324562  - accuracy: 0.90625\n",
      "At: 211 [==========>] Loss 0.13268130634322675  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.16557744381983508  - accuracy: 0.78125\n",
      "At: 213 [==========>] Loss 0.1687446669500617  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.18417567667131218  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.11588151334424926  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.1789226933128999  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.1911618420222284  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.15582943315161393  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.16321574450745674  - accuracy: 0.75\n",
      "At: 220 [==========>] Loss 0.15338555718778718  - accuracy: 0.8125\n",
      "At: 221 [==========>] Loss 0.12920772461827554  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.08786652786905341  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.22803355181411117  - accuracy: 0.6875\n",
      "At: 224 [==========>] Loss 0.15361484924450777  - accuracy: 0.75\n",
      "At: 225 [==========>] Loss 0.13038497606562233  - accuracy: 0.8125\n",
      "At: 226 [==========>] Loss 0.15960242702484254  - accuracy: 0.71875\n",
      "At: 227 [==========>] Loss 0.12731377582818446  - accuracy: 0.78125\n",
      "At: 228 [==========>] Loss 0.13994499482789619  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.18232991639280013  - accuracy: 0.75\n",
      "At: 230 [==========>] Loss 0.13072832722611813  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.214644202188933  - accuracy: 0.625\n",
      "At: 232 [==========>] Loss 0.2194100186944718  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.1900828075108999  - accuracy: 0.625\n",
      "At: 234 [==========>] Loss 0.14881857188907113  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.1665931985824881  - accuracy: 0.78125\n",
      "At: 236 [==========>] Loss 0.1541414743163437  - accuracy: 0.78125\n",
      "At: 237 [==========>] Loss 0.10430480536563286  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.11802538646843248  - accuracy: 0.875\n",
      "At: 239 [==========>] Loss 0.11000324848000569  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.14852159768686812  - accuracy: 0.71875\n",
      "At: 241 [==========>] Loss 0.13479594349044696  - accuracy: 0.8125\n",
      "At: 242 [==========>] Loss 0.1303812759043222  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.135391188115357  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.15083411080467654  - accuracy: 0.71875\n",
      "At: 245 [==========>] Loss 0.109382700852863  - accuracy: 0.875\n",
      "At: 246 [==========>] Loss 0.13349293636581977  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.1300498222204073  - accuracy: 0.8125\n",
      "At: 248 [==========>] Loss 0.09869594894039957  - accuracy: 0.90625\n",
      "At: 249 [==========>] Loss 0.10896912399423528  - accuracy: 0.8125\n",
      "At: 250 [==========>] Loss 0.19228880092104364  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.1618575550625579  - accuracy: 0.65625\n",
      "At: 252 [==========>] Loss 0.08652452514046807  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.15726988706820427  - accuracy: 0.75\n",
      "At: 254 [==========>] Loss 0.09335202856385191  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.15391073694721752  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.1595082496155286  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.1014588307665943  - accuracy: 0.875\n",
      "At: 258 [==========>] Loss 0.11964730471839524  - accuracy: 0.84375\n",
      "At: 259 [==========>] Loss 0.15215432901000187  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.1347023243770737  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.07953173247657105  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.14004131305487927  - accuracy: 0.84375\n",
      "At: 263 [==========>] Loss 0.10875680348574845  - accuracy: 0.84375\n",
      "At: 264 [==========>] Loss 0.09064574786504584  - accuracy: 0.90625\n",
      "At: 265 [==========>] Loss 0.15968531499356858  - accuracy: 0.8125\n",
      "At: 266 [==========>] Loss 0.18879363262542664  - accuracy: 0.75\n",
      "At: 267 [==========>] Loss 0.12423393796726553  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.16060281838240917  - accuracy: 0.78125\n",
      "At: 269 [==========>] Loss 0.09529898209580309  - accuracy: 0.875\n",
      "At: 270 [==========>] Loss 0.1952618869738404  - accuracy: 0.65625\n",
      "At: 271 [==========>] Loss 0.1196406379089827  - accuracy: 0.8125\n",
      "At: 272 [==========>] Loss 0.10299185284824774  - accuracy: 0.84375\n",
      "At: 273 [==========>] Loss 0.13133593844026778  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.1871446068601486  - accuracy: 0.75\n",
      "At: 275 [==========>] Loss 0.07058861736101174  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.1682992437802251  - accuracy: 0.65625\n",
      "At: 277 [==========>] Loss 0.11871467915620443  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.10961470881244037  - accuracy: 0.875\n",
      "At: 279 [==========>] Loss 0.13648421750552428  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.13375759611334728  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.1308247037554616  - accuracy: 0.84375\n",
      "At: 282 [==========>] Loss 0.1693169137868303  - accuracy: 0.75\n",
      "At: 283 [==========>] Loss 0.12003476520658712  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.0895704097415965  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.1334487761797395  - accuracy: 0.78125\n",
      "At: 286 [==========>] Loss 0.08370385726956216  - accuracy: 0.875\n",
      "At: 287 [==========>] Loss 0.15370975309248364  - accuracy: 0.8125\n",
      "At: 288 [==========>] Loss 0.10923485342764387  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.08215797215268819  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.12032379435017787  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.11857847007790688  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.12207619026846034  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.17735795412996627  - accuracy: 0.75\n",
      "At: 294 [==========>] Loss 0.16175161531502386  - accuracy: 0.71875\n",
      "At: 295 [==========>] Loss 0.12755507949066733  - accuracy: 0.8125\n",
      "At: 296 [==========>] Loss 0.10241380288142149  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.11689880774816191  - accuracy: 0.875\n",
      "At: 298 [==========>] Loss 0.15067633193329533  - accuracy: 0.8125\n",
      "At: 299 [==========>] Loss 0.14778727588614338  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.15505339226914436  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.15652354981244265  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.1268885539630623  - accuracy: 0.84375\n",
      "At: 303 [==========>] Loss 0.09863141008583208  - accuracy: 0.84375\n",
      "At: 304 [==========>] Loss 0.1378622201161791  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.2202338155480736  - accuracy: 0.71875\n",
      "At: 306 [==========>] Loss 0.12611080560608806  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.18943641182434756  - accuracy: 0.6875\n",
      "At: 308 [==========>] Loss 0.13074435467488366  - accuracy: 0.78125\n",
      "At: 309 [==========>] Loss 0.10790060965300485  - accuracy: 0.84375\n",
      "At: 310 [==========>] Loss 0.1647552276602291  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.06961394813216107  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.11761097715742423  - accuracy: 0.875\n",
      "At: 313 [==========>] Loss 0.09628702035315198  - accuracy: 0.875\n",
      "At: 314 [==========>] Loss 0.2013001765516318  - accuracy: 0.6875\n",
      "At: 315 [==========>] Loss 0.08218956930385739  - accuracy: 0.90625\n",
      "At: 316 [==========>] Loss 0.1481722308378532  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.2624909684116714  - accuracy: 0.5625\n",
      "At: 318 [==========>] Loss 0.1471823824739479  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.10767585369213947  - accuracy: 0.90625\n",
      "At: 320 [==========>] Loss 0.1491257550927554  - accuracy: 0.75\n",
      "At: 321 [==========>] Loss 0.18415431979381003  - accuracy: 0.71875\n",
      "At: 322 [==========>] Loss 0.10027103313531952  - accuracy: 0.84375\n",
      "At: 323 [==========>] Loss 0.11205064502840126  - accuracy: 0.84375\n",
      "At: 324 [==========>] Loss 0.16370357457768425  - accuracy: 0.71875\n",
      "At: 325 [==========>] Loss 0.11006202360194801  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.1607701088117044  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.10115582322515429  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.10224392265666729  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.1301778611258646  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.1452687138344279  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.19274918370505262  - accuracy: 0.71875\n",
      "At: 332 [==========>] Loss 0.20424181961569163  - accuracy: 0.65625\n",
      "At: 333 [==========>] Loss 0.1585943806433346  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.09003961014544853  - accuracy: 0.90625\n",
      "At: 335 [==========>] Loss 0.09936042501984588  - accuracy: 0.875\n",
      "At: 336 [==========>] Loss 0.12592268199833972  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.14997733156797505  - accuracy: 0.78125\n",
      "At: 338 [==========>] Loss 0.1088567090165217  - accuracy: 0.84375\n",
      "At: 339 [==========>] Loss 0.1386522843811337  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.13296233859294243  - accuracy: 0.8125\n",
      "At: 341 [==========>] Loss 0.09477768916317944  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.1302481008737973  - accuracy: 0.75\n",
      "At: 343 [==========>] Loss 0.23493115947175078  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.16407621471705341  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.14564089584759296  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.1290998375882173  - accuracy: 0.78125\n",
      "At: 347 [==========>] Loss 0.06683519807474622  - accuracy: 0.96875\n",
      "At: 348 [==========>] Loss 0.10997896539515278  - accuracy: 0.84375\n",
      "At: 349 [==========>] Loss 0.1385089271935252  - accuracy: 0.8125\n",
      "At: 350 [==========>] Loss 0.12778108233581983  - accuracy: 0.8125\n",
      "At: 351 [==========>] Loss 0.1732077014617675  - accuracy: 0.75\n",
      "At: 352 [==========>] Loss 0.10158065472693312  - accuracy: 0.90625\n",
      "At: 353 [==========>] Loss 0.12462382785111303  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.1483895029187703  - accuracy: 0.8125\n",
      "At: 355 [==========>] Loss 0.10652862460346826  - accuracy: 0.84375\n",
      "At: 356 [==========>] Loss 0.14537069359340954  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.12916028797386067  - accuracy: 0.78125\n",
      "At: 358 [==========>] Loss 0.1249443726559464  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.09368628812107002  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.13288418254067605  - accuracy: 0.84375\n",
      "At: 361 [==========>] Loss 0.082971427922299  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.10234548200875791  - accuracy: 0.875\n",
      "At: 363 [==========>] Loss 0.07217888083916889  - accuracy: 0.90625\n",
      "At: 364 [==========>] Loss 0.16901613220606265  - accuracy: 0.78125\n",
      "At: 365 [==========>] Loss 0.10941853602736624  - accuracy: 0.8125\n",
      "At: 366 [==========>] Loss 0.1501331880288928  - accuracy: 0.84375\n",
      "At: 367 [==========>] Loss 0.16990311241388883  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.18652782163366524  - accuracy: 0.71875\n",
      "At: 369 [==========>] Loss 0.1523395887399369  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.18340109714186795  - accuracy: 0.6875\n",
      "At: 371 [==========>] Loss 0.09413830444140554  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.0944317493977466  - accuracy: 0.90625\n",
      "At: 373 [==========>] Loss 0.16819656994756665  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.1065565112682825  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.14830482314073  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.11325875132218557  - accuracy: 0.84375\n",
      "At: 377 [==========>] Loss 0.19767352465079105  - accuracy: 0.75\n",
      "At: 378 [==========>] Loss 0.13795127818717118  - accuracy: 0.8125\n",
      "At: 379 [==========>] Loss 0.14423159658939522  - accuracy: 0.75\n",
      "At: 380 [==========>] Loss 0.16018019704353975  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.1720731895130625  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.08323079269698186  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.14792022318794518  - accuracy: 0.75\n",
      "At: 384 [==========>] Loss 0.1857013876894852  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.12051466010470366  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.18458931678784374  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.0680078696223722  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.19555723150449428  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.14361220590286194  - accuracy: 0.8125\n",
      "At: 390 [==========>] Loss 0.10629506913114463  - accuracy: 0.875\n",
      "At: 391 [==========>] Loss 0.106169292744098  - accuracy: 0.84375\n",
      "At: 392 [==========>] Loss 0.12537817327065093  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.21076741371248148  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.10325871526943028  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.1562359918117436  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.16021244867765816  - accuracy: 0.75\n",
      "At: 397 [==========>] Loss 0.13366034819405814  - accuracy: 0.78125\n",
      "At: 398 [==========>] Loss 0.19067464815835228  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.1406968442924788  - accuracy: 0.75\n",
      "At: 400 [==========>] Loss 0.16605523520229004  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.10425589384286824  - accuracy: 0.8125\n",
      "At: 402 [==========>] Loss 0.11717564406777699  - accuracy: 0.8125\n",
      "At: 403 [==========>] Loss 0.05687292171241068  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.10765486696789825  - accuracy: 0.9375\n",
      "At: 405 [==========>] Loss 0.19107680842525018  - accuracy: 0.6875\n",
      "At: 406 [==========>] Loss 0.12404627071676416  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.12110703643665471  - accuracy: 0.8125\n",
      "At: 408 [==========>] Loss 0.1823082810447926  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.20526400408439543  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.11059745305138668  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10435703007268832  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.13872414233100555  - accuracy: 0.78125\n",
      "At: 413 [==========>] Loss 0.12022268951179516  - accuracy: 0.8125\n",
      "At: 414 [==========>] Loss 0.16251508564258244  - accuracy: 0.6875\n",
      "At: 415 [==========>] Loss 0.13226966342117213  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.18580542644395365  - accuracy: 0.6875\n",
      "At: 417 [==========>] Loss 0.1324728602853923  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.12795766163033162  - accuracy: 0.8125\n",
      "At: 419 [==========>] Loss 0.10820581974055224  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.12746475603025098  - accuracy: 0.78125\n",
      "At: 421 [==========>] Loss 0.12652470118583747  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.11762484614353683  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.10957593117644601  - accuracy: 0.875\n",
      "At: 424 [==========>] Loss 0.14456299549177531  - accuracy: 0.8125\n",
      "At: 425 [==========>] Loss 0.19794135087148598  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.122071676154244  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.18934010620671626  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.27364742875397735  - accuracy: 0.5\n",
      "At: 429 [==========>] Loss 0.18334119326477538  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.1232018051459223  - accuracy: 0.78125\n",
      "At: 431 [==========>] Loss 0.09639911139778651  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.1124810165716819  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.09655263167954106  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.12248519465148017  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.15880771236037905  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.1403250464248879  - accuracy: 0.875\n",
      "At: 437 [==========>] Loss 0.13561036366404775  - accuracy: 0.84375\n",
      "At: 438 [==========>] Loss 0.11064495247159123  - accuracy: 0.90625\n",
      "At: 439 [==========>] Loss 0.09003110424744366  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.09040022963288415  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.17105523212155171  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.13435254205309655  - accuracy: 0.78125\n",
      "At: 443 [==========>] Loss 0.13021327676139577  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.1288205023417417  - accuracy: 0.84375\n",
      "At: 445 [==========>] Loss 0.11412373689028847  - accuracy: 0.84375\n",
      "At: 446 [==========>] Loss 0.19099251493815464  - accuracy: 0.71875\n",
      "At: 447 [==========>] Loss 0.15098566906955863  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.15483884991677782  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.10048687151972698  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.12775274256009606  - accuracy: 0.78125\n",
      "At: 451 [==========>] Loss 0.12977806077249116  - accuracy: 0.8125\n",
      "At: 452 [==========>] Loss 0.11600588575517437  - accuracy: 0.90625\n",
      "At: 453 [==========>] Loss 0.14325850180509964  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.18671864571417127  - accuracy: 0.71875\n",
      "At: 455 [==========>] Loss 0.19609255292192823  - accuracy: 0.71875\n",
      "At: 456 [==========>] Loss 0.1390636573281257  - accuracy: 0.90625\n",
      "At: 457 [==========>] Loss 0.1313378051029468  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.09114909082736719  - accuracy: 0.8125\n",
      "At: 459 [==========>] Loss 0.18203132319040233  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.11894769172718975  - accuracy: 0.84375\n",
      "At: 461 [==========>] Loss 0.19394183230742873  - accuracy: 0.78125\n",
      "At: 462 [==========>] Loss 0.16917561871366793  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.15702204464944922  - accuracy: 0.75\n",
      "At: 464 [==========>] Loss 0.1681636849225989  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.1886261254219893  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.09044294676290207  - accuracy: 0.90625\n",
      "At: 467 [==========>] Loss 0.16202329746412247  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.11359812398499576  - accuracy: 0.84375\n",
      "At: 469 [==========>] Loss 0.12275625439974805  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.11038052841719281  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.17558522751492142  - accuracy: 0.71875\n",
      "At: 472 [==========>] Loss 0.11912600406968601  - accuracy: 0.84375\n",
      "At: 473 [==========>] Loss 0.1529544220395751  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.15512587535226513  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.13501899264096934  - accuracy: 0.84375\n",
      "At: 476 [==========>] Loss 0.1812907048919188  - accuracy: 0.75\n",
      "At: 477 [==========>] Loss 0.11321365951539027  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.1063171475557043  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.14481013887025793  - accuracy: 0.84375\n",
      "At: 480 [==========>] Loss 0.15702189827294846  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.0924546834869241  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.07358289360968327  - accuracy: 0.90625\n",
      "At: 483 [==========>] Loss 0.11357632497698344  - accuracy: 0.8125\n",
      "At: 484 [==========>] Loss 0.09370712848208326  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.10731867974339851  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.175515143543996  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.15225142668465774  - accuracy: 0.78125\n",
      "At: 488 [==========>] Loss 0.13184124253016843  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.12443992036420407  - accuracy: 0.84375\n",
      "At: 490 [==========>] Loss 0.12032011263867376  - accuracy: 0.84375\n",
      "At: 491 [==========>] Loss 0.1310387476206893  - accuracy: 0.84375\n",
      "At: 492 [==========>] Loss 0.20459614034126405  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.09138140799161606  - accuracy: 0.90625\n",
      "At: 494 [==========>] Loss 0.14257438790907098  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.12092893071252123  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.1584914408856231  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.156893691785953  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.0770675848195655  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.1723700368504272  - accuracy: 0.6875\n",
      "At: 500 [==========>] Loss 0.10365235131025757  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.13124088373413784  - accuracy: 0.875\n",
      "At: 502 [==========>] Loss 0.14627828147240013  - accuracy: 0.75\n",
      "At: 503 [==========>] Loss 0.08203824931949319  - accuracy: 0.90625\n",
      "At: 504 [==========>] Loss 0.0827943805894499  - accuracy: 0.9375\n",
      "At: 505 [==========>] Loss 0.18084615163725146  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.23940239405678537  - accuracy: 0.6875\n",
      "At: 507 [==========>] Loss 0.13025087459013382  - accuracy: 0.8125\n",
      "At: 508 [==========>] Loss 0.07664205993235852  - accuracy: 0.96875\n",
      "At: 509 [==========>] Loss 0.15416102835559325  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.19172645984255204  - accuracy: 0.75\n",
      "At: 511 [==========>] Loss 0.13379112668795226  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.21218224711206768  - accuracy: 0.71875\n",
      "At: 513 [==========>] Loss 0.19792185652302113  - accuracy: 0.75\n",
      "At: 514 [==========>] Loss 0.128998226840619  - accuracy: 0.84375\n",
      "At: 515 [==========>] Loss 0.11529445851417536  - accuracy: 0.875\n",
      "At: 516 [==========>] Loss 0.17443485722479712  - accuracy: 0.71875\n",
      "At: 517 [==========>] Loss 0.11558739172157993  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.15990906580872455  - accuracy: 0.75\n",
      "At: 519 [==========>] Loss 0.1176759816922309  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.12036969240621392  - accuracy: 0.875\n",
      "At: 521 [==========>] Loss 0.10383371863537927  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.14147975227069431  - accuracy: 0.78125\n",
      "At: 523 [==========>] Loss 0.16004113102561177  - accuracy: 0.78125\n",
      "At: 524 [==========>] Loss 0.09207346341497288  - accuracy: 0.90625\n",
      "At: 525 [==========>] Loss 0.14028745331495068  - accuracy: 0.8125\n",
      "At: 526 [==========>] Loss 0.18464830837986604  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.21546677678879458  - accuracy: 0.6875\n",
      "At: 528 [==========>] Loss 0.15643374473083765  - accuracy: 0.84375\n",
      "At: 529 [==========>] Loss 0.10047929026866634  - accuracy: 0.875\n",
      "At: 530 [==========>] Loss 0.1785614637252746  - accuracy: 0.78125\n",
      "At: 531 [==========>] Loss 0.16343023875017695  - accuracy: 0.6875\n",
      "At: 532 [==========>] Loss 0.117174081809993  - accuracy: 0.8125\n",
      "At: 533 [==========>] Loss 0.08233431173785474  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.1531710931957589  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.14726952970816512  - accuracy: 0.75\n",
      "At: 536 [==========>] Loss 0.16035565436567362  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.05908608090586627  - accuracy: 0.96875\n",
      "At: 538 [==========>] Loss 0.11262116524875865  - accuracy: 0.8125\n",
      "At: 539 [==========>] Loss 0.09107697697581359  - accuracy: 0.875\n",
      "At: 540 [==========>] Loss 0.24663180065672874  - accuracy: 0.65625\n",
      "At: 541 [==========>] Loss 0.15888445096365897  - accuracy: 0.71875\n",
      "At: 542 [==========>] Loss 0.15463052856406662  - accuracy: 0.78125\n",
      "At: 543 [==========>] Loss 0.13863367203946125  - accuracy: 0.84375\n",
      "At: 544 [==========>] Loss 0.18777193448738183  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.09349307215425665  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.16799993454962672  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.1098241845590293  - accuracy: 0.875\n",
      "At: 548 [==========>] Loss 0.11322481230097078  - accuracy: 0.84375\n",
      "At: 549 [==========>] Loss 0.10571252680757065  - accuracy: 0.84375\n",
      "At: 550 [==========>] Loss 0.1061762142298473  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.1303154920935154  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.1262155404563691  - accuracy: 0.875\n",
      "At: 553 [==========>] Loss 0.1325331745184211  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.08866482654743682  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.1381336772522666  - accuracy: 0.84375\n",
      "At: 556 [==========>] Loss 0.15548764733643766  - accuracy: 0.8125\n",
      "At: 557 [==========>] Loss 0.11365916644240483  - accuracy: 0.875\n",
      "At: 558 [==========>] Loss 0.12856561455075266  - accuracy: 0.875\n",
      "At: 559 [==========>] Loss 0.1815792390875137  - accuracy: 0.75\n",
      "At: 560 [==========>] Loss 0.1253304688724687  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.13634450843974166  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.08512682156942808  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.10544919740870964  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.18547961253214845  - accuracy: 0.6875\n",
      "At: 565 [==========>] Loss 0.1158721312925387  - accuracy: 0.8125\n",
      "At: 566 [==========>] Loss 0.12346168235058853  - accuracy: 0.875\n",
      "At: 567 [==========>] Loss 0.1550735609934794  - accuracy: 0.78125\n",
      "At: 568 [==========>] Loss 0.19280623162533134  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.13616360060472166  - accuracy: 0.84375\n",
      "At: 570 [==========>] Loss 0.07557203404795827  - accuracy: 0.90625\n",
      "At: 571 [==========>] Loss 0.11715052145009439  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.12326187229968955  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.09619827751604312  - accuracy: 0.90625\n",
      "At: 574 [==========>] Loss 0.11287004479791947  - accuracy: 0.8125\n",
      "At: 575 [==========>] Loss 0.11594922019335496  - accuracy: 0.84375\n",
      "At: 576 [==========>] Loss 0.06482701717618848  - accuracy: 0.9375\n",
      "At: 577 [==========>] Loss 0.16601832828160404  - accuracy: 0.78125\n",
      "At: 578 [==========>] Loss 0.14628282463588343  - accuracy: 0.78125\n",
      "At: 579 [==========>] Loss 0.11208730349341561  - accuracy: 0.875\n",
      "At: 580 [==========>] Loss 0.13007277226728464  - accuracy: 0.78125\n",
      "At: 581 [==========>] Loss 0.14164728152203188  - accuracy: 0.75\n",
      "At: 582 [==========>] Loss 0.1390329397243073  - accuracy: 0.8125\n",
      "At: 583 [==========>] Loss 0.16814284919830258  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.11759664131173027  - accuracy: 0.875\n",
      "At: 585 [==========>] Loss 0.1452144353704543  - accuracy: 0.78125\n",
      "At: 586 [==========>] Loss 0.1052707357972244  - accuracy: 0.78125\n",
      "At: 587 [==========>] Loss 0.12352739023565194  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.15200865127539237  - accuracy: 0.75\n",
      "At: 589 [==========>] Loss 0.12805669105777012  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.054237330693674826  - accuracy: 0.9375\n",
      "At: 591 [==========>] Loss 0.13293047026892052  - accuracy: 0.84375\n",
      "At: 592 [==========>] Loss 0.06938271943399721  - accuracy: 0.9375\n",
      "At: 593 [==========>] Loss 0.1461986813021653  - accuracy: 0.8125\n",
      "At: 594 [==========>] Loss 0.1413647826496774  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.14767150514324529  - accuracy: 0.78125\n",
      "At: 596 [==========>] Loss 0.10934158703949926  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.19554421039310182  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.12830097523235676  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.11238956278827886  - accuracy: 0.90625\n",
      "At: 600 [==========>] Loss 0.09047044351491121  - accuracy: 0.90625\n",
      "At: 601 [==========>] Loss 0.12230208221243667  - accuracy: 0.75\n",
      "At: 602 [==========>] Loss 0.11371727709849444  - accuracy: 0.84375\n",
      "At: 603 [==========>] Loss 0.12543796919757216  - accuracy: 0.84375\n",
      "At: 604 [==========>] Loss 0.17061184122762035  - accuracy: 0.71875\n",
      "At: 605 [==========>] Loss 0.09955736825133454  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.15178384425302985  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.15267499603466642  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.14379744630092756  - accuracy: 0.78125\n",
      "At: 609 [==========>] Loss 0.11305196411115077  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.13760616188938607  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.09597898737347885  - accuracy: 0.8125\n",
      "At: 612 [==========>] Loss 0.12862132460236697  - accuracy: 0.875\n",
      "At: 613 [==========>] Loss 0.1435468417788791  - accuracy: 0.78125\n",
      "At: 614 [==========>] Loss 0.10451755531018206  - accuracy: 0.875\n",
      "At: 615 [==========>] Loss 0.15919243442257158  - accuracy: 0.8125\n",
      "At: 616 [==========>] Loss 0.15817460243054957  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.12123244958989313  - accuracy: 0.78125\n",
      "At: 618 [==========>] Loss 0.1941899241792982  - accuracy: 0.6875\n",
      "At: 619 [==========>] Loss 0.1204059190071062  - accuracy: 0.875\n",
      "At: 620 [==========>] Loss 0.16575967659978003  - accuracy: 0.78125\n",
      "At: 621 [==========>] Loss 0.06994726033227545  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.15994392327589763  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.13561435418817072  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.09897864556932268  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.111419099626574  - accuracy: 0.875\n",
      "At: 626 [==========>] Loss 0.13804571422072093  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.12328302278244892  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.11748517006651786  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.17250682567522851  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.20297819955478996  - accuracy: 0.625\n",
      "At: 631 [==========>] Loss 0.1771287782433561  - accuracy: 0.6875\n",
      "At: 632 [==========>] Loss 0.12949179810407763  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.15648666927189656  - accuracy: 0.78125\n",
      "At: 634 [==========>] Loss 0.1592586752630899  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.10411233627938267  - accuracy: 0.90625\n",
      "At: 636 [==========>] Loss 0.15509588265844543  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.11808595762466159  - accuracy: 0.875\n",
      "At: 638 [==========>] Loss 0.10971077649053398  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.1445644122356492  - accuracy: 0.78125\n",
      "At: 640 [==========>] Loss 0.19370921306641273  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.11607843234011393  - accuracy: 0.75\n",
      "At: 642 [==========>] Loss 0.15884293068832384  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.10382397185217637  - accuracy: 0.84375\n",
      "At: 644 [==========>] Loss 0.05472193154048017  - accuracy: 1.0\n",
      "At: 645 [==========>] Loss 0.12219038770257515  - accuracy: 0.875\n",
      "At: 646 [==========>] Loss 0.13546791908279174  - accuracy: 0.78125\n",
      "At: 647 [==========>] Loss 0.167735625030766  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.1421663026956807  - accuracy: 0.71875\n",
      "At: 649 [==========>] Loss 0.16302501617188853  - accuracy: 0.75\n",
      "At: 650 [==========>] Loss 0.08486353237394634  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.14859565843671357  - accuracy: 0.78125\n",
      "At: 652 [==========>] Loss 0.0847369014045488  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.11337000781973267  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.08974728581242916  - accuracy: 0.90625\n",
      "At: 655 [==========>] Loss 0.12537746102626235  - accuracy: 0.84375\n",
      "At: 656 [==========>] Loss 0.12623816901721097  - accuracy: 0.8125\n",
      "At: 657 [==========>] Loss 0.14432846245225928  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.13132765709721966  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.1564632978164698  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.12986497543310244  - accuracy: 0.75\n",
      "At: 661 [==========>] Loss 0.14275877986754093  - accuracy: 0.84375\n",
      "At: 662 [==========>] Loss 0.12623604117716938  - accuracy: 0.78125\n",
      "At: 663 [==========>] Loss 0.09720316376325236  - accuracy: 0.9375\n",
      "At: 664 [==========>] Loss 0.1299600912738107  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.16068846995832123  - accuracy: 0.78125\n",
      "At: 666 [==========>] Loss 0.16990057661029412  - accuracy: 0.8125\n",
      "At: 667 [==========>] Loss 0.1098565144218297  - accuracy: 0.875\n",
      "At: 668 [==========>] Loss 0.1453676904083106  - accuracy: 0.78125\n",
      "At: 669 [==========>] Loss 0.15094084919957793  - accuracy: 0.75\n",
      "At: 670 [==========>] Loss 0.19840183319843335  - accuracy: 0.65625\n",
      "At: 671 [==========>] Loss 0.10071787811853528  - accuracy: 0.90625\n",
      "At: 672 [==========>] Loss 0.1412080637185883  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.07170768995385021  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.12521552050776974  - accuracy: 0.78125\n",
      "At: 675 [==========>] Loss 0.07588759730855804  - accuracy: 0.90625\n",
      "At: 676 [==========>] Loss 0.12523998199997755  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.15785707411855437  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.1275338606608334  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.10831371179311522  - accuracy: 0.875\n",
      "At: 680 [==========>] Loss 0.12633261853599767  - accuracy: 0.75\n",
      "At: 681 [==========>] Loss 0.13736390712455654  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.1423329535416536  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.15845851372130032  - accuracy: 0.75\n",
      "At: 684 [==========>] Loss 0.09738480322928145  - accuracy: 0.875\n",
      "At: 685 [==========>] Loss 0.12269599994986362  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.14217418978178561  - accuracy: 0.75\n",
      "At: 687 [==========>] Loss 0.06078891539487089  - accuracy: 0.9375\n",
      "At: 688 [==========>] Loss 0.07703335368566977  - accuracy: 0.9375\n",
      "At: 689 [==========>] Loss 0.14578303707677467  - accuracy: 0.84375\n",
      "At: 690 [==========>] Loss 0.11489214709715069  - accuracy: 0.875\n",
      "At: 691 [==========>] Loss 0.09852679945729453  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.11353019650624274  - accuracy: 0.84375\n",
      "At: 693 [==========>] Loss 0.14635859118121614  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.19428366541072267  - accuracy: 0.71875\n",
      "At: 695 [==========>] Loss 0.1680733194817624  - accuracy: 0.71875\n",
      "At: 696 [==========>] Loss 0.16272889781005362  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.18809958353178197  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.11053163821816253  - accuracy: 0.8125\n",
      "At: 699 [==========>] Loss 0.11078893716996499  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.08754685236835637  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.1341894483060176  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.09973149774638845  - accuracy: 0.84375\n",
      "At: 703 [==========>] Loss 0.16658037329841935  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.15335170597715625  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.19613777377179964  - accuracy: 0.71875\n",
      "At: 706 [==========>] Loss 0.13983280597691644  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.08014138687364314  - accuracy: 0.875\n",
      "At: 708 [==========>] Loss 0.14519377765199487  - accuracy: 0.8125\n",
      "At: 709 [==========>] Loss 0.16513507776934594  - accuracy: 0.75\n",
      "At: 710 [==========>] Loss 0.13748261252361849  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.20202671550984524  - accuracy: 0.75\n",
      "At: 712 [==========>] Loss 0.14089885531056662  - accuracy: 0.8125\n",
      "At: 713 [==========>] Loss 0.16262035931237664  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.19798561002338533  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.11404268255287571  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.11008621645859683  - accuracy: 0.8125\n",
      "At: 717 [==========>] Loss 0.0735634083189177  - accuracy: 0.9375\n",
      "At: 718 [==========>] Loss 0.2089924699954645  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.11214719610033089  - accuracy: 0.875\n",
      "At: 720 [==========>] Loss 0.12254019590471113  - accuracy: 0.875\n",
      "At: 721 [==========>] Loss 0.11112716040498352  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.14393094048962923  - accuracy: 0.8125\n",
      "At: 723 [==========>] Loss 0.1382860860546747  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.12314255913532125  - accuracy: 0.8125\n",
      "At: 725 [==========>] Loss 0.14136627567145998  - accuracy: 0.875\n",
      "At: 726 [==========>] Loss 0.1916236496280335  - accuracy: 0.71875\n",
      "At: 727 [==========>] Loss 0.14096624202698443  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.16778128192100877  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.16603865711278804  - accuracy: 0.71875\n",
      "At: 730 [==========>] Loss 0.17059289718061443  - accuracy: 0.71875\n",
      "At: 731 [==========>] Loss 0.17537837039125642  - accuracy: 0.71875\n",
      "At: 732 [==========>] Loss 0.1516817344364471  - accuracy: 0.71875\n",
      "At: 733 [==========>] Loss 0.1077804706937378  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.16065800229820415  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.11958047081048379  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.08805292271076483  - accuracy: 0.9375\n",
      "At: 737 [==========>] Loss 0.13941408985693926  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.09966349492055177  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.06294473130611627  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.10846511357276455  - accuracy: 0.84375\n",
      "At: 741 [==========>] Loss 0.10176123124713435  - accuracy: 0.90625\n",
      "At: 742 [==========>] Loss 0.1276964310365429  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.1267360611723719  - accuracy: 0.875\n",
      "At: 744 [==========>] Loss 0.14483655555763997  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.15952008842940527  - accuracy: 0.6875\n",
      "At: 746 [==========>] Loss 0.14150440382302543  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.1290516219623516  - accuracy: 0.875\n",
      "At: 748 [==========>] Loss 0.1379788332750257  - accuracy: 0.8125\n",
      "At: 749 [==========>] Loss 0.12797723878703376  - accuracy: 0.75\n",
      "At: 750 [==========>] Loss 0.11308771986037976  - accuracy: 0.875\n",
      "At: 751 [==========>] Loss 0.151917221738402  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.06985537544724105  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.1411097937282152  - accuracy: 0.78125\n",
      "At: 754 [==========>] Loss 0.16003517499637857  - accuracy: 0.78125\n",
      "At: 755 [==========>] Loss 0.09600707603621354  - accuracy: 0.84375\n",
      "At: 756 [==========>] Loss 0.2068289615347123  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.08284392493077732  - accuracy: 0.90625\n",
      "At: 758 [==========>] Loss 0.11715765180737334  - accuracy: 0.875\n",
      "At: 759 [==========>] Loss 0.0797451144268532  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.18868567884804688  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.1454942865648597  - accuracy: 0.78125\n",
      "At: 762 [==========>] Loss 0.11864870566432967  - accuracy: 0.875\n",
      "At: 763 [==========>] Loss 0.14628558763309407  - accuracy: 0.78125\n",
      "At: 764 [==========>] Loss 0.12499043413075846  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.14385662730246834  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.10317009328282045  - accuracy: 0.875\n",
      "At: 767 [==========>] Loss 0.12091353528226104  - accuracy: 0.875\n",
      "At: 768 [==========>] Loss 0.1774682319298955  - accuracy: 0.6875\n",
      "At: 769 [==========>] Loss 0.14445232434794258  - accuracy: 0.75\n",
      "At: 770 [==========>] Loss 0.11776288973470267  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.18489074710244408  - accuracy: 0.6875\n",
      "At: 772 [==========>] Loss 0.11343919145423073  - accuracy: 0.90625\n",
      "At: 773 [==========>] Loss 0.07967753608450713  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.12630238808175  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.18371206204123922  - accuracy: 0.6875\n",
      "At: 776 [==========>] Loss 0.1568201494279021  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.07306850540648571  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.17360857235245944  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.1278026452613334  - accuracy: 0.75\n",
      "At: 780 [==========>] Loss 0.06760385104766443  - accuracy: 0.9375\n",
      "At: 781 [==========>] Loss 0.15874335987690597  - accuracy: 0.8125\n",
      "At: 782 [==========>] Loss 0.14366915078791886  - accuracy: 0.8125\n",
      "At: 783 [==========>] Loss 0.20498377560644632  - accuracy: 0.6875\n",
      "At: 784 [==========>] Loss 0.17477659367531834  - accuracy: 0.6875\n",
      "At: 785 [==========>] Loss 0.16845636817071885  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.13681274782216996  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.15665548528149512  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.09774656626570159  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.14317367902199513  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.10418421704755584  - accuracy: 0.875\n",
      "At: 791 [==========>] Loss 0.17186322445664298  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.18328586052636803  - accuracy: 0.75\n",
      "At: 793 [==========>] Loss 0.12345767084544786  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.13807235484589842  - accuracy: 0.75\n",
      "At: 795 [==========>] Loss 0.09408175434829957  - accuracy: 0.875\n",
      "At: 796 [==========>] Loss 0.13624975596919758  - accuracy: 0.84375\n",
      "At: 797 [==========>] Loss 0.1464445377597139  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.14945698396178259  - accuracy: 0.78125\n",
      "At: 799 [==========>] Loss 0.05825277302375927  - accuracy: 0.96875\n",
      "At: 800 [==========>] Loss 0.15491854980606018  - accuracy: 0.75\n",
      "At: 801 [==========>] Loss 0.11243884448854424  - accuracy: 0.90625\n",
      "At: 802 [==========>] Loss 0.1871049741287939  - accuracy: 0.6875\n",
      "At: 803 [==========>] Loss 0.14993988752103893  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.14974693544091644  - accuracy: 0.84375\n",
      "At: 805 [==========>] Loss 0.14120209955163632  - accuracy: 0.78125\n",
      "At: 806 [==========>] Loss 0.1084416908963976  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.09824275383203065  - accuracy: 0.90625\n",
      "At: 808 [==========>] Loss 0.1307507195928187  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.09791281292561282  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.16974378634885912  - accuracy: 0.75\n",
      "At: 811 [==========>] Loss 0.13140099386340268  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.11607763514433778  - accuracy: 0.875\n",
      "At: 813 [==========>] Loss 0.19327531019614494  - accuracy: 0.6875\n",
      "At: 814 [==========>] Loss 0.1866104904564193  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.13029068101530772  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.16105343669673788  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.08031908470893726  - accuracy: 0.84375\n",
      "At: 818 [==========>] Loss 0.10575607269282768  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.11873075460701965  - accuracy: 0.875\n",
      "At: 820 [==========>] Loss 0.14190382491856235  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.15334330141861088  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.15623541917612077  - accuracy: 0.8125\n",
      "At: 823 [==========>] Loss 0.15910480000499289  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.1337027693975388  - accuracy: 0.8125\n",
      "At: 825 [==========>] Loss 0.17681578287711286  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.11269961721127383  - accuracy: 0.875\n",
      "At: 827 [==========>] Loss 0.09604106609672504  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.06182108996768575  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09576134592047511  - accuracy: 0.84375\n",
      "At: 830 [==========>] Loss 0.08984383383763658  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.09926916814249967  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.10579040749777383  - accuracy: 0.84375\n",
      "At: 833 [==========>] Loss 0.16121945311203198  - accuracy: 0.78125\n",
      "At: 834 [==========>] Loss 0.12866252829523755  - accuracy: 0.78125\n",
      "At: 835 [==========>] Loss 0.08939999645185909  - accuracy: 0.9375\n",
      "At: 836 [==========>] Loss 0.11469827767005905  - accuracy: 0.875\n",
      "At: 837 [==========>] Loss 0.12194035944314059  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.0977261066356151  - accuracy: 0.875\n",
      "At: 839 [==========>] Loss 0.10269368731759923  - accuracy: 0.875\n",
      "At: 840 [==========>] Loss 0.12589550735096422  - accuracy: 0.84375\n",
      "At: 841 [==========>] Loss 0.07271094009724186  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.06659211739828286  - accuracy: 0.96875\n",
      "At: 843 [==========>] Loss 0.1474727155289672  - accuracy: 0.8125\n",
      "At: 844 [==========>] Loss 0.10450199510570762  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.12827241749397394  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.14316941584373127  - accuracy: 0.75\n",
      "At: 847 [==========>] Loss 0.06215046539018794  - accuracy: 0.9375\n",
      "At: 848 [==========>] Loss 0.1307054276483674  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.14679232128206104  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.08788317233041507  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.10332347291671395  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.11723200890496585  - accuracy: 0.875\n",
      "At: 853 [==========>] Loss 0.16680690078703367  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.20548786669033262  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.09353720324148417  - accuracy: 0.84375\n",
      "At: 856 [==========>] Loss 0.09325091884940677  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.09656884689397081  - accuracy: 0.96875\n",
      "At: 858 [==========>] Loss 0.24339605604993927  - accuracy: 0.625\n",
      "At: 859 [==========>] Loss 0.1232174897697826  - accuracy: 0.875\n",
      "At: 860 [==========>] Loss 0.1179117232963856  - accuracy: 0.84375\n",
      "At: 861 [==========>] Loss 0.09887690074849553  - accuracy: 0.84375\n",
      "At: 862 [==========>] Loss 0.0865028601529078  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.12364735489312124  - accuracy: 0.84375\n",
      "At: 864 [==========>] Loss 0.17069505187262224  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.21323714994198287  - accuracy: 0.6875\n",
      "At: 866 [==========>] Loss 0.15787865439529347  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.09508420941424404  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.1722742376112828  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.17617691140795821  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.14793411481812824  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.07089582455820531  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.08841776789792351  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.17958014695420677  - accuracy: 0.71875\n",
      "At: 874 [==========>] Loss 0.16741933378594517  - accuracy: 0.6875\n",
      "At: 875 [==========>] Loss 0.11634681596351534  - accuracy: 0.90625\n",
      "At: 876 [==========>] Loss 0.14759518871119473  - accuracy: 0.75\n",
      "At: 877 [==========>] Loss 0.16087441116962284  - accuracy: 0.84375\n",
      "At: 878 [==========>] Loss 0.03508569142328063  - accuracy: 1.0\n",
      "At: 879 [==========>] Loss 0.128119535670629  - accuracy: 0.84375\n",
      "At: 880 [==========>] Loss 0.11243996280862208  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.16046857140052573  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.10296627922781632  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.15717668749409575  - accuracy: 0.8125\n",
      "At: 884 [==========>] Loss 0.11273522995465038  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.11001500868884674  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.08666023760825013  - accuracy: 0.90625\n",
      "At: 887 [==========>] Loss 0.13672096376689868  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.1609455649267458  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.10809970851340943  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.13245799524390878  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.12047129587766092  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.12565038062019468  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.13439561794020616  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.16021624145156327  - accuracy: 0.78125\n",
      "At: 895 [==========>] Loss 0.0870032685558152  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.12932939363793783  - accuracy: 0.8125\n",
      "At: 897 [==========>] Loss 0.1481585539317203  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.1534968536561161  - accuracy: 0.75\n",
      "At: 899 [==========>] Loss 0.09043870578987825  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.15754043379552315  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.1742215074415441  - accuracy: 0.71875\n",
      "At: 902 [==========>] Loss 0.11432331658084019  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.13252072189326258  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.10004999602886643  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.0737352896531388  - accuracy: 0.875\n",
      "At: 906 [==========>] Loss 0.09580954021190555  - accuracy: 0.84375\n",
      "At: 907 [==========>] Loss 0.15403613629632687  - accuracy: 0.75\n",
      "At: 908 [==========>] Loss 0.11898761732975949  - accuracy: 0.8125\n",
      "At: 909 [==========>] Loss 0.09422209848175103  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.13245917741480284  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.1592826828893063  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.14825114743172985  - accuracy: 0.71875\n",
      "At: 913 [==========>] Loss 0.1000613918951136  - accuracy: 0.90625\n",
      "At: 914 [==========>] Loss 0.1326871927695311  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.16739393652101892  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.15450701871763187  - accuracy: 0.78125\n",
      "At: 917 [==========>] Loss 0.1448665111563241  - accuracy: 0.78125\n",
      "At: 918 [==========>] Loss 0.17332859517843388  - accuracy: 0.75\n",
      "At: 919 [==========>] Loss 0.12477479051631446  - accuracy: 0.78125\n",
      "At: 920 [==========>] Loss 0.10703110734885418  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.14928300133904265  - accuracy: 0.75\n",
      "At: 922 [==========>] Loss 0.1554521018138045  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.0938129250525376  - accuracy: 0.90625\n",
      "At: 924 [==========>] Loss 0.16429887015347694  - accuracy: 0.78125\n",
      "At: 925 [==========>] Loss 0.15887777184063184  - accuracy: 0.78125\n",
      "At: 926 [==========>] Loss 0.14209332933426022  - accuracy: 0.8125\n",
      "At: 927 [==========>] Loss 0.09623879178198577  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.13473748283351278  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.14688588272603803  - accuracy: 0.71875\n",
      "At: 930 [==========>] Loss 0.1172906027668273  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.17196725554125802  - accuracy: 0.71875\n",
      "At: 932 [==========>] Loss 0.08522261070082071  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.08618034094327019  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.14294268068095528  - accuracy: 0.8125\n",
      "At: 935 [==========>] Loss 0.05872498171630742  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.15128753490438818  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.18754466719317112  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.10835483406725052  - accuracy: 0.875\n",
      "At: 939 [==========>] Loss 0.08305461794525278  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.2143250117373745  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.11251912163565697  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.12750577478177202  - accuracy: 0.8125\n",
      "At: 943 [==========>] Loss 0.11862404737843768  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.0797271158374305  - accuracy: 0.9375\n",
      "At: 945 [==========>] Loss 0.09418718106981114  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.12143996681633266  - accuracy: 0.875\n",
      "At: 947 [==========>] Loss 0.1313856853636069  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.1613157463172138  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.07244061937936425  - accuracy: 0.875\n",
      "At: 950 [==========>] Loss 0.11597934515880534  - accuracy: 0.84375\n",
      "At: 951 [==========>] Loss 0.09887571413807278  - accuracy: 0.875\n",
      "At: 952 [==========>] Loss 0.08035263714080638  - accuracy: 0.9375\n",
      "At: 953 [==========>] Loss 0.08005370396327621  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.1091769596080453  - accuracy: 0.78125\n",
      "At: 955 [==========>] Loss 0.11811982148336982  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.06753830555105526  - accuracy: 0.875\n",
      "At: 957 [==========>] Loss 0.13175547991004627  - accuracy: 0.875\n",
      "At: 958 [==========>] Loss 0.08828686252681145  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.12754668083774673  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.11036580424558054  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.1123462286603078  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.10508718516072149  - accuracy: 0.84375\n",
      "At: 963 [==========>] Loss 0.09295719827769144  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.1643860257029612  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.14338588019426968  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.1504708294045322  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.10288641484792645  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.14138442219300973  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.14429046359680664  - accuracy: 0.78125\n",
      "At: 970 [==========>] Loss 0.11754277878425606  - accuracy: 0.75\n",
      "At: 971 [==========>] Loss 0.09950523489164459  - accuracy: 0.875\n",
      "At: 972 [==========>] Loss 0.06977058806971394  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.11110984740707372  - accuracy: 0.90625\n",
      "At: 974 [==========>] Loss 0.06736546544882009  - accuracy: 0.9375\n",
      "At: 975 [==========>] Loss 0.12641078545891687  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.12731035007644173  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.10269060208255702  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.16213828092028723  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.1022548581922379  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.14709659607236614  - accuracy: 0.75\n",
      "At: 981 [==========>] Loss 0.155775513069114  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.07987082266185272  - accuracy: 0.875\n",
      "At: 983 [==========>] Loss 0.09642726777466562  - accuracy: 0.84375\n",
      "At: 984 [==========>] Loss 0.11210040644819659  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.17381242686260637  - accuracy: 0.75\n",
      "At: 986 [==========>] Loss 0.11650285529088042  - accuracy: 0.84375\n",
      "At: 987 [==========>] Loss 0.12782035732651414  - accuracy: 0.78125\n",
      "At: 988 [==========>] Loss 0.10268326339017328  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.12831605977432686  - accuracy: 0.78125\n",
      "At: 990 [==========>] Loss 0.13564500776224084  - accuracy: 0.75\n",
      "At: 991 [==========>] Loss 0.13183511892420785  - accuracy: 0.84375\n",
      "At: 992 [==========>] Loss 0.20869267763516317  - accuracy: 0.6875\n",
      "At: 993 [==========>] Loss 0.1514651511291432  - accuracy: 0.78125\n",
      "At: 994 [==========>] Loss 0.14727326601279606  - accuracy: 0.75\n",
      "At: 995 [==========>] Loss 0.17021786299947525  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.061852693851379016  - accuracy: 0.90625\n",
      "At: 997 [==========>] Loss 0.1321770074961115  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.123515239323858  - accuracy: 0.875\n",
      "At: 999 [==========>] Loss 0.13693520483598176  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.21725849210466058  - accuracy: 0.625\n",
      "At: 1001 [==========>] Loss 0.15025919040703775  - accuracy: 0.71875\n",
      "At: 1002 [==========>] Loss 0.2056276078313512  - accuracy: 0.65625\n",
      "At: 1003 [==========>] Loss 0.1423227945404222  - accuracy: 0.75\n",
      "At: 1004 [==========>] Loss 0.10771209328503387  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.07574053858184487  - accuracy: 0.9375\n",
      "At: 1006 [==========>] Loss 0.07766691783690888  - accuracy: 0.9375\n",
      "At: 1007 [==========>] Loss 0.12656953198037002  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.16368034303387155  - accuracy: 0.78125\n",
      "At: 1009 [==========>] Loss 0.13988117485431378  - accuracy: 0.8125\n",
      "At: 1010 [==========>] Loss 0.1393311542145164  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.1524849227726004  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.11662420333368655  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.07558672282615832  - accuracy: 0.875\n",
      "At: 1014 [==========>] Loss 0.08863998403914203  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.19683817118127317  - accuracy: 0.6875\n",
      "At: 1016 [==========>] Loss 0.16546997902445276  - accuracy: 0.71875\n",
      "At: 1017 [==========>] Loss 0.1595985358714227  - accuracy: 0.75\n",
      "At: 1018 [==========>] Loss 0.14348522943432104  - accuracy: 0.8125\n",
      "At: 1019 [==========>] Loss 0.18566053402377247  - accuracy: 0.78125\n",
      "At: 1020 [==========>] Loss 0.1262565499004377  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.15968359697426607  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.12338646783008521  - accuracy: 0.84375\n",
      "At: 1023 [==========>] Loss 0.16278968073226238  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.18538707510927382  - accuracy: 0.78125\n",
      "At: 1025 [==========>] Loss 0.18423193068394214  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.11309587147176665  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.11589066100486226  - accuracy: 0.875\n",
      "At: 1028 [==========>] Loss 0.22491671853843492  - accuracy: 0.625\n",
      "At: 1029 [==========>] Loss 0.08721675591554134  - accuracy: 0.875\n",
      "At: 1030 [==========>] Loss 0.1077352658575345  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.19046046130615968  - accuracy: 0.71875\n",
      "At: 1032 [==========>] Loss 0.13958410064958487  - accuracy: 0.78125\n",
      "At: 1033 [==========>] Loss 0.1276266871068017  - accuracy: 0.84375\n",
      "At: 1034 [==========>] Loss 0.08582884346920433  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.10215797972546635  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.14102958575627397  - accuracy: 0.8125\n",
      "At: 1037 [==========>] Loss 0.14268280764341373  - accuracy: 0.84375\n",
      "At: 1038 [==========>] Loss 0.07860631624464041  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.10235452878356387  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.10390312944470839  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.1363122879470849  - accuracy: 0.75\n",
      "At: 1042 [==========>] Loss 0.11429902745543619  - accuracy: 0.78125\n",
      "At: 1043 [==========>] Loss 0.1877681755296216  - accuracy: 0.75\n",
      "At: 1044 [==========>] Loss 0.12711986134936698  - accuracy: 0.84375\n",
      "At: 1045 [==========>] Loss 0.1309939908488268  - accuracy: 0.84375\n",
      "At: 1046 [==========>] Loss 0.1782451714282264  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.13125321377387658  - accuracy: 0.8125\n",
      "At: 1048 [==========>] Loss 0.1706694819362195  - accuracy: 0.75\n",
      "At: 1049 [==========>] Loss 0.14646398991057816  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.1603373656324661  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.09468228812065116  - accuracy: 0.875\n",
      "At: 1052 [==========>] Loss 0.09712858043457437  - accuracy: 0.90625\n",
      "At: 1053 [==========>] Loss 0.08913946801296381  - accuracy: 0.9375\n",
      "At: 1054 [==========>] Loss 0.11688326675699165  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.17016194922265693  - accuracy: 0.78125\n",
      "At: 1056 [==========>] Loss 0.1457114196842751  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.14397751309847598  - accuracy: 0.8125\n",
      "At: 1058 [==========>] Loss 0.05308242420969855  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.08791295593877263  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.09954790289836543  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.09043158052096709  - accuracy: 0.90625\n",
      "At: 1062 [==========>] Loss 0.1641493818316458  - accuracy: 0.65625\n",
      "At: 1063 [==========>] Loss 0.12090254912420104  - accuracy: 0.78125\n",
      "At: 1064 [==========>] Loss 0.12612083889320566  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.08957253385451816  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.07955877239801296  - accuracy: 0.90625\n",
      "At: 1067 [==========>] Loss 0.11846234869463519  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.12241046927612473  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.15273574153616506  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.1205830914505921  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.11329056303229185  - accuracy: 0.84375\n",
      "At: 1072 [==========>] Loss 0.12505411904748773  - accuracy: 0.84375\n",
      "At: 1073 [==========>] Loss 0.16933623278430962  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.16581070787115887  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.12055284832081149  - accuracy: 0.84375\n",
      "At: 1076 [==========>] Loss 0.12778379745110738  - accuracy: 0.84375\n",
      "At: 1077 [==========>] Loss 0.08895419932856084  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.0870314017749762  - accuracy: 0.9375\n",
      "At: 1079 [==========>] Loss 0.1171598866287727  - accuracy: 0.84375\n",
      "At: 1080 [==========>] Loss 0.15088487906922507  - accuracy: 0.8125\n",
      "At: 1081 [==========>] Loss 0.12429772886985394  - accuracy: 0.8125\n",
      "At: 1082 [==========>] Loss 0.12050122347746256  - accuracy: 0.875\n",
      "At: 1083 [==========>] Loss 0.1411502309571525  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.08348982595158144  - accuracy: 0.96875\n",
      "At: 1085 [==========>] Loss 0.1225672196488813  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.12550638292352925  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.13162145164987443  - accuracy: 0.84375\n",
      "At: 1088 [==========>] Loss 0.17398565042864822  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.08611258600909552  - accuracy: 0.9375\n",
      "At: 1090 [==========>] Loss 0.08910728009518701  - accuracy: 0.875\n",
      "At: 1091 [==========>] Loss 0.17417493161835954  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.1177130605245712  - accuracy: 0.78125\n",
      "At: 1093 [==========>] Loss 0.15737398091592758  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.15244085417971814  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.1282832986329439  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.10108056819527743  - accuracy: 0.84375\n",
      "At: 1097 [==========>] Loss 0.0660958398042334  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.1583848381323422  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.15861320884232388  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.07566754010756296  - accuracy: 0.9375\n",
      "At: 1101 [==========>] Loss 0.11755703411422849  - accuracy: 0.8125\n",
      "At: 1102 [==========>] Loss 0.11490866746687387  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.09985358901512469  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.07094448997309967  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.07204333616511702  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.09392589392910225  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.18146310027554702  - accuracy: 0.78125\n",
      "At: 1108 [==========>] Loss 0.08021936772088085  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.062282579210958845  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.11787359483882834  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.17824859485908495  - accuracy: 0.78125\n",
      "At: 1112 [==========>] Loss 0.1898194185303237  - accuracy: 0.75\n",
      "At: 1113 [==========>] Loss 0.15533795624598212  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.08933219219147995  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.13148586981161386  - accuracy: 0.84375\n",
      "At: 1116 [==========>] Loss 0.10667248000313193  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.054769635104017625  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.13352069276344541  - accuracy: 0.8125\n",
      "At: 1119 [==========>] Loss 0.14871869737724278  - accuracy: 0.78125\n",
      "At: 1120 [==========>] Loss 0.09494537520092652  - accuracy: 0.90625\n",
      "At: 1121 [==========>] Loss 0.1196655435790233  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.06836495827034536  - accuracy: 0.9375\n",
      "At: 1123 [==========>] Loss 0.13655854332490924  - accuracy: 0.78125\n",
      "At: 1124 [==========>] Loss 0.14136638920658295  - accuracy: 0.8125\n",
      "At: 1125 [==========>] Loss 0.15227377696383004  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.07496558795601727  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.13402784881100288  - accuracy: 0.75\n",
      "At: 1128 [==========>] Loss 0.07101447702553601  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.1643114762661315  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.13278764009628252  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.10111282475765312  - accuracy: 0.84375\n",
      "At: 1132 [==========>] Loss 0.11916075653576262  - accuracy: 0.75\n",
      "At: 1133 [==========>] Loss 0.10690844930559856  - accuracy: 0.8125\n",
      "At: 1134 [==========>] Loss 0.086212759577681  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.10898757541475856  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.16321636048082155  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.08960078779898345  - accuracy: 0.875\n",
      "At: 1138 [==========>] Loss 0.08596799779664299  - accuracy: 0.90625\n",
      "At: 1139 [==========>] Loss 0.07227796094430487  - accuracy: 0.96875\n",
      "At: 1140 [==========>] Loss 0.16925404805174715  - accuracy: 0.71875\n",
      "At: 1141 [==========>] Loss 0.14327467856633722  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.13798989368075645  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.06934729086635387  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.10899801578266602  - accuracy: 0.78125\n",
      "At: 1145 [==========>] Loss 0.13601780410565256  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.13204104883363837  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.21663566481643376  - accuracy: 0.6875\n",
      "At: 1148 [==========>] Loss 0.07575702683756239  - accuracy: 0.9375\n",
      "At: 1149 [==========>] Loss 0.11108722976420597  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.12042591771865924  - accuracy: 0.8125\n",
      "At: 1151 [==========>] Loss 0.16969316518376298  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.1280732504113366  - accuracy: 0.78125\n",
      "At: 1153 [==========>] Loss 0.16368973441126355  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.11985331863884556  - accuracy: 0.8125\n",
      "At: 1155 [==========>] Loss 0.10052841210548003  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.1546426265723161  - accuracy: 0.8125\n",
      "At: 1157 [==========>] Loss 0.11574080493813932  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.16517872614176682  - accuracy: 0.78125\n",
      "At: 1159 [==========>] Loss 0.12623928786369062  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.1003098152895735  - accuracy: 0.84375\n",
      "At: 1161 [==========>] Loss 0.11662957001223487  - accuracy: 0.78125\n",
      "At: 1162 [==========>] Loss 0.1334156016823875  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.18693086586067212  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.08948967781961503  - accuracy: 0.90625\n",
      "At: 1165 [==========>] Loss 0.17066758880166397  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.09813738857377613  - accuracy: 0.875\n",
      "At: 1167 [==========>] Loss 0.15116320866078087  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.13332027774720523  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.08573813625353893  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.19328108714856881  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.06506191249180164  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.1168085669286015  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.11052927807290677  - accuracy: 0.90625\n",
      "At: 1174 [==========>] Loss 0.18702640779540172  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.09295774923453903  - accuracy: 0.90625\n",
      "At: 1176 [==========>] Loss 0.12636631120683225  - accuracy: 0.78125\n",
      "At: 1177 [==========>] Loss 0.08925810148272924  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.1611776697956303  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.11277826783108788  - accuracy: 0.78125\n",
      "At: 1180 [==========>] Loss 0.1945349838444468  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.09678452857530573  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.10312650942877818  - accuracy: 0.84375\n",
      "At: 1183 [==========>] Loss 0.1321871091985346  - accuracy: 0.875\n",
      "At: 1184 [==========>] Loss 0.16327424282861316  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.08796011661532421  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.12656032334897097  - accuracy: 0.84375\n",
      "At: 1187 [==========>] Loss 0.11402084476722177  - accuracy: 0.8125\n",
      "At: 1188 [==========>] Loss 0.07113444743605547  - accuracy: 0.96875\n",
      "At: 1189 [==========>] Loss 0.14096591898377275  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.0910347278876812  - accuracy: 0.9375\n",
      "At: 1191 [==========>] Loss 0.1797074125982288  - accuracy: 0.8125\n",
      "At: 1192 [==========>] Loss 0.07030292247404307  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.11854421068349937  - accuracy: 0.84375\n",
      "At: 1194 [==========>] Loss 0.12442473269099444  - accuracy: 0.90625\n",
      "At: 1195 [==========>] Loss 0.12252377245091814  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.10334824935883008  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.10659482196781073  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.0859868466392274  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.19495770261215406  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.08978669313172033  - accuracy: 0.90625\n",
      "At: 1201 [==========>] Loss 0.10919507517818072  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.16110202794755918  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.1513376412302908  - accuracy: 0.75\n",
      "At: 1204 [==========>] Loss 0.09884620710929704  - accuracy: 0.84375\n",
      "At: 1205 [==========>] Loss 0.06134824209778149  - accuracy: 0.90625\n",
      "At: 1206 [==========>] Loss 0.1043017471424006  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.1537903507090612  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.11069003873318606  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.10913183803404729  - accuracy: 0.78125\n",
      "At: 1210 [==========>] Loss 0.1257441581373978  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.14631493626181294  - accuracy: 0.84375\n",
      "At: 1212 [==========>] Loss 0.0897942109689122  - accuracy: 0.8125\n",
      "At: 1213 [==========>] Loss 0.17862675836915887  - accuracy: 0.75\n",
      "At: 1214 [==========>] Loss 0.14892259633677277  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.12678274162508624  - accuracy: 0.8125\n",
      "At: 1216 [==========>] Loss 0.11453935402564638  - accuracy: 0.78125\n",
      "At: 1217 [==========>] Loss 0.09227787220660631  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.09231160960453563  - accuracy: 0.875\n",
      "At: 1219 [==========>] Loss 0.12351026367425397  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.12572394893115352  - accuracy: 0.84375\n",
      "At: 1221 [==========>] Loss 0.09222425170662793  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.18176229818651782  - accuracy: 0.65625\n",
      "At: 1223 [==========>] Loss 0.0923931730413835  - accuracy: 0.9375\n",
      "At: 1224 [==========>] Loss 0.08757432880510106  - accuracy: 0.875\n",
      "At: 1225 [==========>] Loss 0.08847263353886653  - accuracy: 0.84375\n",
      "At: 1226 [==========>] Loss 0.10067288625100713  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.14182909611633332  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.143880661832359  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.12232491471179707  - accuracy: 0.8125\n",
      "At: 1230 [==========>] Loss 0.1535118543525858  - accuracy: 0.78125\n",
      "At: 1231 [==========>] Loss 0.127430179942427  - accuracy: 0.84375\n",
      "At: 1232 [==========>] Loss 0.0750360726475142  - accuracy: 0.96875\n",
      "At: 1233 [==========>] Loss 0.11113426700633339  - accuracy: 0.78125\n",
      "At: 1234 [==========>] Loss 0.1409373351685274  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.08237561706747383  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.15163984763935529  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.07476266293884662  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.09203528024319085  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.16006963415188386  - accuracy: 0.84375\n",
      "At: 1240 [==========>] Loss 0.09933297828858537  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.09899711222521743  - accuracy: 0.84375\n",
      "At: 1242 [==========>] Loss 0.13074391661022267  - accuracy: 0.78125\n",
      "At: 1243 [==========>] Loss 0.1250967037443457  - accuracy: 0.84375\n",
      "At: 1244 [==========>] Loss 0.1463211962205685  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.09244897697169255  - accuracy: 0.84375\n",
      "At: 1246 [==========>] Loss 0.08404196855855264  - accuracy: 0.84375\n",
      "At: 1247 [==========>] Loss 0.15874376661394157  - accuracy: 0.71875\n",
      "At: 1248 [==========>] Loss 0.0915355273481076  - accuracy: 0.875\n",
      "At: 1249 [==========>] Loss 0.12487307610398635  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.11010414567541148  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.1316196174824203  - accuracy: 0.78125\n",
      "At: 1252 [==========>] Loss 0.07530510361455245  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.10182311989069284  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.16505045055638584  - accuracy: 0.78125\n",
      "At: 1255 [==========>] Loss 0.08369308381145005  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.12277446288828328  - accuracy: 0.8125\n",
      "At: 1257 [==========>] Loss 0.12517572659010112  - accuracy: 0.875\n",
      "At: 1258 [==========>] Loss 0.07566185890500074  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.13131568270537283  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.12756022174646137  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.12628349892717317  - accuracy: 0.84375\n",
      "At: 1262 [==========>] Loss 0.13656182734496342  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.10607859628350495  - accuracy: 0.875\n",
      "At: 1264 [==========>] Loss 0.08636179940044815  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.13304252631962651  - accuracy: 0.8125\n",
      "At: 1266 [==========>] Loss 0.14610082693355939  - accuracy: 0.6875\n",
      "At: 1267 [==========>] Loss 0.13150755956412186  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.12927351582684726  - accuracy: 0.8125\n",
      "At: 1269 [==========>] Loss 0.09812765613093347  - accuracy: 0.875\n",
      "At: 1270 [==========>] Loss 0.1113366736412856  - accuracy: 0.90625\n",
      "At: 1271 [==========>] Loss 0.15300697183245934  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.06411125888009553  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.19776848270417316  - accuracy: 0.6875\n",
      "At: 1274 [==========>] Loss 0.10259627498959717  - accuracy: 0.8125\n",
      "At: 1275 [==========>] Loss 0.08588407749942335  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.10004279517213305  - accuracy: 0.78125\n",
      "At: 1277 [==========>] Loss 0.08617463521023344  - accuracy: 0.84375\n",
      "At: 1278 [==========>] Loss 0.15050792563505266  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.10252306092605212  - accuracy: 0.84375\n",
      "At: 1280 [==========>] Loss 0.11252355116879836  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.12775720587230455  - accuracy: 0.8125\n",
      "At: 1282 [==========>] Loss 0.12914498335265925  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.15153851697818208  - accuracy: 0.78125\n",
      "At: 1284 [==========>] Loss 0.1571778114764316  - accuracy: 0.78125\n",
      "At: 1285 [==========>] Loss 0.07017052371564458  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.1333797229940385  - accuracy: 0.84375\n",
      "At: 1287 [==========>] Loss 0.12429761083246273  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.14917589560940836  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.1140898903889897  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.12022612125450059  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.14450950152153075  - accuracy: 0.84375\n",
      "At: 1292 [==========>] Loss 0.11254729552673105  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.15040936546617756  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.10404468517583629  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.16708711939079385  - accuracy: 0.8125\n",
      "At: 1296 [==========>] Loss 0.14631335276350876  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.13506038905074857  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.09038688956502146  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.15153704071466745  - accuracy: 0.75\n",
      "At: 1300 [==========>] Loss 0.13154350748391427  - accuracy: 0.8125\n",
      "At: 1301 [==========>] Loss 0.12397069644802522  - accuracy: 0.84375\n",
      "At: 1302 [==========>] Loss 0.07307634607313095  - accuracy: 0.90625\n",
      "At: 1303 [==========>] Loss 0.10583676865520365  - accuracy: 0.90625\n",
      "At: 1304 [==========>] Loss 0.10374936072654195  - accuracy: 0.78125\n",
      "At: 1305 [==========>] Loss 0.11339737253696039  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.0874517771090512  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.11710766659398135  - accuracy: 0.875\n",
      "At: 1308 [==========>] Loss 0.07965560706005087  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.15380796463274288  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.16577565471718053  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.15890043259144684  - accuracy: 0.78125\n",
      "At: 1312 [==========>] Loss 0.07667867090780317  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.1721621132375198  - accuracy: 0.71875\n",
      "At: 1314 [==========>] Loss 0.05769459071388857  - accuracy: 0.90625\n",
      "At: 1315 [==========>] Loss 0.14836888594370523  - accuracy: 0.8125\n",
      "At: 1316 [==========>] Loss 0.1500912937720535  - accuracy: 0.8125\n",
      "At: 1317 [==========>] Loss 0.09752023329370371  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.11518055446195544  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.11873797633119701  - accuracy: 0.875\n",
      "At: 1320 [==========>] Loss 0.13668087918267088  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.06859181667457367  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.14631365931452212  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.1008868588865544  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.13560269291082505  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.09582965013219245  - accuracy: 0.84375\n",
      "At: 1326 [==========>] Loss 0.08924291218255573  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.1433265452168091  - accuracy: 0.78125\n",
      "At: 1328 [==========>] Loss 0.07589813034799608  - accuracy: 0.875\n",
      "At: 1329 [==========>] Loss 0.0726695390306524  - accuracy: 0.875\n",
      "At: 1330 [==========>] Loss 0.09852690828990138  - accuracy: 0.875\n",
      "At: 1331 [==========>] Loss 0.14562608753586642  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.12638134023195405  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.137472419054508  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.09895121199104098  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.12193015755110902  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.10734320245554624  - accuracy: 0.8125\n",
      "At: 1337 [==========>] Loss 0.15816203745135088  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.1349712363865789  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.1415890353198882  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.1311071759882447  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.09292728565151573  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.11614470347796421  - accuracy: 0.8125\n",
      "At: 1343 [==========>] Loss 0.19032409575706621  - accuracy: 0.71875\n",
      "At: 1344 [==========>] Loss 0.15209148160419697  - accuracy: 0.78125\n",
      "At: 1345 [==========>] Loss 0.09608847127196413  - accuracy: 0.875\n",
      "At: 1346 [==========>] Loss 0.09659651957731374  - accuracy: 0.8125\n",
      "At: 1347 [==========>] Loss 0.10247732564888838  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09424348648910011  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.15337078370431642  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.1293363036898265  - accuracy: 0.90625\n",
      "At: 1351 [==========>] Loss 0.10845751385963981  - accuracy: 0.875\n",
      "At: 1352 [==========>] Loss 0.09636547459387747  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.16348827783651132  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.18286465185920203  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.07921387049472291  - accuracy: 0.84375\n",
      "At: 1356 [==========>] Loss 0.10871087457180806  - accuracy: 0.875\n",
      "At: 1357 [==========>] Loss 0.12177563106762393  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.1213220114932917  - accuracy: 0.875\n",
      "At: 1359 [==========>] Loss 0.06271485816378158  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.17545012499030488  - accuracy: 0.84375\n",
      "At: 1361 [==========>] Loss 0.10944354634421198  - accuracy: 0.84375\n",
      "At: 1362 [==========>] Loss 0.14796944106747018  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.09636170168879067  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.14525079256979861  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.1145390572740994  - accuracy: 0.78125\n",
      "At: 1366 [==========>] Loss 0.11801475556081689  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.10899535902236442  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.1361072108786549  - accuracy: 0.78125\n",
      "At: 1369 [==========>] Loss 0.08054483679122018  - accuracy: 0.96875\n",
      "At: 1370 [==========>] Loss 0.11143221764699843  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.2140743930435291  - accuracy: 0.6875\n",
      "At: 1372 [==========>] Loss 0.10841314986123804  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.14148766773902927  - accuracy: 0.8125\n",
      "At: 1374 [==========>] Loss 0.12802407978296693  - accuracy: 0.75\n",
      "At: 1375 [==========>] Loss 0.1342549376434541  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.10553437918473713  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.17518085474046086  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.1347554958461951  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.15820049201394254  - accuracy: 0.78125\n",
      "At: 1380 [==========>] Loss 0.13450593950397774  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.09212921295029045  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.11735344365083525  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.10278603969631228  - accuracy: 0.90625\n",
      "At: 1384 [==========>] Loss 0.10065122790746955  - accuracy: 0.84375\n",
      "At: 1385 [==========>] Loss 0.15500823741643444  - accuracy: 0.8125\n",
      "At: 1386 [==========>] Loss 0.16070096481372056  - accuracy: 0.78125\n",
      "At: 1387 [==========>] Loss 0.06197860895775704  - accuracy: 0.9375\n",
      "At: 1388 [==========>] Loss 0.16257387161188316  - accuracy: 0.75\n",
      "At: 1389 [==========>] Loss 0.10326759627037385  - accuracy: 0.84375\n",
      "At: 1390 [==========>] Loss 0.15058319388776187  - accuracy: 0.78125\n",
      "At: 1391 [==========>] Loss 0.11193998047919883  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.07121757100379283  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.12783298993285308  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.08485230447566108  - accuracy: 0.90625\n",
      "At: 1395 [==========>] Loss 0.21836754152226784  - accuracy: 0.65625\n",
      "At: 1396 [==========>] Loss 0.06590451051033264  - accuracy: 0.96875\n",
      "At: 1397 [==========>] Loss 0.1384572090496543  - accuracy: 0.78125\n",
      "At: 1398 [==========>] Loss 0.10216143467279488  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.10983218099318051  - accuracy: 0.8125\n",
      "At: 1400 [==========>] Loss 0.13313754014882512  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.08151620493917831  - accuracy: 0.90625\n",
      "At: 1402 [==========>] Loss 0.1504061731016004  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.13029392814086643  - accuracy: 0.78125\n",
      "At: 1404 [==========>] Loss 0.09462133089386897  - accuracy: 0.90625\n",
      "At: 1405 [==========>] Loss 0.07230756596520044  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.1564021020459932  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.0966811882903448  - accuracy: 0.90625\n",
      "At: 1408 [==========>] Loss 0.1574567667567146  - accuracy: 0.78125\n",
      "At: 1409 [==========>] Loss 0.03296405818560661  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.14676854912901022  - accuracy: 0.75\n",
      "At: 1411 [==========>] Loss 0.15063556060697803  - accuracy: 0.8125\n",
      "At: 1412 [==========>] Loss 0.1286297940210575  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.10513649884024198  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.19007295921935446  - accuracy: 0.71875\n",
      "At: 1415 [==========>] Loss 0.09212931804611338  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.13571988616143116  - accuracy: 0.78125\n",
      "At: 1417 [==========>] Loss 0.09960503744595156  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.1362624868564739  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.1022100958017792  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.0835043258772376  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.10012452161610216  - accuracy: 0.84375\n",
      "At: 1422 [==========>] Loss 0.13949335051426753  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.16476955017100348  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.1360844313923992  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.10605483164150392  - accuracy: 0.84375\n",
      "At: 1426 [==========>] Loss 0.11232312520906454  - accuracy: 0.875\n",
      "At: 1427 [==========>] Loss 0.11637856423445284  - accuracy: 0.875\n",
      "At: 1428 [==========>] Loss 0.10839961643227233  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.13612237737775812  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.07718709487688954  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.09953565900762662  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.09975248647246979  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.09875812202402072  - accuracy: 0.90625\n",
      "At: 1434 [==========>] Loss 0.14612784586988978  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.12287253573328405  - accuracy: 0.875\n",
      "At: 1436 [==========>] Loss 0.07432041769805736  - accuracy: 0.875\n",
      "At: 1437 [==========>] Loss 0.1302713027125803  - accuracy: 0.84375\n",
      "At: 1438 [==========>] Loss 0.16931750335065915  - accuracy: 0.75\n",
      "At: 1439 [==========>] Loss 0.1120246878441306  - accuracy: 0.875\n",
      "At: 1440 [==========>] Loss 0.10851905027256989  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.0816520001113635  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.11468600381436367  - accuracy: 0.90625\n",
      "At: 1443 [==========>] Loss 0.12803142960633826  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.13198070350256255  - accuracy: 0.8125\n",
      "At: 1445 [==========>] Loss 0.2004087869336519  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.17648205355244379  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.16360778000269327  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.0778786407554079  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.13665906536044412  - accuracy: 0.84375\n",
      "At: 1450 [==========>] Loss 0.12280932679404899  - accuracy: 0.875\n",
      "At: 1451 [==========>] Loss 0.13209476594793676  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.08367464620281453  - accuracy: 0.90625\n",
      "At: 1453 [==========>] Loss 0.05800258529265177  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.15745420385274933  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.10388042046045341  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.1116468026851011  - accuracy: 0.8125\n",
      "At: 1457 [==========>] Loss 0.08431775714330715  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.13452789989702352  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.1410323486252144  - accuracy: 0.84375\n",
      "At: 1460 [==========>] Loss 0.1828254726544476  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.10350258422681069  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.1802575920851669  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.09246221705321286  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.1745475830034453  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.1186154892420815  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.09121978241199388  - accuracy: 0.875\n",
      "At: 1467 [==========>] Loss 0.18584582899384852  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.15170460055512194  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.17114651310452092  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.1301684410775002  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.11522552350102547  - accuracy: 0.875\n",
      "At: 1472 [==========>] Loss 0.08264183952264324  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.08190255316188291  - accuracy: 0.90625\n",
      "At: 1474 [==========>] Loss 0.1918683590049773  - accuracy: 0.78125\n",
      "At: 1475 [==========>] Loss 0.14664126182205994  - accuracy: 0.8125\n",
      "At: 1476 [==========>] Loss 0.11703633511168836  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.10478965611467687  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.09193927725451981  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.13837817610208308  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.09991755309303266  - accuracy: 0.90625\n",
      "At: 1481 [==========>] Loss 0.13492008047381715  - accuracy: 0.8125\n",
      "At: 1482 [==========>] Loss 0.10130114904302331  - accuracy: 0.90625\n",
      "At: 1483 [==========>] Loss 0.20698552413009563  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.13618205564833136  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.14346403150620085  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.07214921472251393  - accuracy: 0.9375\n",
      "At: 1487 [==========>] Loss 0.0762612157162833  - accuracy: 0.875\n",
      "At: 1488 [==========>] Loss 0.14161534884874155  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.21192635197992227  - accuracy: 0.6875\n",
      "At: 1490 [==========>] Loss 0.10638893825026491  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.14403361305573373  - accuracy: 0.8125\n",
      "At: 1492 [==========>] Loss 0.1417141510484414  - accuracy: 0.75\n",
      "At: 1493 [==========>] Loss 0.15190634492505423  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.1399794056639493  - accuracy: 0.8125\n",
      "At: 1495 [==========>] Loss 0.13549429475996083  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.098990432979553  - accuracy: 0.8125\n",
      "At: 1497 [==========>] Loss 0.1681783203872555  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.13727045757624864  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.11771838078716879  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.10075324107129147  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.08624118134382248  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.17149217370829578  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.12297472356833739  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.10420643771925465  - accuracy: 0.96875\n",
      "At: 1505 [==========>] Loss 0.13166636292188735  - accuracy: 0.75\n",
      "At: 1506 [==========>] Loss 0.14418100962205077  - accuracy: 0.78125\n",
      "At: 1507 [==========>] Loss 0.12074472449537034  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.18293624409645382  - accuracy: 0.6875\n",
      "At: 1509 [==========>] Loss 0.12501285252457783  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.11914837213431963  - accuracy: 0.75\n",
      "At: 1511 [==========>] Loss 0.1193404763135095  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.08017531665370138  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.13515725621652286  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.14874792571691842  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.11361786903923432  - accuracy: 0.84375\n",
      "At: 1516 [==========>] Loss 0.11484745618176641  - accuracy: 0.84375\n",
      "At: 1517 [==========>] Loss 0.1471590880401155  - accuracy: 0.75\n",
      "At: 1518 [==========>] Loss 0.10463213996853571  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.14361451008638076  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.1253084255440645  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.07656741711234194  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.17916130032242572  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.0998843396247204  - accuracy: 0.875\n",
      "At: 1524 [==========>] Loss 0.11571002433909149  - accuracy: 0.8125\n",
      "At: 1525 [==========>] Loss 0.12723859611380622  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.11473809399450695  - accuracy: 0.8125\n",
      "At: 1527 [==========>] Loss 0.15274308322690622  - accuracy: 0.78125\n",
      "At: 1528 [==========>] Loss 0.14281781591413537  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.08566202258442124  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.05342515000524472  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.1247305581819577  - accuracy: 0.8125\n",
      "At: 1532 [==========>] Loss 0.18144957996190214  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.12489771316161898  - accuracy: 0.84375\n",
      "At: 1534 [==========>] Loss 0.1190158605580836  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.1557637683122721  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.13637483763461133  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.11779487933290403  - accuracy: 0.8125\n",
      "At: 1538 [==========>] Loss 0.1274321078412326  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.0939809915052518  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1596998163328118  - accuracy: 0.71875\n",
      "At: 1541 [==========>] Loss 0.14108639147411034  - accuracy: 0.78125\n",
      "At: 1542 [==========>] Loss 0.09280959993052305  - accuracy: 0.90625\n",
      "At: 1543 [==========>] Loss 0.11482199161748757  - accuracy: 0.875\n",
      "At: 1544 [==========>] Loss 0.13147431774340212  - accuracy: 0.8125\n",
      "At: 1545 [==========>] Loss 0.18188420388833432  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.10950060375450411  - accuracy: 0.84375\n",
      "At: 1547 [==========>] Loss 0.14173081909670454  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.15240973795897758  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.14835131595958082  - accuracy: 0.75\n",
      "At: 1550 [==========>] Loss 0.07228529484714027  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.1951365020048677  - accuracy: 0.75\n",
      "At: 1552 [==========>] Loss 0.09759127062366255  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.08081017961078907  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.12859509831765326  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.11211543023373369  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1455008824297945  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.08955489358812492  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.13554296919003633  - accuracy: 0.78125\n",
      "At: 1559 [==========>] Loss 0.08247064394268537  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.11310526002398741  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.13554956171550955  - accuracy: 0.84375\n",
      "At: 1562 [==========>] Loss 0.09447988392921436  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.11412449020436066  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.0892173675114424  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.13994113991350332  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.130810453319856  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.12735201889953462  - accuracy: 0.8125\n",
      "At: 1568 [==========>] Loss 0.09309389149741282  - accuracy: 0.8125\n",
      "At: 1569 [==========>] Loss 0.1355773331715568  - accuracy: 0.78125\n",
      "At: 1570 [==========>] Loss 0.09713832370850867  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.12454473541852099  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.13226342902094612  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.0736815300100466  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.1409407779721586  - accuracy: 0.78125\n",
      "At: 1575 [==========>] Loss 0.10514873426139612  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.12961797424187838  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.0996748006386521  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.0843793429351076  - accuracy: 0.875\n",
      "At: 1579 [==========>] Loss 0.0880059603623847  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.12065493650889045  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.08965359714678081  - accuracy: 0.90625\n",
      "At: 1582 [==========>] Loss 0.18929296479403385  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.09318519769004538  - accuracy: 0.84375\n",
      "At: 1584 [==========>] Loss 0.12093684663631846  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.09549195637016236  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.1357430295946353  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.11124992912860415  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.13068603946298055  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.14919747124290997  - accuracy: 0.71875\n",
      "At: 1590 [==========>] Loss 0.1294063534849481  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.1205179064468152  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.08327262209366923  - accuracy: 0.90625\n",
      "At: 1593 [==========>] Loss 0.1842539694291523  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.0945365291519954  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.13978552363421304  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.183685267300107  - accuracy: 0.71875\n",
      "At: 1597 [==========>] Loss 0.13649055520888712  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.18066081609645174  - accuracy: 0.75\n",
      "At: 1599 [==========>] Loss 0.21022209939410746  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.1395511067735963  - accuracy: 0.875\n",
      "At: 1601 [==========>] Loss 0.09011568204735601  - accuracy: 0.90625\n",
      "At: 1602 [==========>] Loss 0.13775924068782758  - accuracy: 0.78125\n",
      "At: 1603 [==========>] Loss 0.1998423650676285  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.22357994695986483  - accuracy: 0.6875\n",
      "At: 1605 [==========>] Loss 0.0866761626561306  - accuracy: 0.8125\n",
      "At: 1606 [==========>] Loss 0.11363428007045856  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.19544145896706255  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.15060779837223806  - accuracy: 0.8125\n",
      "At: 1609 [==========>] Loss 0.14216310542839544  - accuracy: 0.8125\n",
      "At: 1610 [==========>] Loss 0.17517823392544737  - accuracy: 0.75\n",
      "At: 1611 [==========>] Loss 0.07812257301538025  - accuracy: 0.875\n",
      "At: 1612 [==========>] Loss 0.07750715169193759  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.14160331150244554  - accuracy: 0.875\n",
      "At: 1614 [==========>] Loss 0.16587904868927567  - accuracy: 0.71875\n",
      "At: 1615 [==========>] Loss 0.10469368504851448  - accuracy: 0.84375\n",
      "At: 1616 [==========>] Loss 0.14794262457328614  - accuracy: 0.71875\n",
      "At: 1617 [==========>] Loss 0.07999874027161535  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.12353352762756087  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.2213726288172616  - accuracy: 0.625\n",
      "At: 1620 [==========>] Loss 0.09759114740721961  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.09680668427043146  - accuracy: 0.84375\n",
      "At: 1622 [==========>] Loss 0.15628449767089486  - accuracy: 0.8125\n",
      "At: 1623 [==========>] Loss 0.09082052940012139  - accuracy: 0.875\n",
      "At: 1624 [==========>] Loss 0.1552124572398349  - accuracy: 0.84375\n",
      "At: 1625 [==========>] Loss 0.14194541257099152  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.10144393891610426  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.10567272195932985  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.16979596309136985  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.1254078272086041  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.10397007657586424  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.11703419091640042  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.09697401419757626  - accuracy: 0.875\n",
      "At: 1633 [==========>] Loss 0.08820240432944772  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.094317484238321  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.2179676594709134  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.10182362270192466  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.08407681148376284  - accuracy: 0.875\n",
      "At: 1638 [==========>] Loss 0.14909122515145054  - accuracy: 0.8125\n",
      "At: 1639 [==========>] Loss 0.1642745849134023  - accuracy: 0.65625\n",
      "At: 1640 [==========>] Loss 0.1396067373096514  - accuracy: 0.75\n",
      "At: 1641 [==========>] Loss 0.10152761116828288  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.11672116001955125  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.1169605939736092  - accuracy: 0.8125\n",
      "At: 1644 [==========>] Loss 0.11409587798280581  - accuracy: 0.875\n",
      "At: 1645 [==========>] Loss 0.09173907787949709  - accuracy: 0.84375\n",
      "At: 1646 [==========>] Loss 0.16727395472796028  - accuracy: 0.75\n",
      "At: 1647 [==========>] Loss 0.16350999091965096  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.16101220534663957  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.07891250696010473  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.08554166882309247  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.1344002473187921  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.07616241187832319  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.10001501363839209  - accuracy: 0.8125\n",
      "At: 1654 [==========>] Loss 0.0696048068113852  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.16446420819304658  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.09368991217666181  - accuracy: 0.90625\n",
      "At: 1657 [==========>] Loss 0.14608108066855907  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.20817568525278057  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.0962190372058115  - accuracy: 0.84375\n",
      "At: 1660 [==========>] Loss 0.04587228020475674  - accuracy: 1.0\n",
      "At: 1661 [==========>] Loss 0.07687351959395855  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.13926548966487712  - accuracy: 0.75\n",
      "At: 1663 [==========>] Loss 0.1573793573800626  - accuracy: 0.8125\n",
      "At: 1664 [==========>] Loss 0.11162812610918535  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.1025167322053756  - accuracy: 0.8125\n",
      "At: 1666 [==========>] Loss 0.09288412624034872  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.1464255965343509  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.1502238731658858  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.1405072876008016  - accuracy: 0.8125\n",
      "At: 1670 [==========>] Loss 0.058387181259345625  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.12413753335347796  - accuracy: 0.8125\n",
      "At: 1672 [==========>] Loss 0.12149865689199593  - accuracy: 0.84375\n",
      "At: 1673 [==========>] Loss 0.10268571460192533  - accuracy: 0.8125\n",
      "At: 1674 [==========>] Loss 0.15485360471405293  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.1857237875657897  - accuracy: 0.6875\n",
      "At: 1676 [==========>] Loss 0.1650487135789704  - accuracy: 0.75\n",
      "At: 1677 [==========>] Loss 0.10485637188541647  - accuracy: 0.90625\n",
      "At: 1678 [==========>] Loss 0.18597993589400086  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.11412105926873778  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.16039565533858202  - accuracy: 0.84375\n",
      "At: 1681 [==========>] Loss 0.14549095961997852  - accuracy: 0.78125\n",
      "At: 1682 [==========>] Loss 0.1050377058968951  - accuracy: 0.84375\n",
      "At: 1683 [==========>] Loss 0.16809598543235188  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.1358731244429691  - accuracy: 0.84375\n",
      "At: 1685 [==========>] Loss 0.12511127850223352  - accuracy: 0.875\n",
      "At: 1686 [==========>] Loss 0.11738340685167584  - accuracy: 0.875\n",
      "At: 1687 [==========>] Loss 0.1639482105334379  - accuracy: 0.75\n",
      "At: 1688 [==========>] Loss 0.05127657213728602  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.12218762546297714  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.08686335917268709  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.12777769915900355  - accuracy: 0.8125\n",
      "At: 1692 [==========>] Loss 0.15334377712540218  - accuracy: 0.78125\n",
      "At: 1693 [==========>] Loss 0.11491972327119378  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.08859300562186383  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.11925483938678055  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.16557173999813551  - accuracy: 0.78125\n",
      "At: 1697 [==========>] Loss 0.12042409915818596  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.07892939447274915  - accuracy: 0.8125\n",
      "At: 1699 [==========>] Loss 0.11515566682058706  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.1336151541152006  - accuracy: 0.75\n",
      "At: 1701 [==========>] Loss 0.10675588231680233  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.1079228927662993  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.17805519254692687  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.0875061214339007  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.11864752283157698  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.1733473811564103  - accuracy: 0.75\n",
      "At: 1707 [==========>] Loss 0.19675363876764002  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.0929187163087587  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.20604712406810846  - accuracy: 0.65625\n",
      "At: 1710 [==========>] Loss 0.14842852860413014  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.08048614015116562  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.09766490055129945  - accuracy: 0.90625\n",
      "At: 1713 [==========>] Loss 0.09890606830806947  - accuracy: 0.90625\n",
      "At: 1714 [==========>] Loss 0.1694925528775829  - accuracy: 0.71875\n",
      "At: 1715 [==========>] Loss 0.10990455428868981  - accuracy: 0.84375\n",
      "At: 1716 [==========>] Loss 0.07792168271704955  - accuracy: 0.875\n",
      "At: 1717 [==========>] Loss 0.09721101487357584  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.14281504359644304  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.09981945334473846  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.06468337580114332  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.17864280301406255  - accuracy: 0.75\n",
      "At: 1722 [==========>] Loss 0.048841341047253184  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.18497162550433477  - accuracy: 0.75\n",
      "At: 1724 [==========>] Loss 0.09871634918825836  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.14055767917393322  - accuracy: 0.8125\n",
      "At: 1726 [==========>] Loss 0.09895018954646817  - accuracy: 0.90625\n",
      "At: 1727 [==========>] Loss 0.1317750739601785  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.10390631438061823  - accuracy: 0.90625\n",
      "At: 1729 [==========>] Loss 0.19471356916458976  - accuracy: 0.625\n",
      "At: 1730 [==========>] Loss 0.1336177904920344  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.10402878763133802  - accuracy: 0.90625\n",
      "At: 1732 [==========>] Loss 0.061119838307272675  - accuracy: 0.9375\n",
      "At: 1733 [==========>] Loss 0.18263727878620603  - accuracy: 0.6875\n",
      "At: 1734 [==========>] Loss 0.1156448573256918  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.1860859418128809  - accuracy: 0.75\n",
      "At: 1736 [==========>] Loss 0.10540620159206654  - accuracy: 0.875\n",
      "At: 1737 [==========>] Loss 0.14324566563561875  - accuracy: 0.8125\n",
      "At: 1738 [==========>] Loss 0.11914495397434102  - accuracy: 0.84375\n",
      "At: 1739 [==========>] Loss 0.12328404972904661  - accuracy: 0.875\n",
      "At: 1740 [==========>] Loss 0.11791665181133715  - accuracy: 0.8125\n",
      "At: 1741 [==========>] Loss 0.16587362474317577  - accuracy: 0.71875\n",
      "At: 1742 [==========>] Loss 0.06235321782992359  - accuracy: 0.90625\n",
      "At: 1743 [==========>] Loss 0.1500075725480077  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.09540329855112537  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.12008513255716317  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.1506668301277955  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.13829963847711282  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.10666009124168019  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.11006353984663311  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.10130891880798845  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.19203113525195628  - accuracy: 0.75\n",
      "At: 1752 [==========>] Loss 0.10851757979392615  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.08154674380077317  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.08581155408188833  - accuracy: 0.9375\n",
      "At: 1755 [==========>] Loss 0.07579686287321166  - accuracy: 0.875\n",
      "At: 1756 [==========>] Loss 0.15390951083444918  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.15556012363300203  - accuracy: 0.71875\n",
      "At: 1758 [==========>] Loss 0.07128685433602841  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.09382269548925298  - accuracy: 0.8125\n",
      "At: 1760 [==========>] Loss 0.08576735221252366  - accuracy: 0.9375\n",
      "At: 1761 [==========>] Loss 0.11593246810779881  - accuracy: 0.8125\n",
      "At: 1762 [==========>] Loss 0.16865189327549324  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.11790129237772291  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.132997549187238  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.1541930146871354  - accuracy: 0.84375\n",
      "At: 1766 [==========>] Loss 0.061519285017511235  - accuracy: 0.96875\n",
      "At: 1767 [==========>] Loss 0.07434828901370963  - accuracy: 0.9375\n",
      "At: 1768 [==========>] Loss 0.09182531073466293  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.060947638684369365  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.0716985364286471  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.14320820263184425  - accuracy: 0.8125\n",
      "At: 1772 [==========>] Loss 0.1306505126012065  - accuracy: 0.75\n",
      "At: 1773 [==========>] Loss 0.10377386472683417  - accuracy: 0.90625\n",
      "At: 1774 [==========>] Loss 0.15781966372220357  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.11382659089145611  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.11148200757593094  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.1144185037644443  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.09650452719243019  - accuracy: 0.8125\n",
      "At: 1779 [==========>] Loss 0.10136716151363856  - accuracy: 0.875\n",
      "At: 1780 [==========>] Loss 0.09934213254576618  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.15361258484454957  - accuracy: 0.8125\n",
      "At: 1782 [==========>] Loss 0.09964418772152495  - accuracy: 0.8125\n",
      "At: 1783 [==========>] Loss 0.1517367714707718  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.07465844535826531  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.08634000785248225  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.1227679573572136  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.12154987551523744  - accuracy: 0.875\n",
      "At: 1788 [==========>] Loss 0.11955858680132533  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.10858785910641544  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.14714450599201492  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.07224504247769017  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.12248881151193866  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.09740525172745604  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.15562752594933577  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.08818867597991084  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.11348903495099058  - accuracy: 0.84375\n",
      "At: 1797 [==========>] Loss 0.13112818683853256  - accuracy: 0.84375\n",
      "At: 1798 [==========>] Loss 0.11089211587547514  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.07389746095580688  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.0998542099938419  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.1659077305301691  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.1556543354865732  - accuracy: 0.78125\n",
      "At: 1803 [==========>] Loss 0.16611489727564535  - accuracy: 0.75\n",
      "At: 1804 [==========>] Loss 0.12785193316037202  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.0573816797366914  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.1364917284909386  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.16856970290053827  - accuracy: 0.75\n",
      "At: 1808 [==========>] Loss 0.1611474148422084  - accuracy: 0.78125\n",
      "At: 1809 [==========>] Loss 0.08453055342703689  - accuracy: 0.875\n",
      "At: 1810 [==========>] Loss 0.13320471713916274  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.12389865851235343  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.10209885191444978  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.1275649524302867  - accuracy: 0.8125\n",
      "At: 1814 [==========>] Loss 0.11779477598350804  - accuracy: 0.78125\n",
      "At: 1815 [==========>] Loss 0.176142922412244  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.03514328855168996  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.1369938717101517  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.14857539220353816  - accuracy: 0.71875\n",
      "At: 1819 [==========>] Loss 0.18126002685831455  - accuracy: 0.6875\n",
      "At: 1820 [==========>] Loss 0.10267739903132786  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.10703664976578837  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.15142932147566543  - accuracy: 0.75\n",
      "At: 1823 [==========>] Loss 0.1768249313539465  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.1516416251905563  - accuracy: 0.8125\n",
      "At: 1825 [==========>] Loss 0.10462094421054226  - accuracy: 0.84375\n",
      "At: 1826 [==========>] Loss 0.06887669204646027  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.1098005214575769  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.1501311901580874  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.15677198709698548  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.13178908497041691  - accuracy: 0.84375\n",
      "At: 1831 [==========>] Loss 0.12631206598531933  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.12389247235576269  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.08760963569616328  - accuracy: 0.90625\n",
      "At: 1834 [==========>] Loss 0.07252206514283699  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.13949498735300156  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.11886060454843447  - accuracy: 0.8125\n",
      "At: 1837 [==========>] Loss 0.04760220878740128  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.11102022333304154  - accuracy: 0.78125\n",
      "At: 1839 [==========>] Loss 0.09061968365186404  - accuracy: 0.90625\n",
      "At: 1840 [==========>] Loss 0.12218330832451257  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.091461277323589  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.13315268277658945  - accuracy: 0.75\n",
      "At: 1843 [==========>] Loss 0.11455103457736246  - accuracy: 0.78125\n",
      "At: 1844 [==========>] Loss 0.107519819295359  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.16948068202998384  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.15017574060964162  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.03583196600737469  - accuracy: 0.96875\n",
      "At: 1848 [==========>] Loss 0.06256616951440633  - accuracy: 0.875\n",
      "At: 1849 [==========>] Loss 0.1720226736723605  - accuracy: 0.71875\n",
      "At: 1850 [==========>] Loss 0.04232968785514262  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.17207669030920736  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.0701778026121167  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.12776608348609517  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.14969690019565587  - accuracy: 0.78125\n",
      "At: 1855 [==========>] Loss 0.1567008405829512  - accuracy: 0.78125\n",
      "At: 1856 [==========>] Loss 0.12305475793563034  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.14591317400855025  - accuracy: 0.78125\n",
      "At: 1858 [==========>] Loss 0.12118509306321965  - accuracy: 0.8125\n",
      "At: 1859 [==========>] Loss 0.14890478390369946  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.1074980118092797  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.08655426219876947  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.1900234105472942  - accuracy: 0.75\n",
      "At: 1863 [==========>] Loss 0.16278560789388727  - accuracy: 0.71875\n",
      "At: 1864 [==========>] Loss 0.1386717403632635  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.09356867158615792  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.2013758794888943  - accuracy: 0.625\n",
      "At: 1867 [==========>] Loss 0.11836536139825327  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.1471208672573556  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.17507982395954025  - accuracy: 0.625\n",
      "At: 1870 [==========>] Loss 0.09312278721897191  - accuracy: 0.875\n",
      "At: 1871 [==========>] Loss 0.1352858017275867  - accuracy: 0.8125\n",
      "At: 1872 [==========>] Loss 0.13936723882099122  - accuracy: 0.71875\n",
      "At: 1873 [==========>] Loss 0.09057419308919124  - accuracy: 0.875\n",
      "At: 1874 [==========>] Loss 0.16701937237715006  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.09492000062347031  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.20873134386079856  - accuracy: 0.6875\n",
      "At: 1877 [==========>] Loss 0.0985047873477395  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.09041348406988112  - accuracy: 0.875\n",
      "At: 1879 [==========>] Loss 0.12510024805004166  - accuracy: 0.78125\n",
      "At: 1880 [==========>] Loss 0.08260784431174081  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.0776157877888122  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.13030472923155062  - accuracy: 0.8125\n",
      "At: 1883 [==========>] Loss 0.13260411407545103  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.10411084205697554  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.0977114913630672  - accuracy: 0.90625\n",
      "At: 1886 [==========>] Loss 0.11953374485776373  - accuracy: 0.84375\n",
      "At: 1887 [==========>] Loss 0.0990542949936278  - accuracy: 0.8125\n",
      "At: 1888 [==========>] Loss 0.14667355002616944  - accuracy: 0.75\n",
      "At: 1889 [==========>] Loss 0.10268935913346905  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.14978168722649735  - accuracy: 0.8125\n",
      "At: 1891 [==========>] Loss 0.08074633521927572  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.08012227352582692  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.0639952994427666  - accuracy: 0.96875\n",
      "At: 1894 [==========>] Loss 0.09770934060366623  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.07334174877607755  - accuracy: 0.90625\n",
      "At: 1896 [==========>] Loss 0.10484210965244323  - accuracy: 0.90625\n",
      "At: 1897 [==========>] Loss 0.09003532730733223  - accuracy: 0.875\n",
      "At: 1898 [==========>] Loss 0.12376630007830082  - accuracy: 0.8125\n",
      "At: 1899 [==========>] Loss 0.10848124712513706  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.1070300171430147  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.10737951051189207  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.14775838936377494  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.13509859657274992  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.05736008815355418  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.13373534025223546  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.09080910338269288  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.0815185441811828  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.09705573547737514  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.1081814569537215  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.06577663778305914  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.13617072499248906  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.13057546061741382  - accuracy: 0.8125\n",
      "At: 1913 [==========>] Loss 0.16312516279072756  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.06803114088475523  - accuracy: 0.9375\n",
      "At: 1915 [==========>] Loss 0.12899395137011888  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.1520250543605245  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.20666288919102466  - accuracy: 0.71875\n",
      "At: 1918 [==========>] Loss 0.1616069345142106  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.07352201337247688  - accuracy: 0.9375\n",
      "At: 1920 [==========>] Loss 0.10992562465408565  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.11604709540711794  - accuracy: 0.875\n",
      "At: 1922 [==========>] Loss 0.12786770013937587  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.18319415325217508  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.135124954899701  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.171876701548679  - accuracy: 0.78125\n",
      "At: 1926 [==========>] Loss 0.08806741098761192  - accuracy: 0.875\n",
      "At: 1927 [==========>] Loss 0.08786253933895376  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.10909217158711215  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.17189560576951765  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.1452253160783368  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.13357537524998783  - accuracy: 0.78125\n",
      "At: 1932 [==========>] Loss 0.1747388474383124  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.08391406958573472  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.14953255632939746  - accuracy: 0.84375\n",
      "At: 1935 [==========>] Loss 0.1264834325057195  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.09257185155324404  - accuracy: 0.90625\n",
      "At: 1937 [==========>] Loss 0.14611313108156873  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.16360468239739254  - accuracy: 0.78125\n",
      "At: 1939 [==========>] Loss 0.11743698960347773  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.11477846203922319  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.12066285208506264  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.1409354516494288  - accuracy: 0.84375\n",
      "At: 1943 [==========>] Loss 0.13130737115001678  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.12607211775488797  - accuracy: 0.84375\n",
      "At: 1945 [==========>] Loss 0.17425199035068784  - accuracy: 0.75\n",
      "At: 1946 [==========>] Loss 0.11137195580879665  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.10694291618698945  - accuracy: 0.875\n",
      "At: 1948 [==========>] Loss 0.14264436495004593  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.06074512799238229  - accuracy: 0.96875\n",
      "At: 1950 [==========>] Loss 0.13855865046104052  - accuracy: 0.75\n",
      "At: 1951 [==========>] Loss 0.14025156304382197  - accuracy: 0.78125\n",
      "At: 1952 [==========>] Loss 0.09474160310540365  - accuracy: 0.84375\n",
      "At: 1953 [==========>] Loss 0.06393608542233294  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.18742280469182526  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.06948979726189325  - accuracy: 0.875\n",
      "At: 1956 [==========>] Loss 0.11193706453358783  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.09352084170450552  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.08668437459103742  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.12622575072410913  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.06971837405482573  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.19422239417566742  - accuracy: 0.71875\n",
      "At: 1962 [==========>] Loss 0.200106140107232  - accuracy: 0.71875\n",
      "At: 1963 [==========>] Loss 0.08858865109508045  - accuracy: 0.9375\n",
      "At: 1964 [==========>] Loss 0.18484597781971826  - accuracy: 0.65625\n",
      "At: 1965 [==========>] Loss 0.14989586162245866  - accuracy: 0.75\n",
      "At: 1966 [==========>] Loss 0.13017101443676346  - accuracy: 0.78125\n",
      "At: 1967 [==========>] Loss 0.1360321229319032  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.19970169136509308  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.15791491905459037  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.09671288622115695  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.20101111297859844  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.0887054429502065  - accuracy: 0.84375\n",
      "At: 1973 [==========>] Loss 0.15001526800994647  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.1437962562739462  - accuracy: 0.78125\n",
      "At: 1975 [==========>] Loss 0.16952844100949468  - accuracy: 0.75\n",
      "At: 1976 [==========>] Loss 0.06046954635809128  - accuracy: 0.9375\n",
      "At: 1977 [==========>] Loss 0.10311255522158047  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.15067201066274116  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.11603240715989796  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.136328229398801  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.15798908374247805  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.06263942554963385  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.12939431273568225  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.07580886997200223  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.14366362039893876  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.17624718638632714  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.10997906295178159  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.10947963352069448  - accuracy: 0.84375\n",
      "At: 1989 [==========>] Loss 0.09131125225612141  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.11594143797438317  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.14704556757768494  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.112426299749315  - accuracy: 0.8125\n",
      "At: 1993 [==========>] Loss 0.1465375869694944  - accuracy: 0.84375\n",
      "At: 1994 [==========>] Loss 0.12996157503128314  - accuracy: 0.78125\n",
      "At: 1995 [==========>] Loss 0.19847080696980843  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.08500638383483719  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.19492840255649635  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.15263314556333177  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.08636019368561301  - accuracy: 0.875\n",
      "At: 2000 [==========>] Loss 0.14231223317646602  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.09331974620769533  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.08792972767218807  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.12052803936566822  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.14049635579082476  - accuracy: 0.8125\n",
      "At: 2005 [==========>] Loss 0.1467468921646017  - accuracy: 0.8125\n",
      "At: 2006 [==========>] Loss 0.12404031182502742  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.12962035808117556  - accuracy: 0.8125\n",
      "At: 2008 [==========>] Loss 0.1436688922770163  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.1450062394329023  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.11816091992454208  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.10827633919120407  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.10863570122320092  - accuracy: 0.8125\n",
      "At: 2013 [==========>] Loss 0.0713223128448823  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.19827472785475272  - accuracy: 0.6875\n",
      "At: 2015 [==========>] Loss 0.057695634170317416  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.11782676393108346  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.09903477994176887  - accuracy: 0.84375\n",
      "At: 2018 [==========>] Loss 0.08410249554323035  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.13440846467379008  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.0699082066220197  - accuracy: 0.90625\n",
      "At: 2021 [==========>] Loss 0.12324767179035667  - accuracy: 0.84375\n",
      "At: 2022 [==========>] Loss 0.11829596273055355  - accuracy: 0.84375\n",
      "At: 2023 [==========>] Loss 0.08966260964572492  - accuracy: 0.9375\n",
      "At: 2024 [==========>] Loss 0.1029032032227441  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.15557094337254013  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.0949149409418242  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.13572154056451835  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.1262883939555739  - accuracy: 0.78125\n",
      "At: 2029 [==========>] Loss 0.1296156283501888  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.1429030956122101  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.16614279852683633  - accuracy: 0.78125\n",
      "At: 2032 [==========>] Loss 0.1503303668329126  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.14791051133787403  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.21556349650646991  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.1171368486982324  - accuracy: 0.84375\n",
      "At: 2036 [==========>] Loss 0.11058225248686704  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.12302222223752646  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.11947348339898778  - accuracy: 0.78125\n",
      "At: 2039 [==========>] Loss 0.10233935003531269  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.11653075572687749  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.05848402553619545  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.11643690466798444  - accuracy: 0.78125\n",
      "At: 2043 [==========>] Loss 0.09024740653304035  - accuracy: 0.90625\n",
      "At: 2044 [==========>] Loss 0.07908254738246763  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.24079418405104816  - accuracy: 0.71875\n",
      "At: 2046 [==========>] Loss 0.07552928454555793  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.07946131415467517  - accuracy: 0.875\n",
      "At: 2048 [==========>] Loss 0.129565764701693  - accuracy: 0.8125\n",
      "At: 2049 [==========>] Loss 0.13876357733794467  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.19574029105102675  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.18783996017833435  - accuracy: 0.71875\n",
      "At: 2052 [==========>] Loss 0.06469234384468368  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.10332828697950538  - accuracy: 0.8125\n",
      "At: 2054 [==========>] Loss 0.11573656780235358  - accuracy: 0.8125\n",
      "At: 2055 [==========>] Loss 0.08157903808712012  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.12204860546160627  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.1162020348590789  - accuracy: 0.84375\n",
      "At: 2058 [==========>] Loss 0.12506189706284598  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.21580288203236705  - accuracy: 0.65625\n",
      "At: 2060 [==========>] Loss 0.11042748322741344  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.1290443887086249  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.12770952752877301  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.09990248809704477  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.18804979038450775  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.06332477970411785  - accuracy: 0.90625\n",
      "At: 2066 [==========>] Loss 0.15821460553416947  - accuracy: 0.75\n",
      "At: 2067 [==========>] Loss 0.08626813795309944  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.11294980840684979  - accuracy: 0.78125\n",
      "At: 2069 [==========>] Loss 0.10519525538125263  - accuracy: 0.8125\n",
      "At: 2070 [==========>] Loss 0.12131213132987592  - accuracy: 0.84375\n",
      "At: 2071 [==========>] Loss 0.12571347715926887  - accuracy: 0.78125\n",
      "At: 2072 [==========>] Loss 0.08187678783476748  - accuracy: 0.875\n",
      "At: 2073 [==========>] Loss 0.12731940010391457  - accuracy: 0.78125\n",
      "At: 2074 [==========>] Loss 0.11237371024756604  - accuracy: 0.8125\n",
      "At: 2075 [==========>] Loss 0.11156559426098697  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.12963653004682485  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.1613337836849058  - accuracy: 0.75\n",
      "At: 2078 [==========>] Loss 0.10541746352186934  - accuracy: 0.875\n",
      "At: 2079 [==========>] Loss 0.10249003335325763  - accuracy: 0.84375\n",
      "At: 2080 [==========>] Loss 0.10988496944980274  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.1550944587261481  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.1193941940196538  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.2043594962488948  - accuracy: 0.6875\n",
      "At: 2084 [==========>] Loss 0.09930060087269192  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.10173585340566788  - accuracy: 0.875\n",
      "At: 2086 [==========>] Loss 0.12113431379637221  - accuracy: 0.84375\n",
      "At: 2087 [==========>] Loss 0.16650564210157237  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.08659389426295572  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.13166169478005438  - accuracy: 0.8125\n",
      "At: 2090 [==========>] Loss 0.08784768441503613  - accuracy: 0.90625\n",
      "At: 2091 [==========>] Loss 0.1462070128950505  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.0847946485941257  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.14938090279483957  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.12853122262602473  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.12042883660526993  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.1616772665117785  - accuracy: 0.71875\n",
      "At: 2097 [==========>] Loss 0.12362488998391372  - accuracy: 0.8125\n",
      "At: 2098 [==========>] Loss 0.12514147854260624  - accuracy: 0.75\n",
      "At: 2099 [==========>] Loss 0.11197691689495451  - accuracy: 0.75\n",
      "At: 2100 [==========>] Loss 0.05230488588180386  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.13889043368449075  - accuracy: 0.84375\n",
      "At: 2102 [==========>] Loss 0.09126662411009437  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.1409156897574454  - accuracy: 0.8125\n",
      "At: 2104 [==========>] Loss 0.11503375757095435  - accuracy: 0.84375\n",
      "At: 2105 [==========>] Loss 0.15599912491635506  - accuracy: 0.6875\n",
      "At: 2106 [==========>] Loss 0.154841343206595  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.0827573640233569  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.11856185255356678  - accuracy: 0.84375\n",
      "At: 2109 [==========>] Loss 0.124951137766863  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.06426000607237899  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.10898677982706438  - accuracy: 0.90625\n",
      "At: 2112 [==========>] Loss 0.11385883384476822  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.09217490276962162  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.12875300157067748  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.11325471226004064  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.13326599112560517  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.11636106664209844  - accuracy: 0.75\n",
      "At: 2118 [==========>] Loss 0.13237460851280486  - accuracy: 0.78125\n",
      "At: 2119 [==========>] Loss 0.0898009857855685  - accuracy: 0.90625\n",
      "At: 2120 [==========>] Loss 0.13735116650824952  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.13943945176864056  - accuracy: 0.78125\n",
      "At: 2122 [==========>] Loss 0.1324665867395569  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.18693427755764574  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.13268225263492295  - accuracy: 0.84375\n",
      "At: 2125 [==========>] Loss 0.1048765636653449  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.0771382775737093  - accuracy: 0.875\n",
      "At: 2127 [==========>] Loss 0.09877680676250472  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.12098029357364073  - accuracy: 0.8125\n",
      "At: 2129 [==========>] Loss 0.15757347766260527  - accuracy: 0.84375\n",
      "At: 2130 [==========>] Loss 0.07029123895751248  - accuracy: 0.84375\n",
      "At: 2131 [==========>] Loss 0.09070196966128138  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.21278743067715478  - accuracy: 0.71875\n",
      "At: 2133 [==========>] Loss 0.15613975129156787  - accuracy: 0.8125\n",
      "At: 2134 [==========>] Loss 0.11941042635659893  - accuracy: 0.78125\n",
      "At: 2135 [==========>] Loss 0.0930205902600244  - accuracy: 0.84375\n",
      "At: 2136 [==========>] Loss 0.14173415759107885  - accuracy: 0.78125\n",
      "At: 2137 [==========>] Loss 0.11220965019477375  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.11132432440234963  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.2079965584658225  - accuracy: 0.71875\n",
      "At: 2140 [==========>] Loss 0.08136480903796095  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.12454298869872363  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.11892014747303453  - accuracy: 0.8125\n",
      "At: 2143 [==========>] Loss 0.08771264468228915  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.0786690160997921  - accuracy: 0.875\n",
      "At: 2145 [==========>] Loss 0.10501798652860928  - accuracy: 0.8125\n",
      "At: 2146 [==========>] Loss 0.15850641968630066  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.12924643142895656  - accuracy: 0.8125\n",
      "At: 2148 [==========>] Loss 0.18139457394625544  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.11915046982878608  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.1287360358188639  - accuracy: 0.8125\n",
      "At: 2151 [==========>] Loss 0.11744299989588818  - accuracy: 0.875\n",
      "At: 2152 [==========>] Loss 0.21441370213866845  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.20406814811432705  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.14302873741347163  - accuracy: 0.8125\n",
      "At: 2155 [==========>] Loss 0.12626033827420047  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.09603893512103921  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.13464170667923359  - accuracy: 0.78125\n",
      "At: 2158 [==========>] Loss 0.14397506715097885  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.06656418198887146  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.13746635656388329  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.08913927772144412  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.11798297318027628  - accuracy: 0.78125\n",
      "At: 2163 [==========>] Loss 0.13416063478464244  - accuracy: 0.8125\n",
      "At: 2164 [==========>] Loss 0.15506621210998384  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.1012367596826817  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.1101310643534554  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.09379703607766432  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.08505473367379479  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.11502173741376984  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11792999166268997  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.1567138464870326  - accuracy: 0.6875\n",
      "At: 2172 [==========>] Loss 0.09158673540453459  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.1530248058472351  - accuracy: 0.75\n",
      "At: 2174 [==========>] Loss 0.10963049719531168  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.10842712898079761  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.11086399667296769  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.13337032081865136  - accuracy: 0.84375\n",
      "At: 2178 [==========>] Loss 0.1000212592369186  - accuracy: 0.84375\n",
      "At: 2179 [==========>] Loss 0.13625783665347993  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.12356058146535195  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.1649996953547253  - accuracy: 0.75\n",
      "At: 2182 [==========>] Loss 0.12280047344062382  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.22973058993785017  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.08763142395676149  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.1059301646941759  - accuracy: 0.875\n",
      "At: 2186 [==========>] Loss 0.16601867608693313  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.16331695407898003  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.0746944706988989  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.08348413297355067  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.14009387427372533  - accuracy: 0.75\n",
      "At: 2191 [==========>] Loss 0.12104928269239575  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.0864952897518607  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.14320739868462382  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.08736602449957429  - accuracy: 0.875\n",
      "At: 2195 [==========>] Loss 0.18208568172285688  - accuracy: 0.6875\n",
      "At: 2196 [==========>] Loss 0.12264001195699568  - accuracy: 0.84375\n",
      "At: 2197 [==========>] Loss 0.09133406819919046  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.09855918118053497  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.05759156671115408  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.044831729889416735  - accuracy: 0.96875\n",
      "At: 2201 [==========>] Loss 0.11327200073117344  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.10210748649561732  - accuracy: 0.84375\n",
      "At: 2203 [==========>] Loss 0.09334320621620223  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.10424900218096733  - accuracy: 0.875\n",
      "At: 2205 [==========>] Loss 0.12100496638619428  - accuracy: 0.8125\n",
      "At: 2206 [==========>] Loss 0.10230870908960912  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.08201826701783918  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.138331756272285  - accuracy: 0.8125\n",
      "At: 2209 [==========>] Loss 0.21007883510500056  - accuracy: 0.6875\n",
      "At: 2210 [==========>] Loss 0.12343659079114958  - accuracy: 0.875\n",
      "At: 2211 [==========>] Loss 0.1555949917918596  - accuracy: 0.78125\n",
      "At: 2212 [==========>] Loss 0.10217555722562544  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.11449139555457415  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.13598043787601038  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.12139165587930102  - accuracy: 0.84375\n",
      "At: 2216 [==========>] Loss 0.14826376306844447  - accuracy: 0.8125\n",
      "At: 2217 [==========>] Loss 0.12355376702620939  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.15520669093498524  - accuracy: 0.84375\n",
      "At: 2219 [==========>] Loss 0.08072479458256414  - accuracy: 0.875\n",
      "At: 2220 [==========>] Loss 0.11468131006812352  - accuracy: 0.8125\n",
      "At: 2221 [==========>] Loss 0.17070279649114073  - accuracy: 0.75\n",
      "At: 2222 [==========>] Loss 0.09368420674564214  - accuracy: 0.875\n",
      "At: 2223 [==========>] Loss 0.14677926221874024  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.16632254984961942  - accuracy: 0.78125\n",
      "At: 2225 [==========>] Loss 0.12497420761100675  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.12685844964262588  - accuracy: 0.75\n",
      "At: 2227 [==========>] Loss 0.18927617532249297  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.08617975136674771  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.16506862747010592  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.12059354073601569  - accuracy: 0.90625\n",
      "At: 2231 [==========>] Loss 0.1491749532340606  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.1684495107417115  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.1607754301194714  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.13950594849348394  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.14592441900826153  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.09553094127889242  - accuracy: 0.84375\n",
      "At: 2237 [==========>] Loss 0.13700666187564964  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.14327585285735014  - accuracy: 0.84375\n",
      "At: 2239 [==========>] Loss 0.18108004872599448  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.13363311941392533  - accuracy: 0.75\n",
      "At: 2241 [==========>] Loss 0.12224084090714277  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.15805888090055126  - accuracy: 0.875\n",
      "At: 2243 [==========>] Loss 0.08306909657628324  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.09206317803027023  - accuracy: 0.90625\n",
      "At: 2245 [==========>] Loss 0.08098773608333543  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.14276955538061042  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.11147959575425818  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.1775953591914402  - accuracy: 0.75\n",
      "At: 2249 [==========>] Loss 0.10485212939944652  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.09917825171150238  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.08901673131531848  - accuracy: 0.84375\n",
      "At: 2252 [==========>] Loss 0.12473072267711086  - accuracy: 0.84375\n",
      "At: 2253 [==========>] Loss 0.12775779430259496  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.12176280825194107  - accuracy: 0.75\n",
      "At: 2255 [==========>] Loss 0.12557587837725004  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.14541861860367714  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.08225879878033915  - accuracy: 0.875\n",
      "At: 2258 [==========>] Loss 0.13828822334122276  - accuracy: 0.71875\n",
      "At: 2259 [==========>] Loss 0.13506381353618058  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.18323098180970956  - accuracy: 0.75\n",
      "At: 2261 [==========>] Loss 0.10141998512121575  - accuracy: 0.875\n",
      "At: 2262 [==========>] Loss 0.16606879837346586  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.1559957175976962  - accuracy: 0.78125\n",
      "At: 2264 [==========>] Loss 0.11458977071449468  - accuracy: 0.8125\n",
      "At: 2265 [==========>] Loss 0.13556922017753953  - accuracy: 0.78125\n",
      "At: 2266 [==========>] Loss 0.11546889538786909  - accuracy: 0.875\n",
      "At: 2267 [==========>] Loss 0.075231476015365  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.12141258524682118  - accuracy: 0.875\n",
      "At: 2269 [==========>] Loss 0.046538348781926975  - accuracy: 0.9375\n",
      "At: 2270 [==========>] Loss 0.09059422956214823  - accuracy: 0.90625\n",
      "At: 2271 [==========>] Loss 0.1601089142812797  - accuracy: 0.75\n",
      "At: 2272 [==========>] Loss 0.09490803752667495  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.10474698484205333  - accuracy: 0.90625\n",
      "At: 2274 [==========>] Loss 0.10493115276003373  - accuracy: 0.84375\n",
      "At: 2275 [==========>] Loss 0.10748362027455688  - accuracy: 0.84375\n",
      "At: 2276 [==========>] Loss 0.0835537976989549  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.1405774051567779  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.08397740539919797  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.14281225232182498  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.1474195092188849  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.11710341744406964  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.06433488250197512  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.17029389456198296  - accuracy: 0.6875\n",
      "At: 2284 [==========>] Loss 0.14283638704260532  - accuracy: 0.625\n",
      "At: 2285 [==========>] Loss 0.13052782533615728  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.12672665905526892  - accuracy: 0.875\n",
      "At: 2287 [==========>] Loss 0.1385497653902059  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.09781276698429649  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.17759738263905703  - accuracy: 0.8125\n",
      "At: 2290 [==========>] Loss 0.05991357027686848  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.14185765911180176  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.08537282553718586  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.06527964318195223  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.06609421241689377  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.1580957334867177  - accuracy: 0.8125\n",
      "At: 2296 [==========>] Loss 0.16188599958585245  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.07812679403795043  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.11203029052999569  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.10510920263431606  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.11227311886820317  - accuracy: 0.8125\n",
      "At: 2301 [==========>] Loss 0.2024439523471671  - accuracy: 0.65625\n",
      "At: 2302 [==========>] Loss 0.17247214980066775  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.05251623859671542  - accuracy: 0.96875\n",
      "At: 2304 [==========>] Loss 0.08859720206639196  - accuracy: 0.90625\n",
      "At: 2305 [==========>] Loss 0.12037835652368055  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.1283075785267236  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.16361854778348767  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.18611847305285706  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.13696290021489743  - accuracy: 0.78125\n",
      "At: 2310 [==========>] Loss 0.13208491650977827  - accuracy: 0.84375\n",
      "At: 2311 [==========>] Loss 0.1368976878851098  - accuracy: 0.8125\n",
      "At: 2312 [==========>] Loss 0.11009493885942762  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.08365312060365773  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.12005238626673961  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.1284195801657818  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.14916962870064843  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.1601809617864356  - accuracy: 0.78125\n",
      "At: 2318 [==========>] Loss 0.19277638956511806  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.11704291580854873  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.11406238971694116  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.14466633461715478  - accuracy: 0.75\n",
      "At: 2322 [==========>] Loss 0.16488760470003316  - accuracy: 0.75\n",
      "At: 2323 [==========>] Loss 0.152508904526171  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.176714644842824  - accuracy: 0.71875\n",
      "At: 2325 [==========>] Loss 0.13273817147016842  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.06730867590185322  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.05511546619469389  - accuracy: 0.96875\n",
      "At: 2328 [==========>] Loss 0.1212861059192098  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.1078175192235676  - accuracy: 0.8125\n",
      "At: 2330 [==========>] Loss 0.16594332520203758  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.1058147121769268  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.10850106549184552  - accuracy: 0.8125\n",
      "At: 2333 [==========>] Loss 0.12107688095626269  - accuracy: 0.84375\n",
      "At: 2334 [==========>] Loss 0.180309389719048  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.09445319165818578  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.09453194204740992  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.15552989930566116  - accuracy: 0.71875\n",
      "At: 2338 [==========>] Loss 0.10880849062312317  - accuracy: 0.84375\n",
      "At: 2339 [==========>] Loss 0.08333200957404972  - accuracy: 0.9375\n",
      "At: 2340 [==========>] Loss 0.15874093420738736  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.12582738964153323  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.157223763844279  - accuracy: 0.78125\n",
      "At: 2343 [==========>] Loss 0.0993566249400748  - accuracy: 0.90625\n",
      "At: 2344 [==========>] Loss 0.17862627625696542  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.14133729586510613  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.0896762361315345  - accuracy: 0.875\n",
      "At: 2347 [==========>] Loss 0.13181335348615802  - accuracy: 0.78125\n",
      "At: 2348 [==========>] Loss 0.06055412827427699  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.10000632109837604  - accuracy: 0.8125\n",
      "At: 2350 [==========>] Loss 0.12017031418540167  - accuracy: 0.8125\n",
      "At: 2351 [==========>] Loss 0.09192544330497894  - accuracy: 0.90625\n",
      "At: 2352 [==========>] Loss 0.11431989158675115  - accuracy: 0.8125\n",
      "At: 2353 [==========>] Loss 0.09988063105460185  - accuracy: 0.78125\n",
      "At: 2354 [==========>] Loss 0.15970601275308316  - accuracy: 0.75\n",
      "At: 2355 [==========>] Loss 0.05029799534170118  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.1209432771307772  - accuracy: 0.78125\n",
      "At: 2357 [==========>] Loss 0.13576669019937543  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.1728912993784907  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.1958937557559116  - accuracy: 0.59375\n",
      "At: 2360 [==========>] Loss 0.12081775538547397  - accuracy: 0.875\n",
      "At: 2361 [==========>] Loss 0.15193406522154404  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.0777776989628029  - accuracy: 0.84375\n",
      "At: 2363 [==========>] Loss 0.16915227204438485  - accuracy: 0.8125\n",
      "At: 2364 [==========>] Loss 0.06759378943588588  - accuracy: 0.9375\n",
      "At: 2365 [==========>] Loss 0.09832920166414263  - accuracy: 0.84375\n",
      "At: 2366 [==========>] Loss 0.21154816380588803  - accuracy: 0.65625\n",
      "At: 2367 [==========>] Loss 0.09641407082631796  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.0952359933709239  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.10300008473499941  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.08919650333785235  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.08821252927379407  - accuracy: 0.90625\n",
      "At: 2372 [==========>] Loss 0.14160780436137665  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.10850373593461798  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.10857505505793552  - accuracy: 0.875\n",
      "At: 2375 [==========>] Loss 0.049237765623393034  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.10002953801314102  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.10304687420613692  - accuracy: 0.875\n",
      "At: 2378 [==========>] Loss 0.1321209982408237  - accuracy: 0.78125\n",
      "At: 2379 [==========>] Loss 0.1469796842709925  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.05145915789139296  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.10076331861800292  - accuracy: 0.875\n",
      "At: 2382 [==========>] Loss 0.10317635263136274  - accuracy: 0.8125\n",
      "At: 2383 [==========>] Loss 0.14039595525064175  - accuracy: 0.78125\n",
      "At: 2384 [==========>] Loss 0.09649828564377169  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.11386253680085215  - accuracy: 0.875\n",
      "At: 2386 [==========>] Loss 0.11148024154741984  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.08300338553271175  - accuracy: 0.875\n",
      "At: 2388 [==========>] Loss 0.13314475023874625  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.044365838758044514  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.061298897197352015  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.17389149606540114  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.107389446165563  - accuracy: 0.875\n",
      "At: 2393 [==========>] Loss 0.09252512893642327  - accuracy: 0.90625\n",
      "At: 2394 [==========>] Loss 0.05809696715991861  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.10121326530710781  - accuracy: 0.875\n",
      "At: 2396 [==========>] Loss 0.06821396946710534  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.08524181826570737  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.09718673968515831  - accuracy: 0.84375\n",
      "At: 2399 [==========>] Loss 0.1537681522898923  - accuracy: 0.8125\n",
      "At: 2400 [==========>] Loss 0.09207574507858711  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.10801804265933106  - accuracy: 0.84375\n",
      "At: 2402 [==========>] Loss 0.11695993584443845  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.18294303220721417  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.1572667455851129  - accuracy: 0.8125\n",
      "At: 2405 [==========>] Loss 0.06949223226724466  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.1429825575739388  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.11326165338725525  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.103206787364788  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.1557036070865197  - accuracy: 0.84375\n",
      "At: 2410 [==========>] Loss 0.1778760181415509  - accuracy: 0.78125\n",
      "At: 2411 [==========>] Loss 0.09703479765049264  - accuracy: 0.84375\n",
      "At: 2412 [==========>] Loss 0.09757253354665334  - accuracy: 0.8125\n",
      "At: 2413 [==========>] Loss 0.07095746533706272  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.08119705491605561  - accuracy: 0.875\n",
      "At: 2415 [==========>] Loss 0.09774091878653278  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.11555080010082111  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.16787946961438563  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.10559803970747465  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.14922076861077005  - accuracy: 0.78125\n",
      "At: 2420 [==========>] Loss 0.13599113644200778  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.08993134122814962  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.16988669612666984  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.13398976349148806  - accuracy: 0.78125\n",
      "At: 2424 [==========>] Loss 0.1020029112146909  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08609931483550479  - accuracy: 0.9375\n",
      "At: 2426 [==========>] Loss 0.1906633569198928  - accuracy: 0.625\n",
      "At: 2427 [==========>] Loss 0.1438603820394945  - accuracy: 0.78125\n",
      "At: 2428 [==========>] Loss 0.07018938523309444  - accuracy: 0.9375\n",
      "At: 2429 [==========>] Loss 0.1191955309951858  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.15094563303120767  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.13683848831473214  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.07585167109511373  - accuracy: 0.90625\n",
      "At: 2433 [==========>] Loss 0.053841128845626265  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.04629157691553755  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.14962712773981157  - accuracy: 0.8125\n",
      "At: 2436 [==========>] Loss 0.1240297442997134  - accuracy: 0.8125\n",
      "At: 2437 [==========>] Loss 0.1771151581257938  - accuracy: 0.75\n",
      "At: 2438 [==========>] Loss 0.11020553985848083  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.13933267905705066  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.10079975965830135  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.1265959011698154  - accuracy: 0.78125\n",
      "At: 2442 [==========>] Loss 0.15197210040774256  - accuracy: 0.75\n",
      "At: 2443 [==========>] Loss 0.11481445497103929  - accuracy: 0.8125\n",
      "At: 2444 [==========>] Loss 0.08397660437159518  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.08626274296375741  - accuracy: 0.875\n",
      "At: 2446 [==========>] Loss 0.1640198214825342  - accuracy: 0.8125\n",
      "At: 2447 [==========>] Loss 0.16161125668873752  - accuracy: 0.71875\n",
      "At: 2448 [==========>] Loss 0.11499280618756177  - accuracy: 0.875\n",
      "At: 2449 [==========>] Loss 0.08112621078542587  - accuracy: 0.875\n",
      "At: 2450 [==========>] Loss 0.06355853122517456  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.06929061448333532  - accuracy: 0.875\n",
      "At: 2452 [==========>] Loss 0.12140436140516991  - accuracy: 0.875\n",
      "At: 2453 [==========>] Loss 0.10624878433938185  - accuracy: 0.875\n",
      "At: 2454 [==========>] Loss 0.15884241096426127  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1539200716808783  - accuracy: 0.78125\n",
      "At: 2456 [==========>] Loss 0.13713705454733344  - accuracy: 0.75\n",
      "At: 2457 [==========>] Loss 0.15078300131463296  - accuracy: 0.8125\n",
      "At: 2458 [==========>] Loss 0.07303087600204435  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.10836412287722635  - accuracy: 0.875\n",
      "At: 2460 [==========>] Loss 0.06813226783877002  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.06849440314856697  - accuracy: 0.90625\n",
      "At: 2462 [==========>] Loss 0.14798828547319365  - accuracy: 0.78125\n",
      "At: 2463 [==========>] Loss 0.11024149765704895  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.15823354172772344  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.14812844942234787  - accuracy: 0.78125\n",
      "At: 2466 [==========>] Loss 0.0935754474659396  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.08835000744223215  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.09220422343055762  - accuracy: 0.90625\n",
      "At: 2469 [==========>] Loss 0.14908646892663138  - accuracy: 0.8125\n",
      "At: 2470 [==========>] Loss 0.12871475585021153  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.09417029165555009  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.10374318828245893  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.11743783790472326  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.06173088695940521  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.11271009049840586  - accuracy: 0.90625\n",
      "At: 2476 [==========>] Loss 0.07944867012549615  - accuracy: 0.875\n",
      "At: 2477 [==========>] Loss 0.1419452595352539  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.09532800870252207  - accuracy: 0.90625\n",
      "At: 2479 [==========>] Loss 0.09438546169809324  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.11203644671518367  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.0736165459102647  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.16019978334352636  - accuracy: 0.75\n",
      "At: 2483 [==========>] Loss 0.12557601766394066  - accuracy: 0.8125\n",
      "At: 2484 [==========>] Loss 0.09205183869948805  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.0894424699439686  - accuracy: 0.90625\n",
      "At: 2486 [==========>] Loss 0.12454510597593473  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.13296193022779856  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.1645372679337102  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.18272525778838275  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.11210348351533213  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.13303512231081296  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.12249277278961329  - accuracy: 0.78125\n",
      "At: 2493 [==========>] Loss 0.07353766019105053  - accuracy: 0.90625\n",
      "At: 2494 [==========>] Loss 0.10695750859321204  - accuracy: 0.84375\n",
      "At: 2495 [==========>] Loss 0.09474779440652127  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.05828938407324985  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.20846858749779124  - accuracy: 0.65625\n",
      "At: 2498 [==========>] Loss 0.0941700129314096  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.057811944689952544  - accuracy: 0.9375\n",
      "At: 2500 [==========>] Loss 0.1706182976406847  - accuracy: 0.6875\n",
      "At: 2501 [==========>] Loss 0.15179089353706413  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.12579359700942988  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.12499347454915123  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.1411746400473759  - accuracy: 0.8125\n",
      "At: 2505 [==========>] Loss 0.09471797212470652  - accuracy: 0.875\n",
      "At: 2506 [==========>] Loss 0.1304404847448354  - accuracy: 0.84375\n",
      "At: 2507 [==========>] Loss 0.13802340477117364  - accuracy: 0.8125\n",
      "At: 2508 [==========>] Loss 0.1360349322006328  - accuracy: 0.78125\n",
      "At: 2509 [==========>] Loss 0.1572859546772562  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.13385520140750473  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.16220296986073127  - accuracy: 0.71875\n",
      "At: 2512 [==========>] Loss 0.10228673892655368  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.16180380105021752  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.10936540388829587  - accuracy: 0.84375\n",
      "At: 2515 [==========>] Loss 0.1918999820618223  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.17157308105418054  - accuracy: 0.71875\n",
      "At: 2517 [==========>] Loss 0.11087077931999473  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.12279696966931652  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.11540427673336472  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.13458947300402097  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.12327412456705222  - accuracy: 0.78125\n",
      "At: 2522 [==========>] Loss 0.2321756667607088  - accuracy: 0.6875\n",
      "At: 2523 [==========>] Loss 0.11700192060609692  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.1824958402196663  - accuracy: 0.78125\n",
      "At: 2525 [==========>] Loss 0.07034044006639312  - accuracy: 0.84375\n",
      "At: 2526 [==========>] Loss 0.1286539630157844  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.12910528113856512  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.10636607502275507  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.13199652636922138  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.13577118943621141  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.045809425590275565  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.11742269272759331  - accuracy: 0.8125\n",
      "At: 2533 [==========>] Loss 0.10843043577082298  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.07529518664198287  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.07992286259113122  - accuracy: 0.90625\n",
      "At: 2536 [==========>] Loss 0.17187140229652095  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.12897261375232516  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.16084182382187961  - accuracy: 0.71875\n",
      "At: 2539 [==========>] Loss 0.09782837524095656  - accuracy: 0.875\n",
      "At: 2540 [==========>] Loss 0.1463162918090451  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.06731699929630866  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.05231641245141162  - accuracy: 0.96875\n",
      "At: 2543 [==========>] Loss 0.16524509348704436  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.15706057264691198  - accuracy: 0.8125\n",
      "At: 2545 [==========>] Loss 0.07224093456225619  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.15490277959055015  - accuracy: 0.8125\n",
      "At: 2547 [==========>] Loss 0.07311037775744206  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.11702151613943844  - accuracy: 0.8125\n",
      "At: 2549 [==========>] Loss 0.10815001819431709  - accuracy: 0.84375\n",
      "At: 2550 [==========>] Loss 0.11574309077294645  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.13595021910001015  - accuracy: 0.75\n",
      "At: 2552 [==========>] Loss 0.10445715610500056  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.07017447886455744  - accuracy: 0.9375\n",
      "At: 2554 [==========>] Loss 0.1362100733518643  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.1844872398987356  - accuracy: 0.6875\n",
      "At: 2556 [==========>] Loss 0.091678354436309  - accuracy: 0.9375\n",
      "At: 2557 [==========>] Loss 0.13420818008350274  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.10208450878704019  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.09724655619532731  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.17131925675965262  - accuracy: 0.75\n",
      "At: 2561 [==========>] Loss 0.08298607696260019  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.11298644738036595  - accuracy: 0.84375\n",
      "At: 2563 [==========>] Loss 0.1284524244553506  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.10830375575510995  - accuracy: 0.8125\n",
      "At: 2565 [==========>] Loss 0.1251775097177737  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.17582995973489843  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.11708759143437425  - accuracy: 0.875\n",
      "At: 2568 [==========>] Loss 0.11793771715590359  - accuracy: 0.78125\n",
      "At: 2569 [==========>] Loss 0.0761391716460265  - accuracy: 0.96875\n",
      "At: 2570 [==========>] Loss 0.18126159826967322  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.08972892528986627  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.17576947033560641  - accuracy: 0.71875\n",
      "At: 2573 [==========>] Loss 0.21138463462652285  - accuracy: 0.6875\n",
      "At: 2574 [==========>] Loss 0.1350568488619551  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.07281516628856188  - accuracy: 0.875\n",
      "At: 2576 [==========>] Loss 0.10342800615891634  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.22814081549295365  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.17632920924708845  - accuracy: 0.75\n",
      "At: 2579 [==========>] Loss 0.17198301191818915  - accuracy: 0.6875\n",
      "At: 2580 [==========>] Loss 0.14379423558569449  - accuracy: 0.8125\n",
      "At: 2581 [==========>] Loss 0.06582594058037206  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.19237194188475165  - accuracy: 0.75\n",
      "At: 2583 [==========>] Loss 0.09120764422242775  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.1856615842146546  - accuracy: 0.71875\n",
      "At: 2585 [==========>] Loss 0.10200529664016544  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.07545381700949277  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.17156771546961647  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.14410768808238456  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.13167711121654962  - accuracy: 0.8125\n",
      "At: 2590 [==========>] Loss 0.17366315411854447  - accuracy: 0.8125\n",
      "At: 2591 [==========>] Loss 0.12665212399771555  - accuracy: 0.875\n",
      "At: 2592 [==========>] Loss 0.11625064758224204  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.07271723538430494  - accuracy: 0.9375\n",
      "At: 2594 [==========>] Loss 0.08629374787616081  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.10281305214416724  - accuracy: 0.84375\n",
      "At: 2596 [==========>] Loss 0.11296216979798362  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.08259022140703028  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.12163935896306656  - accuracy: 0.8125\n",
      "At: 2599 [==========>] Loss 0.14551292301861918  - accuracy: 0.71875\n",
      "At: 2600 [==========>] Loss 0.13320953198921884  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.16898228322646136  - accuracy: 0.75\n",
      "At: 2602 [==========>] Loss 0.08857867567227214  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.12383156153602294  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.08493942836709928  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.12722007140754904  - accuracy: 0.78125\n",
      "At: 2606 [==========>] Loss 0.13506613281652852  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.1236013699879677  - accuracy: 0.875\n",
      "At: 2608 [==========>] Loss 0.07477945887732626  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.08970092663353349  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.14503644969869445  - accuracy: 0.78125\n",
      "At: 2611 [==========>] Loss 0.1316560132095645  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.09742684985975544  - accuracy: 0.84375\n",
      "At: 2613 [==========>] Loss 0.07906313203718718  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.11220894699174164  - accuracy: 0.875\n",
      "At: 2615 [==========>] Loss 0.08338770357376132  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.05671672757276858  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.08037465834114671  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.07096734740426278  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.07859287122146918  - accuracy: 0.90625\n",
      "At: 2620 [==========>] Loss 0.10719351007848313  - accuracy: 0.84375\n",
      "At: 2621 [==========>] Loss 0.0959436906935586  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.11523131394477486  - accuracy: 0.8125\n",
      "At: 2623 [==========>] Loss 0.09084028786858059  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.1388062022997437  - accuracy: 0.78125\n",
      "At: 2625 [==========>] Loss 0.06286190462899349  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.061607994379218406  - accuracy: 0.875\n",
      "At: 2627 [==========>] Loss 0.1369719890942409  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.08414602183681619  - accuracy: 0.875\n",
      "At: 2629 [==========>] Loss 0.09479084432745871  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.08474908410610867  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.15106496470175124  - accuracy: 0.75\n",
      "At: 2632 [==========>] Loss 0.12583705516719024  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.10067781322833363  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.06964941680652201  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.20636043597943984  - accuracy: 0.625\n",
      "At: 2636 [==========>] Loss 0.1361429025786174  - accuracy: 0.8125\n",
      "At: 2637 [==========>] Loss 0.13925168400543125  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.07455707092215832  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.08695718818933476  - accuracy: 0.90625\n",
      "At: 2640 [==========>] Loss 0.13223229300086145  - accuracy: 0.8125\n",
      "At: 2641 [==========>] Loss 0.14995505443581592  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.19119185081222007  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.08300129436084337  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.1542951665654205  - accuracy: 0.84375\n",
      "At: 2645 [==========>] Loss 0.05546440323091159  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.1490645721699157  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.09983679187252545  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.1472035225856193  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.10602780772515326  - accuracy: 0.84375\n",
      "At: 2650 [==========>] Loss 0.12145109067513109  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.16385740161012097  - accuracy: 0.75\n",
      "At: 2652 [==========>] Loss 0.11421175920927468  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.1168345198127958  - accuracy: 0.875\n",
      "At: 2654 [==========>] Loss 0.14330343433781179  - accuracy: 0.75\n",
      "At: 2655 [==========>] Loss 0.24037820650788574  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.033946173865757906  - accuracy: 0.9375\n",
      "At: 2657 [==========>] Loss 0.11618676773492731  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.09495414628636356  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.06772402540714763  - accuracy: 0.875\n",
      "At: 2660 [==========>] Loss 0.10752899011501657  - accuracy: 0.8125\n",
      "At: 2661 [==========>] Loss 0.09087534279753168  - accuracy: 0.84375\n",
      "At: 2662 [==========>] Loss 0.06960677282282925  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.0918807921547568  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.09408584326243642  - accuracy: 0.90625\n",
      "At: 2665 [==========>] Loss 0.11000242279900563  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.1468563935595  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.12579778597698643  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.17546516928968076  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.15782335856882923  - accuracy: 0.75\n",
      "At: 2670 [==========>] Loss 0.12082728832000891  - accuracy: 0.8125\n",
      "At: 2671 [==========>] Loss 0.07286355743295429  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.10352335563330205  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.12733843345248252  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.10369377612152794  - accuracy: 0.8125\n",
      "At: 2675 [==========>] Loss 0.14401266159947512  - accuracy: 0.71875\n",
      "At: 2676 [==========>] Loss 0.11280992789714647  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.11186514287697688  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.06256564495104419  - accuracy: 0.875\n",
      "At: 2679 [==========>] Loss 0.08481826070162013  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.08245721980907092  - accuracy: 0.90625\n",
      "At: 2681 [==========>] Loss 0.09783742823947499  - accuracy: 0.875\n",
      "At: 2682 [==========>] Loss 0.12179602530776507  - accuracy: 0.875\n",
      "At: 2683 [==========>] Loss 0.19255352325874997  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09573803914598597  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.12010177106147639  - accuracy: 0.8125\n",
      "At: 2686 [==========>] Loss 0.061816490777928154  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.1289628749816679  - accuracy: 0.8125\n",
      "At: 2688 [==========>] Loss 0.17874564393208464  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.11239640959791568  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.13572112297932712  - accuracy: 0.78125\n",
      "Epochs  2 / 10\n",
      "At: 1 [==========>] Loss 0.18626885209599522  - accuracy: 0.75\n",
      "At: 2 [==========>] Loss 0.2648190623621237  - accuracy: 0.5625\n",
      "At: 3 [==========>] Loss 0.16405912172165388  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.23836206225268877  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.10077801331187207  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.1547862511567461  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.18405406309889716  - accuracy: 0.71875\n",
      "At: 8 [==========>] Loss 0.22060123562061312  - accuracy: 0.75\n",
      "At: 9 [==========>] Loss 0.33525103535031797  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.2883468444747357  - accuracy: 0.59375\n",
      "At: 11 [==========>] Loss 0.22135276455226566  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.23023554847532698  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.23289829027570913  - accuracy: 0.75\n",
      "At: 14 [==========>] Loss 0.10780366810699964  - accuracy: 0.875\n",
      "At: 15 [==========>] Loss 0.19495538681406882  - accuracy: 0.8125\n",
      "At: 16 [==========>] Loss 0.2022069889091939  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.19783694414363506  - accuracy: 0.75\n",
      "At: 18 [==========>] Loss 0.25623676565836073  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.19823604275208756  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.15884933843717058  - accuracy: 0.84375\n",
      "At: 21 [==========>] Loss 0.2562575629427955  - accuracy: 0.6875\n",
      "At: 22 [==========>] Loss 0.21014177807026718  - accuracy: 0.6875\n",
      "At: 23 [==========>] Loss 0.10765665112938248  - accuracy: 0.875\n",
      "At: 24 [==========>] Loss 0.291853686928009  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.24317428254155793  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.2527517060449112  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.24702392901851508  - accuracy: 0.65625\n",
      "At: 28 [==========>] Loss 0.17105453492826103  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.2130293520268412  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.2484046160118789  - accuracy: 0.75\n",
      "At: 31 [==========>] Loss 0.3109801617877634  - accuracy: 0.59375\n",
      "At: 32 [==========>] Loss 0.24014688255547095  - accuracy: 0.6875\n",
      "At: 33 [==========>] Loss 0.1059703463799399  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.13745283705746597  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.1957094455987562  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.24720740878568961  - accuracy: 0.6875\n",
      "At: 37 [==========>] Loss 0.2599766063511304  - accuracy: 0.71875\n",
      "At: 38 [==========>] Loss 0.25155710677412624  - accuracy: 0.59375\n",
      "At: 39 [==========>] Loss 0.19689608542010684  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.2557512522107209  - accuracy: 0.65625\n",
      "At: 41 [==========>] Loss 0.13461799359260018  - accuracy: 0.84375\n",
      "At: 42 [==========>] Loss 0.15593480656264558  - accuracy: 0.75\n",
      "At: 43 [==========>] Loss 0.19562158110854416  - accuracy: 0.75\n",
      "At: 44 [==========>] Loss 0.1983132707374603  - accuracy: 0.71875\n",
      "At: 45 [==========>] Loss 0.08948913396937655  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.2402445884447022  - accuracy: 0.71875\n",
      "At: 47 [==========>] Loss 0.23247603545261286  - accuracy: 0.6875\n",
      "At: 48 [==========>] Loss 0.17832956018596044  - accuracy: 0.75\n",
      "At: 49 [==========>] Loss 0.14959703440559557  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.2086049878538743  - accuracy: 0.75\n",
      "At: 51 [==========>] Loss 0.23387423926340617  - accuracy: 0.6875\n",
      "At: 52 [==========>] Loss 0.25960122586547085  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.21235151684404005  - accuracy: 0.6875\n",
      "At: 54 [==========>] Loss 0.17644714610462098  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.2582268092535466  - accuracy: 0.65625\n",
      "At: 56 [==========>] Loss 0.18949924249197916  - accuracy: 0.71875\n",
      "At: 57 [==========>] Loss 0.1640536980543571  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.24465697476493775  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.20037794075008458  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.22963418449799325  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.28384694876732164  - accuracy: 0.625\n",
      "At: 62 [==========>] Loss 0.21975611496190517  - accuracy: 0.71875\n",
      "At: 63 [==========>] Loss 0.1966409188194784  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.2327413503498543  - accuracy: 0.65625\n",
      "At: 65 [==========>] Loss 0.2827863935213264  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.22332085409647315  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.2599860773863088  - accuracy: 0.6875\n",
      "At: 68 [==========>] Loss 0.14315144830775225  - accuracy: 0.8125\n",
      "At: 69 [==========>] Loss 0.17019947354180168  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.23539674672956398  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.21576721617306815  - accuracy: 0.71875\n",
      "At: 72 [==========>] Loss 0.17696235701850466  - accuracy: 0.78125\n",
      "At: 73 [==========>] Loss 0.19302776383002254  - accuracy: 0.78125\n",
      "At: 74 [==========>] Loss 0.23021959765244585  - accuracy: 0.75\n",
      "At: 75 [==========>] Loss 0.23544235298475788  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.2766670200455149  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.24732958090350657  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.18091679605386285  - accuracy: 0.78125\n",
      "At: 79 [==========>] Loss 0.17942769518852553  - accuracy: 0.78125\n",
      "At: 80 [==========>] Loss 0.2626288250809169  - accuracy: 0.65625\n",
      "At: 81 [==========>] Loss 0.19100023850907527  - accuracy: 0.75\n",
      "At: 82 [==========>] Loss 0.2822482488308905  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.1711531919760588  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.24052525001856176  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.2506253273300276  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.18703626561426606  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.19609803523618763  - accuracy: 0.75\n",
      "At: 88 [==========>] Loss 0.3799773229113499  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.19023216892715897  - accuracy: 0.78125\n",
      "At: 90 [==========>] Loss 0.20314373323206886  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.19994009584299083  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.08637551430493105  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.13356975483472341  - accuracy: 0.875\n",
      "At: 94 [==========>] Loss 0.15455859932308047  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.17594352303849808  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.14778282209120036  - accuracy: 0.84375\n",
      "At: 97 [==========>] Loss 0.11830291145831176  - accuracy: 0.84375\n",
      "At: 98 [==========>] Loss 0.26860742126572257  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.12730289898972927  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.1315253916543092  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.14264313444237486  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.2112638989348461  - accuracy: 0.75\n",
      "At: 103 [==========>] Loss 0.1864496010089945  - accuracy: 0.75\n",
      "At: 104 [==========>] Loss 0.15753521064953202  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.2159788544024121  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.22968686614911232  - accuracy: 0.65625\n",
      "At: 107 [==========>] Loss 0.21673166058640742  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.28022701147378587  - accuracy: 0.59375\n",
      "At: 109 [==========>] Loss 0.11008018563065966  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.28261784171047843  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.10147644403898337  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.1405726161931166  - accuracy: 0.84375\n",
      "At: 113 [==========>] Loss 0.20010582014968248  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.17835627248427152  - accuracy: 0.78125\n",
      "At: 115 [==========>] Loss 0.17200351980368428  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.2462398498214966  - accuracy: 0.71875\n",
      "At: 117 [==========>] Loss 0.18278418638129734  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.2671296368208359  - accuracy: 0.6875\n",
      "At: 119 [==========>] Loss 0.12077221862458756  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.17582868401269935  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.13448859484323256  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.18772990308410753  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.21140260512843798  - accuracy: 0.71875\n",
      "At: 124 [==========>] Loss 0.2502008242980521  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.19217595054788242  - accuracy: 0.75\n",
      "At: 126 [==========>] Loss 0.20840432136065168  - accuracy: 0.75\n",
      "At: 127 [==========>] Loss 0.23464198072874906  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.29057641750711405  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.15455988349208788  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.24961139187542702  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.21454320081046546  - accuracy: 0.71875\n",
      "At: 132 [==========>] Loss 0.25788581144628603  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.23106060952574076  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.17209601512576586  - accuracy: 0.75\n",
      "At: 135 [==========>] Loss 0.18627624666836198  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.18260325098149047  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.09946045272351606  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.19065182793394242  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.13043152436812946  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.15704277970930136  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.30680125932121616  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.18562516643363944  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.2031194693269382  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.14077300213925198  - accuracy: 0.84375\n",
      "At: 145 [==========>] Loss 0.13175571839952172  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.1282169454667341  - accuracy: 0.8125\n",
      "At: 147 [==========>] Loss 0.19868178843851003  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.14756768911060741  - accuracy: 0.8125\n",
      "At: 149 [==========>] Loss 0.2189925845002001  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.17401485351754306  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.2067877308414589  - accuracy: 0.75\n",
      "At: 152 [==========>] Loss 0.11153197262028554  - accuracy: 0.8125\n",
      "At: 153 [==========>] Loss 0.1961818738286179  - accuracy: 0.78125\n",
      "At: 154 [==========>] Loss 0.17716860307231053  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.2181904157934705  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1253326934985875  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.2584231112730038  - accuracy: 0.75\n",
      "At: 158 [==========>] Loss 0.17558105279297703  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.16571749019809673  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.16046677235743873  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.15782660250203367  - accuracy: 0.8125\n",
      "At: 162 [==========>] Loss 0.2390803705862229  - accuracy: 0.65625\n",
      "At: 163 [==========>] Loss 0.20196208386904563  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.18947603855978104  - accuracy: 0.75\n",
      "At: 165 [==========>] Loss 0.24082916445452224  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.2165735659520778  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.14539903455319703  - accuracy: 0.8125\n",
      "At: 168 [==========>] Loss 0.2002783599176968  - accuracy: 0.78125\n",
      "At: 169 [==========>] Loss 0.172515083864401  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.16309411117289382  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.2175548271699745  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.19671482204363472  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.275016140106072  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.159938450280026  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.17650434719724672  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.1982257038688282  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.13234289277736516  - accuracy: 0.84375\n",
      "At: 178 [==========>] Loss 0.2350770881146197  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.1678838231872866  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.19225128051769824  - accuracy: 0.78125\n",
      "At: 181 [==========>] Loss 0.06857410815296536  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.19338800236053194  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.15360670463751458  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.19825503090258934  - accuracy: 0.71875\n",
      "At: 185 [==========>] Loss 0.12304232632351139  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.1802968355505286  - accuracy: 0.75\n",
      "At: 187 [==========>] Loss 0.17283724801124548  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.19797328557552496  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.2391919307943268  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.15723434403118977  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.35019333075410397  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.1922574271698475  - accuracy: 0.75\n",
      "At: 193 [==========>] Loss 0.2266254523992783  - accuracy: 0.71875\n",
      "At: 194 [==========>] Loss 0.203747036802196  - accuracy: 0.71875\n",
      "At: 195 [==========>] Loss 0.17349666394526253  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.20079210591562777  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.15584644890776134  - accuracy: 0.84375\n",
      "At: 198 [==========>] Loss 0.15190799916991438  - accuracy: 0.78125\n",
      "At: 199 [==========>] Loss 0.11421332290186692  - accuracy: 0.84375\n",
      "At: 200 [==========>] Loss 0.22916015795169808  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.1424280920880456  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.11486543289114204  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.18878029088233472  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.18709297906660818  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.14278984828354122  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.11085027131669828  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.1304009419343548  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.24551480522264318  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.18526165346719692  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.10835122489912782  - accuracy: 0.90625\n",
      "At: 211 [==========>] Loss 0.20135388582171904  - accuracy: 0.75\n",
      "At: 212 [==========>] Loss 0.19097880050968802  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.19810984919209707  - accuracy: 0.75\n",
      "At: 214 [==========>] Loss 0.23332475987618334  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.1310167715557385  - accuracy: 0.8125\n",
      "At: 216 [==========>] Loss 0.17304902156672064  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.26624450028162683  - accuracy: 0.625\n",
      "At: 218 [==========>] Loss 0.1672082488720621  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.23454146329399578  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.23698719432145793  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.17801120290232864  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.07684293253567759  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.30014188027435584  - accuracy: 0.625\n",
      "At: 224 [==========>] Loss 0.19530338081040904  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.15543326241368038  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.19696496975331979  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.20278194632622387  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.18463119604096564  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.20629207346707595  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.14434364740359565  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.2450871667396879  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.2390868784208662  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.22693975402156977  - accuracy: 0.71875\n",
      "At: 234 [==========>] Loss 0.11745035710546209  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.2101953390739483  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.20600757786634463  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.14129943728574842  - accuracy: 0.8125\n",
      "At: 238 [==========>] Loss 0.14931542182343674  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.1265473189304912  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.2507253040947494  - accuracy: 0.65625\n",
      "At: 241 [==========>] Loss 0.13284401785617914  - accuracy: 0.875\n",
      "At: 242 [==========>] Loss 0.15174881123440406  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.12370203674423108  - accuracy: 0.90625\n",
      "At: 244 [==========>] Loss 0.15106064002833977  - accuracy: 0.84375\n",
      "At: 245 [==========>] Loss 0.10631370369359167  - accuracy: 0.84375\n",
      "At: 246 [==========>] Loss 0.1597133583009029  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.17402590006426832  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.12891947151725808  - accuracy: 0.84375\n",
      "At: 249 [==========>] Loss 0.09503046117638919  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.2511564569075416  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.23880439853272006  - accuracy: 0.6875\n",
      "At: 252 [==========>] Loss 0.08008409556011867  - accuracy: 0.9375\n",
      "At: 253 [==========>] Loss 0.20400383760841662  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.09070999970245057  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.16446202001836513  - accuracy: 0.84375\n",
      "At: 256 [==========>] Loss 0.22772996225990422  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.13076245447369972  - accuracy: 0.8125\n",
      "At: 258 [==========>] Loss 0.2003737714496599  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.1898813897779954  - accuracy: 0.78125\n",
      "At: 260 [==========>] Loss 0.1643237858186495  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.10467289033645263  - accuracy: 0.875\n",
      "At: 262 [==========>] Loss 0.16386800321978529  - accuracy: 0.75\n",
      "At: 263 [==========>] Loss 0.11801042705531455  - accuracy: 0.90625\n",
      "At: 264 [==========>] Loss 0.14131907773734168  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.23533649041277654  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.28631912571007617  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.14612472467222798  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.21684930382176995  - accuracy: 0.75\n",
      "At: 269 [==========>] Loss 0.10846039146054431  - accuracy: 0.875\n",
      "At: 270 [==========>] Loss 0.32061420771352056  - accuracy: 0.5625\n",
      "At: 271 [==========>] Loss 0.19594306619523352  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.10861383573404526  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.21461482478564442  - accuracy: 0.71875\n",
      "At: 274 [==========>] Loss 0.20153234046167154  - accuracy: 0.71875\n",
      "At: 275 [==========>] Loss 0.0825046860056893  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.20256447904859376  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.11437850708340726  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.14694691685159433  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.13172135341061322  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.15755350856944167  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.14830390162586146  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.2148631609605317  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.16559313344063648  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.10816053482245759  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.1750740694639995  - accuracy: 0.71875\n",
      "At: 286 [==========>] Loss 0.09146783575619899  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.19507661649104813  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.12183827220471494  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.12283147277707873  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.11433405398372044  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.14586805639853725  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.2078619770242484  - accuracy: 0.75\n",
      "At: 293 [==========>] Loss 0.22953581885684332  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.19214641681716782  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.19964141092928833  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.1474794346690811  - accuracy: 0.84375\n",
      "At: 297 [==========>] Loss 0.12171624873106757  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.1595737616263028  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.19677144947125846  - accuracy: 0.6875\n",
      "At: 300 [==========>] Loss 0.19127121694133392  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.14771402455526017  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.12516550858382178  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.1071451581645364  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.15574372935299383  - accuracy: 0.78125\n",
      "At: 305 [==========>] Loss 0.24947378117511415  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.1300294624727248  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.29524143400943204  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.15338783001883177  - accuracy: 0.78125\n",
      "At: 309 [==========>] Loss 0.08351558726398771  - accuracy: 0.9375\n",
      "At: 310 [==========>] Loss 0.2545722621766989  - accuracy: 0.6875\n",
      "At: 311 [==========>] Loss 0.06417471416336248  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.1410025000327324  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.12656600698942583  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.22694605988665245  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.15835174642765848  - accuracy: 0.8125\n",
      "At: 316 [==========>] Loss 0.23588803489940005  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.3500805831192132  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.14064851975938347  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.12273591679974019  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.2221084222198671  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.19711776928182978  - accuracy: 0.75\n",
      "At: 322 [==========>] Loss 0.1121906760358107  - accuracy: 0.90625\n",
      "At: 323 [==========>] Loss 0.11103728147698345  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.15637568681146158  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.1337009458170345  - accuracy: 0.84375\n",
      "At: 326 [==========>] Loss 0.21254473233477517  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.13005942456677258  - accuracy: 0.8125\n",
      "At: 328 [==========>] Loss 0.1297806102740368  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.14034013445255966  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.1622768254281793  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.19884805118993387  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.2717486822616149  - accuracy: 0.625\n",
      "At: 333 [==========>] Loss 0.1537830729711993  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.10515305828417593  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.1435581750172628  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.15443864072588995  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.2077500268422778  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.1394692963671811  - accuracy: 0.75\n",
      "At: 339 [==========>] Loss 0.19392471696091435  - accuracy: 0.71875\n",
      "At: 340 [==========>] Loss 0.1396555937508402  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.14547967446889715  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.20082768506893273  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.2684375194035821  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.22294622399551114  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.21692455168383296  - accuracy: 0.6875\n",
      "At: 346 [==========>] Loss 0.1399771755513997  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.11015641675870319  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.13146447908166994  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.18174098257880789  - accuracy: 0.78125\n",
      "At: 350 [==========>] Loss 0.13154899730858527  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.3193815217602074  - accuracy: 0.5625\n",
      "At: 352 [==========>] Loss 0.12214504953723454  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.17368382229315055  - accuracy: 0.78125\n",
      "At: 354 [==========>] Loss 0.19890918050234213  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.10945152227856442  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.19615289392404942  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.15246009953368134  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.14520255926548775  - accuracy: 0.8125\n",
      "At: 359 [==========>] Loss 0.0905265292117221  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.19823242871189933  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.09840607895731536  - accuracy: 0.8125\n",
      "At: 362 [==========>] Loss 0.16834284851457404  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.12789105029968223  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.2137670142628414  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.15636193832557088  - accuracy: 0.8125\n",
      "At: 366 [==========>] Loss 0.1899729202625143  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.18436962493831988  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.18786540632722945  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.15548562419552125  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.1786804253944178  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.12245012221690638  - accuracy: 0.8125\n",
      "At: 372 [==========>] Loss 0.11742832425495955  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.23986537138053998  - accuracy: 0.71875\n",
      "At: 374 [==========>] Loss 0.05843465784032808  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.18766751747748056  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.0891204201796218  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.23305423963173277  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.1708279103164924  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.12940589573550892  - accuracy: 0.78125\n",
      "At: 380 [==========>] Loss 0.13296671684560155  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.1843154425294013  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.09832683233773987  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.19556404240441766  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.16882058792146665  - accuracy: 0.8125\n",
      "At: 385 [==========>] Loss 0.1841558742666884  - accuracy: 0.78125\n",
      "At: 386 [==========>] Loss 0.23014038079490398  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.08264244436936669  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.2408481101013122  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.19463160906995403  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.11880988687874464  - accuracy: 0.875\n",
      "At: 391 [==========>] Loss 0.12647738355505256  - accuracy: 0.84375\n",
      "At: 392 [==========>] Loss 0.14254015870113723  - accuracy: 0.78125\n",
      "At: 393 [==========>] Loss 0.2661306194127795  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.09927599119715907  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.1965548944720677  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.15800435205892505  - accuracy: 0.8125\n",
      "At: 397 [==========>] Loss 0.17349136036541182  - accuracy: 0.75\n",
      "At: 398 [==========>] Loss 0.23351361657748312  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.21394233087525125  - accuracy: 0.71875\n",
      "At: 400 [==========>] Loss 0.2225354093633276  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.131837794568568  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.12820323839866504  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.03988980675863539  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.12621815862562186  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.20514134711409943  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.1572756573854397  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.14076155032798454  - accuracy: 0.84375\n",
      "At: 408 [==========>] Loss 0.1948057372945689  - accuracy: 0.75\n",
      "At: 409 [==========>] Loss 0.2538794013302098  - accuracy: 0.65625\n",
      "At: 410 [==========>] Loss 0.1762761616996664  - accuracy: 0.78125\n",
      "At: 411 [==========>] Loss 0.0999050757424865  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.22010704807882125  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.09708223518238775  - accuracy: 0.875\n",
      "At: 414 [==========>] Loss 0.18557389953480008  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.1518934703227645  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.1954462190220192  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.16223059184515448  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.14540239782975123  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.12463943180328196  - accuracy: 0.84375\n",
      "At: 420 [==========>] Loss 0.17325162797201094  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.13047910210812486  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.12458353482998054  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.13088309489164807  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.23413825614188238  - accuracy: 0.65625\n",
      "At: 425 [==========>] Loss 0.20402296376274864  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.15654283489168688  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.1853030063048132  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.2810419742756628  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.1822022491974521  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.1212259954882651  - accuracy: 0.84375\n",
      "At: 431 [==========>] Loss 0.11383611552231673  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.15389485381082513  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.11310338785778185  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.15929180692805014  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.22035810679345078  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.17702505906208793  - accuracy: 0.75\n",
      "At: 437 [==========>] Loss 0.15731666637542668  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.15697913894670115  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.1081052177993309  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.09094163374881689  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.20887030184941685  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.2083483576750057  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.1529721864986177  - accuracy: 0.84375\n",
      "At: 444 [==========>] Loss 0.1888054722539534  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.18089047248149642  - accuracy: 0.75\n",
      "At: 446 [==========>] Loss 0.2480036981472659  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.1647891589516046  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.2133708611622641  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.11970335141134766  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.15492364916371434  - accuracy: 0.78125\n",
      "At: 451 [==========>] Loss 0.14265671950356462  - accuracy: 0.71875\n",
      "At: 452 [==========>] Loss 0.1364012365990408  - accuracy: 0.84375\n",
      "At: 453 [==========>] Loss 0.11951086736326907  - accuracy: 0.84375\n",
      "At: 454 [==========>] Loss 0.20391479840676527  - accuracy: 0.75\n",
      "At: 455 [==========>] Loss 0.17813056276440642  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.1518949746180001  - accuracy: 0.78125\n",
      "At: 457 [==========>] Loss 0.16749048942828074  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.13041970481606308  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.25845129195147676  - accuracy: 0.65625\n",
      "At: 460 [==========>] Loss 0.14306770682577508  - accuracy: 0.78125\n",
      "At: 461 [==========>] Loss 0.21440671242791995  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.19391881650177045  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.1769799856117519  - accuracy: 0.78125\n",
      "At: 464 [==========>] Loss 0.23877908813424603  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.22315318298488757  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.11345565780321348  - accuracy: 0.8125\n",
      "At: 467 [==========>] Loss 0.21816331290267676  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.10849181824700498  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.15860576548821434  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.09211639376350378  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.19006763759966167  - accuracy: 0.71875\n",
      "At: 472 [==========>] Loss 0.13339508184509946  - accuracy: 0.78125\n",
      "At: 473 [==========>] Loss 0.1604359359061449  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.14689530518475596  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.17847905377910547  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.18948500128307005  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.1512821551341048  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.13902997253664848  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.17069348763583722  - accuracy: 0.78125\n",
      "At: 480 [==========>] Loss 0.2189160386049661  - accuracy: 0.6875\n",
      "At: 481 [==========>] Loss 0.11980623419386427  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.11864778241115398  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.12332035730550783  - accuracy: 0.84375\n",
      "At: 484 [==========>] Loss 0.12022049520251359  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.1490050714041411  - accuracy: 0.8125\n",
      "At: 486 [==========>] Loss 0.24287397518730552  - accuracy: 0.6875\n",
      "At: 487 [==========>] Loss 0.11195988441786214  - accuracy: 0.84375\n",
      "At: 488 [==========>] Loss 0.14782532256557657  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.17645263284922363  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.20206993077620833  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.17868445415650935  - accuracy: 0.8125\n",
      "At: 492 [==========>] Loss 0.2240085043301744  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.12302778314571501  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.2216641321044021  - accuracy: 0.65625\n",
      "At: 495 [==========>] Loss 0.13538377099134552  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.22711553950215504  - accuracy: 0.71875\n",
      "At: 497 [==========>] Loss 0.17559625917294414  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.0915098851737674  - accuracy: 0.875\n",
      "At: 499 [==========>] Loss 0.17645890662123043  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.14135718526453167  - accuracy: 0.84375\n",
      "At: 501 [==========>] Loss 0.17167744462451573  - accuracy: 0.6875\n",
      "At: 502 [==========>] Loss 0.12416443899636162  - accuracy: 0.8125\n",
      "At: 503 [==========>] Loss 0.1500763722350613  - accuracy: 0.8125\n",
      "At: 504 [==========>] Loss 0.11936387084735357  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.17984101501212588  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.2921364680046757  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.12266855799483634  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.1021029003061261  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.21078788041349425  - accuracy: 0.71875\n",
      "At: 510 [==========>] Loss 0.18434972979092934  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.1319766639478367  - accuracy: 0.84375\n",
      "At: 512 [==========>] Loss 0.18626361637540811  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.2507359202485287  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.1950352921828642  - accuracy: 0.71875\n",
      "At: 515 [==========>] Loss 0.08600650611372668  - accuracy: 0.90625\n",
      "At: 516 [==========>] Loss 0.19544171197229201  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.14444984446342868  - accuracy: 0.78125\n",
      "At: 518 [==========>] Loss 0.16915127546621234  - accuracy: 0.71875\n",
      "At: 519 [==========>] Loss 0.13029023431428172  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.16051492236135848  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.16248168806301977  - accuracy: 0.78125\n",
      "At: 522 [==========>] Loss 0.1489880820237712  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.20031782292807937  - accuracy: 0.6875\n",
      "At: 524 [==========>] Loss 0.14046011670685832  - accuracy: 0.8125\n",
      "At: 525 [==========>] Loss 0.19998178876581682  - accuracy: 0.65625\n",
      "At: 526 [==========>] Loss 0.2295524588140877  - accuracy: 0.6875\n",
      "At: 527 [==========>] Loss 0.2633009524558385  - accuracy: 0.625\n",
      "At: 528 [==========>] Loss 0.19444271644297828  - accuracy: 0.75\n",
      "At: 529 [==========>] Loss 0.12821609798485728  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.17960593329302293  - accuracy: 0.75\n",
      "At: 531 [==========>] Loss 0.1532143505550603  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.1018096754696712  - accuracy: 0.90625\n",
      "At: 533 [==========>] Loss 0.09575341224942588  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.17676753223102967  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.12381878015769114  - accuracy: 0.78125\n",
      "At: 536 [==========>] Loss 0.18742208486158507  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.10801433160610383  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.1934155810299099  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.1321879209654952  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.2431849205918281  - accuracy: 0.71875\n",
      "At: 541 [==========>] Loss 0.1588948876661469  - accuracy: 0.75\n",
      "At: 542 [==========>] Loss 0.16643024743786214  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.160241486345417  - accuracy: 0.75\n",
      "At: 544 [==========>] Loss 0.28482440879098936  - accuracy: 0.625\n",
      "At: 545 [==========>] Loss 0.12288583965431393  - accuracy: 0.84375\n",
      "At: 546 [==========>] Loss 0.18706420037893906  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.16550167932833354  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.11486004515250722  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.15526632585710426  - accuracy: 0.75\n",
      "At: 550 [==========>] Loss 0.13108006812012027  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.13358914854286152  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.1642505975689556  - accuracy: 0.75\n",
      "At: 553 [==========>] Loss 0.13681917941088406  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.09659192720110626  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.18259207130377195  - accuracy: 0.6875\n",
      "At: 556 [==========>] Loss 0.12859811851746147  - accuracy: 0.84375\n",
      "At: 557 [==========>] Loss 0.14867508437820778  - accuracy: 0.75\n",
      "At: 558 [==========>] Loss 0.1373955146092317  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.23612022066835445  - accuracy: 0.65625\n",
      "At: 560 [==========>] Loss 0.15912308552924262  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.15710015462962437  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.07255820761410978  - accuracy: 0.9375\n",
      "At: 563 [==========>] Loss 0.13645558788844342  - accuracy: 0.8125\n",
      "At: 564 [==========>] Loss 0.20681959285173962  - accuracy: 0.78125\n",
      "At: 565 [==========>] Loss 0.11234123990811937  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.15892109968284845  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.19045746288699844  - accuracy: 0.78125\n",
      "At: 568 [==========>] Loss 0.2312915825353479  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.16081170788561022  - accuracy: 0.78125\n",
      "At: 570 [==========>] Loss 0.13306747711717823  - accuracy: 0.8125\n",
      "At: 571 [==========>] Loss 0.14175643403006055  - accuracy: 0.78125\n",
      "At: 572 [==========>] Loss 0.15289793968157378  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.08840989850757469  - accuracy: 0.90625\n",
      "At: 574 [==========>] Loss 0.19007428966166498  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.15037412289274515  - accuracy: 0.78125\n",
      "At: 576 [==========>] Loss 0.0947393017614802  - accuracy: 0.90625\n",
      "At: 577 [==========>] Loss 0.18872626528306846  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.20634586717838713  - accuracy: 0.75\n",
      "At: 579 [==========>] Loss 0.11212612535337588  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.16426572321933705  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.1673497470562343  - accuracy: 0.8125\n",
      "At: 582 [==========>] Loss 0.14457998599071714  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.22295060323923138  - accuracy: 0.6875\n",
      "At: 584 [==========>] Loss 0.11198261768400036  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.1764687465457634  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.060250792519903534  - accuracy: 0.9375\n",
      "At: 587 [==========>] Loss 0.1585990067041944  - accuracy: 0.8125\n",
      "At: 588 [==========>] Loss 0.14813331443522065  - accuracy: 0.75\n",
      "At: 589 [==========>] Loss 0.19121913066637103  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.06898757004790812  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.13926748733083497  - accuracy: 0.78125\n",
      "At: 592 [==========>] Loss 0.09850078757103281  - accuracy: 0.875\n",
      "At: 593 [==========>] Loss 0.22162581105226117  - accuracy: 0.6875\n",
      "At: 594 [==========>] Loss 0.16001866278046425  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.18015530361870044  - accuracy: 0.78125\n",
      "At: 596 [==========>] Loss 0.125815765281806  - accuracy: 0.875\n",
      "At: 597 [==========>] Loss 0.2262121928662896  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.12827582896526202  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.16350640089134044  - accuracy: 0.75\n",
      "At: 600 [==========>] Loss 0.11245037019711918  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.13671877375142047  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.12700369088524965  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.17582843758849503  - accuracy: 0.6875\n",
      "At: 604 [==========>] Loss 0.25951366063331827  - accuracy: 0.71875\n",
      "At: 605 [==========>] Loss 0.08493610354051956  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.18542573114868133  - accuracy: 0.78125\n",
      "At: 607 [==========>] Loss 0.1245349630622857  - accuracy: 0.875\n",
      "At: 608 [==========>] Loss 0.14761611865273222  - accuracy: 0.8125\n",
      "At: 609 [==========>] Loss 0.13957554556334267  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.1642520977405849  - accuracy: 0.78125\n",
      "At: 611 [==========>] Loss 0.13553000731278472  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.1529884134679284  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.17730272385424867  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.11302579467417652  - accuracy: 0.875\n",
      "At: 615 [==========>] Loss 0.17076551417167948  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.1828061475364221  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.13936202632397385  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.2330443980042141  - accuracy: 0.6875\n",
      "At: 619 [==========>] Loss 0.15250639787145243  - accuracy: 0.8125\n",
      "At: 620 [==========>] Loss 0.1578321052953917  - accuracy: 0.8125\n",
      "At: 621 [==========>] Loss 0.07161614916234686  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.16433297374071626  - accuracy: 0.6875\n",
      "At: 623 [==========>] Loss 0.12400490763438242  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.08649785687048378  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.1388966833777146  - accuracy: 0.78125\n",
      "At: 626 [==========>] Loss 0.14696217384743188  - accuracy: 0.78125\n",
      "At: 627 [==========>] Loss 0.12917784920398645  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.14179202902403848  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.2313405847226537  - accuracy: 0.71875\n",
      "At: 630 [==========>] Loss 0.25409041213139605  - accuracy: 0.5625\n",
      "At: 631 [==========>] Loss 0.18140013971832944  - accuracy: 0.71875\n",
      "At: 632 [==========>] Loss 0.1414987752057798  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.20285476361219018  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.18470860186583188  - accuracy: 0.71875\n",
      "At: 635 [==========>] Loss 0.15757912280774422  - accuracy: 0.8125\n",
      "At: 636 [==========>] Loss 0.13233894417378173  - accuracy: 0.84375\n",
      "At: 637 [==========>] Loss 0.13369610218696798  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.14554602910187853  - accuracy: 0.875\n",
      "At: 639 [==========>] Loss 0.11953969574309697  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.22432110997143456  - accuracy: 0.65625\n",
      "At: 641 [==========>] Loss 0.17011910757277252  - accuracy: 0.75\n",
      "At: 642 [==========>] Loss 0.2239463135097565  - accuracy: 0.6875\n",
      "At: 643 [==========>] Loss 0.16466281436898322  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.08142551832995659  - accuracy: 0.84375\n",
      "At: 645 [==========>] Loss 0.1303764657333335  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.12980662175909707  - accuracy: 0.875\n",
      "At: 647 [==========>] Loss 0.18661160026986015  - accuracy: 0.71875\n",
      "At: 648 [==========>] Loss 0.1796658911343259  - accuracy: 0.75\n",
      "At: 649 [==========>] Loss 0.1757129175904113  - accuracy: 0.71875\n",
      "At: 650 [==========>] Loss 0.09757872852812392  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.15901142568997417  - accuracy: 0.75\n",
      "At: 652 [==========>] Loss 0.12303461809536131  - accuracy: 0.8125\n",
      "At: 653 [==========>] Loss 0.1569358496482507  - accuracy: 0.78125\n",
      "At: 654 [==========>] Loss 0.14962136378722374  - accuracy: 0.8125\n",
      "At: 655 [==========>] Loss 0.13273876340622975  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.12946124914168944  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.1341188916112013  - accuracy: 0.84375\n",
      "At: 658 [==========>] Loss 0.1523519423191837  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.16442488966872526  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.09404109151604528  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.18629792006130072  - accuracy: 0.78125\n",
      "At: 662 [==========>] Loss 0.11125672653785382  - accuracy: 0.875\n",
      "At: 663 [==========>] Loss 0.10264263493716849  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.1338125556642521  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.1944548858117583  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.19014718322811325  - accuracy: 0.6875\n",
      "At: 667 [==========>] Loss 0.11537735958050457  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.2047416499592672  - accuracy: 0.71875\n",
      "At: 669 [==========>] Loss 0.13404685170251499  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.24036100841400485  - accuracy: 0.6875\n",
      "At: 671 [==========>] Loss 0.11813454549507456  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.14666289540111754  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.0792548671788005  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.14513799348109344  - accuracy: 0.78125\n",
      "At: 675 [==========>] Loss 0.11777499230694932  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.20299246862072107  - accuracy: 0.75\n",
      "At: 677 [==========>] Loss 0.17928542234899764  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.15352393197398784  - accuracy: 0.78125\n",
      "At: 679 [==========>] Loss 0.13195446544144704  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.12020952146671514  - accuracy: 0.78125\n",
      "At: 681 [==========>] Loss 0.14537386985267933  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.1498267879702706  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.1322249707752033  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.14054759886621881  - accuracy: 0.8125\n",
      "At: 685 [==========>] Loss 0.1588115532744409  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.10556935171335582  - accuracy: 0.90625\n",
      "At: 687 [==========>] Loss 0.05151233606870757  - accuracy: 0.96875\n",
      "At: 688 [==========>] Loss 0.12988324039299737  - accuracy: 0.84375\n",
      "At: 689 [==========>] Loss 0.16324494926647953  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.1470051953431543  - accuracy: 0.78125\n",
      "At: 691 [==========>] Loss 0.11311308994039404  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.10733179397530192  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.15298553282488997  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.1973320204327837  - accuracy: 0.6875\n",
      "At: 695 [==========>] Loss 0.14209342002933562  - accuracy: 0.84375\n",
      "At: 696 [==========>] Loss 0.16211237276223087  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.19257878417968552  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.14464599824879043  - accuracy: 0.8125\n",
      "At: 699 [==========>] Loss 0.12194563838777546  - accuracy: 0.84375\n",
      "At: 700 [==========>] Loss 0.08615502809212995  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.16365550535091172  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.10511552235097427  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.1796489903062527  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.1817297132490226  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.21933748872260078  - accuracy: 0.65625\n",
      "At: 706 [==========>] Loss 0.14757191477953208  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.08769259198841003  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.15775347273382923  - accuracy: 0.78125\n",
      "At: 709 [==========>] Loss 0.20009301083800707  - accuracy: 0.75\n",
      "At: 710 [==========>] Loss 0.13894506281831842  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.1900682492668329  - accuracy: 0.6875\n",
      "At: 712 [==========>] Loss 0.16811511841827276  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.22524377651608873  - accuracy: 0.6875\n",
      "At: 714 [==========>] Loss 0.21164296956228523  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.14318743146490048  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.12000428312183654  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.10764947825325721  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.16226528494411183  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.14107508524553858  - accuracy: 0.8125\n",
      "At: 720 [==========>] Loss 0.14044021380475197  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.09897117044282017  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.20331468217725004  - accuracy: 0.6875\n",
      "At: 723 [==========>] Loss 0.12223586723381995  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.06331431957324198  - accuracy: 0.9375\n",
      "At: 725 [==========>] Loss 0.17274522679997306  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.1923000600242528  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.12497724148833929  - accuracy: 0.84375\n",
      "At: 728 [==========>] Loss 0.17495058608918615  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.20338428692033114  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.20729581104456535  - accuracy: 0.71875\n",
      "At: 731 [==========>] Loss 0.17458019258086743  - accuracy: 0.6875\n",
      "At: 732 [==========>] Loss 0.1763549009554975  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.11749000216963223  - accuracy: 0.84375\n",
      "At: 734 [==========>] Loss 0.1876150002538945  - accuracy: 0.6875\n",
      "At: 735 [==========>] Loss 0.12370108795127951  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.10019196140947495  - accuracy: 0.84375\n",
      "At: 737 [==========>] Loss 0.15425023820288858  - accuracy: 0.8125\n",
      "At: 738 [==========>] Loss 0.10350629484712048  - accuracy: 0.8125\n",
      "At: 739 [==========>] Loss 0.061986139312705  - accuracy: 0.96875\n",
      "At: 740 [==========>] Loss 0.14686387799890588  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.15126515914703914  - accuracy: 0.8125\n",
      "At: 742 [==========>] Loss 0.14120613235677557  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.17334599142127802  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.14194897298409437  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.22065388986860301  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.15196706532048712  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.0971737497028916  - accuracy: 0.875\n",
      "At: 748 [==========>] Loss 0.17259113490512953  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.17560640279588452  - accuracy: 0.75\n",
      "At: 750 [==========>] Loss 0.1494649339853451  - accuracy: 0.78125\n",
      "At: 751 [==========>] Loss 0.11961994740770293  - accuracy: 0.84375\n",
      "At: 752 [==========>] Loss 0.07909252098639977  - accuracy: 0.9375\n",
      "At: 753 [==========>] Loss 0.17812623825329893  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.18770626670083956  - accuracy: 0.6875\n",
      "At: 755 [==========>] Loss 0.09208117350944883  - accuracy: 0.90625\n",
      "At: 756 [==========>] Loss 0.23180831867363683  - accuracy: 0.6875\n",
      "At: 757 [==========>] Loss 0.12297979832777127  - accuracy: 0.84375\n",
      "At: 758 [==========>] Loss 0.1422791934157258  - accuracy: 0.78125\n",
      "At: 759 [==========>] Loss 0.09154261894708418  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.19181864087608333  - accuracy: 0.65625\n",
      "At: 761 [==========>] Loss 0.17849933388101985  - accuracy: 0.75\n",
      "At: 762 [==========>] Loss 0.10299348464336006  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.16943428714882997  - accuracy: 0.8125\n",
      "At: 764 [==========>] Loss 0.14061653955595493  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.20282128329848587  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.1346191182798003  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.11509871001255356  - accuracy: 0.90625\n",
      "At: 768 [==========>] Loss 0.15657588000958717  - accuracy: 0.75\n",
      "At: 769 [==========>] Loss 0.13477504454155131  - accuracy: 0.8125\n",
      "At: 770 [==========>] Loss 0.14504555467481114  - accuracy: 0.8125\n",
      "At: 771 [==========>] Loss 0.2123989418816111  - accuracy: 0.6875\n",
      "At: 772 [==========>] Loss 0.1329908227959378  - accuracy: 0.8125\n",
      "At: 773 [==========>] Loss 0.08314167551684243  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.15498308687815673  - accuracy: 0.78125\n",
      "At: 775 [==========>] Loss 0.24427685772631297  - accuracy: 0.625\n",
      "At: 776 [==========>] Loss 0.18328862317770264  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.1140411700656365  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.1925593885193616  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.12903620481051134  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.09205664298246244  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.1718436077324828  - accuracy: 0.8125\n",
      "At: 782 [==========>] Loss 0.16362685894179302  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.19759158285034628  - accuracy: 0.71875\n",
      "At: 784 [==========>] Loss 0.21388651422024277  - accuracy: 0.65625\n",
      "At: 785 [==========>] Loss 0.23189371437545586  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.14487150699984097  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.16742931476990228  - accuracy: 0.8125\n",
      "At: 788 [==========>] Loss 0.07537129082311568  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.16128129115777423  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.11395649339860103  - accuracy: 0.84375\n",
      "At: 791 [==========>] Loss 0.20384551848336183  - accuracy: 0.71875\n",
      "At: 792 [==========>] Loss 0.20606331704095818  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.11498259551735046  - accuracy: 0.84375\n",
      "At: 794 [==========>] Loss 0.15200610746058574  - accuracy: 0.6875\n",
      "At: 795 [==========>] Loss 0.12343446221981215  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.14974784109586742  - accuracy: 0.84375\n",
      "At: 797 [==========>] Loss 0.19214920537122626  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.17749897527716652  - accuracy: 0.71875\n",
      "At: 799 [==========>] Loss 0.07903050068192433  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.15917228268103012  - accuracy: 0.78125\n",
      "At: 801 [==========>] Loss 0.12742258706532483  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.20016613473499031  - accuracy: 0.6875\n",
      "At: 803 [==========>] Loss 0.18989097925951798  - accuracy: 0.71875\n",
      "At: 804 [==========>] Loss 0.14753049383695355  - accuracy: 0.8125\n",
      "At: 805 [==========>] Loss 0.21215140339398356  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.11859588779099213  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.1297374738335731  - accuracy: 0.8125\n",
      "At: 808 [==========>] Loss 0.1401315402559213  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.09575794181629678  - accuracy: 0.90625\n",
      "At: 810 [==========>] Loss 0.16644186066739858  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.13506092871765818  - accuracy: 0.78125\n",
      "At: 812 [==========>] Loss 0.15085976016189181  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.21855351236167236  - accuracy: 0.71875\n",
      "At: 814 [==========>] Loss 0.21460888972372505  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.14941436480076142  - accuracy: 0.78125\n",
      "At: 816 [==========>] Loss 0.17340973248447158  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.08569682722927463  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.15534568432641022  - accuracy: 0.78125\n",
      "At: 819 [==========>] Loss 0.14511360590406655  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.1417359817558904  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.16151588326711114  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.16619239049412973  - accuracy: 0.71875\n",
      "At: 823 [==========>] Loss 0.1480675782773267  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.16538572782764543  - accuracy: 0.6875\n",
      "At: 825 [==========>] Loss 0.2101643169349981  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.15567936682305605  - accuracy: 0.78125\n",
      "At: 827 [==========>] Loss 0.08975962590652133  - accuracy: 0.9375\n",
      "At: 828 [==========>] Loss 0.07349968862938562  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.1055327546570261  - accuracy: 0.8125\n",
      "At: 830 [==========>] Loss 0.10058303437036256  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.08781460065807273  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.1359056309218628  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.18202543335875332  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.11465301776317038  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.09834326534958807  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.13652872789417614  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.14305176583309562  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.15046119942013875  - accuracy: 0.78125\n",
      "At: 839 [==========>] Loss 0.15150601585000148  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.14813251600441418  - accuracy: 0.78125\n",
      "At: 841 [==========>] Loss 0.0658748307818941  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.07495260714300407  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.17736878592056332  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.14568521498368003  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.1527617372585787  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.11530529519468495  - accuracy: 0.84375\n",
      "At: 847 [==========>] Loss 0.09445318095480969  - accuracy: 0.875\n",
      "At: 848 [==========>] Loss 0.15344031797274202  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.19772057921485514  - accuracy: 0.71875\n",
      "At: 850 [==========>] Loss 0.09789400111917118  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.10473883103561031  - accuracy: 0.78125\n",
      "At: 852 [==========>] Loss 0.18997455593248133  - accuracy: 0.71875\n",
      "At: 853 [==========>] Loss 0.17248892393463094  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.24078931791026525  - accuracy: 0.65625\n",
      "At: 855 [==========>] Loss 0.07810513904841368  - accuracy: 0.9375\n",
      "At: 856 [==========>] Loss 0.08193872385813765  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.07654658729817862  - accuracy: 0.96875\n",
      "At: 858 [==========>] Loss 0.23361557650101747  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.14751665657995428  - accuracy: 0.78125\n",
      "At: 860 [==========>] Loss 0.14165092206089058  - accuracy: 0.875\n",
      "At: 861 [==========>] Loss 0.10863194348404517  - accuracy: 0.84375\n",
      "At: 862 [==========>] Loss 0.11941091392627812  - accuracy: 0.84375\n",
      "At: 863 [==========>] Loss 0.16277527226596633  - accuracy: 0.71875\n",
      "At: 864 [==========>] Loss 0.17175654340002772  - accuracy: 0.6875\n",
      "At: 865 [==========>] Loss 0.21042457479433146  - accuracy: 0.71875\n",
      "At: 866 [==========>] Loss 0.1721919253606915  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.10146153039268341  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.17596656739184416  - accuracy: 0.75\n",
      "At: 869 [==========>] Loss 0.17914952859089514  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.16194068186156496  - accuracy: 0.75\n",
      "At: 871 [==========>] Loss 0.07701446268402228  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.10831233148604288  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.17735563449011138  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.15962143648086274  - accuracy: 0.75\n",
      "At: 875 [==========>] Loss 0.10091888528946559  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.12196097371062621  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.15258959820694773  - accuracy: 0.71875\n",
      "At: 878 [==========>] Loss 0.06129897751785629  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.14642120837144668  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.13816416775171608  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.16773311515857464  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.121749195943004  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.14605445359541286  - accuracy: 0.71875\n",
      "At: 884 [==========>] Loss 0.15990878997749614  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.14897841623006727  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.11931632082908006  - accuracy: 0.84375\n",
      "At: 887 [==========>] Loss 0.1783508630138382  - accuracy: 0.75\n",
      "At: 888 [==========>] Loss 0.15426003923800355  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.10883920030543795  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.19088953064046044  - accuracy: 0.75\n",
      "At: 891 [==========>] Loss 0.10035489182599408  - accuracy: 0.90625\n",
      "At: 892 [==========>] Loss 0.13810645712465458  - accuracy: 0.78125\n",
      "At: 893 [==========>] Loss 0.17138581615380527  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.1624237655256517  - accuracy: 0.75\n",
      "At: 895 [==========>] Loss 0.12611094908833453  - accuracy: 0.8125\n",
      "At: 896 [==========>] Loss 0.12873612998992068  - accuracy: 0.78125\n",
      "At: 897 [==========>] Loss 0.14125117808555732  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.1619065150938359  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.106473367042532  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.1673443090152071  - accuracy: 0.71875\n",
      "At: 901 [==========>] Loss 0.1768445369043709  - accuracy: 0.71875\n",
      "At: 902 [==========>] Loss 0.10489798639165088  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.1404108444428656  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.1444631223700426  - accuracy: 0.71875\n",
      "At: 905 [==========>] Loss 0.10171249460308254  - accuracy: 0.875\n",
      "At: 906 [==========>] Loss 0.09001059126364863  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.15924593896971118  - accuracy: 0.71875\n",
      "At: 908 [==========>] Loss 0.17616167650428932  - accuracy: 0.75\n",
      "At: 909 [==========>] Loss 0.10428816189860474  - accuracy: 0.90625\n",
      "At: 910 [==========>] Loss 0.1561136896044613  - accuracy: 0.75\n",
      "At: 911 [==========>] Loss 0.15338684549806422  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.1143239330979017  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.12596411309958044  - accuracy: 0.84375\n",
      "At: 914 [==========>] Loss 0.13153139062428496  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.1521700375100763  - accuracy: 0.8125\n",
      "At: 916 [==========>] Loss 0.18956516428941353  - accuracy: 0.71875\n",
      "At: 917 [==========>] Loss 0.184826537723494  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.17055241887863887  - accuracy: 0.6875\n",
      "At: 919 [==========>] Loss 0.1236607820447027  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.15744432986513718  - accuracy: 0.75\n",
      "At: 921 [==========>] Loss 0.1665791938694486  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.12784491290160158  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.12806410340792096  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.22737001857038128  - accuracy: 0.71875\n",
      "At: 925 [==========>] Loss 0.1490266909315392  - accuracy: 0.84375\n",
      "At: 926 [==========>] Loss 0.1162963526221856  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.1256694373609572  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.12535951125898517  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.13810891502260633  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.12567530059316173  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.17171907102243306  - accuracy: 0.8125\n",
      "At: 932 [==========>] Loss 0.09150622239324174  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.08150112520927685  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.11504009495366427  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.04480857649606185  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.18321882695976308  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.19510964437690792  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.1421297533308804  - accuracy: 0.84375\n",
      "At: 939 [==========>] Loss 0.12448481599206154  - accuracy: 0.84375\n",
      "At: 940 [==========>] Loss 0.2129410329621359  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.11715028389214349  - accuracy: 0.8125\n",
      "At: 942 [==========>] Loss 0.16854182673299134  - accuracy: 0.71875\n",
      "At: 943 [==========>] Loss 0.13623025282630413  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.10800028671552869  - accuracy: 0.8125\n",
      "At: 945 [==========>] Loss 0.14090937714830568  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.12054807678768487  - accuracy: 0.8125\n",
      "At: 947 [==========>] Loss 0.14357514331763727  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.1882313907584463  - accuracy: 0.6875\n",
      "At: 949 [==========>] Loss 0.055865299207001534  - accuracy: 0.9375\n",
      "At: 950 [==========>] Loss 0.12104706795922694  - accuracy: 0.84375\n",
      "At: 951 [==========>] Loss 0.11070855527626647  - accuracy: 0.875\n",
      "At: 952 [==========>] Loss 0.09392476763322617  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.06389065423297124  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.10796430481639607  - accuracy: 0.9375\n",
      "At: 955 [==========>] Loss 0.12489511059235396  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.10139916713879461  - accuracy: 0.875\n",
      "At: 957 [==========>] Loss 0.12676080427045663  - accuracy: 0.8125\n",
      "At: 958 [==========>] Loss 0.11094373590953319  - accuracy: 0.84375\n",
      "At: 959 [==========>] Loss 0.13742970458828818  - accuracy: 0.78125\n",
      "At: 960 [==========>] Loss 0.09880327007821552  - accuracy: 0.875\n",
      "At: 961 [==========>] Loss 0.10292392147830216  - accuracy: 0.875\n",
      "At: 962 [==========>] Loss 0.10269873998806958  - accuracy: 0.84375\n",
      "At: 963 [==========>] Loss 0.10542575325589507  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.18698104960750295  - accuracy: 0.78125\n",
      "At: 965 [==========>] Loss 0.13886176848148662  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.13090550348712882  - accuracy: 0.75\n",
      "At: 967 [==========>] Loss 0.10752817170715162  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.1620696733026205  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.1317954439190802  - accuracy: 0.8125\n",
      "At: 970 [==========>] Loss 0.13933668485112694  - accuracy: 0.78125\n",
      "At: 971 [==========>] Loss 0.11116726940642202  - accuracy: 0.875\n",
      "At: 972 [==========>] Loss 0.07611688595995322  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.11879181171164824  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.09044147017138106  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.1387032380830956  - accuracy: 0.75\n",
      "At: 976 [==========>] Loss 0.12275094225172305  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.11359647394765739  - accuracy: 0.8125\n",
      "At: 978 [==========>] Loss 0.1593432437189326  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.08564347370811677  - accuracy: 0.90625\n",
      "At: 980 [==========>] Loss 0.1451946688801734  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.21817920695283372  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.09053908419150544  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.11824270794547354  - accuracy: 0.875\n",
      "At: 984 [==========>] Loss 0.13319226169178952  - accuracy: 0.8125\n",
      "At: 985 [==========>] Loss 0.18261889054009756  - accuracy: 0.71875\n",
      "At: 986 [==========>] Loss 0.13588617379206186  - accuracy: 0.875\n",
      "At: 987 [==========>] Loss 0.10452794626691264  - accuracy: 0.84375\n",
      "At: 988 [==========>] Loss 0.12872579532390974  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.1408679403430347  - accuracy: 0.78125\n",
      "At: 990 [==========>] Loss 0.12756833504131015  - accuracy: 0.8125\n",
      "At: 991 [==========>] Loss 0.15844324327895973  - accuracy: 0.78125\n",
      "At: 992 [==========>] Loss 0.22506457919511708  - accuracy: 0.6875\n",
      "At: 993 [==========>] Loss 0.14748152644731483  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.16254956184584135  - accuracy: 0.75\n",
      "At: 995 [==========>] Loss 0.17074824835061977  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.08466186883015897  - accuracy: 0.90625\n",
      "At: 997 [==========>] Loss 0.15396078013287712  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.122976694103899  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.14128915834491093  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.21674181840991177  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.16282717854722412  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.22300923988285737  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.15109285209974044  - accuracy: 0.75\n",
      "At: 1004 [==========>] Loss 0.13746737989561358  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.09085368660900718  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.1100296910925697  - accuracy: 0.84375\n",
      "At: 1007 [==========>] Loss 0.11996815529443436  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.21204623202302283  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.12644495084620544  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.16127843637832684  - accuracy: 0.8125\n",
      "At: 1011 [==========>] Loss 0.1608355921554911  - accuracy: 0.75\n",
      "At: 1012 [==========>] Loss 0.1108102906375747  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.07674068911464649  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.08987646773837582  - accuracy: 0.90625\n",
      "At: 1015 [==========>] Loss 0.22638220581585644  - accuracy: 0.65625\n",
      "At: 1016 [==========>] Loss 0.1496766974242196  - accuracy: 0.75\n",
      "At: 1017 [==========>] Loss 0.1866958840284619  - accuracy: 0.71875\n",
      "At: 1018 [==========>] Loss 0.15187430221041445  - accuracy: 0.75\n",
      "At: 1019 [==========>] Loss 0.19054889914056217  - accuracy: 0.78125\n",
      "At: 1020 [==========>] Loss 0.13533532312447116  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.1308657364190588  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.136121253572162  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.19197279314998442  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.1965453666101865  - accuracy: 0.6875\n",
      "At: 1025 [==========>] Loss 0.20479051845616986  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.13048299316852097  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.09747611555293817  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.24002579980862532  - accuracy: 0.625\n",
      "At: 1029 [==========>] Loss 0.11199160916128398  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.10996814817090801  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.18898173687606984  - accuracy: 0.78125\n",
      "At: 1032 [==========>] Loss 0.14801514960267206  - accuracy: 0.78125\n",
      "At: 1033 [==========>] Loss 0.13590957717361907  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.08886846214201138  - accuracy: 0.875\n",
      "At: 1035 [==========>] Loss 0.1111739719377467  - accuracy: 0.8125\n",
      "At: 1036 [==========>] Loss 0.16273887094658646  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.17157733891653232  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.11378423769755605  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.10405078312935337  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.13809375079908215  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.16200585193992628  - accuracy: 0.75\n",
      "At: 1042 [==========>] Loss 0.14015056797030878  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.21254330371446584  - accuracy: 0.65625\n",
      "At: 1044 [==========>] Loss 0.12570749462306643  - accuracy: 0.84375\n",
      "At: 1045 [==========>] Loss 0.17092857660490496  - accuracy: 0.78125\n",
      "At: 1046 [==========>] Loss 0.13656904484313753  - accuracy: 0.84375\n",
      "At: 1047 [==========>] Loss 0.1262018091196721  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.2064195167587787  - accuracy: 0.75\n",
      "At: 1049 [==========>] Loss 0.16462825733126027  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.17079763734870593  - accuracy: 0.78125\n",
      "At: 1051 [==========>] Loss 0.06654021912950245  - accuracy: 0.96875\n",
      "At: 1052 [==========>] Loss 0.12648699529333346  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.11314459937399399  - accuracy: 0.8125\n",
      "At: 1054 [==========>] Loss 0.11908520343911985  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.21308561726914585  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.1290763772032581  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.1687638479549211  - accuracy: 0.75\n",
      "At: 1058 [==========>] Loss 0.0495189484233342  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.11024557993407016  - accuracy: 0.84375\n",
      "At: 1060 [==========>] Loss 0.090823471875838  - accuracy: 0.90625\n",
      "At: 1061 [==========>] Loss 0.12229952960884949  - accuracy: 0.84375\n",
      "At: 1062 [==========>] Loss 0.1710785092100233  - accuracy: 0.6875\n",
      "At: 1063 [==========>] Loss 0.14701668040786608  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.15685866129785267  - accuracy: 0.78125\n",
      "At: 1065 [==========>] Loss 0.10236476556567471  - accuracy: 0.8125\n",
      "At: 1066 [==========>] Loss 0.1019445833893891  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.13899563091911693  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.1015673321655055  - accuracy: 0.875\n",
      "At: 1069 [==========>] Loss 0.15112020620986685  - accuracy: 0.75\n",
      "At: 1070 [==========>] Loss 0.1071638945132569  - accuracy: 0.90625\n",
      "At: 1071 [==========>] Loss 0.1344551523338973  - accuracy: 0.78125\n",
      "At: 1072 [==========>] Loss 0.13340421432134453  - accuracy: 0.875\n",
      "At: 1073 [==========>] Loss 0.18906624436115316  - accuracy: 0.71875\n",
      "At: 1074 [==========>] Loss 0.21782800473520697  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.13719111036397036  - accuracy: 0.8125\n",
      "At: 1076 [==========>] Loss 0.16026423655145744  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.09024889763359244  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.06917173672161714  - accuracy: 0.90625\n",
      "At: 1079 [==========>] Loss 0.15459572497010948  - accuracy: 0.8125\n",
      "At: 1080 [==========>] Loss 0.19887050317628224  - accuracy: 0.75\n",
      "At: 1081 [==========>] Loss 0.13805198030050572  - accuracy: 0.75\n",
      "At: 1082 [==========>] Loss 0.12243362253630218  - accuracy: 0.8125\n",
      "At: 1083 [==========>] Loss 0.124193070067107  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.1183326079041793  - accuracy: 0.875\n",
      "At: 1085 [==========>] Loss 0.10491437497096784  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.12596994834433184  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.13833876749100565  - accuracy: 0.78125\n",
      "At: 1088 [==========>] Loss 0.15263868330872649  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.13349783158913658  - accuracy: 0.8125\n",
      "At: 1090 [==========>] Loss 0.09107494970976779  - accuracy: 0.9375\n",
      "At: 1091 [==========>] Loss 0.21546957055760696  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.13686682408757012  - accuracy: 0.8125\n",
      "At: 1093 [==========>] Loss 0.1345750781156746  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.13734580465546037  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.1382368277067937  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.09599421574200667  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.05742402162191025  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.150235085689544  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.17470052506376033  - accuracy: 0.78125\n",
      "At: 1100 [==========>] Loss 0.09074092146385038  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.11370135403261739  - accuracy: 0.84375\n",
      "At: 1102 [==========>] Loss 0.12801834732881223  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.10164662438799439  - accuracy: 0.875\n",
      "At: 1104 [==========>] Loss 0.10565778934545753  - accuracy: 0.875\n",
      "At: 1105 [==========>] Loss 0.08193833166257931  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.07747956397552336  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.1774963631735868  - accuracy: 0.75\n",
      "At: 1108 [==========>] Loss 0.0941683823972577  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.07515450835036672  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.12513691400302937  - accuracy: 0.8125\n",
      "At: 1111 [==========>] Loss 0.2100872150076642  - accuracy: 0.6875\n",
      "At: 1112 [==========>] Loss 0.20226602914825412  - accuracy: 0.75\n",
      "At: 1113 [==========>] Loss 0.15345528037070108  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.09391914836262641  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.1149242538233981  - accuracy: 0.84375\n",
      "At: 1116 [==========>] Loss 0.14501608119151616  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.0741496493629911  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.13194925245898173  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.15337165214707915  - accuracy: 0.71875\n",
      "At: 1120 [==========>] Loss 0.09167783181995295  - accuracy: 0.84375\n",
      "At: 1121 [==========>] Loss 0.15165881933054176  - accuracy: 0.75\n",
      "At: 1122 [==========>] Loss 0.07302575460342213  - accuracy: 0.9375\n",
      "At: 1123 [==========>] Loss 0.1627342671412204  - accuracy: 0.75\n",
      "At: 1124 [==========>] Loss 0.16062270472063703  - accuracy: 0.71875\n",
      "At: 1125 [==========>] Loss 0.16397899544655642  - accuracy: 0.71875\n",
      "At: 1126 [==========>] Loss 0.10454145941448019  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.14083240374108913  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.06538560613303272  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.16005079710370707  - accuracy: 0.75\n",
      "At: 1130 [==========>] Loss 0.12250634846865033  - accuracy: 0.875\n",
      "At: 1131 [==========>] Loss 0.12356790722830822  - accuracy: 0.8125\n",
      "At: 1132 [==========>] Loss 0.13014366895942772  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.15471528759220726  - accuracy: 0.75\n",
      "At: 1134 [==========>] Loss 0.10801169272082406  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.11607188768975649  - accuracy: 0.84375\n",
      "At: 1136 [==========>] Loss 0.16225548648468413  - accuracy: 0.78125\n",
      "At: 1137 [==========>] Loss 0.1425063846855805  - accuracy: 0.8125\n",
      "At: 1138 [==========>] Loss 0.07486487149166615  - accuracy: 0.90625\n",
      "At: 1139 [==========>] Loss 0.13219422734925707  - accuracy: 0.84375\n",
      "At: 1140 [==========>] Loss 0.188467274891437  - accuracy: 0.6875\n",
      "At: 1141 [==========>] Loss 0.17629632040232884  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.1527767448165087  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.059309979543404695  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.0965310150017615  - accuracy: 0.875\n",
      "At: 1145 [==========>] Loss 0.15218891793691453  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.10545097056302183  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.2382937574817906  - accuracy: 0.625\n",
      "At: 1148 [==========>] Loss 0.08580189046275702  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.12367673053886159  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.17047740818774226  - accuracy: 0.71875\n",
      "At: 1151 [==========>] Loss 0.16977867565083155  - accuracy: 0.75\n",
      "At: 1152 [==========>] Loss 0.12697276382002318  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.19859454855920028  - accuracy: 0.71875\n",
      "At: 1154 [==========>] Loss 0.12481614634043768  - accuracy: 0.875\n",
      "At: 1155 [==========>] Loss 0.10732652878411407  - accuracy: 0.875\n",
      "At: 1156 [==========>] Loss 0.13639156789655213  - accuracy: 0.8125\n",
      "At: 1157 [==========>] Loss 0.13029733619407835  - accuracy: 0.875\n",
      "At: 1158 [==========>] Loss 0.16376448361209653  - accuracy: 0.75\n",
      "At: 1159 [==========>] Loss 0.13656346924215715  - accuracy: 0.78125\n",
      "At: 1160 [==========>] Loss 0.08156097347018826  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.10418537495077536  - accuracy: 0.8125\n",
      "At: 1162 [==========>] Loss 0.13587681354171083  - accuracy: 0.84375\n",
      "At: 1163 [==========>] Loss 0.20770771152593775  - accuracy: 0.6875\n",
      "At: 1164 [==========>] Loss 0.06058605040969289  - accuracy: 0.96875\n",
      "At: 1165 [==========>] Loss 0.16551975533255742  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.0792214216466473  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.13719742473054147  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.13289900310073452  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.0963371957881821  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.1648540391029248  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.08530429375535684  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.12702630945488752  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.12245245174267999  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.18064349967507254  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.12963107288676937  - accuracy: 0.875\n",
      "At: 1176 [==========>] Loss 0.11467796063453062  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.08046083452755784  - accuracy: 0.9375\n",
      "At: 1178 [==========>] Loss 0.16212143346966573  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.1381006568797694  - accuracy: 0.75\n",
      "At: 1180 [==========>] Loss 0.1972190774439776  - accuracy: 0.6875\n",
      "At: 1181 [==========>] Loss 0.11947081137084144  - accuracy: 0.84375\n",
      "At: 1182 [==========>] Loss 0.12030313733997373  - accuracy: 0.78125\n",
      "At: 1183 [==========>] Loss 0.15490178229992674  - accuracy: 0.84375\n",
      "At: 1184 [==========>] Loss 0.1618306928949675  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.1159364095063645  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.14975266130620885  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.13806614138583068  - accuracy: 0.78125\n",
      "At: 1188 [==========>] Loss 0.10462965160451226  - accuracy: 0.8125\n",
      "At: 1189 [==========>] Loss 0.16661088312515537  - accuracy: 0.78125\n",
      "At: 1190 [==========>] Loss 0.11480051744190133  - accuracy: 0.8125\n",
      "At: 1191 [==========>] Loss 0.17217263856349324  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.06652070583144296  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.1468728807523248  - accuracy: 0.71875\n",
      "At: 1194 [==========>] Loss 0.17965405438560256  - accuracy: 0.8125\n",
      "At: 1195 [==========>] Loss 0.11195135872433104  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.12560639850239047  - accuracy: 0.8125\n",
      "At: 1197 [==========>] Loss 0.11122798212620316  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.1026168791386288  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.17733783065056136  - accuracy: 0.75\n",
      "At: 1200 [==========>] Loss 0.11173145290499867  - accuracy: 0.78125\n",
      "At: 1201 [==========>] Loss 0.12395068811671625  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.16964514481222354  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.1664051294003591  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.08686473100389228  - accuracy: 0.9375\n",
      "At: 1205 [==========>] Loss 0.08850816901983471  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.10874815379820564  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.15413579527690346  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.08723110108802434  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.11545971621321041  - accuracy: 0.84375\n",
      "At: 1210 [==========>] Loss 0.10432756911663923  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.13620290920286202  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.11773701025765519  - accuracy: 0.8125\n",
      "At: 1213 [==========>] Loss 0.2205318557429402  - accuracy: 0.59375\n",
      "At: 1214 [==========>] Loss 0.18042214553775082  - accuracy: 0.75\n",
      "At: 1215 [==========>] Loss 0.1498389839847255  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.13545857742587208  - accuracy: 0.8125\n",
      "At: 1217 [==========>] Loss 0.07012472673423302  - accuracy: 0.84375\n",
      "At: 1218 [==========>] Loss 0.11365008566872256  - accuracy: 0.8125\n",
      "At: 1219 [==========>] Loss 0.11477011102218011  - accuracy: 0.875\n",
      "At: 1220 [==========>] Loss 0.1460326082976059  - accuracy: 0.8125\n",
      "At: 1221 [==========>] Loss 0.0969257698227235  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.17535524519127932  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.11159310876450539  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.09967421793599196  - accuracy: 0.875\n",
      "At: 1225 [==========>] Loss 0.08702625959121477  - accuracy: 0.90625\n",
      "At: 1226 [==========>] Loss 0.10579107453762249  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.14069703054782712  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.11749462726170218  - accuracy: 0.8125\n",
      "At: 1229 [==========>] Loss 0.13239126366593268  - accuracy: 0.8125\n",
      "At: 1230 [==========>] Loss 0.17061458837060386  - accuracy: 0.71875\n",
      "At: 1231 [==========>] Loss 0.14562468722903144  - accuracy: 0.75\n",
      "At: 1232 [==========>] Loss 0.08682200724976094  - accuracy: 0.875\n",
      "At: 1233 [==========>] Loss 0.11163284079962849  - accuracy: 0.8125\n",
      "At: 1234 [==========>] Loss 0.13038207301952714  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.06447241643393728  - accuracy: 0.96875\n",
      "At: 1236 [==========>] Loss 0.16557825908665114  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.07546437415821927  - accuracy: 0.96875\n",
      "At: 1238 [==========>] Loss 0.11500999825319705  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.15819875888275037  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.09210359253878678  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.12810507244469146  - accuracy: 0.84375\n",
      "At: 1242 [==========>] Loss 0.15385465375827617  - accuracy: 0.75\n",
      "At: 1243 [==========>] Loss 0.16326206858362358  - accuracy: 0.8125\n",
      "At: 1244 [==========>] Loss 0.17114250417031623  - accuracy: 0.75\n",
      "At: 1245 [==========>] Loss 0.13232405418906473  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.09188907779156485  - accuracy: 0.875\n",
      "At: 1247 [==========>] Loss 0.1544960034781832  - accuracy: 0.71875\n",
      "At: 1248 [==========>] Loss 0.09775717801863004  - accuracy: 0.90625\n",
      "At: 1249 [==========>] Loss 0.1355361875015042  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.13730202914467626  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.1062699851760336  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.1017804814778381  - accuracy: 0.84375\n",
      "At: 1253 [==========>] Loss 0.11341330240567123  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.17424164844585102  - accuracy: 0.78125\n",
      "At: 1255 [==========>] Loss 0.1279024142588457  - accuracy: 0.84375\n",
      "At: 1256 [==========>] Loss 0.11425873373129775  - accuracy: 0.875\n",
      "At: 1257 [==========>] Loss 0.139183470570054  - accuracy: 0.875\n",
      "At: 1258 [==========>] Loss 0.10186299334605861  - accuracy: 0.875\n",
      "At: 1259 [==========>] Loss 0.15256600350890173  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.12937873733925148  - accuracy: 0.75\n",
      "At: 1261 [==========>] Loss 0.14492587747513574  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.1559192600464131  - accuracy: 0.75\n",
      "At: 1263 [==========>] Loss 0.07778250345444448  - accuracy: 0.96875\n",
      "At: 1264 [==========>] Loss 0.09402746464494782  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.13724419490364617  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.1270182450528276  - accuracy: 0.75\n",
      "At: 1267 [==========>] Loss 0.12768996301912794  - accuracy: 0.75\n",
      "At: 1268 [==========>] Loss 0.16230988156831283  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.13233896890201913  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.13102992506398375  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.17921149373357026  - accuracy: 0.71875\n",
      "At: 1272 [==========>] Loss 0.0811901976207832  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.20758466380405144  - accuracy: 0.75\n",
      "At: 1274 [==========>] Loss 0.13691095433941802  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.09089915287838476  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.13109535952151352  - accuracy: 0.78125\n",
      "At: 1277 [==========>] Loss 0.07925504126592983  - accuracy: 0.9375\n",
      "At: 1278 [==========>] Loss 0.15528066311452654  - accuracy: 0.78125\n",
      "At: 1279 [==========>] Loss 0.07194042614545249  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.1229593054586871  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.14354666416750572  - accuracy: 0.8125\n",
      "At: 1282 [==========>] Loss 0.12140304824009118  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.14052084497967718  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.14747269463631762  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.08656050922481685  - accuracy: 0.9375\n",
      "At: 1286 [==========>] Loss 0.12628506398986303  - accuracy: 0.84375\n",
      "At: 1287 [==========>] Loss 0.12683630083525985  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.15209671465790686  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.10046545245308372  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.1611273973569808  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.18477549640056629  - accuracy: 0.6875\n",
      "At: 1292 [==========>] Loss 0.09573714215798668  - accuracy: 0.90625\n",
      "At: 1293 [==========>] Loss 0.1322367129746156  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.12117417015536769  - accuracy: 0.84375\n",
      "At: 1295 [==========>] Loss 0.15188452508258238  - accuracy: 0.71875\n",
      "At: 1296 [==========>] Loss 0.13051900106348524  - accuracy: 0.84375\n",
      "At: 1297 [==========>] Loss 0.10876484740882969  - accuracy: 0.875\n",
      "At: 1298 [==========>] Loss 0.12525831871379284  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.15465669091930787  - accuracy: 0.75\n",
      "At: 1300 [==========>] Loss 0.14475704581996562  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.12199288311386237  - accuracy: 0.8125\n",
      "At: 1302 [==========>] Loss 0.08059422330627715  - accuracy: 0.90625\n",
      "At: 1303 [==========>] Loss 0.12335360213869154  - accuracy: 0.84375\n",
      "At: 1304 [==========>] Loss 0.11465736890597539  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.1392793421156735  - accuracy: 0.75\n",
      "At: 1306 [==========>] Loss 0.08443360403210369  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.13965850362694912  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.04586889431350333  - accuracy: 0.96875\n",
      "At: 1309 [==========>] Loss 0.15838072475765969  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.16341406298119865  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.15557243422026534  - accuracy: 0.71875\n",
      "At: 1312 [==========>] Loss 0.06385020411936917  - accuracy: 0.96875\n",
      "At: 1313 [==========>] Loss 0.15666861848156732  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.06551710762481247  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.17893377382324333  - accuracy: 0.71875\n",
      "At: 1316 [==========>] Loss 0.1623740060248922  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.09558909139528256  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.11963865453448845  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.12963747128928485  - accuracy: 0.8125\n",
      "At: 1320 [==========>] Loss 0.15602033388985792  - accuracy: 0.75\n",
      "At: 1321 [==========>] Loss 0.06112304924103975  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.177175127124295  - accuracy: 0.65625\n",
      "At: 1323 [==========>] Loss 0.10693313605420382  - accuracy: 0.90625\n",
      "At: 1324 [==========>] Loss 0.14371584751883504  - accuracy: 0.84375\n",
      "At: 1325 [==========>] Loss 0.12103907603128676  - accuracy: 0.8125\n",
      "At: 1326 [==========>] Loss 0.08181379183450198  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.1458166524022757  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.10475760484851235  - accuracy: 0.875\n",
      "At: 1329 [==========>] Loss 0.10830214089760769  - accuracy: 0.84375\n",
      "At: 1330 [==========>] Loss 0.11115715546320915  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.1699325630982091  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.1325303863303989  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.14977360507666637  - accuracy: 0.75\n",
      "At: 1334 [==========>] Loss 0.10247033234183374  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.11634564792219626  - accuracy: 0.875\n",
      "At: 1336 [==========>] Loss 0.10694279077770721  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.16869210875666424  - accuracy: 0.78125\n",
      "At: 1338 [==========>] Loss 0.11975329851874364  - accuracy: 0.875\n",
      "At: 1339 [==========>] Loss 0.1303260392318752  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.13845792474971264  - accuracy: 0.875\n",
      "At: 1341 [==========>] Loss 0.09979838511178217  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.12992324036880704  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.2098547527958451  - accuracy: 0.6875\n",
      "At: 1344 [==========>] Loss 0.16680701617039367  - accuracy: 0.78125\n",
      "At: 1345 [==========>] Loss 0.12331393424056199  - accuracy: 0.8125\n",
      "At: 1346 [==========>] Loss 0.11746154053506269  - accuracy: 0.875\n",
      "At: 1347 [==========>] Loss 0.09873231402559898  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.11129374037306938  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.15403246894736422  - accuracy: 0.75\n",
      "At: 1350 [==========>] Loss 0.13027700396110528  - accuracy: 0.8125\n",
      "At: 1351 [==========>] Loss 0.15185885313306074  - accuracy: 0.78125\n",
      "At: 1352 [==========>] Loss 0.08737215790308914  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.1848333364689289  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.20105776101321196  - accuracy: 0.6875\n",
      "At: 1355 [==========>] Loss 0.08830715584852344  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.10131098518559546  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.12420334928105024  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.15117140057038578  - accuracy: 0.8125\n",
      "At: 1359 [==========>] Loss 0.08282480738291104  - accuracy: 0.84375\n",
      "At: 1360 [==========>] Loss 0.1612605249522947  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.07730517398646952  - accuracy: 0.9375\n",
      "At: 1362 [==========>] Loss 0.15601699827144616  - accuracy: 0.78125\n",
      "At: 1363 [==========>] Loss 0.09962619824843971  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.15987938135440197  - accuracy: 0.78125\n",
      "At: 1365 [==========>] Loss 0.13716543955841404  - accuracy: 0.78125\n",
      "At: 1366 [==========>] Loss 0.18544920565617795  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.0950934306505436  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.18302938451323608  - accuracy: 0.71875\n",
      "At: 1369 [==========>] Loss 0.10232093377569554  - accuracy: 0.84375\n",
      "At: 1370 [==========>] Loss 0.11458931605563616  - accuracy: 0.875\n",
      "At: 1371 [==========>] Loss 0.17512194874610368  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.1136580429529257  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.1438021968783892  - accuracy: 0.78125\n",
      "At: 1374 [==========>] Loss 0.13967459431059692  - accuracy: 0.75\n",
      "At: 1375 [==========>] Loss 0.12305709597144375  - accuracy: 0.8125\n",
      "At: 1376 [==========>] Loss 0.11376848051193086  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.18805475391167187  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.12625304851189978  - accuracy: 0.8125\n",
      "At: 1379 [==========>] Loss 0.15234062904791962  - accuracy: 0.75\n",
      "At: 1380 [==========>] Loss 0.1479760140416252  - accuracy: 0.75\n",
      "At: 1381 [==========>] Loss 0.08179188390976727  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.13308912169061024  - accuracy: 0.78125\n",
      "At: 1383 [==========>] Loss 0.10931785329080895  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.12478524137587566  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.16084943707835306  - accuracy: 0.75\n",
      "At: 1386 [==========>] Loss 0.19377061914945906  - accuracy: 0.71875\n",
      "At: 1387 [==========>] Loss 0.05242270396623118  - accuracy: 0.96875\n",
      "At: 1388 [==========>] Loss 0.159040648606119  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.11928477746292973  - accuracy: 0.8125\n",
      "At: 1390 [==========>] Loss 0.15450985389594313  - accuracy: 0.75\n",
      "At: 1391 [==========>] Loss 0.13226042522490794  - accuracy: 0.78125\n",
      "At: 1392 [==========>] Loss 0.126357882058434  - accuracy: 0.84375\n",
      "At: 1393 [==========>] Loss 0.14834440544163202  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.118039234651756  - accuracy: 0.8125\n",
      "At: 1395 [==========>] Loss 0.21053397621209263  - accuracy: 0.65625\n",
      "At: 1396 [==========>] Loss 0.05707523493525582  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.13592160514570611  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.10241194681212684  - accuracy: 0.8125\n",
      "At: 1399 [==========>] Loss 0.15905040556132582  - accuracy: 0.8125\n",
      "At: 1400 [==========>] Loss 0.1515922413146399  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.09732307781239347  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.15478918075805415  - accuracy: 0.8125\n",
      "At: 1403 [==========>] Loss 0.18582298860879806  - accuracy: 0.6875\n",
      "At: 1404 [==========>] Loss 0.12161640449302843  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.09762926835025529  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.14839827100823716  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.11038763225291962  - accuracy: 0.84375\n",
      "At: 1408 [==========>] Loss 0.152588999217584  - accuracy: 0.78125\n",
      "At: 1409 [==========>] Loss 0.029454258935782046  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.16578645456157107  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.1437856719884517  - accuracy: 0.8125\n",
      "At: 1412 [==========>] Loss 0.13733501810396945  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.1041085933657129  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.1849747947363044  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.06589464123826465  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.1419472995132558  - accuracy: 0.71875\n",
      "At: 1417 [==========>] Loss 0.10937609698188372  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.12262103924761783  - accuracy: 0.84375\n",
      "At: 1419 [==========>] Loss 0.12120294970463025  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.11270244956559075  - accuracy: 0.8125\n",
      "At: 1421 [==========>] Loss 0.12373716220639605  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.14535958647011255  - accuracy: 0.78125\n",
      "At: 1423 [==========>] Loss 0.14648285054031587  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.14304973852017888  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.09257236460440268  - accuracy: 0.875\n",
      "At: 1426 [==========>] Loss 0.15914413905435248  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.09519245909830393  - accuracy: 0.9375\n",
      "At: 1428 [==========>] Loss 0.09757164154861768  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.15298714987454298  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.08612464036882855  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.12334396613459445  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.10267542216576314  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.08365982057901589  - accuracy: 0.90625\n",
      "At: 1434 [==========>] Loss 0.14336452968548152  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.12832568627409238  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.08352311084681523  - accuracy: 0.875\n",
      "At: 1437 [==========>] Loss 0.11740791269531059  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.14777708287849045  - accuracy: 0.78125\n",
      "At: 1439 [==========>] Loss 0.10358771306403783  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.13917548101704766  - accuracy: 0.78125\n",
      "At: 1441 [==========>] Loss 0.09318148909235736  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.11773885797677525  - accuracy: 0.875\n",
      "At: 1443 [==========>] Loss 0.13483701990712976  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.12249983489402622  - accuracy: 0.8125\n",
      "At: 1445 [==========>] Loss 0.20672588837772515  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.1953214079192225  - accuracy: 0.78125\n",
      "At: 1447 [==========>] Loss 0.16007965668328195  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.09065744051492097  - accuracy: 0.84375\n",
      "At: 1449 [==========>] Loss 0.1387473152464482  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.12689336422826222  - accuracy: 0.8125\n",
      "At: 1451 [==========>] Loss 0.1338635203075725  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.10764032093370043  - accuracy: 0.78125\n",
      "At: 1453 [==========>] Loss 0.05402318726235465  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.16107843908152802  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.1205173089952676  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.09363437139003986  - accuracy: 0.8125\n",
      "At: 1457 [==========>] Loss 0.09739460036366687  - accuracy: 0.875\n",
      "At: 1458 [==========>] Loss 0.1717295633808007  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.14300770141923147  - accuracy: 0.75\n",
      "At: 1460 [==========>] Loss 0.17239486191390427  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.12290019335067093  - accuracy: 0.8125\n",
      "At: 1462 [==========>] Loss 0.20598524596005163  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.07303105567877177  - accuracy: 0.90625\n",
      "At: 1464 [==========>] Loss 0.21323269348958826  - accuracy: 0.6875\n",
      "At: 1465 [==========>] Loss 0.13052485268172787  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.08570877621845617  - accuracy: 0.9375\n",
      "At: 1467 [==========>] Loss 0.19028725387185513  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.1770529439060586  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.16274387382287397  - accuracy: 0.75\n",
      "At: 1470 [==========>] Loss 0.15772872985419506  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.14828147989557972  - accuracy: 0.78125\n",
      "At: 1472 [==========>] Loss 0.07957116892094071  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.13875580159774642  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.18223049610087405  - accuracy: 0.78125\n",
      "At: 1475 [==========>] Loss 0.1559432554186423  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.12889412903740177  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.10137645591429086  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.11515910111221467  - accuracy: 0.8125\n",
      "At: 1479 [==========>] Loss 0.13508948429850273  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.09977939693159171  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.1285725114737518  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.08985831949405439  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.22447645389753806  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.12960423917789543  - accuracy: 0.84375\n",
      "At: 1485 [==========>] Loss 0.1623561650904583  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.08489939576026775  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.06390743036394965  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.1447739768446284  - accuracy: 0.78125\n",
      "At: 1489 [==========>] Loss 0.2227199140227599  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.10853655657132581  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.17485045032851534  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.1587391665574911  - accuracy: 0.75\n",
      "At: 1493 [==========>] Loss 0.18926564276234087  - accuracy: 0.6875\n",
      "At: 1494 [==========>] Loss 0.15783424904748306  - accuracy: 0.75\n",
      "At: 1495 [==========>] Loss 0.13964073476735162  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.12248779698684273  - accuracy: 0.8125\n",
      "At: 1497 [==========>] Loss 0.1563872232777175  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.13579580635527538  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.11575292672898446  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.12865337168503524  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.07875420303685268  - accuracy: 0.96875\n",
      "At: 1502 [==========>] Loss 0.1629075574367982  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.13181335083236964  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.1475128097616614  - accuracy: 0.71875\n",
      "At: 1505 [==========>] Loss 0.11835956982092186  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.17856254463412521  - accuracy: 0.71875\n",
      "At: 1507 [==========>] Loss 0.1428036688902478  - accuracy: 0.78125\n",
      "At: 1508 [==========>] Loss 0.20010206282940235  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.11504491979856168  - accuracy: 0.78125\n",
      "At: 1510 [==========>] Loss 0.10749198382172788  - accuracy: 0.8125\n",
      "At: 1511 [==========>] Loss 0.11454163940904606  - accuracy: 0.90625\n",
      "At: 1512 [==========>] Loss 0.10898549312476598  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.14892012516334024  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.15370515411245028  - accuracy: 0.75\n",
      "At: 1515 [==========>] Loss 0.13347431884057326  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.12312960432004057  - accuracy: 0.84375\n",
      "At: 1517 [==========>] Loss 0.15747540287451242  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.12177789663285771  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.18308401891827014  - accuracy: 0.78125\n",
      "At: 1520 [==========>] Loss 0.11270559791839964  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.07876551444212664  - accuracy: 0.84375\n",
      "At: 1522 [==========>] Loss 0.18020358253708457  - accuracy: 0.71875\n",
      "At: 1523 [==========>] Loss 0.1100112808455653  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.1376655277195823  - accuracy: 0.8125\n",
      "At: 1525 [==========>] Loss 0.1346431587007122  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.12660931764954528  - accuracy: 0.75\n",
      "At: 1527 [==========>] Loss 0.13568743101625547  - accuracy: 0.78125\n",
      "At: 1528 [==========>] Loss 0.13990571953465392  - accuracy: 0.75\n",
      "At: 1529 [==========>] Loss 0.07372238576885481  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.06149348358837588  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.11291512451278103  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.193245789925417  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.12645463033083254  - accuracy: 0.8125\n",
      "At: 1534 [==========>] Loss 0.10385007547512434  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.1441395700629356  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.15136893917354968  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10400926923547557  - accuracy: 0.875\n",
      "At: 1538 [==========>] Loss 0.13921326184720254  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.10643476597228552  - accuracy: 0.84375\n",
      "At: 1540 [==========>] Loss 0.1649550523068996  - accuracy: 0.8125\n",
      "At: 1541 [==========>] Loss 0.1252802726595807  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.10216749524025168  - accuracy: 0.90625\n",
      "At: 1543 [==========>] Loss 0.15298005477952986  - accuracy: 0.75\n",
      "At: 1544 [==========>] Loss 0.16586867009696118  - accuracy: 0.75\n",
      "At: 1545 [==========>] Loss 0.20403169751849803  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.14539104032606534  - accuracy: 0.75\n",
      "At: 1547 [==========>] Loss 0.1678760161460845  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.12344565982646621  - accuracy: 0.90625\n",
      "At: 1549 [==========>] Loss 0.13776988209310498  - accuracy: 0.84375\n",
      "At: 1550 [==========>] Loss 0.0929690170285323  - accuracy: 0.875\n",
      "At: 1551 [==========>] Loss 0.15994551159979617  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.10516252372409315  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.08837626420910435  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.14322039650713736  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.11946578713898354  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1622769413971844  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.09364459658195459  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.1311202362333495  - accuracy: 0.84375\n",
      "At: 1559 [==========>] Loss 0.10362533072691221  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.12337955118925353  - accuracy: 0.84375\n",
      "At: 1561 [==========>] Loss 0.15524869393868673  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.11331791664905477  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.10595396241464597  - accuracy: 0.84375\n",
      "At: 1564 [==========>] Loss 0.12912851997220381  - accuracy: 0.8125\n",
      "At: 1565 [==========>] Loss 0.12953994317582043  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.13678520893383972  - accuracy: 0.84375\n",
      "At: 1567 [==========>] Loss 0.15198727529747152  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.08543107352618712  - accuracy: 0.9375\n",
      "At: 1569 [==========>] Loss 0.10196680657404661  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.09739175215087256  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.14287796560726146  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.12379700521015989  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.06254687970441818  - accuracy: 0.875\n",
      "At: 1574 [==========>] Loss 0.15874511526591636  - accuracy: 0.71875\n",
      "At: 1575 [==========>] Loss 0.094373826431595  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.12533117215566653  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.10090957202094504  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.08800020582516235  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.09751496208130547  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.13865009929728112  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.09267721416497017  - accuracy: 0.90625\n",
      "At: 1582 [==========>] Loss 0.18889475458651817  - accuracy: 0.6875\n",
      "At: 1583 [==========>] Loss 0.10205552416391672  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.12297414896328779  - accuracy: 0.875\n",
      "At: 1585 [==========>] Loss 0.10540230880619669  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.1603753344547623  - accuracy: 0.84375\n",
      "At: 1587 [==========>] Loss 0.12147127372961915  - accuracy: 0.8125\n",
      "At: 1588 [==========>] Loss 0.1117471467030821  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.14119542290665874  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.13475144748635975  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.12298732351281894  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.09408658536791105  - accuracy: 0.90625\n",
      "At: 1593 [==========>] Loss 0.18907491572798585  - accuracy: 0.8125\n",
      "At: 1594 [==========>] Loss 0.11992864756607549  - accuracy: 0.8125\n",
      "At: 1595 [==========>] Loss 0.12890831431636868  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.20801134103269203  - accuracy: 0.6875\n",
      "At: 1597 [==========>] Loss 0.14915742237076812  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.15880719341337682  - accuracy: 0.75\n",
      "At: 1599 [==========>] Loss 0.21521438592412928  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.15936435059696824  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.09200604886936482  - accuracy: 0.875\n",
      "At: 1602 [==========>] Loss 0.13299914715203634  - accuracy: 0.75\n",
      "At: 1603 [==========>] Loss 0.18277478035029998  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.23936440191193348  - accuracy: 0.625\n",
      "At: 1605 [==========>] Loss 0.09704360094283401  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.10537634041653235  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.16414096572955614  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.12679365574053436  - accuracy: 0.84375\n",
      "At: 1609 [==========>] Loss 0.18297411816173254  - accuracy: 0.6875\n",
      "At: 1610 [==========>] Loss 0.17148499758871938  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.08373612399065161  - accuracy: 0.84375\n",
      "At: 1612 [==========>] Loss 0.09539744770856937  - accuracy: 0.84375\n",
      "At: 1613 [==========>] Loss 0.13138600201823847  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.13055721679666119  - accuracy: 0.875\n",
      "At: 1615 [==========>] Loss 0.06574865148978348  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.13491922155677377  - accuracy: 0.75\n",
      "At: 1617 [==========>] Loss 0.09863016787114184  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11584742778435658  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.19242433078670168  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.13354770179708503  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.11066518943807936  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.16432737393229802  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.10169995328974116  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.14234507410084926  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.14877776521175642  - accuracy: 0.84375\n",
      "At: 1626 [==========>] Loss 0.0969592823907833  - accuracy: 0.875\n",
      "At: 1627 [==========>] Loss 0.1020430648484648  - accuracy: 0.84375\n",
      "At: 1628 [==========>] Loss 0.15415475523390443  - accuracy: 0.75\n",
      "At: 1629 [==========>] Loss 0.12997302549765816  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.09142228252545373  - accuracy: 0.9375\n",
      "At: 1631 [==========>] Loss 0.11638347250047924  - accuracy: 0.84375\n",
      "At: 1632 [==========>] Loss 0.10159959495645392  - accuracy: 0.84375\n",
      "At: 1633 [==========>] Loss 0.0831906913680357  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.14716397288304678  - accuracy: 0.78125\n",
      "At: 1635 [==========>] Loss 0.24394951261455955  - accuracy: 0.65625\n",
      "At: 1636 [==========>] Loss 0.11147801874800363  - accuracy: 0.875\n",
      "At: 1637 [==========>] Loss 0.07633963901073593  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.13440274411963743  - accuracy: 0.8125\n",
      "At: 1639 [==========>] Loss 0.14921399214914619  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.11775040195325774  - accuracy: 0.75\n",
      "At: 1641 [==========>] Loss 0.0847887441258634  - accuracy: 0.90625\n",
      "At: 1642 [==========>] Loss 0.10529189217081811  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.10531869886400065  - accuracy: 0.8125\n",
      "At: 1644 [==========>] Loss 0.1202829631079568  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.08071416782125282  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.1358727471921598  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.17628789083540802  - accuracy: 0.71875\n",
      "At: 1648 [==========>] Loss 0.13776584997096294  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.09401958904056162  - accuracy: 0.875\n",
      "At: 1650 [==========>] Loss 0.09929618863562947  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.14114039027002528  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.07088089050704036  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.10128676648794813  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.06962899987824911  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.20116384013606944  - accuracy: 0.75\n",
      "At: 1656 [==========>] Loss 0.12337906465553612  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.1487556272349213  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.19660068953313503  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.10554929275779415  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.05188338548364052  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.09055674178292603  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.11083958940513872  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.16440202418808547  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.11476969039677468  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.11585170711382115  - accuracy: 0.8125\n",
      "At: 1666 [==========>] Loss 0.09623452227357965  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.15730635859108946  - accuracy: 0.78125\n",
      "At: 1668 [==========>] Loss 0.16204182037334003  - accuracy: 0.8125\n",
      "At: 1669 [==========>] Loss 0.13704562515278462  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.07013914271982113  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.12151021139397  - accuracy: 0.8125\n",
      "At: 1672 [==========>] Loss 0.1553213863724825  - accuracy: 0.8125\n",
      "At: 1673 [==========>] Loss 0.07952190468540533  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.18174155450396853  - accuracy: 0.71875\n",
      "At: 1675 [==========>] Loss 0.19238154507053443  - accuracy: 0.6875\n",
      "At: 1676 [==========>] Loss 0.2314341935220012  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.13257629594953402  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.1563018938648204  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.11629424505884702  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.17038581497901506  - accuracy: 0.71875\n",
      "At: 1681 [==========>] Loss 0.13162555742787532  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.08595158472636952  - accuracy: 0.90625\n",
      "At: 1683 [==========>] Loss 0.18153812547470155  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.11269511422288492  - accuracy: 0.875\n",
      "At: 1685 [==========>] Loss 0.12749647606809184  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.11070646743357968  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.17936999263803088  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.07004827245416323  - accuracy: 0.90625\n",
      "At: 1689 [==========>] Loss 0.10469824291074009  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.12423705486744417  - accuracy: 0.75\n",
      "At: 1691 [==========>] Loss 0.11456745528533221  - accuracy: 0.875\n",
      "At: 1692 [==========>] Loss 0.164735450188126  - accuracy: 0.71875\n",
      "At: 1693 [==========>] Loss 0.1156338455500328  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.09502199535916449  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.15861335226807  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.15858026977791953  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.11987277873565086  - accuracy: 0.84375\n",
      "At: 1698 [==========>] Loss 0.09560726758339251  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.0997504344052749  - accuracy: 0.8125\n",
      "At: 1700 [==========>] Loss 0.1311259132105727  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.10140676069023105  - accuracy: 0.8125\n",
      "At: 1702 [==========>] Loss 0.11149148283414759  - accuracy: 0.8125\n",
      "At: 1703 [==========>] Loss 0.1894596638967026  - accuracy: 0.6875\n",
      "At: 1704 [==========>] Loss 0.09501967193196106  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.12526922989111827  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.16788375198565336  - accuracy: 0.75\n",
      "At: 1707 [==========>] Loss 0.2122285494641378  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.09022880073942138  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.18547653801023337  - accuracy: 0.6875\n",
      "At: 1710 [==========>] Loss 0.1712590108716377  - accuracy: 0.6875\n",
      "At: 1711 [==========>] Loss 0.0972989879051977  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.12161734122503842  - accuracy: 0.84375\n",
      "At: 1713 [==========>] Loss 0.07319503315777368  - accuracy: 0.90625\n",
      "At: 1714 [==========>] Loss 0.1759823518675785  - accuracy: 0.75\n",
      "At: 1715 [==========>] Loss 0.12284192148045348  - accuracy: 0.78125\n",
      "At: 1716 [==========>] Loss 0.06825060460760432  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.10630734610692313  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.14302175632947778  - accuracy: 0.8125\n",
      "At: 1719 [==========>] Loss 0.11787859782308044  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.07233123800242211  - accuracy: 0.90625\n",
      "At: 1721 [==========>] Loss 0.15110603733697536  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.07191634407213493  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.20535388348029776  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.0916779547275559  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.15638548456917423  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.13365822672911867  - accuracy: 0.8125\n",
      "At: 1727 [==========>] Loss 0.13713507455137983  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.1302553097291961  - accuracy: 0.84375\n",
      "At: 1729 [==========>] Loss 0.19742321226957163  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.13089816380781313  - accuracy: 0.78125\n",
      "At: 1731 [==========>] Loss 0.12302978908262505  - accuracy: 0.84375\n",
      "At: 1732 [==========>] Loss 0.07006646519999074  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.17453734787524505  - accuracy: 0.6875\n",
      "At: 1734 [==========>] Loss 0.09547935914833666  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.1750768462351876  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.1329731353022211  - accuracy: 0.8125\n",
      "At: 1737 [==========>] Loss 0.15217661753547823  - accuracy: 0.75\n",
      "At: 1738 [==========>] Loss 0.10489980524138211  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.10276002980051942  - accuracy: 0.875\n",
      "At: 1740 [==========>] Loss 0.1593944249491222  - accuracy: 0.78125\n",
      "At: 1741 [==========>] Loss 0.14057707115391913  - accuracy: 0.8125\n",
      "At: 1742 [==========>] Loss 0.07237274039695092  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.15718162535495078  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.09949095289625033  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.10483877354866845  - accuracy: 0.875\n",
      "At: 1746 [==========>] Loss 0.17169139762540017  - accuracy: 0.71875\n",
      "At: 1747 [==========>] Loss 0.11943658090213465  - accuracy: 0.84375\n",
      "At: 1748 [==========>] Loss 0.10530361594593579  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.11218549339523927  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.09073758100717055  - accuracy: 0.90625\n",
      "At: 1751 [==========>] Loss 0.20015463230214559  - accuracy: 0.71875\n",
      "At: 1752 [==========>] Loss 0.11546605468210315  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.09136459161458363  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.11551346730638362  - accuracy: 0.90625\n",
      "At: 1755 [==========>] Loss 0.08551363102448772  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.15264869948822002  - accuracy: 0.75\n",
      "At: 1757 [==========>] Loss 0.1734183039489109  - accuracy: 0.71875\n",
      "At: 1758 [==========>] Loss 0.08749981374627275  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.10231341951659663  - accuracy: 0.8125\n",
      "At: 1760 [==========>] Loss 0.09655480660542698  - accuracy: 0.84375\n",
      "At: 1761 [==========>] Loss 0.1385929716151092  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.15856817691443917  - accuracy: 0.8125\n",
      "At: 1763 [==========>] Loss 0.11535913922823829  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.14595494686607835  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.13052160881956726  - accuracy: 0.875\n",
      "At: 1766 [==========>] Loss 0.08114896094857585  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.08630216116295035  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.11890365616235238  - accuracy: 0.78125\n",
      "At: 1769 [==========>] Loss 0.06905280547469358  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.07614396330714371  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.17002368392997325  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.13055505002780038  - accuracy: 0.78125\n",
      "At: 1773 [==========>] Loss 0.10598374865297167  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.1415956042653851  - accuracy: 0.875\n",
      "At: 1775 [==========>] Loss 0.1213407036239471  - accuracy: 0.8125\n",
      "At: 1776 [==========>] Loss 0.12022858205585486  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.12344998801536296  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.11711453738628073  - accuracy: 0.8125\n",
      "At: 1779 [==========>] Loss 0.07845109125591998  - accuracy: 0.9375\n",
      "At: 1780 [==========>] Loss 0.11339591471163385  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.18470412531279629  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.11803955343831829  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.13844956862647506  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.07528987968401403  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.09100747603820174  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.13933776469842585  - accuracy: 0.75\n",
      "At: 1787 [==========>] Loss 0.13154703900547216  - accuracy: 0.84375\n",
      "At: 1788 [==========>] Loss 0.10006120216493673  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.11237563759674117  - accuracy: 0.84375\n",
      "At: 1790 [==========>] Loss 0.1610902705575432  - accuracy: 0.65625\n",
      "At: 1791 [==========>] Loss 0.057835685675867374  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.11276707476102041  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.08349364251356609  - accuracy: 0.9375\n",
      "At: 1794 [==========>] Loss 0.16395507729158176  - accuracy: 0.6875\n",
      "At: 1795 [==========>] Loss 0.09138709626352537  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.14704123497703364  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.12275391145695388  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.11910993731592114  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.09530603282792785  - accuracy: 0.84375\n",
      "At: 1800 [==========>] Loss 0.1356100029617084  - accuracy: 0.78125\n",
      "At: 1801 [==========>] Loss 0.1913934534443916  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.13045172045270154  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.1610913981283751  - accuracy: 0.71875\n",
      "At: 1804 [==========>] Loss 0.14381255847143126  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.0377529295617962  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.1666853153658959  - accuracy: 0.75\n",
      "At: 1807 [==========>] Loss 0.16493863190731373  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.17457748596635936  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.09022970614933644  - accuracy: 0.875\n",
      "At: 1810 [==========>] Loss 0.16468105680300898  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.13341347854600616  - accuracy: 0.875\n",
      "At: 1812 [==========>] Loss 0.1110959224584222  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.15360800430648086  - accuracy: 0.78125\n",
      "At: 1814 [==========>] Loss 0.11391010052524661  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.14687397709700747  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.04434801296803133  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.14818031417609795  - accuracy: 0.75\n",
      "At: 1818 [==========>] Loss 0.16461975246660632  - accuracy: 0.6875\n",
      "At: 1819 [==========>] Loss 0.1778817403258741  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.10581633098184359  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.10755649294394151  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.16995193142163378  - accuracy: 0.78125\n",
      "At: 1823 [==========>] Loss 0.19866583352732042  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.18957411325377213  - accuracy: 0.6875\n",
      "At: 1825 [==========>] Loss 0.10745976925943991  - accuracy: 0.875\n",
      "At: 1826 [==========>] Loss 0.06586931957559751  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11556450944504494  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.14328585511245157  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.17382117225999522  - accuracy: 0.78125\n",
      "At: 1830 [==========>] Loss 0.1632394888425304  - accuracy: 0.75\n",
      "At: 1831 [==========>] Loss 0.12981582970009437  - accuracy: 0.78125\n",
      "At: 1832 [==========>] Loss 0.11712617318265027  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.12874661803284204  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.09787149540762835  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.14427607755709562  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.10923698783699878  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.04170239822295775  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.09648870490195074  - accuracy: 0.875\n",
      "At: 1839 [==========>] Loss 0.09029411984600408  - accuracy: 0.875\n",
      "At: 1840 [==========>] Loss 0.1357941334818041  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.11939723256755883  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.12902953219665722  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.09220232301519932  - accuracy: 0.84375\n",
      "At: 1844 [==========>] Loss 0.11810197476415983  - accuracy: 0.84375\n",
      "At: 1845 [==========>] Loss 0.19085744058031578  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.1429144877761549  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.0578489745622146  - accuracy: 0.90625\n",
      "At: 1848 [==========>] Loss 0.05281642363032901  - accuracy: 0.96875\n",
      "At: 1849 [==========>] Loss 0.19382507743160188  - accuracy: 0.71875\n",
      "At: 1850 [==========>] Loss 0.04148660062338769  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.1661735098765036  - accuracy: 0.71875\n",
      "At: 1852 [==========>] Loss 0.0830210002573235  - accuracy: 0.875\n",
      "At: 1853 [==========>] Loss 0.11521838646283473  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.13417278468860994  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.19680038133917963  - accuracy: 0.75\n",
      "At: 1856 [==========>] Loss 0.13358561239759228  - accuracy: 0.8125\n",
      "At: 1857 [==========>] Loss 0.18060626423727777  - accuracy: 0.71875\n",
      "At: 1858 [==========>] Loss 0.11678636943395562  - accuracy: 0.875\n",
      "At: 1859 [==========>] Loss 0.15078049957246997  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.14036975217466433  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.08574033102803182  - accuracy: 0.90625\n",
      "At: 1862 [==========>] Loss 0.14406289363071184  - accuracy: 0.75\n",
      "At: 1863 [==========>] Loss 0.17350372592865082  - accuracy: 0.71875\n",
      "At: 1864 [==========>] Loss 0.14119850045356225  - accuracy: 0.78125\n",
      "At: 1865 [==========>] Loss 0.0882465382672322  - accuracy: 0.90625\n",
      "At: 1866 [==========>] Loss 0.19604468275095638  - accuracy: 0.71875\n",
      "At: 1867 [==========>] Loss 0.12997916142953908  - accuracy: 0.78125\n",
      "At: 1868 [==========>] Loss 0.17339308128013337  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.1906509865418114  - accuracy: 0.65625\n",
      "At: 1870 [==========>] Loss 0.14047214907235717  - accuracy: 0.8125\n",
      "At: 1871 [==========>] Loss 0.130059546718098  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.13062380520093508  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.09768262649361849  - accuracy: 0.84375\n",
      "At: 1874 [==========>] Loss 0.13816113429688393  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.08327002175314924  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.2038532491786239  - accuracy: 0.71875\n",
      "At: 1877 [==========>] Loss 0.10364376704108931  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.10043244391372219  - accuracy: 0.875\n",
      "At: 1879 [==========>] Loss 0.14012842644991882  - accuracy: 0.78125\n",
      "At: 1880 [==========>] Loss 0.08173810643616908  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.07876908308865607  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.13014229169952093  - accuracy: 0.78125\n",
      "At: 1883 [==========>] Loss 0.16118047235144337  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.13094582601653673  - accuracy: 0.78125\n",
      "At: 1885 [==========>] Loss 0.10240204276072817  - accuracy: 0.875\n",
      "At: 1886 [==========>] Loss 0.13281354401743578  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.08528603006299625  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.13112417464721834  - accuracy: 0.875\n",
      "At: 1889 [==========>] Loss 0.08882393110127318  - accuracy: 0.90625\n",
      "At: 1890 [==========>] Loss 0.15480267050156474  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.08039185513958551  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.09436902445817338  - accuracy: 0.8125\n",
      "At: 1893 [==========>] Loss 0.08697516305644824  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.09182237112073104  - accuracy: 0.84375\n",
      "At: 1895 [==========>] Loss 0.07057074811970988  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.09099643168569074  - accuracy: 0.875\n",
      "At: 1897 [==========>] Loss 0.06898142268246199  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.10360653420432228  - accuracy: 0.875\n",
      "At: 1899 [==========>] Loss 0.09867348137251354  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.12860108231102974  - accuracy: 0.8125\n",
      "At: 1901 [==========>] Loss 0.102999268564528  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.15961113458998347  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.14288390299574877  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.051971002159772975  - accuracy: 0.9375\n",
      "At: 1905 [==========>] Loss 0.16738941851480965  - accuracy: 0.6875\n",
      "At: 1906 [==========>] Loss 0.10902036606027127  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.09758444024623114  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.11974446558532437  - accuracy: 0.78125\n",
      "At: 1909 [==========>] Loss 0.12876862447268922  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.09015310158911802  - accuracy: 0.8125\n",
      "At: 1911 [==========>] Loss 0.10940567452657622  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.1132821546778451  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.1582442849312801  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.0676022014326344  - accuracy: 0.9375\n",
      "At: 1915 [==========>] Loss 0.11322600548866973  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.14523695750234877  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.17736878565709852  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.16469257822544722  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.10382862435551662  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10240991586821135  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.12414630554629572  - accuracy: 0.875\n",
      "At: 1922 [==========>] Loss 0.1314457093194019  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.16616862718296005  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.11692718982983842  - accuracy: 0.84375\n",
      "At: 1925 [==========>] Loss 0.19070790188943393  - accuracy: 0.6875\n",
      "At: 1926 [==========>] Loss 0.10530954528850937  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.09764127038558382  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.1341320091903157  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.17025284746239996  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.14387676421403484  - accuracy: 0.75\n",
      "At: 1931 [==========>] Loss 0.12012461709853592  - accuracy: 0.875\n",
      "At: 1932 [==========>] Loss 0.1558190243569693  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.10589906855554473  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.14319525774420816  - accuracy: 0.84375\n",
      "At: 1935 [==========>] Loss 0.13091664382485474  - accuracy: 0.84375\n",
      "At: 1936 [==========>] Loss 0.09771074141527064  - accuracy: 0.84375\n",
      "At: 1937 [==========>] Loss 0.15074249441037504  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.1879376637541871  - accuracy: 0.8125\n",
      "At: 1939 [==========>] Loss 0.08804689883374012  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.12736319111977545  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.12451693926366343  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.14907695902534063  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.1547587241035956  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.11939491144241525  - accuracy: 0.84375\n",
      "At: 1945 [==========>] Loss 0.14648414764518672  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.11346967367234355  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.1059765663835629  - accuracy: 0.875\n",
      "At: 1948 [==========>] Loss 0.11166799508515386  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.07124451280450378  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.14250662308040468  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.14719328969274365  - accuracy: 0.84375\n",
      "At: 1952 [==========>] Loss 0.09888279475645502  - accuracy: 0.84375\n",
      "At: 1953 [==========>] Loss 0.08282363662721284  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.1951310088346022  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.06721780708025062  - accuracy: 0.96875\n",
      "At: 1956 [==========>] Loss 0.10143919178759378  - accuracy: 0.875\n",
      "At: 1957 [==========>] Loss 0.10448176821312935  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.09549177083887272  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.12450495177258253  - accuracy: 0.78125\n",
      "At: 1960 [==========>] Loss 0.08104667458711672  - accuracy: 0.875\n",
      "At: 1961 [==========>] Loss 0.19224583409224222  - accuracy: 0.6875\n",
      "At: 1962 [==========>] Loss 0.2232486400322855  - accuracy: 0.65625\n",
      "At: 1963 [==========>] Loss 0.08512567696646607  - accuracy: 0.9375\n",
      "At: 1964 [==========>] Loss 0.17606306623168066  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.14542150384102626  - accuracy: 0.8125\n",
      "At: 1966 [==========>] Loss 0.102637296189461  - accuracy: 0.875\n",
      "At: 1967 [==========>] Loss 0.15693171579495324  - accuracy: 0.75\n",
      "At: 1968 [==========>] Loss 0.17615039617463663  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.15054098979119768  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.08729695230060378  - accuracy: 0.90625\n",
      "At: 1971 [==========>] Loss 0.230674342392276  - accuracy: 0.6875\n",
      "At: 1972 [==========>] Loss 0.0974338335637743  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.12349536324155336  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.15684037781914173  - accuracy: 0.78125\n",
      "At: 1975 [==========>] Loss 0.15772476036076102  - accuracy: 0.71875\n",
      "At: 1976 [==========>] Loss 0.05608997869216678  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.10416190202672353  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.15497162510578855  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.11601805289931845  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.13786222046880636  - accuracy: 0.90625\n",
      "At: 1981 [==========>] Loss 0.17965944260680472  - accuracy: 0.6875\n",
      "At: 1982 [==========>] Loss 0.08520323293836804  - accuracy: 0.875\n",
      "At: 1983 [==========>] Loss 0.16852374854467295  - accuracy: 0.71875\n",
      "At: 1984 [==========>] Loss 0.1060323326632977  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.14407814194567037  - accuracy: 0.75\n",
      "At: 1986 [==========>] Loss 0.20871955898077246  - accuracy: 0.6875\n",
      "At: 1987 [==========>] Loss 0.11079888086140392  - accuracy: 0.78125\n",
      "At: 1988 [==========>] Loss 0.08268264316154139  - accuracy: 0.9375\n",
      "At: 1989 [==========>] Loss 0.09339532529114271  - accuracy: 0.8125\n",
      "At: 1990 [==========>] Loss 0.1252414902744149  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.13951330431983516  - accuracy: 0.84375\n",
      "At: 1992 [==========>] Loss 0.10125614458233675  - accuracy: 0.90625\n",
      "At: 1993 [==========>] Loss 0.14428563858970253  - accuracy: 0.8125\n",
      "At: 1994 [==========>] Loss 0.12197995414910848  - accuracy: 0.84375\n",
      "At: 1995 [==========>] Loss 0.18996298661482694  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.11736790768178845  - accuracy: 0.78125\n",
      "At: 1997 [==========>] Loss 0.2014582818755442  - accuracy: 0.75\n",
      "At: 1998 [==========>] Loss 0.15145891942456116  - accuracy: 0.71875\n",
      "At: 1999 [==========>] Loss 0.08883646318003455  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.14392068110906772  - accuracy: 0.75\n",
      "At: 2001 [==========>] Loss 0.08797451786427646  - accuracy: 0.84375\n",
      "At: 2002 [==========>] Loss 0.0823909490050601  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.13233229976605432  - accuracy: 0.78125\n",
      "At: 2004 [==========>] Loss 0.1572856635305339  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.13151763719382725  - accuracy: 0.8125\n",
      "At: 2006 [==========>] Loss 0.1586948448631979  - accuracy: 0.75\n",
      "At: 2007 [==========>] Loss 0.1162927586109554  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.11992199163745464  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.14814565877769484  - accuracy: 0.8125\n",
      "At: 2010 [==========>] Loss 0.12808473210971977  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.11532482085055742  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.10754396836048383  - accuracy: 0.8125\n",
      "At: 2013 [==========>] Loss 0.07944713088165323  - accuracy: 0.96875\n",
      "At: 2014 [==========>] Loss 0.19067539493644636  - accuracy: 0.6875\n",
      "At: 2015 [==========>] Loss 0.0719190101534696  - accuracy: 0.90625\n",
      "At: 2016 [==========>] Loss 0.12363670522447685  - accuracy: 0.875\n",
      "At: 2017 [==========>] Loss 0.09653673350634331  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.084774485688656  - accuracy: 0.84375\n",
      "At: 2019 [==========>] Loss 0.12177184174043162  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.0731275716333464  - accuracy: 0.875\n",
      "At: 2021 [==========>] Loss 0.10628513653654269  - accuracy: 0.78125\n",
      "At: 2022 [==========>] Loss 0.11094606113545664  - accuracy: 0.8125\n",
      "At: 2023 [==========>] Loss 0.09550804338861409  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.1101069629158574  - accuracy: 0.875\n",
      "At: 2025 [==========>] Loss 0.17279244448144895  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.10791268466315995  - accuracy: 0.84375\n",
      "At: 2027 [==========>] Loss 0.15753797081522353  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.1099763131429656  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.13808602283410176  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.1590460943916035  - accuracy: 0.75\n",
      "At: 2031 [==========>] Loss 0.17512642852477306  - accuracy: 0.75\n",
      "At: 2032 [==========>] Loss 0.14098302046521013  - accuracy: 0.8125\n",
      "At: 2033 [==========>] Loss 0.1501474528402389  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.23134465852840352  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.11985004494022616  - accuracy: 0.8125\n",
      "At: 2036 [==========>] Loss 0.11106592888088505  - accuracy: 0.84375\n",
      "At: 2037 [==========>] Loss 0.13054096656549707  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.09930576099406242  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.09558641971337623  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.11938712647808547  - accuracy: 0.875\n",
      "At: 2041 [==========>] Loss 0.055633243006297434  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.10875199440788397  - accuracy: 0.78125\n",
      "At: 2043 [==========>] Loss 0.13089267390339182  - accuracy: 0.8125\n",
      "At: 2044 [==========>] Loss 0.09843963361485505  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.23242000563139043  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.0725266189512744  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.08633112975314819  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.11739509660766753  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.13717760207135365  - accuracy: 0.75\n",
      "At: 2050 [==========>] Loss 0.1836219065671158  - accuracy: 0.71875\n",
      "At: 2051 [==========>] Loss 0.16965212539424618  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.07769293461052242  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.1191521085918188  - accuracy: 0.8125\n",
      "At: 2054 [==========>] Loss 0.13385274490563398  - accuracy: 0.75\n",
      "At: 2055 [==========>] Loss 0.0757031963119008  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.12746048254140804  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.15972388347580418  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.1292176763668042  - accuracy: 0.78125\n",
      "At: 2059 [==========>] Loss 0.18554785883678038  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.1309104643555462  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.1306791655854473  - accuracy: 0.84375\n",
      "At: 2062 [==========>] Loss 0.1481367464610813  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.1119167376517209  - accuracy: 0.84375\n",
      "At: 2064 [==========>] Loss 0.17242907663907933  - accuracy: 0.78125\n",
      "At: 2065 [==========>] Loss 0.050015571331073864  - accuracy: 0.9375\n",
      "At: 2066 [==========>] Loss 0.14043403482378325  - accuracy: 0.78125\n",
      "At: 2067 [==========>] Loss 0.10006593966359688  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.11151418321022902  - accuracy: 0.84375\n",
      "At: 2069 [==========>] Loss 0.08062864617795366  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.15473617229422176  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.12388859776075548  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.07513397106564353  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.10063623927040996  - accuracy: 0.8125\n",
      "At: 2074 [==========>] Loss 0.09887663217615174  - accuracy: 0.875\n",
      "At: 2075 [==========>] Loss 0.10178734202966083  - accuracy: 0.875\n",
      "At: 2076 [==========>] Loss 0.1368273288785212  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.15077942594383115  - accuracy: 0.8125\n",
      "At: 2078 [==========>] Loss 0.11196931404751391  - accuracy: 0.8125\n",
      "At: 2079 [==========>] Loss 0.0797659503025237  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.11424826738768568  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.15542548397329659  - accuracy: 0.75\n",
      "At: 2082 [==========>] Loss 0.14241046968804177  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.18998894606674738  - accuracy: 0.78125\n",
      "At: 2084 [==========>] Loss 0.1063817205684272  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.09858809092874617  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.10132095596246841  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.1710451709681356  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.09864281708730027  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.127308619893158  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.09565296038964254  - accuracy: 0.90625\n",
      "At: 2091 [==========>] Loss 0.11403114577319788  - accuracy: 0.84375\n",
      "At: 2092 [==========>] Loss 0.07756686175930139  - accuracy: 0.875\n",
      "At: 2093 [==========>] Loss 0.1476508012111266  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.13726125091279884  - accuracy: 0.78125\n",
      "At: 2095 [==========>] Loss 0.13096630034511292  - accuracy: 0.84375\n",
      "At: 2096 [==========>] Loss 0.18782197795024536  - accuracy: 0.71875\n",
      "At: 2097 [==========>] Loss 0.12854681122653305  - accuracy: 0.78125\n",
      "At: 2098 [==========>] Loss 0.1353408319741291  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.12054312090327682  - accuracy: 0.78125\n",
      "At: 2100 [==========>] Loss 0.07071049093697847  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.14371606959536873  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.07936256379948639  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.145929493816374  - accuracy: 0.8125\n",
      "At: 2104 [==========>] Loss 0.09680145086283456  - accuracy: 0.875\n",
      "At: 2105 [==========>] Loss 0.17138076116656487  - accuracy: 0.78125\n",
      "At: 2106 [==========>] Loss 0.1521346270353883  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.07644913948372724  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.13373844170107332  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.14048539110573824  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.06926113805632034  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.11449356631644707  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.11487096164667507  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.10583939182787885  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.12892459314760402  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.12804106915057303  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.09823571960751681  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.15129297213745319  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.1423287079586541  - accuracy: 0.78125\n",
      "At: 2119 [==========>] Loss 0.09000876877246039  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.1498355737087615  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.12162926529460596  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.15368380084600713  - accuracy: 0.75\n",
      "At: 2123 [==========>] Loss 0.17912685550103175  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.12593660707209828  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.09054390633824999  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.05730022600639276  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.09118845090265412  - accuracy: 0.90625\n",
      "At: 2128 [==========>] Loss 0.10792384467070126  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.16032857572436507  - accuracy: 0.78125\n",
      "At: 2130 [==========>] Loss 0.05620898521029651  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.10585682608414496  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.21133747776599868  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.16101257293926927  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.12840086470343023  - accuracy: 0.75\n",
      "At: 2135 [==========>] Loss 0.1032904447473803  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.14694191929035655  - accuracy: 0.71875\n",
      "At: 2137 [==========>] Loss 0.1163986680089202  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.12983078454215166  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.17966410122283522  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.08711225084563104  - accuracy: 0.8125\n",
      "At: 2141 [==========>] Loss 0.1372990299802822  - accuracy: 0.75\n",
      "At: 2142 [==========>] Loss 0.11946190652919482  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.0948271518809081  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.08394074812306909  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.09672669527181656  - accuracy: 0.78125\n",
      "At: 2146 [==========>] Loss 0.1688152858203238  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.09377634739848051  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.2005961245225333  - accuracy: 0.6875\n",
      "At: 2149 [==========>] Loss 0.12377533513294023  - accuracy: 0.78125\n",
      "At: 2150 [==========>] Loss 0.10611229921882467  - accuracy: 0.84375\n",
      "At: 2151 [==========>] Loss 0.10656316195505702  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.24708024363233397  - accuracy: 0.625\n",
      "At: 2153 [==========>] Loss 0.18520437910889412  - accuracy: 0.65625\n",
      "At: 2154 [==========>] Loss 0.154416308652905  - accuracy: 0.84375\n",
      "At: 2155 [==========>] Loss 0.14446079329160044  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.11893270347014709  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.10213777392366638  - accuracy: 0.875\n",
      "At: 2158 [==========>] Loss 0.16682411761648747  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.06870083096962702  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.1418773275005433  - accuracy: 0.84375\n",
      "At: 2161 [==========>] Loss 0.12864663284758787  - accuracy: 0.78125\n",
      "At: 2162 [==========>] Loss 0.10478491597982895  - accuracy: 0.84375\n",
      "At: 2163 [==========>] Loss 0.13644215757079647  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.15985070328593198  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.11132248912144055  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.12082044085111085  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.07968672713792227  - accuracy: 0.9375\n",
      "At: 2168 [==========>] Loss 0.08260655706513512  - accuracy: 0.90625\n",
      "At: 2169 [==========>] Loss 0.12228269226921354  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11767222899536037  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.16788229370503438  - accuracy: 0.75\n",
      "At: 2172 [==========>] Loss 0.12806510893234188  - accuracy: 0.78125\n",
      "At: 2173 [==========>] Loss 0.13161754686686286  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.12013349577525369  - accuracy: 0.875\n",
      "At: 2175 [==========>] Loss 0.1190642485846551  - accuracy: 0.875\n",
      "At: 2176 [==========>] Loss 0.14973678614689215  - accuracy: 0.8125\n",
      "At: 2177 [==========>] Loss 0.15955507675582095  - accuracy: 0.75\n",
      "At: 2178 [==========>] Loss 0.09814574714252823  - accuracy: 0.90625\n",
      "At: 2179 [==========>] Loss 0.1268508575775365  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.11204108674484328  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.17155823670603343  - accuracy: 0.6875\n",
      "At: 2182 [==========>] Loss 0.11220110637921418  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.2575447949785024  - accuracy: 0.625\n",
      "At: 2184 [==========>] Loss 0.08634696705838421  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.08171647388097375  - accuracy: 0.9375\n",
      "At: 2186 [==========>] Loss 0.1666598336800621  - accuracy: 0.71875\n",
      "At: 2187 [==========>] Loss 0.1834210898188029  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.08457902554438929  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.08905661934341043  - accuracy: 0.875\n",
      "At: 2190 [==========>] Loss 0.12334096622549189  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.1105343049733743  - accuracy: 0.78125\n",
      "At: 2192 [==========>] Loss 0.11310002132435563  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.1472461092526804  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.10445505230878599  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.17637545683242273  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.14564386215434144  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.07330251793060882  - accuracy: 0.9375\n",
      "At: 2198 [==========>] Loss 0.11148999599172933  - accuracy: 0.84375\n",
      "At: 2199 [==========>] Loss 0.05256517696951492  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.05640385296897493  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.10793546953990447  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.09418863094625275  - accuracy: 0.875\n",
      "At: 2203 [==========>] Loss 0.10028958002926738  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.12482424206918767  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.11603058860311133  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.1096951887668082  - accuracy: 0.75\n",
      "At: 2207 [==========>] Loss 0.1027316331820996  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.16337844377661623  - accuracy: 0.78125\n",
      "At: 2209 [==========>] Loss 0.15160226054448867  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.14191594581127331  - accuracy: 0.75\n",
      "At: 2211 [==========>] Loss 0.1555282413761069  - accuracy: 0.8125\n",
      "At: 2212 [==========>] Loss 0.07929057065066308  - accuracy: 0.90625\n",
      "At: 2213 [==========>] Loss 0.14012300724222898  - accuracy: 0.78125\n",
      "At: 2214 [==========>] Loss 0.11967282472010296  - accuracy: 0.84375\n",
      "At: 2215 [==========>] Loss 0.11175623771497856  - accuracy: 0.84375\n",
      "At: 2216 [==========>] Loss 0.13372400233087098  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.1400874308920733  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.17826206771925468  - accuracy: 0.78125\n",
      "At: 2219 [==========>] Loss 0.0907258456652622  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.12963859333673017  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.17441025110685116  - accuracy: 0.71875\n",
      "At: 2222 [==========>] Loss 0.11529034415155051  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.15291593238691506  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.13863934299404546  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.12940208316163582  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.10087184707337606  - accuracy: 0.875\n",
      "At: 2227 [==========>] Loss 0.20180035280516279  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.07600165771083843  - accuracy: 0.9375\n",
      "At: 2229 [==========>] Loss 0.16780478512498492  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.11351837034332404  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.16528628742748203  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.16224630018302202  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.13706556885806112  - accuracy: 0.84375\n",
      "At: 2234 [==========>] Loss 0.14507897952634244  - accuracy: 0.8125\n",
      "At: 2235 [==========>] Loss 0.11664663639840567  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.07226637063432183  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.12639107048128306  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.16038681741782884  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.15869678527735023  - accuracy: 0.78125\n",
      "At: 2240 [==========>] Loss 0.14895732810531098  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.15193975651297661  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.15974533332645138  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.08542280042003103  - accuracy: 0.84375\n",
      "At: 2244 [==========>] Loss 0.09093399008135158  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.06568353943341482  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.12052986043732423  - accuracy: 0.84375\n",
      "At: 2247 [==========>] Loss 0.11498872058167092  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.16307385228160715  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.11098196741427524  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.08378468809089579  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.09840240238122477  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.10482671754083317  - accuracy: 0.875\n",
      "At: 2253 [==========>] Loss 0.12608859244704013  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.14292540395011494  - accuracy: 0.78125\n",
      "At: 2255 [==========>] Loss 0.15013712844567484  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.12858760927088958  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.10876426891407157  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.15520723628212219  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.13513023076894765  - accuracy: 0.78125\n",
      "At: 2260 [==========>] Loss 0.17806608176808475  - accuracy: 0.75\n",
      "At: 2261 [==========>] Loss 0.08843684420150093  - accuracy: 0.875\n",
      "At: 2262 [==========>] Loss 0.16689654913281493  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.1518055584276429  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.10063937075690924  - accuracy: 0.8125\n",
      "At: 2265 [==========>] Loss 0.10182939839735988  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.11591139124294701  - accuracy: 0.875\n",
      "At: 2267 [==========>] Loss 0.06556177332341452  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.09444241936652725  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.04376671764204058  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.10359219393577607  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.14931732184755353  - accuracy: 0.71875\n",
      "At: 2272 [==========>] Loss 0.08677877545720278  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.11223935926673874  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.10412131592780646  - accuracy: 0.8125\n",
      "At: 2275 [==========>] Loss 0.08683832827079034  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.10929808421327228  - accuracy: 0.8125\n",
      "At: 2277 [==========>] Loss 0.15010276111860996  - accuracy: 0.84375\n",
      "At: 2278 [==========>] Loss 0.09933991089457622  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.14496609295331614  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.13628111858781872  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.09058885423070981  - accuracy: 0.875\n",
      "At: 2282 [==========>] Loss 0.06395925833369445  - accuracy: 0.96875\n",
      "At: 2283 [==========>] Loss 0.16740152029795935  - accuracy: 0.6875\n",
      "At: 2284 [==========>] Loss 0.15131634726936255  - accuracy: 0.71875\n",
      "At: 2285 [==========>] Loss 0.11009680558586564  - accuracy: 0.875\n",
      "At: 2286 [==========>] Loss 0.1369866387966589  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.14787298340840865  - accuracy: 0.78125\n",
      "At: 2288 [==========>] Loss 0.10270318313195907  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.14712669313552745  - accuracy: 0.75\n",
      "At: 2290 [==========>] Loss 0.05821910850587232  - accuracy: 0.96875\n",
      "At: 2291 [==========>] Loss 0.1290669090249873  - accuracy: 0.78125\n",
      "At: 2292 [==========>] Loss 0.08040565780292105  - accuracy: 0.9375\n",
      "At: 2293 [==========>] Loss 0.05599618608445177  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.06105675658551167  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.14143030606023343  - accuracy: 0.6875\n",
      "At: 2296 [==========>] Loss 0.14552486361321373  - accuracy: 0.8125\n",
      "At: 2297 [==========>] Loss 0.07384061859028726  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.1276553729833189  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.11356698226710057  - accuracy: 0.78125\n",
      "At: 2300 [==========>] Loss 0.11313031062595988  - accuracy: 0.8125\n",
      "At: 2301 [==========>] Loss 0.1917301172222654  - accuracy: 0.625\n",
      "At: 2302 [==========>] Loss 0.18900547291303998  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.07060743902367819  - accuracy: 0.9375\n",
      "At: 2304 [==========>] Loss 0.08387979704703996  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.12332508780229887  - accuracy: 0.84375\n",
      "At: 2306 [==========>] Loss 0.12843868649065654  - accuracy: 0.75\n",
      "At: 2307 [==========>] Loss 0.17883002122696998  - accuracy: 0.75\n",
      "At: 2308 [==========>] Loss 0.1901225278453171  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.11365162775027378  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.16326731712860898  - accuracy: 0.78125\n",
      "At: 2311 [==========>] Loss 0.16460949786270052  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.1226800167667455  - accuracy: 0.90625\n",
      "At: 2313 [==========>] Loss 0.0860120098994391  - accuracy: 0.875\n",
      "At: 2314 [==========>] Loss 0.1269534689725963  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.11747115429776511  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.13468643626532542  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.16672632032653045  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.17653166457996083  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.1484517393507012  - accuracy: 0.78125\n",
      "At: 2320 [==========>] Loss 0.103707357651764  - accuracy: 0.84375\n",
      "At: 2321 [==========>] Loss 0.11667242632852157  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.2213230869331366  - accuracy: 0.6875\n",
      "At: 2323 [==========>] Loss 0.16570033924026778  - accuracy: 0.8125\n",
      "At: 2324 [==========>] Loss 0.1562538426732943  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.10942191432705087  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.0713430857630048  - accuracy: 0.90625\n",
      "At: 2327 [==========>] Loss 0.0764647479408786  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12399524785652417  - accuracy: 0.90625\n",
      "At: 2329 [==========>] Loss 0.11598375854062254  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.14398873880942428  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.08131739023809975  - accuracy: 0.90625\n",
      "At: 2332 [==========>] Loss 0.12705093803487263  - accuracy: 0.75\n",
      "At: 2333 [==========>] Loss 0.11148668439439118  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.16102614993200126  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.1170519686377759  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.10889826609815058  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.15796856174485013  - accuracy: 0.75\n",
      "At: 2338 [==========>] Loss 0.09212376632113312  - accuracy: 0.84375\n",
      "At: 2339 [==========>] Loss 0.08965364829686678  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.13683860067695078  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.15325107439355737  - accuracy: 0.6875\n",
      "At: 2342 [==========>] Loss 0.1935616573637361  - accuracy: 0.71875\n",
      "At: 2343 [==========>] Loss 0.0996182559135052  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.18764173421465435  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.13795398911427326  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.08312330680083524  - accuracy: 0.875\n",
      "At: 2347 [==========>] Loss 0.13522853427733944  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.07448665863818421  - accuracy: 0.90625\n",
      "At: 2349 [==========>] Loss 0.11522849561824008  - accuracy: 0.8125\n",
      "At: 2350 [==========>] Loss 0.13695956074638846  - accuracy: 0.84375\n",
      "At: 2351 [==========>] Loss 0.09737272453148302  - accuracy: 0.78125\n",
      "At: 2352 [==========>] Loss 0.12929458131340993  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.09215101856021904  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.17485758642217575  - accuracy: 0.6875\n",
      "At: 2355 [==========>] Loss 0.051451848099211814  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.1396486350661112  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.1524155194937099  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.16192073294493609  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.17363566023932653  - accuracy: 0.75\n",
      "At: 2360 [==========>] Loss 0.13874134970842328  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.17882529524552956  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.09234145151087847  - accuracy: 0.84375\n",
      "At: 2363 [==========>] Loss 0.1687286437412674  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.0878848890794858  - accuracy: 0.875\n",
      "At: 2365 [==========>] Loss 0.0849547777880508  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.1580738534364546  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.10910383806384859  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.12442062266740911  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.11977371589453731  - accuracy: 0.8125\n",
      "At: 2370 [==========>] Loss 0.0857263937616259  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.1038869588780894  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.14445498284710384  - accuracy: 0.8125\n",
      "At: 2373 [==========>] Loss 0.13019594076169994  - accuracy: 0.8125\n",
      "At: 2374 [==========>] Loss 0.11214185675958102  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.06222234192779508  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.11702825066328645  - accuracy: 0.8125\n",
      "At: 2377 [==========>] Loss 0.10063601278003698  - accuracy: 0.90625\n",
      "At: 2378 [==========>] Loss 0.11017086489512401  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.17809262602285442  - accuracy: 0.78125\n",
      "At: 2380 [==========>] Loss 0.07404451276522594  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.08885722943332243  - accuracy: 0.9375\n",
      "At: 2382 [==========>] Loss 0.11078904422701122  - accuracy: 0.8125\n",
      "At: 2383 [==========>] Loss 0.14140835348743994  - accuracy: 0.78125\n",
      "At: 2384 [==========>] Loss 0.10485239187997494  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.13179379114062523  - accuracy: 0.8125\n",
      "At: 2386 [==========>] Loss 0.09464565674763413  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.06933775593485444  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.11475989579424772  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.0590152769615932  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.0686638608631025  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.17779920228819157  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.14262390553316118  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.10518517963776014  - accuracy: 0.90625\n",
      "At: 2394 [==========>] Loss 0.06374588135955558  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.09068524511069882  - accuracy: 0.90625\n",
      "At: 2396 [==========>] Loss 0.07312112599844778  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.08842211137276502  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.10585832022197172  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.16214250952588136  - accuracy: 0.8125\n",
      "At: 2400 [==========>] Loss 0.09302376150686376  - accuracy: 0.90625\n",
      "At: 2401 [==========>] Loss 0.08388354541302857  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.0912166943231959  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.1867922546414847  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.14211172068959746  - accuracy: 0.8125\n",
      "At: 2405 [==========>] Loss 0.08980132165470134  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.12594550568342103  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.12054281546973619  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.10877788339684796  - accuracy: 0.84375\n",
      "At: 2409 [==========>] Loss 0.15166826519547036  - accuracy: 0.78125\n",
      "At: 2410 [==========>] Loss 0.15865925634503014  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.09057117208390401  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.08988429456008522  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.1229694099115538  - accuracy: 0.8125\n",
      "At: 2414 [==========>] Loss 0.06369744566872425  - accuracy: 0.96875\n",
      "At: 2415 [==========>] Loss 0.08987172222139499  - accuracy: 0.90625\n",
      "At: 2416 [==========>] Loss 0.08435380313281801  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.1785472842846048  - accuracy: 0.71875\n",
      "At: 2418 [==========>] Loss 0.13323753593250895  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.13798877411260124  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.14673404049138802  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.10698482144655953  - accuracy: 0.875\n",
      "At: 2422 [==========>] Loss 0.1643865665938735  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.1370555769289592  - accuracy: 0.75\n",
      "At: 2424 [==========>] Loss 0.0947685673678647  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08510969075926705  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.19920285097822435  - accuracy: 0.6875\n",
      "At: 2427 [==========>] Loss 0.150450339679359  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.07716015590866157  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.1339859489243203  - accuracy: 0.8125\n",
      "At: 2430 [==========>] Loss 0.1446941711079935  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.15148670290082458  - accuracy: 0.71875\n",
      "At: 2432 [==========>] Loss 0.07611377546875125  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.07582650041547702  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.038396226267358086  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.16631825757291283  - accuracy: 0.78125\n",
      "At: 2436 [==========>] Loss 0.11584111935528915  - accuracy: 0.78125\n",
      "At: 2437 [==========>] Loss 0.16522171923104356  - accuracy: 0.78125\n",
      "At: 2438 [==========>] Loss 0.09159435599857751  - accuracy: 0.9375\n",
      "At: 2439 [==========>] Loss 0.14311835387177363  - accuracy: 0.75\n",
      "At: 2440 [==========>] Loss 0.11243189312430653  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.11131993827568905  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.13976010838177272  - accuracy: 0.71875\n",
      "At: 2443 [==========>] Loss 0.12155386864547305  - accuracy: 0.84375\n",
      "At: 2444 [==========>] Loss 0.08047826478326987  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.06473345200502376  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.15770633679907126  - accuracy: 0.8125\n",
      "At: 2447 [==========>] Loss 0.16140872773958853  - accuracy: 0.75\n",
      "At: 2448 [==========>] Loss 0.14057639844545505  - accuracy: 0.71875\n",
      "At: 2449 [==========>] Loss 0.07544889637193787  - accuracy: 0.9375\n",
      "At: 2450 [==========>] Loss 0.0727775433254995  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.062430524049204184  - accuracy: 0.875\n",
      "At: 2452 [==========>] Loss 0.131253807131209  - accuracy: 0.875\n",
      "At: 2453 [==========>] Loss 0.13426234897639125  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.15217178756691369  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1449144256769331  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.1441585307686607  - accuracy: 0.78125\n",
      "At: 2457 [==========>] Loss 0.1683927213099904  - accuracy: 0.71875\n",
      "At: 2458 [==========>] Loss 0.08950739026142926  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.1546239626375357  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.07515926989058505  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.07003364494252137  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.15289118227677057  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.10528680735511181  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.1681832532946309  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.14073974226032995  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.09477454078100923  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.09627991166653904  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.09391524506440803  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.14972311237668023  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.15484485975095597  - accuracy: 0.8125\n",
      "At: 2471 [==========>] Loss 0.10859636676561289  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.10492389045013432  - accuracy: 0.8125\n",
      "At: 2473 [==========>] Loss 0.12020734994840256  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.08696745137624515  - accuracy: 0.875\n",
      "At: 2475 [==========>] Loss 0.11002979587736034  - accuracy: 0.84375\n",
      "At: 2476 [==========>] Loss 0.0827505484106173  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.1369283213506885  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.10724427132700727  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.08606258648802101  - accuracy: 0.8125\n",
      "At: 2480 [==========>] Loss 0.12062908842431891  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.08019903418045349  - accuracy: 0.90625\n",
      "At: 2482 [==========>] Loss 0.18518413443459214  - accuracy: 0.75\n",
      "At: 2483 [==========>] Loss 0.09674576517572525  - accuracy: 0.875\n",
      "At: 2484 [==========>] Loss 0.0952850698532697  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.12188919736499929  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.11813812294646565  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.14008016094660913  - accuracy: 0.78125\n",
      "At: 2488 [==========>] Loss 0.16175930253884663  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.1762453875211556  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.11257863276079563  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.1549227652681755  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.12761787963827612  - accuracy: 0.875\n",
      "At: 2493 [==========>] Loss 0.08531017151794537  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.10668935834465913  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.1004328875556068  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.06720318762111535  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.20091471193713395  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.11575927291763602  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.0727114069749533  - accuracy: 0.875\n",
      "At: 2500 [==========>] Loss 0.17914389971652894  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.17430949847267674  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.11020574887606512  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.10884765118294323  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.16147746776522692  - accuracy: 0.75\n",
      "At: 2505 [==========>] Loss 0.10362921299788219  - accuracy: 0.84375\n",
      "At: 2506 [==========>] Loss 0.11596361588449718  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.13417677134381056  - accuracy: 0.71875\n",
      "At: 2508 [==========>] Loss 0.12574350333398876  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.14042291582449395  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.11276474655972263  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.160688040878734  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.10687942860514982  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.15169001230036303  - accuracy: 0.8125\n",
      "At: 2514 [==========>] Loss 0.14119988815355156  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.17462325786186378  - accuracy: 0.78125\n",
      "At: 2516 [==========>] Loss 0.18596120617237683  - accuracy: 0.6875\n",
      "At: 2517 [==========>] Loss 0.11612463587962481  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.1356698309313997  - accuracy: 0.78125\n",
      "At: 2519 [==========>] Loss 0.11158810536877427  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.13007204303402686  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.1103315972687174  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.21121478935232507  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.12643962961384342  - accuracy: 0.78125\n",
      "At: 2524 [==========>] Loss 0.17780896495694387  - accuracy: 0.75\n",
      "At: 2525 [==========>] Loss 0.07901876165479112  - accuracy: 0.9375\n",
      "At: 2526 [==========>] Loss 0.11012086634256708  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.11765574402424173  - accuracy: 0.84375\n",
      "At: 2528 [==========>] Loss 0.09151929394537184  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.14895956173684846  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.14025448340284857  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.07192092125384009  - accuracy: 0.9375\n",
      "At: 2532 [==========>] Loss 0.11529857937681734  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.0985734480628096  - accuracy: 0.9375\n",
      "At: 2534 [==========>] Loss 0.07081577616074806  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.0777600477702377  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.152447354200972  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.11722384567667538  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.17966723519804836  - accuracy: 0.6875\n",
      "At: 2539 [==========>] Loss 0.08916710276267745  - accuracy: 0.875\n",
      "At: 2540 [==========>] Loss 0.14078282132296613  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.06049290818245561  - accuracy: 0.96875\n",
      "At: 2542 [==========>] Loss 0.05692533338389129  - accuracy: 0.90625\n",
      "At: 2543 [==========>] Loss 0.15455433538419813  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.13285405599148156  - accuracy: 0.75\n",
      "At: 2545 [==========>] Loss 0.058305612598180706  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.1302103135970239  - accuracy: 0.875\n",
      "At: 2547 [==========>] Loss 0.08710350344357767  - accuracy: 0.90625\n",
      "At: 2548 [==========>] Loss 0.07958222230246124  - accuracy: 0.9375\n",
      "At: 2549 [==========>] Loss 0.0897771672445332  - accuracy: 0.84375\n",
      "At: 2550 [==========>] Loss 0.13821925896519185  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.13829176569117224  - accuracy: 0.84375\n",
      "At: 2552 [==========>] Loss 0.11753740109566942  - accuracy: 0.8125\n",
      "At: 2553 [==========>] Loss 0.07319588654737127  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.13607522452752402  - accuracy: 0.84375\n",
      "At: 2555 [==========>] Loss 0.16552118535045152  - accuracy: 0.78125\n",
      "At: 2556 [==========>] Loss 0.10260639574448803  - accuracy: 0.84375\n",
      "At: 2557 [==========>] Loss 0.1077577012238368  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.11451954673941944  - accuracy: 0.78125\n",
      "At: 2559 [==========>] Loss 0.1082850165236997  - accuracy: 0.8125\n",
      "At: 2560 [==========>] Loss 0.164018706351999  - accuracy: 0.8125\n",
      "At: 2561 [==========>] Loss 0.10248707230182373  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.10894134894316831  - accuracy: 0.8125\n",
      "At: 2563 [==========>] Loss 0.10525158341045142  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09055055588775082  - accuracy: 0.90625\n",
      "At: 2565 [==========>] Loss 0.11254678781787399  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1723492453432609  - accuracy: 0.78125\n",
      "At: 2567 [==========>] Loss 0.12658624351209458  - accuracy: 0.8125\n",
      "At: 2568 [==========>] Loss 0.11290398882603422  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.06710280312952904  - accuracy: 0.90625\n",
      "At: 2570 [==========>] Loss 0.1816994673987825  - accuracy: 0.75\n",
      "At: 2571 [==========>] Loss 0.10539236437366534  - accuracy: 0.8125\n",
      "At: 2572 [==========>] Loss 0.17694094921661005  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.19025737847137686  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.14360127741623555  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.05697687945904849  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.10597728110907007  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.21840859481512265  - accuracy: 0.6875\n",
      "At: 2578 [==========>] Loss 0.1614432371527788  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.169643470887456  - accuracy: 0.78125\n",
      "At: 2580 [==========>] Loss 0.13898996130210264  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.060185186970527965  - accuracy: 0.96875\n",
      "At: 2582 [==========>] Loss 0.21610021561717513  - accuracy: 0.71875\n",
      "At: 2583 [==========>] Loss 0.08687399959116845  - accuracy: 0.90625\n",
      "At: 2584 [==========>] Loss 0.16938280179255988  - accuracy: 0.8125\n",
      "At: 2585 [==========>] Loss 0.09119591515722275  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.08903273378032592  - accuracy: 0.875\n",
      "At: 2587 [==========>] Loss 0.17221638191841304  - accuracy: 0.65625\n",
      "At: 2588 [==========>] Loss 0.14908202808856863  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.10450044262753304  - accuracy: 0.8125\n",
      "At: 2590 [==========>] Loss 0.19776744591381862  - accuracy: 0.71875\n",
      "At: 2591 [==========>] Loss 0.12779444609109836  - accuracy: 0.78125\n",
      "At: 2592 [==========>] Loss 0.10425368275228422  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.07865256192692928  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.08505947571164893  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.09066360452295423  - accuracy: 0.84375\n",
      "At: 2596 [==========>] Loss 0.12099660938617787  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.06399781346634245  - accuracy: 0.9375\n",
      "At: 2598 [==========>] Loss 0.12298537839645096  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.10824083211541198  - accuracy: 0.84375\n",
      "At: 2600 [==========>] Loss 0.12482112475193458  - accuracy: 0.75\n",
      "At: 2601 [==========>] Loss 0.14459635204275198  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.08107705302503812  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.16799514685916203  - accuracy: 0.75\n",
      "At: 2604 [==========>] Loss 0.11616573979223982  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.13251659801669988  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.11321855248685814  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.12818816816210638  - accuracy: 0.8125\n",
      "At: 2608 [==========>] Loss 0.07703308994649491  - accuracy: 0.90625\n",
      "At: 2609 [==========>] Loss 0.0953810381181722  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.14050518722702154  - accuracy: 0.8125\n",
      "At: 2611 [==========>] Loss 0.13538591201539485  - accuracy: 0.75\n",
      "At: 2612 [==========>] Loss 0.08704399191075061  - accuracy: 0.84375\n",
      "At: 2613 [==========>] Loss 0.1041271627510669  - accuracy: 0.8125\n",
      "At: 2614 [==========>] Loss 0.11100190285531261  - accuracy: 0.875\n",
      "At: 2615 [==========>] Loss 0.08281372494029539  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06320462968871471  - accuracy: 0.875\n",
      "At: 2617 [==========>] Loss 0.10467145365345151  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.08105309547815727  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.11108913814060017  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.09502229399070852  - accuracy: 0.875\n",
      "At: 2621 [==========>] Loss 0.11796823287522858  - accuracy: 0.8125\n",
      "At: 2622 [==========>] Loss 0.09645162438457716  - accuracy: 0.875\n",
      "At: 2623 [==========>] Loss 0.10773713359261565  - accuracy: 0.875\n",
      "At: 2624 [==========>] Loss 0.1210002928184548  - accuracy: 0.875\n",
      "At: 2625 [==========>] Loss 0.06679390764792917  - accuracy: 0.90625\n",
      "At: 2626 [==========>] Loss 0.08473934397785288  - accuracy: 0.90625\n",
      "At: 2627 [==========>] Loss 0.1415375150520883  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.08419411935743933  - accuracy: 0.875\n",
      "At: 2629 [==========>] Loss 0.09702619302037843  - accuracy: 0.90625\n",
      "At: 2630 [==========>] Loss 0.08710897590627997  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.1022805020018183  - accuracy: 0.875\n",
      "At: 2632 [==========>] Loss 0.13529657003784795  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.106018447354596  - accuracy: 0.875\n",
      "At: 2634 [==========>] Loss 0.05860464910038495  - accuracy: 0.9375\n",
      "At: 2635 [==========>] Loss 0.2219904276269647  - accuracy: 0.65625\n",
      "At: 2636 [==========>] Loss 0.13954287289487313  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.16688144456797088  - accuracy: 0.75\n",
      "At: 2638 [==========>] Loss 0.08182779677599665  - accuracy: 0.90625\n",
      "At: 2639 [==========>] Loss 0.060294053231775765  - accuracy: 0.9375\n",
      "At: 2640 [==========>] Loss 0.12176478311866828  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.14934643876440787  - accuracy: 0.78125\n",
      "At: 2642 [==========>] Loss 0.18018068281616217  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.09299614524671954  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.1651447941516227  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.0624459461038453  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.1408521918289175  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.14160681926677207  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.1507052405308656  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.13988135906889426  - accuracy: 0.78125\n",
      "At: 2650 [==========>] Loss 0.11843891069559107  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.17214559458085643  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.11763345989201117  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.11326638915530554  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.14557696446500978  - accuracy: 0.75\n",
      "At: 2655 [==========>] Loss 0.23092823952368302  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.027363505972078452  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.11841530249990659  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.09932863674787386  - accuracy: 0.84375\n",
      "At: 2659 [==========>] Loss 0.0545289279564539  - accuracy: 0.96875\n",
      "At: 2660 [==========>] Loss 0.10275824714902254  - accuracy: 0.84375\n",
      "At: 2661 [==========>] Loss 0.0890098581275702  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.07644803967558453  - accuracy: 0.875\n",
      "At: 2663 [==========>] Loss 0.07878135014924503  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.0909699394998981  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.09561115003836415  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.14841216983028466  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.15166151182073406  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.13865954110352344  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.14259860373107391  - accuracy: 0.75\n",
      "At: 2670 [==========>] Loss 0.1200270726960448  - accuracy: 0.8125\n",
      "At: 2671 [==========>] Loss 0.07316646859076784  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.08599061782107187  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.11983276343568651  - accuracy: 0.875\n",
      "At: 2674 [==========>] Loss 0.11608600787222237  - accuracy: 0.8125\n",
      "At: 2675 [==========>] Loss 0.13522570964702113  - accuracy: 0.78125\n",
      "At: 2676 [==========>] Loss 0.13106336977631267  - accuracy: 0.78125\n",
      "At: 2677 [==========>] Loss 0.10237626976331668  - accuracy: 0.90625\n",
      "At: 2678 [==========>] Loss 0.07397916623527087  - accuracy: 0.875\n",
      "At: 2679 [==========>] Loss 0.08780008233552064  - accuracy: 0.875\n",
      "At: 2680 [==========>] Loss 0.08869317164335239  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.10689614587924694  - accuracy: 0.84375\n",
      "At: 2682 [==========>] Loss 0.13539167699810828  - accuracy: 0.75\n",
      "At: 2683 [==========>] Loss 0.16987623077402325  - accuracy: 0.78125\n",
      "At: 2684 [==========>] Loss 0.11722561009966893  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.1298526373209838  - accuracy: 0.78125\n",
      "At: 2686 [==========>] Loss 0.07579721681108433  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.13199152935808509  - accuracy: 0.78125\n",
      "At: 2688 [==========>] Loss 0.15704567788343343  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.12675213105758174  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.10612359773833208  - accuracy: 0.875\n",
      "Epochs  3 / 10\n",
      "At: 1 [==========>] Loss 0.1163672957951864  - accuracy: 0.8125\n",
      "At: 2 [==========>] Loss 0.16854358796105687  - accuracy: 0.78125\n",
      "At: 3 [==========>] Loss 0.1371944320075535  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.13848979976758105  - accuracy: 0.78125\n",
      "At: 5 [==========>] Loss 0.06430311745445531  - accuracy: 0.9375\n",
      "At: 6 [==========>] Loss 0.13178111740952309  - accuracy: 0.875\n",
      "At: 7 [==========>] Loss 0.1261006552655631  - accuracy: 0.8125\n",
      "At: 8 [==========>] Loss 0.14786068393372398  - accuracy: 0.84375\n",
      "At: 9 [==========>] Loss 0.31784700634405655  - accuracy: 0.5625\n",
      "At: 10 [==========>] Loss 0.21438270944707805  - accuracy: 0.71875\n",
      "At: 11 [==========>] Loss 0.18561102979715532  - accuracy: 0.75\n",
      "At: 12 [==========>] Loss 0.16870470920362052  - accuracy: 0.75\n",
      "At: 13 [==========>] Loss 0.14454303204026203  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.05887682696143351  - accuracy: 0.9375\n",
      "At: 15 [==========>] Loss 0.13090905255039478  - accuracy: 0.8125\n",
      "At: 16 [==========>] Loss 0.18709309087998047  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.17453485995411652  - accuracy: 0.78125\n",
      "At: 18 [==========>] Loss 0.19917819923979002  - accuracy: 0.75\n",
      "At: 19 [==========>] Loss 0.12017822537926728  - accuracy: 0.90625\n",
      "At: 20 [==========>] Loss 0.10596015792324356  - accuracy: 0.875\n",
      "At: 21 [==========>] Loss 0.19764467013615206  - accuracy: 0.75\n",
      "At: 22 [==========>] Loss 0.15617966421397833  - accuracy: 0.84375\n",
      "At: 23 [==========>] Loss 0.08323468156841399  - accuracy: 0.90625\n",
      "At: 24 [==========>] Loss 0.24907951843992807  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.183459238322497  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.22666411012622478  - accuracy: 0.6875\n",
      "At: 27 [==========>] Loss 0.18035549416407987  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.1910659233717979  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.1607995923889631  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.18615623428287817  - accuracy: 0.78125\n",
      "At: 31 [==========>] Loss 0.21896538830394202  - accuracy: 0.71875\n",
      "At: 32 [==========>] Loss 0.19703371630435504  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.06578086850569292  - accuracy: 0.96875\n",
      "At: 34 [==========>] Loss 0.13465076449133856  - accuracy: 0.875\n",
      "At: 35 [==========>] Loss 0.1630879568708361  - accuracy: 0.75\n",
      "At: 36 [==========>] Loss 0.1780446282809217  - accuracy: 0.78125\n",
      "At: 37 [==========>] Loss 0.16827551145937922  - accuracy: 0.75\n",
      "At: 38 [==========>] Loss 0.2207425631308667  - accuracy: 0.6875\n",
      "At: 39 [==========>] Loss 0.20365525983919025  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.17479134381061517  - accuracy: 0.75\n",
      "At: 41 [==========>] Loss 0.06520104391534953  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.13463446534112317  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.1322066557066766  - accuracy: 0.84375\n",
      "At: 44 [==========>] Loss 0.13239124537222816  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.10447629307200677  - accuracy: 0.875\n",
      "At: 46 [==========>] Loss 0.15384301511844403  - accuracy: 0.8125\n",
      "At: 47 [==========>] Loss 0.21270297665011598  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.12476872141958684  - accuracy: 0.84375\n",
      "At: 49 [==========>] Loss 0.11685398573622338  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.09176195795338901  - accuracy: 0.875\n",
      "At: 51 [==========>] Loss 0.17696902052272973  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.21468168506880145  - accuracy: 0.6875\n",
      "At: 53 [==========>] Loss 0.11621535809319901  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.14049972269295907  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.18331834582102785  - accuracy: 0.71875\n",
      "At: 56 [==========>] Loss 0.15037161837180035  - accuracy: 0.78125\n",
      "At: 57 [==========>] Loss 0.09801518919601696  - accuracy: 0.9375\n",
      "At: 58 [==========>] Loss 0.19108144662401028  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.20772417939924454  - accuracy: 0.78125\n",
      "At: 60 [==========>] Loss 0.21251579552284044  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.20631400057454075  - accuracy: 0.75\n",
      "At: 62 [==========>] Loss 0.15809503512066678  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.19773394009607181  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.21249461131729025  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.19987662404969747  - accuracy: 0.71875\n",
      "At: 66 [==========>] Loss 0.15759251272987268  - accuracy: 0.78125\n",
      "At: 67 [==========>] Loss 0.2118727075022014  - accuracy: 0.6875\n",
      "At: 68 [==========>] Loss 0.0886910773688892  - accuracy: 0.90625\n",
      "At: 69 [==========>] Loss 0.13842959235540614  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.18411474543930872  - accuracy: 0.78125\n",
      "At: 71 [==========>] Loss 0.15765157434348775  - accuracy: 0.75\n",
      "At: 72 [==========>] Loss 0.12160134612089121  - accuracy: 0.84375\n",
      "At: 73 [==========>] Loss 0.14441983416760412  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.1672108151712758  - accuracy: 0.8125\n",
      "At: 75 [==========>] Loss 0.16106498743084224  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.20553631822178448  - accuracy: 0.75\n",
      "At: 77 [==========>] Loss 0.1861434426329349  - accuracy: 0.75\n",
      "At: 78 [==========>] Loss 0.07051067431784477  - accuracy: 0.9375\n",
      "At: 79 [==========>] Loss 0.1541150563343846  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.15709369156220826  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.1278300256672371  - accuracy: 0.8125\n",
      "At: 82 [==========>] Loss 0.1846982025487261  - accuracy: 0.71875\n",
      "At: 83 [==========>] Loss 0.1387881704320001  - accuracy: 0.84375\n",
      "At: 84 [==========>] Loss 0.13275216675518597  - accuracy: 0.84375\n",
      "At: 85 [==========>] Loss 0.18515573362077514  - accuracy: 0.6875\n",
      "At: 86 [==========>] Loss 0.18034688044130254  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.14115522363946778  - accuracy: 0.84375\n",
      "At: 88 [==========>] Loss 0.32563885244308766  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.16340620891765834  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.1978710912228317  - accuracy: 0.75\n",
      "At: 91 [==========>] Loss 0.1523491101092214  - accuracy: 0.8125\n",
      "At: 92 [==========>] Loss 0.06004325588967427  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.14718437916308014  - accuracy: 0.875\n",
      "At: 94 [==========>] Loss 0.14392173838009592  - accuracy: 0.84375\n",
      "At: 95 [==========>] Loss 0.14898334434742194  - accuracy: 0.8125\n",
      "At: 96 [==========>] Loss 0.13674911690931651  - accuracy: 0.8125\n",
      "At: 97 [==========>] Loss 0.10161522258619668  - accuracy: 0.875\n",
      "At: 98 [==========>] Loss 0.16381776845104293  - accuracy: 0.75\n",
      "At: 99 [==========>] Loss 0.11209297791351991  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.13754540648437086  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.0898810393132154  - accuracy: 0.90625\n",
      "At: 102 [==========>] Loss 0.21481142435631334  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.12359505628978656  - accuracy: 0.875\n",
      "At: 104 [==========>] Loss 0.11823715728972926  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.14734335730866088  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.11597083230699892  - accuracy: 0.84375\n",
      "At: 107 [==========>] Loss 0.17771735232049155  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.21468286128641514  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.10868547256216088  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.2389554801015027  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.08935639148572722  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.10825266457339319  - accuracy: 0.875\n",
      "At: 113 [==========>] Loss 0.16797286947022816  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.16751646040984441  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.11382464662358266  - accuracy: 0.84375\n",
      "At: 116 [==========>] Loss 0.20935909872187713  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.1525565681163349  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.26626971056630727  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.08965365193633429  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.16432452101445463  - accuracy: 0.75\n",
      "At: 121 [==========>] Loss 0.1556683529761917  - accuracy: 0.78125\n",
      "At: 122 [==========>] Loss 0.14839089641747483  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.1771612385398184  - accuracy: 0.71875\n",
      "At: 124 [==========>] Loss 0.2084088923909758  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.15435733779468658  - accuracy: 0.8125\n",
      "At: 126 [==========>] Loss 0.1986925940944824  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.15820520426709567  - accuracy: 0.8125\n",
      "At: 128 [==========>] Loss 0.16962741748542445  - accuracy: 0.71875\n",
      "At: 129 [==========>] Loss 0.12977275294947327  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.20029744987141296  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.1304666303348217  - accuracy: 0.78125\n",
      "At: 132 [==========>] Loss 0.16473783714878618  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.1820288959517222  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.17034224669901424  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.15725247145482874  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.15050395880411324  - accuracy: 0.84375\n",
      "At: 137 [==========>] Loss 0.07154338576286356  - accuracy: 0.9375\n",
      "At: 138 [==========>] Loss 0.13303385534461348  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.09115887816371752  - accuracy: 0.90625\n",
      "At: 140 [==========>] Loss 0.15984072019814183  - accuracy: 0.78125\n",
      "At: 141 [==========>] Loss 0.24701554548964655  - accuracy: 0.65625\n",
      "At: 142 [==========>] Loss 0.14281047531984542  - accuracy: 0.8125\n",
      "At: 143 [==========>] Loss 0.16275999045569142  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.1306357484295716  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.1516346160395957  - accuracy: 0.78125\n",
      "At: 146 [==========>] Loss 0.10572820681009364  - accuracy: 0.8125\n",
      "At: 147 [==========>] Loss 0.15353786453188603  - accuracy: 0.8125\n",
      "At: 148 [==========>] Loss 0.12328678523309371  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.15688392118412844  - accuracy: 0.8125\n",
      "At: 150 [==========>] Loss 0.1254861779641544  - accuracy: 0.84375\n",
      "At: 151 [==========>] Loss 0.10409931720285112  - accuracy: 0.90625\n",
      "At: 152 [==========>] Loss 0.11571697150974254  - accuracy: 0.84375\n",
      "At: 153 [==========>] Loss 0.1326580938839041  - accuracy: 0.8125\n",
      "At: 154 [==========>] Loss 0.13530204378853755  - accuracy: 0.84375\n",
      "At: 155 [==========>] Loss 0.19411940119422225  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.13705348518456245  - accuracy: 0.8125\n",
      "At: 157 [==========>] Loss 0.20732204547882227  - accuracy: 0.75\n",
      "At: 158 [==========>] Loss 0.13591222996611232  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.15048676039516845  - accuracy: 0.78125\n",
      "At: 160 [==========>] Loss 0.11884167060176146  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.10504669076316796  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.1984652224165932  - accuracy: 0.75\n",
      "At: 163 [==========>] Loss 0.1543683374222533  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.17116143895830382  - accuracy: 0.75\n",
      "At: 165 [==========>] Loss 0.17707949848206478  - accuracy: 0.75\n",
      "At: 166 [==========>] Loss 0.17459721537149453  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.08627487420245307  - accuracy: 0.90625\n",
      "At: 168 [==========>] Loss 0.15966247096387567  - accuracy: 0.8125\n",
      "At: 169 [==========>] Loss 0.1439787089411877  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.1335159470083132  - accuracy: 0.84375\n",
      "At: 171 [==========>] Loss 0.18304509787160414  - accuracy: 0.75\n",
      "At: 172 [==========>] Loss 0.14460397560383975  - accuracy: 0.78125\n",
      "At: 173 [==========>] Loss 0.20490736062135775  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.13944187818471276  - accuracy: 0.8125\n",
      "At: 175 [==========>] Loss 0.15180026008908679  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.1507562008973577  - accuracy: 0.84375\n",
      "At: 177 [==========>] Loss 0.11933315147999769  - accuracy: 0.8125\n",
      "At: 178 [==========>] Loss 0.183530759423452  - accuracy: 0.78125\n",
      "At: 179 [==========>] Loss 0.15687619708455766  - accuracy: 0.78125\n",
      "At: 180 [==========>] Loss 0.1466169608673501  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.06729386414164135  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.19595265898818892  - accuracy: 0.78125\n",
      "At: 183 [==========>] Loss 0.11115613776878926  - accuracy: 0.875\n",
      "At: 184 [==========>] Loss 0.13019433371251715  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.11104375558142424  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.1393067079932291  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.1890040974213979  - accuracy: 0.78125\n",
      "At: 188 [==========>] Loss 0.1392611257774033  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.18147548392259438  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.1169502984242253  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.3017633997070688  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.1214351930988391  - accuracy: 0.84375\n",
      "At: 193 [==========>] Loss 0.1742928966717525  - accuracy: 0.71875\n",
      "At: 194 [==========>] Loss 0.14615420560339693  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.11489987146698238  - accuracy: 0.84375\n",
      "At: 196 [==========>] Loss 0.12359312218106991  - accuracy: 0.84375\n",
      "At: 197 [==========>] Loss 0.16856030301010994  - accuracy: 0.78125\n",
      "At: 198 [==========>] Loss 0.07033931225254314  - accuracy: 0.9375\n",
      "At: 199 [==========>] Loss 0.10736184980617408  - accuracy: 0.90625\n",
      "At: 200 [==========>] Loss 0.18009901803961031  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.15498864202039936  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.13148769399434923  - accuracy: 0.8125\n",
      "At: 203 [==========>] Loss 0.11843186865019063  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.13962432451925882  - accuracy: 0.8125\n",
      "At: 205 [==========>] Loss 0.10090220994817357  - accuracy: 0.84375\n",
      "At: 206 [==========>] Loss 0.10979536148754698  - accuracy: 0.84375\n",
      "At: 207 [==========>] Loss 0.13637117883001387  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.17016581843966389  - accuracy: 0.78125\n",
      "At: 209 [==========>] Loss 0.1965422162606531  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.08845194834544026  - accuracy: 0.90625\n",
      "At: 211 [==========>] Loss 0.15155896401295393  - accuracy: 0.6875\n",
      "At: 212 [==========>] Loss 0.16461287396167262  - accuracy: 0.8125\n",
      "At: 213 [==========>] Loss 0.19792865367589033  - accuracy: 0.71875\n",
      "At: 214 [==========>] Loss 0.18157249878456866  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.12999509553951355  - accuracy: 0.75\n",
      "At: 216 [==========>] Loss 0.13790284983111262  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.17681330578941  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.10404685933135087  - accuracy: 0.84375\n",
      "At: 219 [==========>] Loss 0.17050029280698767  - accuracy: 0.8125\n",
      "At: 220 [==========>] Loss 0.17901782522415496  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.1434058610814269  - accuracy: 0.84375\n",
      "At: 222 [==========>] Loss 0.10781845613625192  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.250486080933772  - accuracy: 0.71875\n",
      "At: 224 [==========>] Loss 0.16227175815968264  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.16313758283303564  - accuracy: 0.75\n",
      "At: 226 [==========>] Loss 0.11356330431328204  - accuracy: 0.875\n",
      "At: 227 [==========>] Loss 0.13827272113311076  - accuracy: 0.8125\n",
      "At: 228 [==========>] Loss 0.1589679033926381  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.1982716803632515  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.12059267456928975  - accuracy: 0.84375\n",
      "At: 231 [==========>] Loss 0.21179769522356123  - accuracy: 0.65625\n",
      "At: 232 [==========>] Loss 0.1829708403029407  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.18635957213589605  - accuracy: 0.78125\n",
      "At: 234 [==========>] Loss 0.12011736300934495  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.16753138321681157  - accuracy: 0.75\n",
      "At: 236 [==========>] Loss 0.2085613769355288  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.13778384998182636  - accuracy: 0.875\n",
      "At: 238 [==========>] Loss 0.12068582485096228  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.09434899354559176  - accuracy: 0.875\n",
      "At: 240 [==========>] Loss 0.1759439012235776  - accuracy: 0.75\n",
      "At: 241 [==========>] Loss 0.14025235113720083  - accuracy: 0.8125\n",
      "At: 242 [==========>] Loss 0.14248835845259497  - accuracy: 0.84375\n",
      "At: 243 [==========>] Loss 0.12093213656694844  - accuracy: 0.8125\n",
      "At: 244 [==========>] Loss 0.14030366660876503  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.07126762220680416  - accuracy: 0.9375\n",
      "At: 246 [==========>] Loss 0.13123490352363032  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.10781969850633258  - accuracy: 0.8125\n",
      "At: 248 [==========>] Loss 0.12786377652748826  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.08878637636756923  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.1676813472293927  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.15261309907209078  - accuracy: 0.8125\n",
      "At: 252 [==========>] Loss 0.0722135772256175  - accuracy: 0.9375\n",
      "At: 253 [==========>] Loss 0.1726653887083867  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.08010055784874354  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.14798475190152238  - accuracy: 0.875\n",
      "At: 256 [==========>] Loss 0.1951170141876333  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.07378661268622796  - accuracy: 0.90625\n",
      "At: 258 [==========>] Loss 0.16624025132658504  - accuracy: 0.8125\n",
      "At: 259 [==========>] Loss 0.12388745649073346  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.14576905220836744  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.1043811609097907  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.11170210472404263  - accuracy: 0.875\n",
      "At: 263 [==========>] Loss 0.08278658205824252  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.08467693561865602  - accuracy: 0.90625\n",
      "At: 265 [==========>] Loss 0.18853626014536065  - accuracy: 0.71875\n",
      "At: 266 [==========>] Loss 0.2244801800811037  - accuracy: 0.6875\n",
      "At: 267 [==========>] Loss 0.1448648590191101  - accuracy: 0.84375\n",
      "At: 268 [==========>] Loss 0.16568866158941137  - accuracy: 0.75\n",
      "At: 269 [==========>] Loss 0.09328087833275882  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.2575941199819628  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.11260863049606644  - accuracy: 0.84375\n",
      "At: 272 [==========>] Loss 0.08325631409407022  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.13675026879949398  - accuracy: 0.78125\n",
      "At: 274 [==========>] Loss 0.18695632720980998  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.0883786630808572  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.15796192036315138  - accuracy: 0.78125\n",
      "At: 277 [==========>] Loss 0.11568621619218425  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.10565670906912404  - accuracy: 0.875\n",
      "At: 279 [==========>] Loss 0.1609471076489265  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.13169706556167388  - accuracy: 0.84375\n",
      "At: 281 [==========>] Loss 0.149476719867879  - accuracy: 0.78125\n",
      "At: 282 [==========>] Loss 0.19215264070952282  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.14104289816120946  - accuracy: 0.78125\n",
      "At: 284 [==========>] Loss 0.10243860446583658  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.16992408572203993  - accuracy: 0.75\n",
      "At: 286 [==========>] Loss 0.09389664846327023  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.1648847922479197  - accuracy: 0.8125\n",
      "At: 288 [==========>] Loss 0.09958343582165882  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.1069547578569737  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.10786316653216234  - accuracy: 0.8125\n",
      "At: 291 [==========>] Loss 0.11047607663610992  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.16104308199702716  - accuracy: 0.78125\n",
      "At: 293 [==========>] Loss 0.17633095582024963  - accuracy: 0.75\n",
      "At: 294 [==========>] Loss 0.15593059736639076  - accuracy: 0.71875\n",
      "At: 295 [==========>] Loss 0.08712324334695459  - accuracy: 0.875\n",
      "At: 296 [==========>] Loss 0.09142296854665477  - accuracy: 0.90625\n",
      "At: 297 [==========>] Loss 0.10309502509799047  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.12985458845006256  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.15457761813725251  - accuracy: 0.78125\n",
      "At: 300 [==========>] Loss 0.1446154514529458  - accuracy: 0.8125\n",
      "At: 301 [==========>] Loss 0.16094657753461966  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.13230797013646292  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.07148313364990556  - accuracy: 0.9375\n",
      "At: 304 [==========>] Loss 0.11520858673823497  - accuracy: 0.875\n",
      "At: 305 [==========>] Loss 0.2092224729865289  - accuracy: 0.71875\n",
      "At: 306 [==========>] Loss 0.11578204950855794  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.23610461219473655  - accuracy: 0.6875\n",
      "At: 308 [==========>] Loss 0.10583784935480542  - accuracy: 0.84375\n",
      "At: 309 [==========>] Loss 0.07574082429865434  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.20755908535539946  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.07324328727406638  - accuracy: 0.96875\n",
      "At: 312 [==========>] Loss 0.11995768427327666  - accuracy: 0.875\n",
      "At: 313 [==========>] Loss 0.09808929316116269  - accuracy: 0.90625\n",
      "At: 314 [==========>] Loss 0.18203395572614822  - accuracy: 0.78125\n",
      "At: 315 [==========>] Loss 0.13801684451963742  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.16304093426212174  - accuracy: 0.8125\n",
      "At: 317 [==========>] Loss 0.2540677809943666  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.13386172486958509  - accuracy: 0.78125\n",
      "At: 319 [==========>] Loss 0.12060889387920445  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.18893747860752222  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.17142190098305837  - accuracy: 0.78125\n",
      "At: 322 [==========>] Loss 0.12176802070457465  - accuracy: 0.84375\n",
      "At: 323 [==========>] Loss 0.11113085635548935  - accuracy: 0.875\n",
      "At: 324 [==========>] Loss 0.11439460950453453  - accuracy: 0.8125\n",
      "At: 325 [==========>] Loss 0.10835550841353783  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.19300573561490875  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.0971922715535191  - accuracy: 0.90625\n",
      "At: 328 [==========>] Loss 0.12571067067087097  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.15485557001105935  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.16169788664700627  - accuracy: 0.71875\n",
      "At: 331 [==========>] Loss 0.14215178615233645  - accuracy: 0.84375\n",
      "At: 332 [==========>] Loss 0.23805219580050038  - accuracy: 0.625\n",
      "At: 333 [==========>] Loss 0.13298783662442631  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.08282313332417415  - accuracy: 0.90625\n",
      "At: 335 [==========>] Loss 0.12013444775323313  - accuracy: 0.875\n",
      "At: 336 [==========>] Loss 0.10161450788598256  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.15358466209938346  - accuracy: 0.78125\n",
      "At: 338 [==========>] Loss 0.09595608694095567  - accuracy: 0.90625\n",
      "At: 339 [==========>] Loss 0.123659732729708  - accuracy: 0.8125\n",
      "At: 340 [==========>] Loss 0.11176744121434393  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.15411053011453818  - accuracy: 0.8125\n",
      "At: 342 [==========>] Loss 0.1598893179130851  - accuracy: 0.75\n",
      "At: 343 [==========>] Loss 0.2588514935233023  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.15596295265546603  - accuracy: 0.75\n",
      "At: 345 [==========>] Loss 0.17848723878558662  - accuracy: 0.65625\n",
      "At: 346 [==========>] Loss 0.12156754589229345  - accuracy: 0.78125\n",
      "At: 347 [==========>] Loss 0.11147103733858044  - accuracy: 0.875\n",
      "At: 348 [==========>] Loss 0.1251009044876093  - accuracy: 0.875\n",
      "At: 349 [==========>] Loss 0.14576122130573843  - accuracy: 0.71875\n",
      "At: 350 [==========>] Loss 0.12042628107662277  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.21691755401645418  - accuracy: 0.65625\n",
      "At: 352 [==========>] Loss 0.08072824871533768  - accuracy: 0.9375\n",
      "At: 353 [==========>] Loss 0.13995238142119185  - accuracy: 0.78125\n",
      "At: 354 [==========>] Loss 0.17034267095322064  - accuracy: 0.75\n",
      "At: 355 [==========>] Loss 0.09367503315634235  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.16357755295416698  - accuracy: 0.78125\n",
      "At: 357 [==========>] Loss 0.1414152747608863  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.11952326630042673  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.09124173566179533  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.13900583540593192  - accuracy: 0.8125\n",
      "At: 361 [==========>] Loss 0.07634432332464683  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.11945481123092697  - accuracy: 0.875\n",
      "At: 363 [==========>] Loss 0.08814315210255975  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.16824386842067143  - accuracy: 0.78125\n",
      "At: 365 [==========>] Loss 0.11806938431585651  - accuracy: 0.875\n",
      "At: 366 [==========>] Loss 0.16175004559685954  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.157415078249806  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.16550511589374384  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.15389268758945726  - accuracy: 0.84375\n",
      "At: 370 [==========>] Loss 0.13223720608478634  - accuracy: 0.78125\n",
      "At: 371 [==========>] Loss 0.13050118087318874  - accuracy: 0.8125\n",
      "At: 372 [==========>] Loss 0.07664169208367563  - accuracy: 0.90625\n",
      "At: 373 [==========>] Loss 0.18613158038925454  - accuracy: 0.78125\n",
      "At: 374 [==========>] Loss 0.07194432547126316  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.11378969607456024  - accuracy: 0.84375\n",
      "At: 376 [==========>] Loss 0.09157477047455881  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.23693724339465952  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.18039249323308468  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.13873842895092647  - accuracy: 0.75\n",
      "At: 380 [==========>] Loss 0.14828864881823745  - accuracy: 0.84375\n",
      "At: 381 [==========>] Loss 0.1289733738435289  - accuracy: 0.78125\n",
      "At: 382 [==========>] Loss 0.09474236299900299  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.1345021739258728  - accuracy: 0.75\n",
      "At: 384 [==========>] Loss 0.15263622919642245  - accuracy: 0.84375\n",
      "At: 385 [==========>] Loss 0.14271598287964785  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.1887364252240726  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.06690981353427222  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.20323103220440486  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.1664713598591056  - accuracy: 0.8125\n",
      "At: 390 [==========>] Loss 0.10601222719943414  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.09760186325537284  - accuracy: 0.875\n",
      "At: 392 [==========>] Loss 0.13109443914852645  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.2191179178220558  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.10244774411469412  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.14728417732486077  - accuracy: 0.8125\n",
      "At: 396 [==========>] Loss 0.14818074112674287  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.13283753561746858  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.17221824714089456  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.17532276985792103  - accuracy: 0.65625\n",
      "At: 400 [==========>] Loss 0.16058218739811234  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.08703655398144614  - accuracy: 0.90625\n",
      "At: 402 [==========>] Loss 0.09628066400719089  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.07050332508540909  - accuracy: 0.90625\n",
      "At: 404 [==========>] Loss 0.1324601261004497  - accuracy: 0.8125\n",
      "At: 405 [==========>] Loss 0.17579462482535307  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.10495491334068732  - accuracy: 0.84375\n",
      "At: 407 [==========>] Loss 0.11024191126699756  - accuracy: 0.875\n",
      "At: 408 [==========>] Loss 0.14313184674742385  - accuracy: 0.8125\n",
      "At: 409 [==========>] Loss 0.23160689042255092  - accuracy: 0.71875\n",
      "At: 410 [==========>] Loss 0.09335546126335256  - accuracy: 0.84375\n",
      "At: 411 [==========>] Loss 0.10900200721909481  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.1445284242451455  - accuracy: 0.8125\n",
      "At: 413 [==========>] Loss 0.11497326571899644  - accuracy: 0.875\n",
      "At: 414 [==========>] Loss 0.14505070141838344  - accuracy: 0.8125\n",
      "At: 415 [==========>] Loss 0.11670202112116018  - accuracy: 0.84375\n",
      "At: 416 [==========>] Loss 0.18354244131995529  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.11666851777291785  - accuracy: 0.8125\n",
      "At: 418 [==========>] Loss 0.11761278960905802  - accuracy: 0.84375\n",
      "At: 419 [==========>] Loss 0.12815419197101072  - accuracy: 0.84375\n",
      "At: 420 [==========>] Loss 0.1439369445784931  - accuracy: 0.78125\n",
      "At: 421 [==========>] Loss 0.12345395815116439  - accuracy: 0.875\n",
      "At: 422 [==========>] Loss 0.1025281953561378  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.10458796434691683  - accuracy: 0.875\n",
      "At: 424 [==========>] Loss 0.1962783835227341  - accuracy: 0.71875\n",
      "At: 425 [==========>] Loss 0.18252795281495895  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.10216957572571422  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.18468105780722527  - accuracy: 0.6875\n",
      "At: 428 [==========>] Loss 0.26733076132836797  - accuracy: 0.59375\n",
      "At: 429 [==========>] Loss 0.1501074186267139  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.09884425649337492  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.1171820211526089  - accuracy: 0.875\n",
      "At: 432 [==========>] Loss 0.12887702436203094  - accuracy: 0.875\n",
      "At: 433 [==========>] Loss 0.09070227346570744  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.11888969561624886  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.205102452032499  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.12994761956888  - accuracy: 0.78125\n",
      "At: 437 [==========>] Loss 0.09947426191178622  - accuracy: 0.875\n",
      "At: 438 [==========>] Loss 0.09886641000348813  - accuracy: 0.875\n",
      "At: 439 [==========>] Loss 0.08426030054012483  - accuracy: 0.84375\n",
      "At: 440 [==========>] Loss 0.08957686848840737  - accuracy: 0.875\n",
      "At: 441 [==========>] Loss 0.14994435983492488  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.1451991671121487  - accuracy: 0.78125\n",
      "At: 443 [==========>] Loss 0.13632777398178858  - accuracy: 0.84375\n",
      "At: 444 [==========>] Loss 0.16833269194330147  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.1807023195479778  - accuracy: 0.75\n",
      "At: 446 [==========>] Loss 0.18494853840198833  - accuracy: 0.75\n",
      "At: 447 [==========>] Loss 0.1507856051080793  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.1845477202717719  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.11079326936577392  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.11566973961859882  - accuracy: 0.84375\n",
      "At: 451 [==========>] Loss 0.0875187028719924  - accuracy: 0.90625\n",
      "At: 452 [==========>] Loss 0.16199259711093497  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.13950443585770844  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.18514789414124155  - accuracy: 0.71875\n",
      "At: 455 [==========>] Loss 0.19736474976561252  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.14537501220765614  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.16534584344120073  - accuracy: 0.8125\n",
      "At: 458 [==========>] Loss 0.0978311918656209  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.21815006543984466  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.13877968384488412  - accuracy: 0.78125\n",
      "At: 461 [==========>] Loss 0.13543002532821  - accuracy: 0.8125\n",
      "At: 462 [==========>] Loss 0.17886551680821017  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.16069934417111903  - accuracy: 0.78125\n",
      "At: 464 [==========>] Loss 0.16484742799539898  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.17421328996092406  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.0859752308161869  - accuracy: 0.9375\n",
      "At: 467 [==========>] Loss 0.19337950977105794  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.08538644431050049  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.1191283299521173  - accuracy: 0.84375\n",
      "At: 470 [==========>] Loss 0.11819619670284054  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.15766874385995072  - accuracy: 0.8125\n",
      "At: 472 [==========>] Loss 0.13761654637302795  - accuracy: 0.78125\n",
      "At: 473 [==========>] Loss 0.18106310309216095  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.13329108260462474  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.13681688123065872  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.14468973889159012  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.13930831048870532  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.10396591553281512  - accuracy: 0.875\n",
      "At: 479 [==========>] Loss 0.14206782856795278  - accuracy: 0.78125\n",
      "At: 480 [==========>] Loss 0.1490653414316146  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.14082026037228207  - accuracy: 0.8125\n",
      "At: 482 [==========>] Loss 0.0809969632108446  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.11092135232078577  - accuracy: 0.8125\n",
      "At: 484 [==========>] Loss 0.1105270562393688  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.13061556598899568  - accuracy: 0.78125\n",
      "At: 486 [==========>] Loss 0.18902770191888107  - accuracy: 0.75\n",
      "At: 487 [==========>] Loss 0.13913614012717263  - accuracy: 0.84375\n",
      "At: 488 [==========>] Loss 0.11378996003273605  - accuracy: 0.90625\n",
      "At: 489 [==========>] Loss 0.19146529965230105  - accuracy: 0.71875\n",
      "At: 490 [==========>] Loss 0.1366978854469191  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.12990791503685173  - accuracy: 0.84375\n",
      "At: 492 [==========>] Loss 0.19483255358829077  - accuracy: 0.65625\n",
      "At: 493 [==========>] Loss 0.10406735630085873  - accuracy: 0.90625\n",
      "At: 494 [==========>] Loss 0.15395532486491637  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.11523845362742875  - accuracy: 0.84375\n",
      "At: 496 [==========>] Loss 0.18842033737707714  - accuracy: 0.71875\n",
      "At: 497 [==========>] Loss 0.1630737800225891  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.07767526866962382  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.1720037655730727  - accuracy: 0.71875\n",
      "At: 500 [==========>] Loss 0.12312185641609696  - accuracy: 0.84375\n",
      "At: 501 [==========>] Loss 0.19135636863655733  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.12400217529437821  - accuracy: 0.875\n",
      "At: 503 [==========>] Loss 0.09755168925865956  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.09349616822764853  - accuracy: 0.90625\n",
      "At: 505 [==========>] Loss 0.17123687428535062  - accuracy: 0.8125\n",
      "At: 506 [==========>] Loss 0.2621185367650395  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.12565925270331757  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.1201482635525016  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.11922310367667231  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.16771191287184017  - accuracy: 0.78125\n",
      "At: 511 [==========>] Loss 0.11497389351767767  - accuracy: 0.90625\n",
      "At: 512 [==========>] Loss 0.18054720400614216  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.18257822861386946  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.13096411148861506  - accuracy: 0.75\n",
      "At: 515 [==========>] Loss 0.08241529149125798  - accuracy: 0.90625\n",
      "At: 516 [==========>] Loss 0.16075615456269682  - accuracy: 0.8125\n",
      "At: 517 [==========>] Loss 0.12634789550743833  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.1851320284483528  - accuracy: 0.78125\n",
      "At: 519 [==========>] Loss 0.15810500685190892  - accuracy: 0.78125\n",
      "At: 520 [==========>] Loss 0.1388555301598075  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.15174529322779876  - accuracy: 0.78125\n",
      "At: 522 [==========>] Loss 0.1381909943550478  - accuracy: 0.84375\n",
      "At: 523 [==========>] Loss 0.15892846163015995  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.10955243666101286  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.14703985003695993  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.17770851391777495  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.22223773156326407  - accuracy: 0.6875\n",
      "At: 528 [==========>] Loss 0.19910661086068343  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.1402875106384407  - accuracy: 0.8125\n",
      "At: 530 [==========>] Loss 0.18235730620352483  - accuracy: 0.75\n",
      "At: 531 [==========>] Loss 0.14980223180967675  - accuracy: 0.84375\n",
      "At: 532 [==========>] Loss 0.11621041486178771  - accuracy: 0.90625\n",
      "At: 533 [==========>] Loss 0.09030993946465977  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.1714553936080136  - accuracy: 0.75\n",
      "At: 535 [==========>] Loss 0.1876239797599658  - accuracy: 0.65625\n",
      "At: 536 [==========>] Loss 0.15431510749883764  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.10183715975421213  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.12960774897399477  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.10596814115590818  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.2582083322639044  - accuracy: 0.6875\n",
      "At: 541 [==========>] Loss 0.12832986360400567  - accuracy: 0.84375\n",
      "At: 542 [==========>] Loss 0.13682419036181726  - accuracy: 0.75\n",
      "At: 543 [==========>] Loss 0.12778527206982587  - accuracy: 0.84375\n",
      "At: 544 [==========>] Loss 0.1711399595103858  - accuracy: 0.75\n",
      "At: 545 [==========>] Loss 0.10640790220884096  - accuracy: 0.78125\n",
      "At: 546 [==========>] Loss 0.1604101927628546  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.11932523358286463  - accuracy: 0.8125\n",
      "At: 548 [==========>] Loss 0.09469824270887475  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.12456653398390072  - accuracy: 0.78125\n",
      "At: 550 [==========>] Loss 0.10239317782246146  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.1320332677067059  - accuracy: 0.75\n",
      "At: 552 [==========>] Loss 0.12376967847516773  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.11368673695978315  - accuracy: 0.84375\n",
      "At: 554 [==========>] Loss 0.08642689220911275  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.1442540796151584  - accuracy: 0.8125\n",
      "At: 556 [==========>] Loss 0.1468446183808455  - accuracy: 0.75\n",
      "At: 557 [==========>] Loss 0.10130658352298358  - accuracy: 0.875\n",
      "At: 558 [==========>] Loss 0.15801113153444224  - accuracy: 0.8125\n",
      "At: 559 [==========>] Loss 0.232266805484294  - accuracy: 0.625\n",
      "At: 560 [==========>] Loss 0.13013089197537592  - accuracy: 0.875\n",
      "At: 561 [==========>] Loss 0.13874650279878514  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.06553556429446353  - accuracy: 0.9375\n",
      "At: 563 [==========>] Loss 0.17280943991767317  - accuracy: 0.78125\n",
      "At: 564 [==========>] Loss 0.14542066831900297  - accuracy: 0.78125\n",
      "At: 565 [==========>] Loss 0.08344964905915629  - accuracy: 0.9375\n",
      "At: 566 [==========>] Loss 0.11556291118444559  - accuracy: 0.84375\n",
      "At: 567 [==========>] Loss 0.16605158161900155  - accuracy: 0.71875\n",
      "At: 568 [==========>] Loss 0.20649362138818925  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.1650626136122628  - accuracy: 0.75\n",
      "At: 570 [==========>] Loss 0.11027428354987404  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.11437445975300446  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.10264860757482795  - accuracy: 0.875\n",
      "At: 573 [==========>] Loss 0.10272405131028808  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.14429754911906972  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.12801783545424086  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.05814741420483528  - accuracy: 0.9375\n",
      "At: 577 [==========>] Loss 0.1542172548605653  - accuracy: 0.8125\n",
      "At: 578 [==========>] Loss 0.17822336522039237  - accuracy: 0.78125\n",
      "At: 579 [==========>] Loss 0.11232017231628459  - accuracy: 0.875\n",
      "At: 580 [==========>] Loss 0.10459915657716815  - accuracy: 0.875\n",
      "At: 581 [==========>] Loss 0.14290742258126826  - accuracy: 0.84375\n",
      "At: 582 [==========>] Loss 0.14617490434844727  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.19498691281094838  - accuracy: 0.71875\n",
      "At: 584 [==========>] Loss 0.09253200460141513  - accuracy: 0.875\n",
      "At: 585 [==========>] Loss 0.1320600018523143  - accuracy: 0.8125\n",
      "At: 586 [==========>] Loss 0.07385545493043982  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.15325873562692466  - accuracy: 0.75\n",
      "At: 588 [==========>] Loss 0.19060224162381872  - accuracy: 0.71875\n",
      "At: 589 [==========>] Loss 0.16428628898337153  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.05236685157200768  - accuracy: 1.0\n",
      "At: 591 [==========>] Loss 0.1498627026262052  - accuracy: 0.84375\n",
      "At: 592 [==========>] Loss 0.10311774112750169  - accuracy: 0.90625\n",
      "At: 593 [==========>] Loss 0.1717972047891121  - accuracy: 0.78125\n",
      "At: 594 [==========>] Loss 0.13524137587403134  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.15409147602328913  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.1355666550915542  - accuracy: 0.8125\n",
      "At: 597 [==========>] Loss 0.18895705981699296  - accuracy: 0.78125\n",
      "At: 598 [==========>] Loss 0.14763301603233764  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.13323377884262283  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.12766671962785256  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.12226042211936758  - accuracy: 0.875\n",
      "At: 602 [==========>] Loss 0.12394685742285089  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.14297530879017595  - accuracy: 0.75\n",
      "At: 604 [==========>] Loss 0.1879223869407265  - accuracy: 0.75\n",
      "At: 605 [==========>] Loss 0.09409930396630035  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.1395974545739363  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.12565699346471837  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.13090426694225732  - accuracy: 0.78125\n",
      "At: 609 [==========>] Loss 0.12172169042263153  - accuracy: 0.8125\n",
      "At: 610 [==========>] Loss 0.1184288726786198  - accuracy: 0.84375\n",
      "At: 611 [==========>] Loss 0.08487040281678891  - accuracy: 0.90625\n",
      "At: 612 [==========>] Loss 0.12105835335303299  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.15369124587566507  - accuracy: 0.78125\n",
      "At: 614 [==========>] Loss 0.14827374306965407  - accuracy: 0.84375\n",
      "At: 615 [==========>] Loss 0.1590734780379854  - accuracy: 0.78125\n",
      "At: 616 [==========>] Loss 0.1467301568551294  - accuracy: 0.84375\n",
      "At: 617 [==========>] Loss 0.1366743099832751  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.17755710719306989  - accuracy: 0.78125\n",
      "At: 619 [==========>] Loss 0.1726510950889646  - accuracy: 0.75\n",
      "At: 620 [==========>] Loss 0.17701799761439901  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.08004685512191169  - accuracy: 0.875\n",
      "At: 622 [==========>] Loss 0.17767773656122504  - accuracy: 0.71875\n",
      "At: 623 [==========>] Loss 0.1417256802258378  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.09098489816505659  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.1057307994558696  - accuracy: 0.84375\n",
      "At: 626 [==========>] Loss 0.12993348524080622  - accuracy: 0.75\n",
      "At: 627 [==========>] Loss 0.12018169699142348  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.13321407832613213  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.17448494064595035  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.19320482766443373  - accuracy: 0.71875\n",
      "At: 631 [==========>] Loss 0.18365088686062223  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.1444141614310249  - accuracy: 0.8125\n",
      "At: 633 [==========>] Loss 0.18720688974958977  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.13765663181681465  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.1400706762654783  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.16718773041332635  - accuracy: 0.71875\n",
      "At: 637 [==========>] Loss 0.10581776086264341  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.11981316514243769  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.1311183176336142  - accuracy: 0.84375\n",
      "At: 640 [==========>] Loss 0.21634481112688814  - accuracy: 0.625\n",
      "At: 641 [==========>] Loss 0.10956487056859265  - accuracy: 0.8125\n",
      "At: 642 [==========>] Loss 0.19354635310143065  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.12108168640440982  - accuracy: 0.8125\n",
      "At: 644 [==========>] Loss 0.08200136302567695  - accuracy: 0.9375\n",
      "At: 645 [==========>] Loss 0.12495719350249634  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.14654888099724805  - accuracy: 0.8125\n",
      "At: 647 [==========>] Loss 0.15949204880172543  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.13653611162670276  - accuracy: 0.8125\n",
      "At: 649 [==========>] Loss 0.13652753561457928  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.09286470871098182  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.16939209166067942  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.11188784667371318  - accuracy: 0.84375\n",
      "At: 653 [==========>] Loss 0.12492343876269071  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.1330006951580594  - accuracy: 0.8125\n",
      "At: 655 [==========>] Loss 0.12316807951241551  - accuracy: 0.84375\n",
      "At: 656 [==========>] Loss 0.13720064841803017  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.15011673780064064  - accuracy: 0.71875\n",
      "At: 658 [==========>] Loss 0.1372537256945014  - accuracy: 0.78125\n",
      "At: 659 [==========>] Loss 0.15087352147558009  - accuracy: 0.75\n",
      "At: 660 [==========>] Loss 0.12137959034570762  - accuracy: 0.8125\n",
      "At: 661 [==========>] Loss 0.13907497929845467  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.11911577013329099  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.11561709286942615  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.13064240604538047  - accuracy: 0.875\n",
      "At: 665 [==========>] Loss 0.15379485404388177  - accuracy: 0.78125\n",
      "At: 666 [==========>] Loss 0.1865675890160175  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.12794173001169695  - accuracy: 0.8125\n",
      "At: 668 [==========>] Loss 0.14327536003119898  - accuracy: 0.8125\n",
      "At: 669 [==========>] Loss 0.16450450767524358  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.22432236671973593  - accuracy: 0.71875\n",
      "At: 671 [==========>] Loss 0.08421268563376573  - accuracy: 0.90625\n",
      "At: 672 [==========>] Loss 0.12296758237382338  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.07053260029191513  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.1379587683160708  - accuracy: 0.78125\n",
      "At: 675 [==========>] Loss 0.09189121617732243  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.16394373257680095  - accuracy: 0.78125\n",
      "At: 677 [==========>] Loss 0.14394975194141607  - accuracy: 0.8125\n",
      "At: 678 [==========>] Loss 0.14158867071672207  - accuracy: 0.75\n",
      "At: 679 [==========>] Loss 0.1303549470985232  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.09946659257461754  - accuracy: 0.875\n",
      "At: 681 [==========>] Loss 0.14705372571340428  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.13334518110548538  - accuracy: 0.8125\n",
      "At: 683 [==========>] Loss 0.15281432594349822  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.1335436651557707  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.1381804614299186  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.11341548295455656  - accuracy: 0.84375\n",
      "At: 687 [==========>] Loss 0.08249583519211814  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.12397828790864572  - accuracy: 0.8125\n",
      "At: 689 [==========>] Loss 0.1544946264179065  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.1339276225070571  - accuracy: 0.875\n",
      "At: 691 [==========>] Loss 0.12840633594939846  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.11456737814394444  - accuracy: 0.78125\n",
      "At: 693 [==========>] Loss 0.14403300174874048  - accuracy: 0.84375\n",
      "At: 694 [==========>] Loss 0.17444872127048403  - accuracy: 0.75\n",
      "At: 695 [==========>] Loss 0.1468295585872178  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.13511391140007906  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.16282792006206945  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.1223577167690032  - accuracy: 0.8125\n",
      "At: 699 [==========>] Loss 0.1199996976052102  - accuracy: 0.84375\n",
      "At: 700 [==========>] Loss 0.07225735267019084  - accuracy: 0.90625\n",
      "At: 701 [==========>] Loss 0.1636405500795174  - accuracy: 0.75\n",
      "At: 702 [==========>] Loss 0.11614695764692282  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.16884322688194003  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.1499583224560832  - accuracy: 0.78125\n",
      "At: 705 [==========>] Loss 0.2266811464225094  - accuracy: 0.65625\n",
      "At: 706 [==========>] Loss 0.1526103683646566  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.09906119907272634  - accuracy: 0.875\n",
      "At: 708 [==========>] Loss 0.13037378413239253  - accuracy: 0.8125\n",
      "At: 709 [==========>] Loss 0.2090191923951249  - accuracy: 0.71875\n",
      "At: 710 [==========>] Loss 0.14484122632708885  - accuracy: 0.75\n",
      "At: 711 [==========>] Loss 0.2458167385095662  - accuracy: 0.65625\n",
      "At: 712 [==========>] Loss 0.16127702732509722  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.21008433945561644  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.22568633473316452  - accuracy: 0.625\n",
      "At: 715 [==========>] Loss 0.10900886830202998  - accuracy: 0.84375\n",
      "At: 716 [==========>] Loss 0.1310548163079965  - accuracy: 0.78125\n",
      "At: 717 [==========>] Loss 0.06604306634941053  - accuracy: 0.96875\n",
      "At: 718 [==========>] Loss 0.1855438749000908  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.09530995242540691  - accuracy: 0.875\n",
      "At: 720 [==========>] Loss 0.13288107545404768  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.10523393767303299  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.17267185790283768  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.12034191994643509  - accuracy: 0.875\n",
      "At: 724 [==========>] Loss 0.07933473397346066  - accuracy: 0.90625\n",
      "At: 725 [==========>] Loss 0.12150478383073965  - accuracy: 0.875\n",
      "At: 726 [==========>] Loss 0.17487388280316407  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.1287106308556312  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.14820712079002007  - accuracy: 0.78125\n",
      "At: 729 [==========>] Loss 0.1798719071708631  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.15814934203717446  - accuracy: 0.8125\n",
      "At: 731 [==========>] Loss 0.17211033260611627  - accuracy: 0.71875\n",
      "At: 732 [==========>] Loss 0.13465309747414722  - accuracy: 0.875\n",
      "At: 733 [==========>] Loss 0.10153191635036055  - accuracy: 0.8125\n",
      "At: 734 [==========>] Loss 0.13671121040929018  - accuracy: 0.8125\n",
      "At: 735 [==========>] Loss 0.12614249083660847  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.0921320703687794  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.1476929950949713  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.11133265085301629  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.06370035932708352  - accuracy: 0.96875\n",
      "At: 740 [==========>] Loss 0.1498150884598634  - accuracy: 0.78125\n",
      "At: 741 [==========>] Loss 0.10161150985471766  - accuracy: 0.875\n",
      "At: 742 [==========>] Loss 0.15067212394290241  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.14595364369273467  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.16533845304502817  - accuracy: 0.78125\n",
      "At: 745 [==========>] Loss 0.19042860158842123  - accuracy: 0.6875\n",
      "At: 746 [==========>] Loss 0.10276919959577477  - accuracy: 0.875\n",
      "At: 747 [==========>] Loss 0.11397459657960861  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.15804708312068372  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.15170438302079278  - accuracy: 0.75\n",
      "At: 750 [==========>] Loss 0.12127843225257831  - accuracy: 0.78125\n",
      "At: 751 [==========>] Loss 0.173883331715343  - accuracy: 0.75\n",
      "At: 752 [==========>] Loss 0.05464909816143616  - accuracy: 0.9375\n",
      "At: 753 [==========>] Loss 0.15093649494426584  - accuracy: 0.8125\n",
      "At: 754 [==========>] Loss 0.18445725436293892  - accuracy: 0.71875\n",
      "At: 755 [==========>] Loss 0.08835789439772027  - accuracy: 0.90625\n",
      "At: 756 [==========>] Loss 0.17517793173925192  - accuracy: 0.75\n",
      "At: 757 [==========>] Loss 0.09140272951119184  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.10830641920033258  - accuracy: 0.875\n",
      "At: 759 [==========>] Loss 0.08938924054979569  - accuracy: 0.9375\n",
      "At: 760 [==========>] Loss 0.17073780804227778  - accuracy: 0.71875\n",
      "At: 761 [==========>] Loss 0.15269083499829006  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.09411616797663033  - accuracy: 0.875\n",
      "At: 763 [==========>] Loss 0.15988028666765441  - accuracy: 0.78125\n",
      "At: 764 [==========>] Loss 0.12170166397426024  - accuracy: 0.8125\n",
      "At: 765 [==========>] Loss 0.15558730739238846  - accuracy: 0.75\n",
      "At: 766 [==========>] Loss 0.13894695270287843  - accuracy: 0.78125\n",
      "At: 767 [==========>] Loss 0.1668719091953002  - accuracy: 0.78125\n",
      "At: 768 [==========>] Loss 0.1592315733904409  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.13903325938230343  - accuracy: 0.84375\n",
      "At: 770 [==========>] Loss 0.1227178394189583  - accuracy: 0.78125\n",
      "At: 771 [==========>] Loss 0.17196857752505357  - accuracy: 0.6875\n",
      "At: 772 [==========>] Loss 0.13521383246327262  - accuracy: 0.84375\n",
      "At: 773 [==========>] Loss 0.08083226739187206  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.14312900313585655  - accuracy: 0.8125\n",
      "At: 775 [==========>] Loss 0.17602023793391985  - accuracy: 0.71875\n",
      "At: 776 [==========>] Loss 0.189749549677111  - accuracy: 0.6875\n",
      "At: 777 [==========>] Loss 0.08885695239237582  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.17515543632641384  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.17267306875848631  - accuracy: 0.75\n",
      "At: 780 [==========>] Loss 0.07209261056509236  - accuracy: 0.9375\n",
      "At: 781 [==========>] Loss 0.16240336492225152  - accuracy: 0.75\n",
      "At: 782 [==========>] Loss 0.16578363549056183  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.19032014767109576  - accuracy: 0.71875\n",
      "At: 784 [==========>] Loss 0.1434906078317292  - accuracy: 0.8125\n",
      "At: 785 [==========>] Loss 0.22247972138962668  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.13074989306725676  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.17691816848617248  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.07694801354546928  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.14195337921198029  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.13403128610710724  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.18179848766393333  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.15948192751202556  - accuracy: 0.78125\n",
      "At: 793 [==========>] Loss 0.09923633730391064  - accuracy: 0.9375\n",
      "At: 794 [==========>] Loss 0.15853051198702978  - accuracy: 0.71875\n",
      "At: 795 [==========>] Loss 0.1252980614732102  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.1239819902404381  - accuracy: 0.84375\n",
      "At: 797 [==========>] Loss 0.18236934064450347  - accuracy: 0.8125\n",
      "At: 798 [==========>] Loss 0.15083604991678298  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.05105380792362629  - accuracy: 0.96875\n",
      "At: 800 [==========>] Loss 0.13370449342690463  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.0888773910587484  - accuracy: 0.875\n",
      "At: 802 [==========>] Loss 0.17970751290005693  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.15779139200823733  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.13719362867936089  - accuracy: 0.75\n",
      "At: 805 [==========>] Loss 0.1397149990262707  - accuracy: 0.8125\n",
      "At: 806 [==========>] Loss 0.09409486004732245  - accuracy: 0.84375\n",
      "At: 807 [==========>] Loss 0.11961526065205805  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.11206961564909906  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.06563284019665216  - accuracy: 0.90625\n",
      "At: 810 [==========>] Loss 0.17257256011531655  - accuracy: 0.8125\n",
      "At: 811 [==========>] Loss 0.11010783022561914  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.13792706295151919  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.2010812682505722  - accuracy: 0.65625\n",
      "At: 814 [==========>] Loss 0.16646826839666518  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.16346412269420751  - accuracy: 0.75\n",
      "At: 816 [==========>] Loss 0.15417005945306406  - accuracy: 0.75\n",
      "At: 817 [==========>] Loss 0.05596447395548025  - accuracy: 0.96875\n",
      "At: 818 [==========>] Loss 0.124851896159647  - accuracy: 0.90625\n",
      "At: 819 [==========>] Loss 0.11290520083045197  - accuracy: 0.78125\n",
      "At: 820 [==========>] Loss 0.14025247111168798  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.1639568253368569  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.1687770007118923  - accuracy: 0.75\n",
      "At: 823 [==========>] Loss 0.13736401678988613  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.13415454058356657  - accuracy: 0.78125\n",
      "At: 825 [==========>] Loss 0.21352834672692683  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.1285801830257943  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.10600176029335322  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.05871263703719219  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09936280435686207  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.07409049743635573  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.09439788825569928  - accuracy: 0.875\n",
      "At: 832 [==========>] Loss 0.1307634105247722  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.18693581841393797  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.10620681973002763  - accuracy: 0.875\n",
      "At: 835 [==========>] Loss 0.1077066657490557  - accuracy: 0.84375\n",
      "At: 836 [==========>] Loss 0.11499526159492211  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.11634137616489174  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.12758363252262217  - accuracy: 0.8125\n",
      "At: 839 [==========>] Loss 0.11153508409474533  - accuracy: 0.84375\n",
      "At: 840 [==========>] Loss 0.11123095779955611  - accuracy: 0.84375\n",
      "At: 841 [==========>] Loss 0.07296595871577248  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.09409549733784567  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.16765987588859724  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.10268381634091797  - accuracy: 0.875\n",
      "At: 845 [==========>] Loss 0.1388801191893629  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.09820205302439311  - accuracy: 0.84375\n",
      "At: 847 [==========>] Loss 0.07407174186851628  - accuracy: 0.9375\n",
      "At: 848 [==========>] Loss 0.1297331270483281  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.14504327173616102  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.10204840130142498  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.08736339270264158  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.16393407833659115  - accuracy: 0.75\n",
      "At: 853 [==========>] Loss 0.1303008534499785  - accuracy: 0.84375\n",
      "At: 854 [==========>] Loss 0.2258459254233232  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.07889940464299458  - accuracy: 0.90625\n",
      "At: 856 [==========>] Loss 0.06955591545345927  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.05695690527375592  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.2360569072484976  - accuracy: 0.71875\n",
      "At: 859 [==========>] Loss 0.10433402611716243  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.1150612550580181  - accuracy: 0.84375\n",
      "At: 861 [==========>] Loss 0.10669341489579443  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.07321732567079617  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.1378269791635206  - accuracy: 0.8125\n",
      "At: 864 [==========>] Loss 0.13982537728027805  - accuracy: 0.8125\n",
      "At: 865 [==========>] Loss 0.22785921062443404  - accuracy: 0.65625\n",
      "At: 866 [==========>] Loss 0.15629890548090467  - accuracy: 0.84375\n",
      "At: 867 [==========>] Loss 0.09327538989362211  - accuracy: 0.90625\n",
      "At: 868 [==========>] Loss 0.18412397807877734  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.20211760030534714  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.13400983727808619  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.09044412517056125  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.09459713359668968  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.1680605440345552  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.19038600124750088  - accuracy: 0.6875\n",
      "At: 875 [==========>] Loss 0.10831868805947956  - accuracy: 0.84375\n",
      "At: 876 [==========>] Loss 0.08909515938529533  - accuracy: 0.90625\n",
      "At: 877 [==========>] Loss 0.14342765615020203  - accuracy: 0.84375\n",
      "At: 878 [==========>] Loss 0.06281275381077982  - accuracy: 0.9375\n",
      "At: 879 [==========>] Loss 0.13429926203172582  - accuracy: 0.78125\n",
      "At: 880 [==========>] Loss 0.11327377249842453  - accuracy: 0.90625\n",
      "At: 881 [==========>] Loss 0.14530757077097473  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.09109352338782295  - accuracy: 0.90625\n",
      "At: 883 [==========>] Loss 0.1501546932085946  - accuracy: 0.75\n",
      "At: 884 [==========>] Loss 0.14721518802125053  - accuracy: 0.84375\n",
      "At: 885 [==========>] Loss 0.13289300062498474  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.1018219191951884  - accuracy: 0.84375\n",
      "At: 887 [==========>] Loss 0.15133527188118817  - accuracy: 0.8125\n",
      "At: 888 [==========>] Loss 0.1372652522610175  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.10040579445348928  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.13273417954908107  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.09480109851939483  - accuracy: 0.90625\n",
      "At: 892 [==========>] Loss 0.11586427028564422  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.1757269910085149  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.1501748413206551  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.0964692550821701  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.08335599314991332  - accuracy: 0.90625\n",
      "At: 897 [==========>] Loss 0.15880036071616827  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.13283607420940902  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.08828374936877864  - accuracy: 0.90625\n",
      "At: 900 [==========>] Loss 0.1774461741496434  - accuracy: 0.6875\n",
      "At: 901 [==========>] Loss 0.16253330968647292  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.11428746651517914  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.15038534652162985  - accuracy: 0.75\n",
      "At: 904 [==========>] Loss 0.13293926024012237  - accuracy: 0.78125\n",
      "At: 905 [==========>] Loss 0.0834171717902222  - accuracy: 0.9375\n",
      "At: 906 [==========>] Loss 0.11384277600537054  - accuracy: 0.875\n",
      "At: 907 [==========>] Loss 0.14562234609653957  - accuracy: 0.8125\n",
      "At: 908 [==========>] Loss 0.15003898605525431  - accuracy: 0.75\n",
      "At: 909 [==========>] Loss 0.08327935349339943  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.137649170104685  - accuracy: 0.78125\n",
      "At: 911 [==========>] Loss 0.14627512450822028  - accuracy: 0.84375\n",
      "At: 912 [==========>] Loss 0.1333506824644662  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.14031472087992114  - accuracy: 0.78125\n",
      "At: 914 [==========>] Loss 0.13948002018168207  - accuracy: 0.75\n",
      "At: 915 [==========>] Loss 0.12237345950959477  - accuracy: 0.8125\n",
      "At: 916 [==========>] Loss 0.16451885438449634  - accuracy: 0.75\n",
      "At: 917 [==========>] Loss 0.16469389461122208  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.16100344396994679  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.14057620950022554  - accuracy: 0.78125\n",
      "At: 920 [==========>] Loss 0.11526434933612266  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.12373071444924674  - accuracy: 0.84375\n",
      "At: 922 [==========>] Loss 0.14614510157756985  - accuracy: 0.78125\n",
      "At: 923 [==========>] Loss 0.0921885302356453  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.18724478803183597  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.14598083291276193  - accuracy: 0.78125\n",
      "At: 926 [==========>] Loss 0.14274783504688535  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.12947847513248256  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.14625540577532148  - accuracy: 0.78125\n",
      "At: 929 [==========>] Loss 0.15147830677279822  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.09973893839948143  - accuracy: 0.84375\n",
      "At: 931 [==========>] Loss 0.18549878556558413  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.09977784257326067  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.09119863985528345  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.12103346125626066  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.046905454604954  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.16161033136141628  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.15616787460943937  - accuracy: 0.78125\n",
      "At: 938 [==========>] Loss 0.11840560246076273  - accuracy: 0.84375\n",
      "At: 939 [==========>] Loss 0.11197616983126082  - accuracy: 0.84375\n",
      "At: 940 [==========>] Loss 0.21169330556209714  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.0989164528400226  - accuracy: 0.90625\n",
      "At: 942 [==========>] Loss 0.15955740144333397  - accuracy: 0.8125\n",
      "At: 943 [==========>] Loss 0.1499806692187407  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.12663360358628617  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.10784514854884386  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.13272647718450475  - accuracy: 0.8125\n",
      "At: 947 [==========>] Loss 0.15141177026500574  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.13836786265396672  - accuracy: 0.78125\n",
      "At: 949 [==========>] Loss 0.06935623858746293  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.1277934945867767  - accuracy: 0.75\n",
      "At: 951 [==========>] Loss 0.1228486330012011  - accuracy: 0.8125\n",
      "At: 952 [==========>] Loss 0.07687318962935483  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.06265847818877784  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.10716540233560282  - accuracy: 0.90625\n",
      "At: 955 [==========>] Loss 0.13213023268343294  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.0925936959825325  - accuracy: 0.875\n",
      "At: 957 [==========>] Loss 0.14119224456136714  - accuracy: 0.84375\n",
      "At: 958 [==========>] Loss 0.07299558559934294  - accuracy: 0.9375\n",
      "At: 959 [==========>] Loss 0.14894903557396855  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.11849593743680605  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.10866123224269941  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.08890214050943623  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.09389808325271265  - accuracy: 0.90625\n",
      "At: 964 [==========>] Loss 0.16998146929837027  - accuracy: 0.78125\n",
      "At: 965 [==========>] Loss 0.130736570754683  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.1523935617536867  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.11150384925770213  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.14172805035062297  - accuracy: 0.875\n",
      "At: 969 [==========>] Loss 0.12724096417794237  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.109644552798916  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.0883784148202588  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.04633999822715014  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.13961784040795283  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.08805933597223022  - accuracy: 0.875\n",
      "At: 975 [==========>] Loss 0.11027500457700681  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.1121495749241271  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.10918949094672493  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.15110466879021942  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.10549860782694409  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.1657013934764356  - accuracy: 0.71875\n",
      "At: 981 [==========>] Loss 0.18183031966702798  - accuracy: 0.78125\n",
      "At: 982 [==========>] Loss 0.06715535769874792  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.13092091515249754  - accuracy: 0.84375\n",
      "At: 984 [==========>] Loss 0.10742273181767421  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.17523823176678519  - accuracy: 0.75\n",
      "At: 986 [==========>] Loss 0.11086018714670497  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.14065879276446822  - accuracy: 0.75\n",
      "At: 988 [==========>] Loss 0.08365645594582807  - accuracy: 0.90625\n",
      "At: 989 [==========>] Loss 0.15263270474759916  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.11371397848627182  - accuracy: 0.84375\n",
      "At: 991 [==========>] Loss 0.16665678577367918  - accuracy: 0.78125\n",
      "At: 992 [==========>] Loss 0.18888352734433347  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.1529494338163543  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.16091724948977193  - accuracy: 0.75\n",
      "At: 995 [==========>] Loss 0.1490693892031772  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.061999610804216364  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.13508496539843462  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.11204216233172004  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.1258093320393216  - accuracy: 0.8125\n",
      "At: 1000 [==========>] Loss 0.20315510075767945  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.14960969333422824  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.252050657276656  - accuracy: 0.65625\n",
      "At: 1003 [==========>] Loss 0.1251943921443191  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.1264922558829737  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.08809783287564188  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.10010618124829551  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.13450131403874238  - accuracy: 0.84375\n",
      "At: 1008 [==========>] Loss 0.18424321716827957  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.11749486602008795  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.15145996849203874  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.1564828267475394  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.11619859747023975  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.08968658280581476  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.09171707597861684  - accuracy: 0.84375\n",
      "At: 1015 [==========>] Loss 0.20644548413074879  - accuracy: 0.6875\n",
      "At: 1016 [==========>] Loss 0.13300522100053996  - accuracy: 0.8125\n",
      "At: 1017 [==========>] Loss 0.1488798988241206  - accuracy: 0.8125\n",
      "At: 1018 [==========>] Loss 0.13606567540090103  - accuracy: 0.8125\n",
      "At: 1019 [==========>] Loss 0.19120339331745564  - accuracy: 0.71875\n",
      "At: 1020 [==========>] Loss 0.12671634598747075  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.1414269714546175  - accuracy: 0.6875\n",
      "At: 1022 [==========>] Loss 0.13406825863092411  - accuracy: 0.8125\n",
      "At: 1023 [==========>] Loss 0.1620160503556335  - accuracy: 0.78125\n",
      "At: 1024 [==========>] Loss 0.18888964766680533  - accuracy: 0.78125\n",
      "At: 1025 [==========>] Loss 0.19197704265819504  - accuracy: 0.78125\n",
      "At: 1026 [==========>] Loss 0.10602185665992944  - accuracy: 0.84375\n",
      "At: 1027 [==========>] Loss 0.10862468451593776  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.2308242888908296  - accuracy: 0.65625\n",
      "At: 1029 [==========>] Loss 0.09917690766483041  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.07081291801272457  - accuracy: 0.9375\n",
      "At: 1031 [==========>] Loss 0.17324493012784362  - accuracy: 0.78125\n",
      "At: 1032 [==========>] Loss 0.13485815556755493  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.14365183104014326  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.105047205047221  - accuracy: 0.875\n",
      "At: 1035 [==========>] Loss 0.10972330322516319  - accuracy: 0.84375\n",
      "At: 1036 [==========>] Loss 0.13977963927675693  - accuracy: 0.84375\n",
      "At: 1037 [==========>] Loss 0.16976498186287917  - accuracy: 0.71875\n",
      "At: 1038 [==========>] Loss 0.09933541984520339  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.10885148107790132  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.11818593175323341  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.15032370514597654  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.13229516079933998  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.18000320552866123  - accuracy: 0.71875\n",
      "At: 1044 [==========>] Loss 0.1302812054390524  - accuracy: 0.78125\n",
      "At: 1045 [==========>] Loss 0.1623083474278719  - accuracy: 0.75\n",
      "At: 1046 [==========>] Loss 0.13637676273923802  - accuracy: 0.78125\n",
      "At: 1047 [==========>] Loss 0.1749050966989616  - accuracy: 0.71875\n",
      "At: 1048 [==========>] Loss 0.17156729280177152  - accuracy: 0.8125\n",
      "At: 1049 [==========>] Loss 0.14340498711866018  - accuracy: 0.84375\n",
      "At: 1050 [==========>] Loss 0.14510494591816714  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.07367836312972312  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.1077342627430883  - accuracy: 0.875\n",
      "At: 1053 [==========>] Loss 0.11097614761567925  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.11324997986470686  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.1639980739240581  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.12359148046051036  - accuracy: 0.75\n",
      "At: 1057 [==========>] Loss 0.16301755650988778  - accuracy: 0.78125\n",
      "At: 1058 [==========>] Loss 0.06166615956111066  - accuracy: 0.9375\n",
      "At: 1059 [==========>] Loss 0.11947072938019246  - accuracy: 0.8125\n",
      "At: 1060 [==========>] Loss 0.11264363896415132  - accuracy: 0.875\n",
      "At: 1061 [==========>] Loss 0.12560442599380844  - accuracy: 0.84375\n",
      "At: 1062 [==========>] Loss 0.1548248985182932  - accuracy: 0.8125\n",
      "At: 1063 [==========>] Loss 0.1182562406897732  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.14261919200858658  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.10726640048852192  - accuracy: 0.8125\n",
      "At: 1066 [==========>] Loss 0.11748464584874231  - accuracy: 0.75\n",
      "At: 1067 [==========>] Loss 0.11515404243877542  - accuracy: 0.875\n",
      "At: 1068 [==========>] Loss 0.09413342306783425  - accuracy: 0.90625\n",
      "At: 1069 [==========>] Loss 0.14266837594987072  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.12180974211367983  - accuracy: 0.8125\n",
      "At: 1071 [==========>] Loss 0.11502123891204491  - accuracy: 0.8125\n",
      "At: 1072 [==========>] Loss 0.10516695410598476  - accuracy: 0.84375\n",
      "At: 1073 [==========>] Loss 0.16001168017864026  - accuracy: 0.78125\n",
      "At: 1074 [==========>] Loss 0.19596119893183123  - accuracy: 0.65625\n",
      "At: 1075 [==========>] Loss 0.13405081985456885  - accuracy: 0.78125\n",
      "At: 1076 [==========>] Loss 0.14679624531436425  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.07961726050386608  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.06736237342259614  - accuracy: 0.9375\n",
      "At: 1079 [==========>] Loss 0.11228957581877608  - accuracy: 0.84375\n",
      "At: 1080 [==========>] Loss 0.1442711669039914  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.1303328368558811  - accuracy: 0.8125\n",
      "At: 1082 [==========>] Loss 0.0952425141711844  - accuracy: 0.875\n",
      "At: 1083 [==========>] Loss 0.08892763394464742  - accuracy: 0.90625\n",
      "At: 1084 [==========>] Loss 0.09117998217230464  - accuracy: 0.9375\n",
      "At: 1085 [==========>] Loss 0.13476040235379683  - accuracy: 0.78125\n",
      "At: 1086 [==========>] Loss 0.09709250746328957  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.11036738173425475  - accuracy: 0.84375\n",
      "At: 1088 [==========>] Loss 0.17079711835605194  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.10256649349019994  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.08580463951742673  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.18063373142673753  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.12472694587043046  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.1730224566474991  - accuracy: 0.78125\n",
      "At: 1094 [==========>] Loss 0.1323813435271927  - accuracy: 0.84375\n",
      "At: 1095 [==========>] Loss 0.11498463370956966  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.13431757618723988  - accuracy: 0.8125\n",
      "At: 1097 [==========>] Loss 0.07445607277730477  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.17148541334503908  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.14140333397033966  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.05905442896829994  - accuracy: 0.9375\n",
      "At: 1101 [==========>] Loss 0.116970361958944  - accuracy: 0.84375\n",
      "At: 1102 [==========>] Loss 0.12473308504197936  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.07774105304928883  - accuracy: 0.875\n",
      "At: 1104 [==========>] Loss 0.06390245304270717  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.08791480560796916  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.0674467563004783  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.19849624907648872  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.08925016037975786  - accuracy: 0.90625\n",
      "At: 1109 [==========>] Loss 0.06646258804981972  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.10431331460282812  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.1927556233746523  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.1381337444161486  - accuracy: 0.84375\n",
      "At: 1113 [==========>] Loss 0.16121328017421194  - accuracy: 0.71875\n",
      "At: 1114 [==========>] Loss 0.058149233704426484  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.1266202610469643  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.11929381622899805  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.07227540647729763  - accuracy: 0.9375\n",
      "At: 1118 [==========>] Loss 0.1335020226050933  - accuracy: 0.78125\n",
      "At: 1119 [==========>] Loss 0.13166399030128387  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.07010095485569567  - accuracy: 0.9375\n",
      "At: 1121 [==========>] Loss 0.12957482906296378  - accuracy: 0.78125\n",
      "At: 1122 [==========>] Loss 0.0834748141858737  - accuracy: 0.96875\n",
      "At: 1123 [==========>] Loss 0.16054807740395816  - accuracy: 0.71875\n",
      "At: 1124 [==========>] Loss 0.11620034430782317  - accuracy: 0.875\n",
      "At: 1125 [==========>] Loss 0.14481407269271004  - accuracy: 0.71875\n",
      "At: 1126 [==========>] Loss 0.10092689474943688  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.12146320869860226  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.07366773289338564  - accuracy: 0.875\n",
      "At: 1129 [==========>] Loss 0.15736737086938912  - accuracy: 0.75\n",
      "At: 1130 [==========>] Loss 0.10661329948895307  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.077931472289344  - accuracy: 0.90625\n",
      "At: 1132 [==========>] Loss 0.11658430577268225  - accuracy: 0.8125\n",
      "At: 1133 [==========>] Loss 0.13579519760207331  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.1040296739594469  - accuracy: 0.84375\n",
      "At: 1135 [==========>] Loss 0.11239662220721416  - accuracy: 0.84375\n",
      "At: 1136 [==========>] Loss 0.16179638906602295  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.0857352119225433  - accuracy: 0.90625\n",
      "At: 1138 [==========>] Loss 0.10778949177134246  - accuracy: 0.875\n",
      "At: 1139 [==========>] Loss 0.10633113359295882  - accuracy: 0.875\n",
      "At: 1140 [==========>] Loss 0.13629438954552348  - accuracy: 0.84375\n",
      "At: 1141 [==========>] Loss 0.14622553259111748  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.15647647595522246  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.05488018674564666  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.1305084408879053  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.12953433531590738  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.1286271683146722  - accuracy: 0.8125\n",
      "At: 1147 [==========>] Loss 0.20534604773387533  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.09326293116050935  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.08817528229905049  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.10777202446278937  - accuracy: 0.90625\n",
      "At: 1151 [==========>] Loss 0.14837889961756295  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.10113782028678434  - accuracy: 0.90625\n",
      "At: 1153 [==========>] Loss 0.20127583382737652  - accuracy: 0.75\n",
      "At: 1154 [==========>] Loss 0.12710395725334395  - accuracy: 0.84375\n",
      "At: 1155 [==========>] Loss 0.12362573465844927  - accuracy: 0.8125\n",
      "At: 1156 [==========>] Loss 0.16173373614027614  - accuracy: 0.71875\n",
      "At: 1157 [==========>] Loss 0.14304356614474556  - accuracy: 0.8125\n",
      "At: 1158 [==========>] Loss 0.1356288348502699  - accuracy: 0.84375\n",
      "At: 1159 [==========>] Loss 0.09962562668370839  - accuracy: 0.875\n",
      "At: 1160 [==========>] Loss 0.07077192705738038  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.124435555987216  - accuracy: 0.8125\n",
      "At: 1162 [==========>] Loss 0.1461343527678721  - accuracy: 0.78125\n",
      "At: 1163 [==========>] Loss 0.18831140474619537  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.06491276429532734  - accuracy: 0.96875\n",
      "At: 1165 [==========>] Loss 0.1659972435496388  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.06071218107264755  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.14869070347873836  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.12686170386601525  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.10092873933501068  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.19701082216996174  - accuracy: 0.71875\n",
      "At: 1171 [==========>] Loss 0.07879729480975842  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.13617833722878356  - accuracy: 0.78125\n",
      "At: 1173 [==========>] Loss 0.10857181466139612  - accuracy: 0.90625\n",
      "At: 1174 [==========>] Loss 0.1747355550969588  - accuracy: 0.78125\n",
      "At: 1175 [==========>] Loss 0.12803975753513058  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.12448602499019255  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.08752603657919608  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.16120914939084469  - accuracy: 0.8125\n",
      "At: 1179 [==========>] Loss 0.12349970694345333  - accuracy: 0.84375\n",
      "At: 1180 [==========>] Loss 0.19130905859921682  - accuracy: 0.78125\n",
      "At: 1181 [==========>] Loss 0.11091079580396297  - accuracy: 0.8125\n",
      "At: 1182 [==========>] Loss 0.14219717671229937  - accuracy: 0.71875\n",
      "At: 1183 [==========>] Loss 0.14730044310709095  - accuracy: 0.84375\n",
      "At: 1184 [==========>] Loss 0.1507039893976087  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.11483865956631417  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.1476818935531174  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.11544088803237892  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.08257414438335106  - accuracy: 0.90625\n",
      "At: 1189 [==========>] Loss 0.13721960576582426  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.10559227506140317  - accuracy: 0.875\n",
      "At: 1191 [==========>] Loss 0.16288596249531678  - accuracy: 0.75\n",
      "At: 1192 [==========>] Loss 0.08939895238108783  - accuracy: 0.90625\n",
      "At: 1193 [==========>] Loss 0.15128988126943366  - accuracy: 0.8125\n",
      "At: 1194 [==========>] Loss 0.13345685160880072  - accuracy: 0.8125\n",
      "At: 1195 [==========>] Loss 0.11209816761800434  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.1368241126556901  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.12547112902837  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.09027868503395092  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.1606908514942904  - accuracy: 0.78125\n",
      "At: 1200 [==========>] Loss 0.06278765972526576  - accuracy: 1.0\n",
      "At: 1201 [==========>] Loss 0.13763152941622514  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.15829059628504064  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.13840252104512746  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.09848945384626315  - accuracy: 0.90625\n",
      "At: 1205 [==========>] Loss 0.0682937840892012  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.12264244425071935  - accuracy: 0.875\n",
      "At: 1207 [==========>] Loss 0.15967244095558347  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.09549213863359202  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.09023652728312742  - accuracy: 0.875\n",
      "At: 1210 [==========>] Loss 0.13099105931345822  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.14589726678124415  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.10356080121768538  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.17181936347244375  - accuracy: 0.78125\n",
      "At: 1214 [==========>] Loss 0.13615292884333416  - accuracy: 0.8125\n",
      "At: 1215 [==========>] Loss 0.15894390810549477  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.09671843166233801  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.0985474227420608  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.10071345425233644  - accuracy: 0.90625\n",
      "At: 1219 [==========>] Loss 0.11822568018810548  - accuracy: 0.8125\n",
      "At: 1220 [==========>] Loss 0.12768806478876932  - accuracy: 0.78125\n",
      "At: 1221 [==========>] Loss 0.07770417631437945  - accuracy: 0.96875\n",
      "At: 1222 [==========>] Loss 0.17360590123329006  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.07271589772549733  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08430516001041516  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.08696424301028763  - accuracy: 0.90625\n",
      "At: 1226 [==========>] Loss 0.09405873564656728  - accuracy: 0.90625\n",
      "At: 1227 [==========>] Loss 0.13263986407477826  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.15969944295857158  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.13530627721629024  - accuracy: 0.75\n",
      "At: 1230 [==========>] Loss 0.16195152953114977  - accuracy: 0.75\n",
      "At: 1231 [==========>] Loss 0.13833016454756486  - accuracy: 0.8125\n",
      "At: 1232 [==========>] Loss 0.09284936867123748  - accuracy: 0.90625\n",
      "At: 1233 [==========>] Loss 0.11568559929830671  - accuracy: 0.8125\n",
      "At: 1234 [==========>] Loss 0.1273537050343874  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.07301829872369273  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.14164357891200458  - accuracy: 0.84375\n",
      "At: 1237 [==========>] Loss 0.06210670149591772  - accuracy: 0.9375\n",
      "At: 1238 [==========>] Loss 0.10894778174846992  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.15222650986665848  - accuracy: 0.8125\n",
      "At: 1240 [==========>] Loss 0.1061414217522377  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.12467730139701184  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.17082713787946285  - accuracy: 0.78125\n",
      "At: 1243 [==========>] Loss 0.1594996082762479  - accuracy: 0.78125\n",
      "At: 1244 [==========>] Loss 0.14150163146897882  - accuracy: 0.8125\n",
      "At: 1245 [==========>] Loss 0.11760768533619513  - accuracy: 0.84375\n",
      "At: 1246 [==========>] Loss 0.06275852044608267  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.15904713232962164  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.1052402096127715  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.15228529935311022  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.1314619791471586  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.10280708095111898  - accuracy: 0.84375\n",
      "At: 1252 [==========>] Loss 0.09549321387722864  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.0987101222494344  - accuracy: 0.90625\n",
      "At: 1254 [==========>] Loss 0.15686264115501822  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.09998518574127699  - accuracy: 0.875\n",
      "At: 1256 [==========>] Loss 0.14355550425064012  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.14214100308879668  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.07599646697436149  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.13348864734054444  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.10249330183593075  - accuracy: 0.875\n",
      "At: 1261 [==========>] Loss 0.13491285255848004  - accuracy: 0.75\n",
      "At: 1262 [==========>] Loss 0.1496171289310441  - accuracy: 0.8125\n",
      "At: 1263 [==========>] Loss 0.11732560102890524  - accuracy: 0.8125\n",
      "At: 1264 [==========>] Loss 0.09599903648670534  - accuracy: 0.875\n",
      "At: 1265 [==========>] Loss 0.1374146739907921  - accuracy: 0.78125\n",
      "At: 1266 [==========>] Loss 0.1305682596612453  - accuracy: 0.71875\n",
      "At: 1267 [==========>] Loss 0.10580248593406044  - accuracy: 0.90625\n",
      "At: 1268 [==========>] Loss 0.1710411694416002  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.1278139680323314  - accuracy: 0.78125\n",
      "At: 1270 [==========>] Loss 0.12925440604325325  - accuracy: 0.78125\n",
      "At: 1271 [==========>] Loss 0.15426378181256953  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.07888109118653598  - accuracy: 0.875\n",
      "At: 1273 [==========>] Loss 0.20025943828642667  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.13724983404977384  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.07488900549490227  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.12437830593989428  - accuracy: 0.8125\n",
      "At: 1277 [==========>] Loss 0.10451923064940308  - accuracy: 0.8125\n",
      "At: 1278 [==========>] Loss 0.14308095748686614  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.09431720374300818  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.11892867985093353  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.1573298634938607  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.127087083140749  - accuracy: 0.78125\n",
      "At: 1283 [==========>] Loss 0.14696960031075762  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.1565116457139382  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.06873424168561333  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.14565110994712482  - accuracy: 0.8125\n",
      "At: 1287 [==========>] Loss 0.11685527323416486  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.1814093338427605  - accuracy: 0.6875\n",
      "At: 1289 [==========>] Loss 0.09796968777995037  - accuracy: 0.875\n",
      "At: 1290 [==========>] Loss 0.1285917231415537  - accuracy: 0.8125\n",
      "At: 1291 [==========>] Loss 0.18270872286753453  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.09875642998831166  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.14099258109098434  - accuracy: 0.8125\n",
      "At: 1294 [==========>] Loss 0.14858656698230524  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.14483051281802534  - accuracy: 0.8125\n",
      "At: 1296 [==========>] Loss 0.13946555433170552  - accuracy: 0.8125\n",
      "At: 1297 [==========>] Loss 0.12394478147925848  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.10759780282781087  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.1547733820031535  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.11995495003159042  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.11039437171844546  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.09090260366064574  - accuracy: 0.875\n",
      "At: 1303 [==========>] Loss 0.09357093300655021  - accuracy: 0.90625\n",
      "At: 1304 [==========>] Loss 0.10722817728180328  - accuracy: 0.875\n",
      "At: 1305 [==========>] Loss 0.14313415171103339  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.07378251963361726  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.16961252274714653  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.07475372818527135  - accuracy: 0.9375\n",
      "At: 1309 [==========>] Loss 0.1525899846878615  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.165749019611561  - accuracy: 0.8125\n",
      "At: 1311 [==========>] Loss 0.14981196317083892  - accuracy: 0.75\n",
      "At: 1312 [==========>] Loss 0.07605164168809789  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.17787682603490546  - accuracy: 0.6875\n",
      "At: 1314 [==========>] Loss 0.05373369740539198  - accuracy: 0.96875\n",
      "At: 1315 [==========>] Loss 0.15036625642825316  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.13330756850367861  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.1040924144508219  - accuracy: 0.875\n",
      "At: 1318 [==========>] Loss 0.12432703805175606  - accuracy: 0.84375\n",
      "At: 1319 [==========>] Loss 0.12301599755976768  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.13886100248400418  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.08237902283741089  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.1459334207248435  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.08852455898122794  - accuracy: 0.9375\n",
      "At: 1324 [==========>] Loss 0.13609845665173428  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.09856314614319374  - accuracy: 0.875\n",
      "At: 1326 [==========>] Loss 0.1082375109255706  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.1488643587027098  - accuracy: 0.71875\n",
      "At: 1328 [==========>] Loss 0.09947955832900955  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.07382176764828015  - accuracy: 0.90625\n",
      "At: 1330 [==========>] Loss 0.1323226101456335  - accuracy: 0.8125\n",
      "At: 1331 [==========>] Loss 0.17598987417459555  - accuracy: 0.75\n",
      "At: 1332 [==========>] Loss 0.11107805761970206  - accuracy: 0.84375\n",
      "At: 1333 [==========>] Loss 0.14858825864539116  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.11101951968530975  - accuracy: 0.84375\n",
      "At: 1335 [==========>] Loss 0.1011025314673911  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.09481887631267649  - accuracy: 0.875\n",
      "At: 1337 [==========>] Loss 0.15354400711012037  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.16094180242703812  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.12382645958178923  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.13822797359068217  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.07536064951716764  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.1099494280604706  - accuracy: 0.875\n",
      "At: 1343 [==========>] Loss 0.16061528568656303  - accuracy: 0.75\n",
      "At: 1344 [==========>] Loss 0.16579606375167893  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.09987986707422326  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.11023168851243154  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.11441077295361224  - accuracy: 0.84375\n",
      "At: 1348 [==========>] Loss 0.121171011295575  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.13641163071890797  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.1213245528552799  - accuracy: 0.8125\n",
      "At: 1351 [==========>] Loss 0.1191790434733987  - accuracy: 0.78125\n",
      "At: 1352 [==========>] Loss 0.08300948079034025  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.17921636837492994  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.17783662742433184  - accuracy: 0.71875\n",
      "At: 1355 [==========>] Loss 0.08094564681801485  - accuracy: 0.875\n",
      "At: 1356 [==========>] Loss 0.08169189721245088  - accuracy: 0.9375\n",
      "At: 1357 [==========>] Loss 0.11631154308654212  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.12051677051400451  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.08036875817735532  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.1853207597825896  - accuracy: 0.75\n",
      "At: 1361 [==========>] Loss 0.113212526406611  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.1321455149083288  - accuracy: 0.78125\n",
      "At: 1363 [==========>] Loss 0.0988961234855854  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.15090704386043344  - accuracy: 0.75\n",
      "At: 1365 [==========>] Loss 0.13724303090896706  - accuracy: 0.78125\n",
      "At: 1366 [==========>] Loss 0.14894765888362588  - accuracy: 0.75\n",
      "At: 1367 [==========>] Loss 0.125408500589967  - accuracy: 0.84375\n",
      "At: 1368 [==========>] Loss 0.18356248614567103  - accuracy: 0.6875\n",
      "At: 1369 [==========>] Loss 0.07717974122313791  - accuracy: 0.875\n",
      "At: 1370 [==========>] Loss 0.14200068521015805  - accuracy: 0.75\n",
      "At: 1371 [==========>] Loss 0.1961920512739984  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.12154370057612349  - accuracy: 0.78125\n",
      "At: 1373 [==========>] Loss 0.13170880508301847  - accuracy: 0.8125\n",
      "At: 1374 [==========>] Loss 0.13642436458133547  - accuracy: 0.78125\n",
      "At: 1375 [==========>] Loss 0.14252692800145417  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.0992876868944825  - accuracy: 0.90625\n",
      "At: 1377 [==========>] Loss 0.17195110126966928  - accuracy: 0.8125\n",
      "At: 1378 [==========>] Loss 0.10809821877800824  - accuracy: 0.875\n",
      "At: 1379 [==========>] Loss 0.14964315987970428  - accuracy: 0.8125\n",
      "At: 1380 [==========>] Loss 0.13610400996847577  - accuracy: 0.78125\n",
      "At: 1381 [==========>] Loss 0.08485751923573628  - accuracy: 0.9375\n",
      "At: 1382 [==========>] Loss 0.12844708160238938  - accuracy: 0.875\n",
      "At: 1383 [==========>] Loss 0.10804832725210224  - accuracy: 0.8125\n",
      "At: 1384 [==========>] Loss 0.10286873244746296  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.16572816500806686  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.18107040573691519  - accuracy: 0.6875\n",
      "At: 1387 [==========>] Loss 0.07988599066155297  - accuracy: 0.9375\n",
      "At: 1388 [==========>] Loss 0.16437787000782803  - accuracy: 0.75\n",
      "At: 1389 [==========>] Loss 0.09965936238524153  - accuracy: 0.90625\n",
      "At: 1390 [==========>] Loss 0.15120757717372452  - accuracy: 0.75\n",
      "At: 1391 [==========>] Loss 0.12874219227586473  - accuracy: 0.78125\n",
      "At: 1392 [==========>] Loss 0.0873507963321482  - accuracy: 0.875\n",
      "At: 1393 [==========>] Loss 0.13144368564254028  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.09402001100232354  - accuracy: 0.84375\n",
      "At: 1395 [==========>] Loss 0.1827010661333745  - accuracy: 0.71875\n",
      "At: 1396 [==========>] Loss 0.058329622877555484  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.13469051601512472  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.125980834896258  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.12688852323279148  - accuracy: 0.84375\n",
      "At: 1400 [==========>] Loss 0.15249399474100084  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.08648454478783968  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.13474441355756756  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.15106503294524595  - accuracy: 0.71875\n",
      "At: 1404 [==========>] Loss 0.12903522687667152  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.08560729976269099  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.1506662599845594  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.11490004797734027  - accuracy: 0.84375\n",
      "At: 1408 [==========>] Loss 0.13817604170231995  - accuracy: 0.78125\n",
      "At: 1409 [==========>] Loss 0.04565747185105735  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.16214833102201479  - accuracy: 0.78125\n",
      "At: 1411 [==========>] Loss 0.12198613973492681  - accuracy: 0.84375\n",
      "At: 1412 [==========>] Loss 0.12185108790421462  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.10363981991389273  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.17176967047238462  - accuracy: 0.75\n",
      "At: 1415 [==========>] Loss 0.08471449603586101  - accuracy: 0.875\n",
      "At: 1416 [==========>] Loss 0.13454936856916455  - accuracy: 0.8125\n",
      "At: 1417 [==========>] Loss 0.09153913320069335  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.16179811063834454  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.12565753351996342  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.09462330367117885  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.09779785266430704  - accuracy: 0.84375\n",
      "At: 1422 [==========>] Loss 0.11421026603975658  - accuracy: 0.78125\n",
      "At: 1423 [==========>] Loss 0.1306735822991772  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.15024025888284637  - accuracy: 0.8125\n",
      "At: 1425 [==========>] Loss 0.09059361778397786  - accuracy: 0.875\n",
      "At: 1426 [==========>] Loss 0.12461721300601165  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.08425648315625123  - accuracy: 0.9375\n",
      "At: 1428 [==========>] Loss 0.10985295401601036  - accuracy: 0.84375\n",
      "At: 1429 [==========>] Loss 0.18720594689712053  - accuracy: 0.75\n",
      "At: 1430 [==========>] Loss 0.07481815486345356  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.14039154523720349  - accuracy: 0.78125\n",
      "At: 1432 [==========>] Loss 0.12086572123184244  - accuracy: 0.8125\n",
      "At: 1433 [==========>] Loss 0.098584013036357  - accuracy: 0.9375\n",
      "At: 1434 [==========>] Loss 0.1413889222353332  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.13428312045262353  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.07574208374639373  - accuracy: 0.875\n",
      "At: 1437 [==========>] Loss 0.11795988213941543  - accuracy: 0.84375\n",
      "At: 1438 [==========>] Loss 0.11682860702120315  - accuracy: 0.8125\n",
      "At: 1439 [==========>] Loss 0.1148670303224372  - accuracy: 0.875\n",
      "At: 1440 [==========>] Loss 0.08891590500247384  - accuracy: 0.90625\n",
      "At: 1441 [==========>] Loss 0.06482817325071444  - accuracy: 0.9375\n",
      "At: 1442 [==========>] Loss 0.12455939767785151  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.1284542121639079  - accuracy: 0.84375\n",
      "At: 1444 [==========>] Loss 0.09810256398554583  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.17202947483556746  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.21158233496819723  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.17237030201849773  - accuracy: 0.75\n",
      "At: 1448 [==========>] Loss 0.0669501291298808  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.12967568869461282  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.11905202550690282  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.08929974924065073  - accuracy: 0.875\n",
      "At: 1452 [==========>] Loss 0.10311768697982171  - accuracy: 0.84375\n",
      "At: 1453 [==========>] Loss 0.0575497128601163  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.184536942473339  - accuracy: 0.75\n",
      "At: 1455 [==========>] Loss 0.11488982624011206  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.10697666372829294  - accuracy: 0.84375\n",
      "At: 1457 [==========>] Loss 0.08604240412992448  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.16762797644317007  - accuracy: 0.78125\n",
      "At: 1459 [==========>] Loss 0.11298294637524871  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.1694853228023187  - accuracy: 0.78125\n",
      "At: 1461 [==========>] Loss 0.1224321633674755  - accuracy: 0.8125\n",
      "At: 1462 [==========>] Loss 0.18876062622948797  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.09175484182980151  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.1793199674864599  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.0984899417097359  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.09269880569728463  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.1928858310692841  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.15085396781220828  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.17973869430767736  - accuracy: 0.75\n",
      "At: 1470 [==========>] Loss 0.1549477513104447  - accuracy: 0.6875\n",
      "At: 1471 [==========>] Loss 0.1475168766735897  - accuracy: 0.75\n",
      "At: 1472 [==========>] Loss 0.09272457212482532  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.11688688720959603  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.21659132742953507  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.1627940379777229  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.11801030712583788  - accuracy: 0.78125\n",
      "At: 1477 [==========>] Loss 0.10771052198564446  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.10481151833511138  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.1205116939285335  - accuracy: 0.8125\n",
      "At: 1480 [==========>] Loss 0.10507619635863336  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.11612492876303399  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.087224797907873  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.19217452106285748  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.1231205615329835  - accuracy: 0.8125\n",
      "At: 1485 [==========>] Loss 0.16582334073692798  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.0762791915295496  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.080447065622438  - accuracy: 0.90625\n",
      "At: 1488 [==========>] Loss 0.1444464437726418  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.17423238932228235  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.08850227306326011  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.18672944467602823  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.12612611276949395  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.21895459052212016  - accuracy: 0.65625\n",
      "At: 1494 [==========>] Loss 0.15163773690394367  - accuracy: 0.8125\n",
      "At: 1495 [==========>] Loss 0.1079285783069365  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.1112122539750584  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.1448974543750738  - accuracy: 0.8125\n",
      "At: 1498 [==========>] Loss 0.1287941016517774  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.11380678885213877  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.08254991232670811  - accuracy: 0.90625\n",
      "At: 1501 [==========>] Loss 0.12030074403613428  - accuracy: 0.8125\n",
      "At: 1502 [==========>] Loss 0.14036786949934665  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.11588076484757685  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.15876950938588064  - accuracy: 0.78125\n",
      "At: 1505 [==========>] Loss 0.12872759909147502  - accuracy: 0.78125\n",
      "At: 1506 [==========>] Loss 0.15991992774481434  - accuracy: 0.71875\n",
      "At: 1507 [==========>] Loss 0.13453188218091158  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.22304978163248018  - accuracy: 0.65625\n",
      "At: 1509 [==========>] Loss 0.09944271939277974  - accuracy: 0.875\n",
      "At: 1510 [==========>] Loss 0.11063306689995267  - accuracy: 0.875\n",
      "At: 1511 [==========>] Loss 0.10860793754627872  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.10862079580141587  - accuracy: 0.875\n",
      "At: 1513 [==========>] Loss 0.1909866080901567  - accuracy: 0.6875\n",
      "At: 1514 [==========>] Loss 0.14721887056801858  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.13073999783201734  - accuracy: 0.84375\n",
      "At: 1516 [==========>] Loss 0.09273833836976636  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.13773758464996666  - accuracy: 0.84375\n",
      "At: 1518 [==========>] Loss 0.11483616390398993  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.18187135172746893  - accuracy: 0.6875\n",
      "At: 1520 [==========>] Loss 0.11577198576889455  - accuracy: 0.8125\n",
      "At: 1521 [==========>] Loss 0.07527596401915114  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.18605715252450858  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.10285948991614052  - accuracy: 0.875\n",
      "At: 1524 [==========>] Loss 0.16248614315176596  - accuracy: 0.71875\n",
      "At: 1525 [==========>] Loss 0.1131407754008312  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.10807123175536071  - accuracy: 0.875\n",
      "At: 1527 [==========>] Loss 0.1550129231127828  - accuracy: 0.75\n",
      "At: 1528 [==========>] Loss 0.13384217341184895  - accuracy: 0.8125\n",
      "At: 1529 [==========>] Loss 0.07470620069455902  - accuracy: 0.875\n",
      "At: 1530 [==========>] Loss 0.03718834302482309  - accuracy: 1.0\n",
      "At: 1531 [==========>] Loss 0.14284941624868674  - accuracy: 0.8125\n",
      "At: 1532 [==========>] Loss 0.19280788525102427  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.13757885881450743  - accuracy: 0.8125\n",
      "At: 1534 [==========>] Loss 0.10906059871213683  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.16733947934327537  - accuracy: 0.78125\n",
      "At: 1536 [==========>] Loss 0.1791710383942252  - accuracy: 0.75\n",
      "At: 1537 [==========>] Loss 0.10473046153470969  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.15279224899036808  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.07992266417089108  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1672364410374711  - accuracy: 0.78125\n",
      "At: 1541 [==========>] Loss 0.1237887241860581  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.10713352654118832  - accuracy: 0.8125\n",
      "At: 1543 [==========>] Loss 0.12953869353111902  - accuracy: 0.84375\n",
      "At: 1544 [==========>] Loss 0.1363687901567185  - accuracy: 0.8125\n",
      "At: 1545 [==========>] Loss 0.21699117568823625  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.11180951444113982  - accuracy: 0.84375\n",
      "At: 1547 [==========>] Loss 0.15006726115747215  - accuracy: 0.84375\n",
      "At: 1548 [==========>] Loss 0.12178106137397857  - accuracy: 0.90625\n",
      "At: 1549 [==========>] Loss 0.14463802377679016  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.059193176540489945  - accuracy: 0.9375\n",
      "At: 1551 [==========>] Loss 0.14791617261512732  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.08864410987416751  - accuracy: 0.90625\n",
      "At: 1553 [==========>] Loss 0.07671430180792302  - accuracy: 0.875\n",
      "At: 1554 [==========>] Loss 0.12913606758570798  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.13108245522145656  - accuracy: 0.78125\n",
      "At: 1556 [==========>] Loss 0.1909449065610159  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.09021495003322534  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.13893727789980864  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.08820640828733953  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.11393677344767023  - accuracy: 0.84375\n",
      "At: 1561 [==========>] Loss 0.1528600244605126  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.09587606763103063  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.10137251011434095  - accuracy: 0.90625\n",
      "At: 1564 [==========>] Loss 0.12204770308275929  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.11642021169293354  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.13371264338964117  - accuracy: 0.875\n",
      "At: 1567 [==========>] Loss 0.14335131394295772  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.06552595605519573  - accuracy: 0.96875\n",
      "At: 1569 [==========>] Loss 0.11982433775330374  - accuracy: 0.8125\n",
      "At: 1570 [==========>] Loss 0.07278703948324158  - accuracy: 0.9375\n",
      "At: 1571 [==========>] Loss 0.15307018292152175  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.1263018102434545  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.07005588820878621  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.11849428150406596  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.09944146025833564  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.14472423215030172  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.09808495203676125  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.07567654142621112  - accuracy: 0.875\n",
      "At: 1579 [==========>] Loss 0.09853292291593053  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.10076721925592812  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.09215591764004041  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.15623014521247014  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.07779053519457554  - accuracy: 0.90625\n",
      "At: 1584 [==========>] Loss 0.12636336239453905  - accuracy: 0.84375\n",
      "At: 1585 [==========>] Loss 0.10835372903427562  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.16773430500260414  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.08136941237233904  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.12874556061197204  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.1469560305600497  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.15459415823574657  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.11030220826295915  - accuracy: 0.90625\n",
      "At: 1592 [==========>] Loss 0.09668270020958149  - accuracy: 0.875\n",
      "At: 1593 [==========>] Loss 0.1629507580285331  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.0924545969676058  - accuracy: 0.9375\n",
      "At: 1595 [==========>] Loss 0.1339894886987138  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.2053323618854327  - accuracy: 0.6875\n",
      "At: 1597 [==========>] Loss 0.16283791177170562  - accuracy: 0.78125\n",
      "At: 1598 [==========>] Loss 0.17705575227425208  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.22169926521552114  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.1388325136836867  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.09265258264985773  - accuracy: 0.875\n",
      "At: 1602 [==========>] Loss 0.13325013854415554  - accuracy: 0.78125\n",
      "At: 1603 [==========>] Loss 0.1814932537401796  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.23834121586472332  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.06757551722350966  - accuracy: 0.90625\n",
      "At: 1606 [==========>] Loss 0.1230899116831641  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.17185489956657407  - accuracy: 0.71875\n",
      "At: 1608 [==========>] Loss 0.14943539990408905  - accuracy: 0.8125\n",
      "At: 1609 [==========>] Loss 0.19495915023874544  - accuracy: 0.6875\n",
      "At: 1610 [==========>] Loss 0.17316547027278553  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.11030156821324096  - accuracy: 0.84375\n",
      "At: 1612 [==========>] Loss 0.09796805111012652  - accuracy: 0.84375\n",
      "At: 1613 [==========>] Loss 0.1555736769351778  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.14706165021506937  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.08136652525860738  - accuracy: 0.9375\n",
      "At: 1616 [==========>] Loss 0.14930728794545117  - accuracy: 0.75\n",
      "At: 1617 [==========>] Loss 0.09364106191036616  - accuracy: 0.84375\n",
      "At: 1618 [==========>] Loss 0.12095596805735563  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.1975184561064323  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.12658068637710818  - accuracy: 0.8125\n",
      "At: 1621 [==========>] Loss 0.11936678710896143  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.1650934939238023  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.09322564150135364  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.15514435190922465  - accuracy: 0.8125\n",
      "At: 1625 [==========>] Loss 0.1447451747348215  - accuracy: 0.78125\n",
      "At: 1626 [==========>] Loss 0.11175548969834678  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.11528605467068699  - accuracy: 0.84375\n",
      "At: 1628 [==========>] Loss 0.1552282282465697  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.13856909556946737  - accuracy: 0.78125\n",
      "At: 1630 [==========>] Loss 0.09197068566707703  - accuracy: 0.9375\n",
      "At: 1631 [==========>] Loss 0.135045227997027  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.10289450630240667  - accuracy: 0.875\n",
      "At: 1633 [==========>] Loss 0.08248306514650729  - accuracy: 0.9375\n",
      "At: 1634 [==========>] Loss 0.125575080267244  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.23137821557708507  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.12775546077560623  - accuracy: 0.84375\n",
      "At: 1637 [==========>] Loss 0.09157238905829526  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.127183242058043  - accuracy: 0.8125\n",
      "At: 1639 [==========>] Loss 0.1534022190611208  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.13346965265866667  - accuracy: 0.84375\n",
      "At: 1641 [==========>] Loss 0.08981122822653498  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.11619107553834317  - accuracy: 0.8125\n",
      "At: 1643 [==========>] Loss 0.11687930944492392  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.11374774581101962  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.07455752546782049  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.1332343428581162  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.17622800673516162  - accuracy: 0.75\n",
      "At: 1648 [==========>] Loss 0.14326735054127418  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.08851424528097655  - accuracy: 0.90625\n",
      "At: 1650 [==========>] Loss 0.10160413336003615  - accuracy: 0.84375\n",
      "At: 1651 [==========>] Loss 0.13796982424220375  - accuracy: 0.8125\n",
      "At: 1652 [==========>] Loss 0.07760821047379918  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.06674688199254439  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.05587496437634328  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.17243758196524203  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.13070854324229328  - accuracy: 0.8125\n",
      "At: 1657 [==========>] Loss 0.12097791811912778  - accuracy: 0.84375\n",
      "At: 1658 [==========>] Loss 0.2228619646378763  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.1081919258344375  - accuracy: 0.84375\n",
      "At: 1660 [==========>] Loss 0.04296865082596205  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.09018736880340777  - accuracy: 0.875\n",
      "At: 1662 [==========>] Loss 0.09502457462711134  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.13890703622807757  - accuracy: 0.8125\n",
      "At: 1664 [==========>] Loss 0.10347046214932146  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.07515289232317637  - accuracy: 0.90625\n",
      "At: 1666 [==========>] Loss 0.08800313530133633  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.1532160159596219  - accuracy: 0.8125\n",
      "At: 1668 [==========>] Loss 0.1873306626570787  - accuracy: 0.71875\n",
      "At: 1669 [==========>] Loss 0.11227662219685912  - accuracy: 0.84375\n",
      "At: 1670 [==========>] Loss 0.04980850646544627  - accuracy: 0.9375\n",
      "At: 1671 [==========>] Loss 0.14176125926821045  - accuracy: 0.8125\n",
      "At: 1672 [==========>] Loss 0.1402614902831134  - accuracy: 0.8125\n",
      "At: 1673 [==========>] Loss 0.0748200666903398  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.1668446986953468  - accuracy: 0.71875\n",
      "At: 1675 [==========>] Loss 0.20459259904056581  - accuracy: 0.6875\n",
      "At: 1676 [==========>] Loss 0.24523227124233019  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.1114940108791203  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.16093271718488888  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.10434707502540831  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.18176395276815246  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.13409145800973668  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.08963059772344795  - accuracy: 0.90625\n",
      "At: 1683 [==========>] Loss 0.18650582601398655  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.08670240138392052  - accuracy: 0.90625\n",
      "At: 1685 [==========>] Loss 0.11128132877230922  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.1108496842559541  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.18272311981411285  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.05807541262494836  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.11736476944074697  - accuracy: 0.875\n",
      "At: 1690 [==========>] Loss 0.11331382982215665  - accuracy: 0.875\n",
      "At: 1691 [==========>] Loss 0.12715701106714067  - accuracy: 0.78125\n",
      "At: 1692 [==========>] Loss 0.17992809627256012  - accuracy: 0.65625\n",
      "At: 1693 [==========>] Loss 0.11994025102525827  - accuracy: 0.84375\n",
      "At: 1694 [==========>] Loss 0.09288216118929714  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.15311218037468183  - accuracy: 0.75\n",
      "At: 1696 [==========>] Loss 0.13803692604432827  - accuracy: 0.8125\n",
      "At: 1697 [==========>] Loss 0.11994797106418997  - accuracy: 0.8125\n",
      "At: 1698 [==========>] Loss 0.08153430621704241  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.10170328961434039  - accuracy: 0.90625\n",
      "At: 1700 [==========>] Loss 0.09845177648689234  - accuracy: 0.875\n",
      "At: 1701 [==========>] Loss 0.10210753022067585  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.09344537480426557  - accuracy: 0.90625\n",
      "At: 1703 [==========>] Loss 0.18621096926213399  - accuracy: 0.6875\n",
      "At: 1704 [==========>] Loss 0.09088746080818813  - accuracy: 0.84375\n",
      "At: 1705 [==========>] Loss 0.12938567364281336  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.1341606007727975  - accuracy: 0.84375\n",
      "At: 1707 [==========>] Loss 0.17993854268137902  - accuracy: 0.78125\n",
      "At: 1708 [==========>] Loss 0.0956463628828589  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.15985519163958758  - accuracy: 0.8125\n",
      "At: 1710 [==========>] Loss 0.14019971833255793  - accuracy: 0.84375\n",
      "At: 1711 [==========>] Loss 0.09292198271827333  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.10725585226374654  - accuracy: 0.875\n",
      "At: 1713 [==========>] Loss 0.1108437688212434  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.16313580904704697  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.14266757211846082  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.0664164535788599  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08712648714427607  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.14183191618696422  - accuracy: 0.8125\n",
      "At: 1719 [==========>] Loss 0.11299997974830522  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.09283028199536603  - accuracy: 0.84375\n",
      "At: 1721 [==========>] Loss 0.16242787049259239  - accuracy: 0.71875\n",
      "At: 1722 [==========>] Loss 0.08247194908063679  - accuracy: 0.84375\n",
      "At: 1723 [==========>] Loss 0.20426328085015844  - accuracy: 0.71875\n",
      "At: 1724 [==========>] Loss 0.08656041868141942  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.1513386130614155  - accuracy: 0.78125\n",
      "At: 1726 [==========>] Loss 0.13154284284213572  - accuracy: 0.75\n",
      "At: 1727 [==========>] Loss 0.12437758898279899  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.11759831010884039  - accuracy: 0.84375\n",
      "At: 1729 [==========>] Loss 0.15715030595873666  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.1347515191651874  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.10237324903174475  - accuracy: 0.84375\n",
      "At: 1732 [==========>] Loss 0.09018795838066325  - accuracy: 0.84375\n",
      "At: 1733 [==========>] Loss 0.1854490479103224  - accuracy: 0.65625\n",
      "At: 1734 [==========>] Loss 0.11000503827674007  - accuracy: 0.8125\n",
      "At: 1735 [==========>] Loss 0.14847426993296722  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.11726407718544082  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.15009985883395968  - accuracy: 0.75\n",
      "At: 1738 [==========>] Loss 0.11627212479533033  - accuracy: 0.875\n",
      "At: 1739 [==========>] Loss 0.10820546373957762  - accuracy: 0.875\n",
      "At: 1740 [==========>] Loss 0.15050189633524916  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.1295131652574853  - accuracy: 0.8125\n",
      "At: 1742 [==========>] Loss 0.058111549464397984  - accuracy: 0.90625\n",
      "At: 1743 [==========>] Loss 0.15071268615716563  - accuracy: 0.75\n",
      "At: 1744 [==========>] Loss 0.09676418690550023  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.12133068027624053  - accuracy: 0.84375\n",
      "At: 1746 [==========>] Loss 0.1929785849567445  - accuracy: 0.71875\n",
      "At: 1747 [==========>] Loss 0.11507985020057958  - accuracy: 0.875\n",
      "At: 1748 [==========>] Loss 0.13136851100428693  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.11430408343370234  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.11530773004111214  - accuracy: 0.84375\n",
      "At: 1751 [==========>] Loss 0.16820524752257618  - accuracy: 0.75\n",
      "At: 1752 [==========>] Loss 0.1060683010814722  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.08948074139500242  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.11597994592677627  - accuracy: 0.875\n",
      "At: 1755 [==========>] Loss 0.09533397643613747  - accuracy: 0.875\n",
      "At: 1756 [==========>] Loss 0.16230962011755123  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.15962914425244593  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.07199049194692886  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.10211015664092676  - accuracy: 0.875\n",
      "At: 1760 [==========>] Loss 0.11680739470410652  - accuracy: 0.84375\n",
      "At: 1761 [==========>] Loss 0.11026214128659412  - accuracy: 0.84375\n",
      "At: 1762 [==========>] Loss 0.16574314892577743  - accuracy: 0.75\n",
      "At: 1763 [==========>] Loss 0.1042527894599389  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.1351194432387384  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.11369321548997748  - accuracy: 0.84375\n",
      "At: 1766 [==========>] Loss 0.08169785323955343  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.09673249962628465  - accuracy: 0.875\n",
      "At: 1768 [==========>] Loss 0.11893677600444377  - accuracy: 0.8125\n",
      "At: 1769 [==========>] Loss 0.08410221466586315  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.06852467883257037  - accuracy: 0.90625\n",
      "At: 1771 [==========>] Loss 0.1857489548387795  - accuracy: 0.71875\n",
      "At: 1772 [==========>] Loss 0.14429191215886547  - accuracy: 0.78125\n",
      "At: 1773 [==========>] Loss 0.08901080833350708  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.14463602410335938  - accuracy: 0.8125\n",
      "At: 1775 [==========>] Loss 0.11729869046866018  - accuracy: 0.8125\n",
      "At: 1776 [==========>] Loss 0.12072398159152536  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11216618290345608  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.12400962778148175  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.09421176158492686  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.11422772062251256  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.1870597032065715  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.11381199113225658  - accuracy: 0.8125\n",
      "At: 1783 [==========>] Loss 0.14599105096662723  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.08832043560708099  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.1004701383340055  - accuracy: 0.84375\n",
      "At: 1786 [==========>] Loss 0.1563311410214069  - accuracy: 0.78125\n",
      "At: 1787 [==========>] Loss 0.11649578686507706  - accuracy: 0.875\n",
      "At: 1788 [==========>] Loss 0.10177312471929754  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.11809938064736424  - accuracy: 0.8125\n",
      "At: 1790 [==========>] Loss 0.14318980320067504  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.0862503554908709  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.12737417190668954  - accuracy: 0.8125\n",
      "At: 1793 [==========>] Loss 0.0909779731875861  - accuracy: 0.90625\n",
      "At: 1794 [==========>] Loss 0.18285257953872958  - accuracy: 0.78125\n",
      "At: 1795 [==========>] Loss 0.0776471580440582  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.13932832967783432  - accuracy: 0.78125\n",
      "At: 1797 [==========>] Loss 0.13542682218075258  - accuracy: 0.8125\n",
      "At: 1798 [==========>] Loss 0.13352946458972334  - accuracy: 0.8125\n",
      "At: 1799 [==========>] Loss 0.08260388431967006  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.11162755898406954  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.16933654412516963  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.12482738676256873  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.19075765770748015  - accuracy: 0.75\n",
      "At: 1804 [==========>] Loss 0.13573433219134506  - accuracy: 0.84375\n",
      "At: 1805 [==========>] Loss 0.03736114316085894  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.17411072378058867  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.1601138546892556  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.15243775174951524  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.11413312910192334  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.16116589189426406  - accuracy: 0.75\n",
      "At: 1811 [==========>] Loss 0.13132240691727864  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.12185333552837532  - accuracy: 0.84375\n",
      "At: 1813 [==========>] Loss 0.1301721665700843  - accuracy: 0.875\n",
      "At: 1814 [==========>] Loss 0.12544124269809676  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.1876009276748286  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.050669780510592344  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.16620895357380142  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.1165832655351746  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.18125312709188782  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.1115929372096493  - accuracy: 0.8125\n",
      "At: 1821 [==========>] Loss 0.0955128633848085  - accuracy: 0.90625\n",
      "At: 1822 [==========>] Loss 0.14482286219238052  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.1985404851397461  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.1617435283928993  - accuracy: 0.8125\n",
      "At: 1825 [==========>] Loss 0.11487717976826932  - accuracy: 0.90625\n",
      "At: 1826 [==========>] Loss 0.06936720776786343  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.10756676835629475  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.14215364385407186  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.17627614638192082  - accuracy: 0.78125\n",
      "At: 1830 [==========>] Loss 0.16459743737564414  - accuracy: 0.75\n",
      "At: 1831 [==========>] Loss 0.12230674625455192  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.12674385704471547  - accuracy: 0.75\n",
      "At: 1833 [==========>] Loss 0.12213700911242283  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.07781417074881175  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.15774664500815416  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.1311534390942542  - accuracy: 0.8125\n",
      "At: 1837 [==========>] Loss 0.049119096309658175  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.08032004473136004  - accuracy: 0.9375\n",
      "At: 1839 [==========>] Loss 0.08599415732952384  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.10569508932456156  - accuracy: 0.875\n",
      "At: 1841 [==========>] Loss 0.10212782204021863  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.11415130898778485  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.1248288725487863  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.1243588146736676  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.17987001807276498  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.14237182189643988  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.08402746943307504  - accuracy: 0.90625\n",
      "At: 1848 [==========>] Loss 0.045949384583050305  - accuracy: 0.96875\n",
      "At: 1849 [==========>] Loss 0.17125622416888434  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.02995142005845891  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.14194666829069333  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.0972727115596558  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.11910474980186515  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.1291337505783283  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.18803618249903495  - accuracy: 0.71875\n",
      "At: 1856 [==========>] Loss 0.12557228146957988  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.15584522239384474  - accuracy: 0.71875\n",
      "At: 1858 [==========>] Loss 0.13071121186179313  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.15370293378293315  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.14469094109296202  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.08539690851953893  - accuracy: 0.96875\n",
      "At: 1862 [==========>] Loss 0.16199737577556367  - accuracy: 0.75\n",
      "At: 1863 [==========>] Loss 0.16563058465153357  - accuracy: 0.75\n",
      "At: 1864 [==========>] Loss 0.16236621486504185  - accuracy: 0.75\n",
      "At: 1865 [==========>] Loss 0.10365874954305375  - accuracy: 0.84375\n",
      "At: 1866 [==========>] Loss 0.22136659052942786  - accuracy: 0.625\n",
      "At: 1867 [==========>] Loss 0.11788980485551713  - accuracy: 0.78125\n",
      "At: 1868 [==========>] Loss 0.1810719770038382  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.17991613436442355  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.12838498675229879  - accuracy: 0.8125\n",
      "At: 1871 [==========>] Loss 0.13191494809682613  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.1317612841913609  - accuracy: 0.8125\n",
      "At: 1873 [==========>] Loss 0.07170258820302497  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.16556488457203927  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.1187393046910916  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.2432329895841323  - accuracy: 0.65625\n",
      "At: 1877 [==========>] Loss 0.07947040493640817  - accuracy: 0.90625\n",
      "At: 1878 [==========>] Loss 0.1256667296228591  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.11496681820409968  - accuracy: 0.875\n",
      "At: 1880 [==========>] Loss 0.08971083036948403  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.08235760536375779  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.11333943645492436  - accuracy: 0.84375\n",
      "At: 1883 [==========>] Loss 0.13497452074776248  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.10038880176176929  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.09341625056542818  - accuracy: 0.84375\n",
      "At: 1886 [==========>] Loss 0.13402376889721712  - accuracy: 0.84375\n",
      "At: 1887 [==========>] Loss 0.09285355451001577  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.1463091789857312  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.09273668359743167  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.16680039076327663  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.07171810508772167  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.08365002602174816  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.08477099556156294  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.08759690485275291  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.07802078307988952  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.12408187182744379  - accuracy: 0.875\n",
      "At: 1897 [==========>] Loss 0.06271233186366533  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.11864931430235967  - accuracy: 0.875\n",
      "At: 1899 [==========>] Loss 0.08952269939941926  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.1505398481514289  - accuracy: 0.8125\n",
      "At: 1901 [==========>] Loss 0.11274706896662236  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.14707760859664534  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.11883647305503134  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.049850471616089076  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.15154646892719253  - accuracy: 0.78125\n",
      "At: 1906 [==========>] Loss 0.11818835920702345  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.09374871665466131  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.09425790016696242  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.10162693534787083  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.06128729974708802  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.1265133568951315  - accuracy: 0.84375\n",
      "At: 1912 [==========>] Loss 0.12011363313752413  - accuracy: 0.90625\n",
      "At: 1913 [==========>] Loss 0.15769245659916736  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.07912542616522092  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.10845336722566355  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.14663143250707888  - accuracy: 0.78125\n",
      "At: 1917 [==========>] Loss 0.1580734897202935  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.16370083155575776  - accuracy: 0.6875\n",
      "At: 1919 [==========>] Loss 0.10093102194712947  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.10442060253528962  - accuracy: 0.78125\n",
      "At: 1921 [==========>] Loss 0.15505785708609424  - accuracy: 0.84375\n",
      "At: 1922 [==========>] Loss 0.13885183721108094  - accuracy: 0.8125\n",
      "At: 1923 [==========>] Loss 0.18037787240349498  - accuracy: 0.6875\n",
      "At: 1924 [==========>] Loss 0.11135612818185206  - accuracy: 0.90625\n",
      "At: 1925 [==========>] Loss 0.18129928528327327  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.09734566773842913  - accuracy: 0.875\n",
      "At: 1927 [==========>] Loss 0.10405109773329463  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12792021882469454  - accuracy: 0.8125\n",
      "At: 1929 [==========>] Loss 0.17216825499561506  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.15225408572066285  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.12778034990499781  - accuracy: 0.78125\n",
      "At: 1932 [==========>] Loss 0.16010079530094926  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.10574596919079621  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.15013975871704877  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.1387146135635639  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.09240685662164655  - accuracy: 0.90625\n",
      "At: 1937 [==========>] Loss 0.15000368440594491  - accuracy: 0.8125\n",
      "At: 1938 [==========>] Loss 0.16515662905933007  - accuracy: 0.78125\n",
      "At: 1939 [==========>] Loss 0.07883940403468184  - accuracy: 0.875\n",
      "At: 1940 [==========>] Loss 0.13969284585471003  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.12343322103868574  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.15625216253704854  - accuracy: 0.8125\n",
      "At: 1943 [==========>] Loss 0.12094424475989277  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.13969051216947356  - accuracy: 0.71875\n",
      "At: 1945 [==========>] Loss 0.17226287726151518  - accuracy: 0.78125\n",
      "At: 1946 [==========>] Loss 0.0823130798377443  - accuracy: 0.875\n",
      "At: 1947 [==========>] Loss 0.10511549769550765  - accuracy: 0.8125\n",
      "At: 1948 [==========>] Loss 0.11640531639329121  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.06408380104945624  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.12909127139306104  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.1483362058831839  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.09689478098098751  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.08656343263024968  - accuracy: 0.9375\n",
      "At: 1954 [==========>] Loss 0.19302787316913061  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.07494888113286487  - accuracy: 0.875\n",
      "At: 1956 [==========>] Loss 0.11019609397015298  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.07346692143348502  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.09632178078969622  - accuracy: 0.90625\n",
      "At: 1959 [==========>] Loss 0.12027652904586053  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.05793642617613759  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.16180626543954385  - accuracy: 0.8125\n",
      "At: 1962 [==========>] Loss 0.17328900008637893  - accuracy: 0.71875\n",
      "At: 1963 [==========>] Loss 0.07062561736062366  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.15679847314121692  - accuracy: 0.75\n",
      "At: 1965 [==========>] Loss 0.1587658866526509  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.11763360230602372  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.1348853218544474  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.16494694481160194  - accuracy: 0.78125\n",
      "At: 1969 [==========>] Loss 0.16547020369222748  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.10902673576688181  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.18838062912779852  - accuracy: 0.625\n",
      "At: 1972 [==========>] Loss 0.10243663502496088  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.12937742177802936  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.11730836793715002  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.16576927443714468  - accuracy: 0.71875\n",
      "At: 1976 [==========>] Loss 0.08669351873775721  - accuracy: 0.875\n",
      "At: 1977 [==========>] Loss 0.1044816902586256  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.18214741586816874  - accuracy: 0.75\n",
      "At: 1979 [==========>] Loss 0.11960934548074928  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.12874288671970438  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.15968274412740596  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.07936913316970523  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.15514588491504228  - accuracy: 0.8125\n",
      "At: 1984 [==========>] Loss 0.09468738864985757  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.11669142876606482  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.19024573248340065  - accuracy: 0.71875\n",
      "At: 1987 [==========>] Loss 0.09297273303474982  - accuracy: 0.875\n",
      "At: 1988 [==========>] Loss 0.0946157585383682  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.11403931763010697  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.09722382641882144  - accuracy: 0.90625\n",
      "At: 1991 [==========>] Loss 0.12917354139888992  - accuracy: 0.875\n",
      "At: 1992 [==========>] Loss 0.14239705230694788  - accuracy: 0.78125\n",
      "At: 1993 [==========>] Loss 0.16518594052988686  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.10249302017442342  - accuracy: 0.84375\n",
      "At: 1995 [==========>] Loss 0.1915387976222218  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.10474273421942643  - accuracy: 0.8125\n",
      "At: 1997 [==========>] Loss 0.19370762136840636  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.15158296780735497  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.08765630443843174  - accuracy: 0.875\n",
      "At: 2000 [==========>] Loss 0.15365208015134782  - accuracy: 0.71875\n",
      "At: 2001 [==========>] Loss 0.1103142926332742  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.06452638025041495  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.12274730012155755  - accuracy: 0.84375\n",
      "At: 2004 [==========>] Loss 0.14581164715807693  - accuracy: 0.84375\n",
      "At: 2005 [==========>] Loss 0.14071776642422418  - accuracy: 0.78125\n",
      "At: 2006 [==========>] Loss 0.1432349398918179  - accuracy: 0.84375\n",
      "At: 2007 [==========>] Loss 0.08489613167323323  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.14923037741080222  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.1355093681662793  - accuracy: 0.84375\n",
      "At: 2010 [==========>] Loss 0.12385834458508635  - accuracy: 0.78125\n",
      "At: 2011 [==========>] Loss 0.12294640135097006  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.1144030802774357  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.12054215740618626  - accuracy: 0.78125\n",
      "At: 2014 [==========>] Loss 0.21986251805868318  - accuracy: 0.6875\n",
      "At: 2015 [==========>] Loss 0.07793705881384128  - accuracy: 0.875\n",
      "At: 2016 [==========>] Loss 0.12960293681520102  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.0836752383696413  - accuracy: 0.90625\n",
      "At: 2018 [==========>] Loss 0.11309022766979271  - accuracy: 0.84375\n",
      "At: 2019 [==========>] Loss 0.12302930432063536  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.08395707497605481  - accuracy: 0.875\n",
      "At: 2021 [==========>] Loss 0.11997134995084979  - accuracy: 0.84375\n",
      "At: 2022 [==========>] Loss 0.1197113630404286  - accuracy: 0.84375\n",
      "At: 2023 [==========>] Loss 0.09746259029655369  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.10821285993897858  - accuracy: 0.8125\n",
      "At: 2025 [==========>] Loss 0.18864838512355458  - accuracy: 0.75\n",
      "At: 2026 [==========>] Loss 0.10553850790929324  - accuracy: 0.84375\n",
      "At: 2027 [==========>] Loss 0.15362229110910153  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.11867389528450409  - accuracy: 0.8125\n",
      "At: 2029 [==========>] Loss 0.15043246882680833  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.15352076453222913  - accuracy: 0.78125\n",
      "At: 2031 [==========>] Loss 0.16605490155144417  - accuracy: 0.75\n",
      "At: 2032 [==========>] Loss 0.15899665217195882  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.16205023464649415  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.226358017582945  - accuracy: 0.65625\n",
      "At: 2035 [==========>] Loss 0.09639152253271067  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.08522257130172156  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.14268601328474895  - accuracy: 0.75\n",
      "At: 2038 [==========>] Loss 0.08999598979428  - accuracy: 0.875\n",
      "At: 2039 [==========>] Loss 0.0961326298377495  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.09917274376481502  - accuracy: 0.875\n",
      "At: 2041 [==========>] Loss 0.05553651631925912  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.09281436170014057  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.11569661692143157  - accuracy: 0.8125\n",
      "At: 2044 [==========>] Loss 0.08389234460832629  - accuracy: 0.90625\n",
      "At: 2045 [==========>] Loss 0.20657477214011022  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.07299376948910291  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.0890213537766228  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.0900860111235535  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.14029285124457852  - accuracy: 0.84375\n",
      "At: 2050 [==========>] Loss 0.1629524305742114  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.151278317966339  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.0843140349519557  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.13646027359008267  - accuracy: 0.8125\n",
      "At: 2054 [==========>] Loss 0.128659877487225  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.0763581375873795  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.11515394032824548  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.1469122839666962  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.1542275472829725  - accuracy: 0.78125\n",
      "At: 2059 [==========>] Loss 0.20397684241126904  - accuracy: 0.65625\n",
      "At: 2060 [==========>] Loss 0.12921221029353921  - accuracy: 0.875\n",
      "At: 2061 [==========>] Loss 0.15182999778652018  - accuracy: 0.78125\n",
      "At: 2062 [==========>] Loss 0.16385897691197027  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.10421859967528949  - accuracy: 0.90625\n",
      "At: 2064 [==========>] Loss 0.1864913278616215  - accuracy: 0.71875\n",
      "At: 2065 [==========>] Loss 0.03962762678386319  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.14719709664637312  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.09617168956679256  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.11268359464722268  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.11081859651563417  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.15673155404556538  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.1272137928420194  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.06655562788075998  - accuracy: 0.875\n",
      "At: 2073 [==========>] Loss 0.10054554860601375  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.10261274169357378  - accuracy: 0.875\n",
      "At: 2075 [==========>] Loss 0.11251022226182035  - accuracy: 0.78125\n",
      "At: 2076 [==========>] Loss 0.12342136388181912  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.15214477709842922  - accuracy: 0.75\n",
      "At: 2078 [==========>] Loss 0.10892204139743705  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07289700082412796  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.11232146854171499  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.15182448294674872  - accuracy: 0.75\n",
      "At: 2082 [==========>] Loss 0.12248715091045256  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.1861606078423372  - accuracy: 0.65625\n",
      "At: 2084 [==========>] Loss 0.11447510249833524  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.09187167855237396  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.09083700896396733  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.1572179538305865  - accuracy: 0.8125\n",
      "At: 2088 [==========>] Loss 0.10914304472763975  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.15656247549811103  - accuracy: 0.78125\n",
      "At: 2090 [==========>] Loss 0.08829970546891878  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.13923501315502151  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.07897044915890913  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.15242467242834717  - accuracy: 0.78125\n",
      "At: 2094 [==========>] Loss 0.10883818388508026  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.110635117678477  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.18123084690466054  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.1259737237695867  - accuracy: 0.84375\n",
      "At: 2098 [==========>] Loss 0.1340974185596444  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.10581352355949844  - accuracy: 0.78125\n",
      "At: 2100 [==========>] Loss 0.06454399095017345  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.15814131280530594  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.08824310922630427  - accuracy: 0.9375\n",
      "At: 2103 [==========>] Loss 0.13992530787163487  - accuracy: 0.84375\n",
      "At: 2104 [==========>] Loss 0.10503045842571829  - accuracy: 0.90625\n",
      "At: 2105 [==========>] Loss 0.16441027857984708  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.1540046829627173  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.105746436600311  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.1373539250675185  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.11918696435246452  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.07371713279912619  - accuracy: 0.875\n",
      "At: 2111 [==========>] Loss 0.12010566523252891  - accuracy: 0.875\n",
      "At: 2112 [==========>] Loss 0.12234058897525535  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.0974333434649727  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.1466744390350968  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.1178118175619426  - accuracy: 0.8125\n",
      "At: 2116 [==========>] Loss 0.10846856508939928  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.12859965806145024  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.12319706121591227  - accuracy: 0.8125\n",
      "At: 2119 [==========>] Loss 0.09533755409931316  - accuracy: 0.84375\n",
      "At: 2120 [==========>] Loss 0.1627853906652144  - accuracy: 0.8125\n",
      "At: 2121 [==========>] Loss 0.13910254074958434  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.1392212314926507  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.18930890965407346  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.12635709322730354  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.0936173523800469  - accuracy: 0.84375\n",
      "At: 2126 [==========>] Loss 0.06355709867264048  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.11294638811519336  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.1044979286519028  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.15346107362983924  - accuracy: 0.78125\n",
      "At: 2130 [==========>] Loss 0.06881309857186999  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.09106050845578631  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.19873964003988842  - accuracy: 0.78125\n",
      "At: 2133 [==========>] Loss 0.13586619229592084  - accuracy: 0.84375\n",
      "At: 2134 [==========>] Loss 0.1077542750722524  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.09514956616140266  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.12675027570704506  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.13296145670158083  - accuracy: 0.84375\n",
      "At: 2138 [==========>] Loss 0.11290308001629393  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.1616059737771211  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.08572169818639463  - accuracy: 0.90625\n",
      "At: 2141 [==========>] Loss 0.11445682583805586  - accuracy: 0.84375\n",
      "At: 2142 [==========>] Loss 0.12295448032528583  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.10484775938418615  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.06833811205697209  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.0881205889018391  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.1736158804255185  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.1075765834483659  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.20740082379833427  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.11733579636718665  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.09382735849804294  - accuracy: 0.84375\n",
      "At: 2151 [==========>] Loss 0.1294789819361219  - accuracy: 0.8125\n",
      "At: 2152 [==========>] Loss 0.2344545114330009  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.18998888691905289  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.14302957671470581  - accuracy: 0.75\n",
      "At: 2155 [==========>] Loss 0.14018929457806023  - accuracy: 0.75\n",
      "At: 2156 [==========>] Loss 0.10594377566941504  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.10435097326254855  - accuracy: 0.875\n",
      "At: 2158 [==========>] Loss 0.16562815655900476  - accuracy: 0.71875\n",
      "At: 2159 [==========>] Loss 0.08872170915736961  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.15453019977276752  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.10110049094497278  - accuracy: 0.9375\n",
      "At: 2162 [==========>] Loss 0.10780402064260348  - accuracy: 0.84375\n",
      "At: 2163 [==========>] Loss 0.13195403841295186  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.17236341284311718  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.10966352267641039  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.10942725799707545  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.09031805680560981  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.086198600241972  - accuracy: 0.875\n",
      "At: 2169 [==========>] Loss 0.09310693950643731  - accuracy: 0.875\n",
      "At: 2170 [==========>] Loss 0.11103265954540402  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.1324745222397462  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.10144665528223727  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.12196281175182165  - accuracy: 0.75\n",
      "At: 2174 [==========>] Loss 0.122848900717733  - accuracy: 0.875\n",
      "At: 2175 [==========>] Loss 0.12910000004683247  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.16292592018580687  - accuracy: 0.78125\n",
      "At: 2177 [==========>] Loss 0.16013049920665867  - accuracy: 0.78125\n",
      "At: 2178 [==========>] Loss 0.11000164094490217  - accuracy: 0.8125\n",
      "At: 2179 [==========>] Loss 0.13564007568584666  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.11115871937985576  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.1616291352323399  - accuracy: 0.75\n",
      "At: 2182 [==========>] Loss 0.10660313218819763  - accuracy: 0.875\n",
      "At: 2183 [==========>] Loss 0.2220637200969724  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.068819008887533  - accuracy: 0.96875\n",
      "At: 2185 [==========>] Loss 0.10523263369671197  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.14901583006916047  - accuracy: 0.71875\n",
      "At: 2187 [==========>] Loss 0.1683344342653204  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.07235702341508546  - accuracy: 0.875\n",
      "At: 2189 [==========>] Loss 0.09870764110668949  - accuracy: 0.84375\n",
      "At: 2190 [==========>] Loss 0.1334490650505339  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.1029490276311174  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.11697816267173222  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.17309878837905845  - accuracy: 0.75\n",
      "At: 2194 [==========>] Loss 0.10561276591868499  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.1804404358597408  - accuracy: 0.65625\n",
      "At: 2196 [==========>] Loss 0.13570854865261786  - accuracy: 0.75\n",
      "At: 2197 [==========>] Loss 0.08317102198012714  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.07574040350176621  - accuracy: 0.9375\n",
      "At: 2199 [==========>] Loss 0.06639911631082354  - accuracy: 0.90625\n",
      "At: 2200 [==========>] Loss 0.05476826729610336  - accuracy: 0.90625\n",
      "At: 2201 [==========>] Loss 0.1044698918446408  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.08100904103934059  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.09850467067798699  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.11945487109692636  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.11791280157280987  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.10595672503973753  - accuracy: 0.78125\n",
      "At: 2207 [==========>] Loss 0.08465038672664343  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.17815731322157347  - accuracy: 0.71875\n",
      "At: 2209 [==========>] Loss 0.17956427048248286  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.10761243970470982  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.1612404147333194  - accuracy: 0.78125\n",
      "At: 2212 [==========>] Loss 0.11514374503508393  - accuracy: 0.8125\n",
      "At: 2213 [==========>] Loss 0.11802475068020546  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.13257800796061045  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.1327518597649009  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.1329735927513708  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.1245511918953744  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.1605335834544817  - accuracy: 0.78125\n",
      "At: 2219 [==========>] Loss 0.07940174246647766  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.1182832943194464  - accuracy: 0.78125\n",
      "At: 2221 [==========>] Loss 0.17934510832599482  - accuracy: 0.75\n",
      "At: 2222 [==========>] Loss 0.12560999300955905  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.15869254341175446  - accuracy: 0.71875\n",
      "At: 2224 [==========>] Loss 0.14461595514179376  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.14696595292064857  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.1329738794242541  - accuracy: 0.75\n",
      "At: 2227 [==========>] Loss 0.21076373953258565  - accuracy: 0.65625\n",
      "At: 2228 [==========>] Loss 0.0584481316770374  - accuracy: 0.9375\n",
      "At: 2229 [==========>] Loss 0.1484194251373037  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.12854096665582038  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.1524906377301625  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.17055980476053917  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.17351460966503562  - accuracy: 0.71875\n",
      "At: 2234 [==========>] Loss 0.15217746781661723  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.10762506194550298  - accuracy: 0.84375\n",
      "At: 2236 [==========>] Loss 0.07300893421866823  - accuracy: 0.9375\n",
      "At: 2237 [==========>] Loss 0.1360222223597227  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.17321320526587353  - accuracy: 0.75\n",
      "At: 2239 [==========>] Loss 0.1806245944727906  - accuracy: 0.78125\n",
      "At: 2240 [==========>] Loss 0.15722394225571135  - accuracy: 0.75\n",
      "At: 2241 [==========>] Loss 0.14981267537333542  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.1707082212641413  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.07597967147369386  - accuracy: 0.9375\n",
      "At: 2244 [==========>] Loss 0.09548231752584108  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.07933376775897763  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.1335077864590992  - accuracy: 0.84375\n",
      "At: 2247 [==========>] Loss 0.1230868482795747  - accuracy: 0.78125\n",
      "At: 2248 [==========>] Loss 0.17966718349712574  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.116943129846115  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.06529996896142995  - accuracy: 0.9375\n",
      "At: 2251 [==========>] Loss 0.09122609017353037  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.10950739103013049  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.12294447692105673  - accuracy: 0.78125\n",
      "At: 2254 [==========>] Loss 0.15984846998338464  - accuracy: 0.71875\n",
      "At: 2255 [==========>] Loss 0.1498223875165159  - accuracy: 0.78125\n",
      "At: 2256 [==========>] Loss 0.15304629224875516  - accuracy: 0.84375\n",
      "At: 2257 [==========>] Loss 0.11205864286978334  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.1549896053999134  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.1288042086425895  - accuracy: 0.875\n",
      "At: 2260 [==========>] Loss 0.20019888455220017  - accuracy: 0.71875\n",
      "At: 2261 [==========>] Loss 0.12312060796974142  - accuracy: 0.78125\n",
      "At: 2262 [==========>] Loss 0.15153578251906002  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.11500011028382097  - accuracy: 0.84375\n",
      "At: 2264 [==========>] Loss 0.09174286215621874  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.12332361614380823  - accuracy: 0.8125\n",
      "At: 2266 [==========>] Loss 0.11803999387766093  - accuracy: 0.8125\n",
      "At: 2267 [==========>] Loss 0.07698429794930708  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.08864614332715223  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.03509094261462771  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.11256150271597484  - accuracy: 0.84375\n",
      "At: 2271 [==========>] Loss 0.16898601646800265  - accuracy: 0.71875\n",
      "At: 2272 [==========>] Loss 0.0711528317431497  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.11920868701927925  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.08742034038165336  - accuracy: 0.875\n",
      "At: 2275 [==========>] Loss 0.10983961033609843  - accuracy: 0.84375\n",
      "At: 2276 [==========>] Loss 0.08916325308127712  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.13587208554413513  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.1175071483609388  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.1437630363877972  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.15050754738070216  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.1113615089230273  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.0893877735012934  - accuracy: 0.875\n",
      "At: 2283 [==========>] Loss 0.1855867921648115  - accuracy: 0.71875\n",
      "At: 2284 [==========>] Loss 0.11617052894812036  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.14838042871052168  - accuracy: 0.6875\n",
      "At: 2286 [==========>] Loss 0.1510371684413433  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.1368734668063248  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.08580589666342806  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.16413195722415513  - accuracy: 0.71875\n",
      "At: 2290 [==========>] Loss 0.058213303514997955  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.14545463137746759  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.07588001039564632  - accuracy: 0.875\n",
      "At: 2293 [==========>] Loss 0.06227598829569399  - accuracy: 0.9375\n",
      "At: 2294 [==========>] Loss 0.05947226693916936  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.13786449232372078  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.18097922097372626  - accuracy: 0.6875\n",
      "At: 2297 [==========>] Loss 0.07826386582988759  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.10753426969897534  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.11807664598508888  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.11848775021940741  - accuracy: 0.8125\n",
      "At: 2301 [==========>] Loss 0.19675223064713038  - accuracy: 0.6875\n",
      "At: 2302 [==========>] Loss 0.14852211743505628  - accuracy: 0.90625\n",
      "At: 2303 [==========>] Loss 0.07646657228635743  - accuracy: 0.875\n",
      "At: 2304 [==========>] Loss 0.09788589469281112  - accuracy: 0.90625\n",
      "At: 2305 [==========>] Loss 0.11053715864562809  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.13894875887919614  - accuracy: 0.75\n",
      "At: 2307 [==========>] Loss 0.1562841755180178  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.20585232028633604  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.13324905414876803  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.1366662175428199  - accuracy: 0.8125\n",
      "At: 2311 [==========>] Loss 0.1705216201687903  - accuracy: 0.75\n",
      "At: 2312 [==========>] Loss 0.1059027951445184  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.09102798339557744  - accuracy: 0.84375\n",
      "At: 2314 [==========>] Loss 0.13887813540162197  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.14626381992275578  - accuracy: 0.75\n",
      "At: 2316 [==========>] Loss 0.15054619440914924  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.1671773789025341  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.1877401216301961  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.12085113141751233  - accuracy: 0.875\n",
      "At: 2320 [==========>] Loss 0.09929427920733272  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.12606900218198755  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.1872606222054921  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.14491023842601605  - accuracy: 0.75\n",
      "At: 2324 [==========>] Loss 0.14656615867048167  - accuracy: 0.8125\n",
      "At: 2325 [==========>] Loss 0.11610473814004528  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.07153274152566415  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.08840220169591495  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.10609258218116249  - accuracy: 0.90625\n",
      "At: 2329 [==========>] Loss 0.1167485754560234  - accuracy: 0.8125\n",
      "At: 2330 [==========>] Loss 0.16797882174466738  - accuracy: 0.6875\n",
      "At: 2331 [==========>] Loss 0.10053402809671066  - accuracy: 0.84375\n",
      "At: 2332 [==========>] Loss 0.10830663415771516  - accuracy: 0.875\n",
      "At: 2333 [==========>] Loss 0.10579072178039066  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.17809738064945826  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.12735481852467645  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.09890926210621878  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.12840913543492533  - accuracy: 0.8125\n",
      "At: 2338 [==========>] Loss 0.09276711264735674  - accuracy: 0.875\n",
      "At: 2339 [==========>] Loss 0.08316389590620868  - accuracy: 0.90625\n",
      "At: 2340 [==========>] Loss 0.16383996204801307  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.13268472468801898  - accuracy: 0.75\n",
      "At: 2342 [==========>] Loss 0.16717481197028708  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.07958661226333723  - accuracy: 0.90625\n",
      "At: 2344 [==========>] Loss 0.19183527350559249  - accuracy: 0.75\n",
      "At: 2345 [==========>] Loss 0.13949559349215873  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.08184035982209074  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.128922537676777  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.05815746883090206  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.09772240045325196  - accuracy: 0.90625\n",
      "At: 2350 [==========>] Loss 0.13269863719081676  - accuracy: 0.90625\n",
      "At: 2351 [==========>] Loss 0.08680870212693002  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.13471222826029783  - accuracy: 0.8125\n",
      "At: 2353 [==========>] Loss 0.09817408297463077  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.1505289831342433  - accuracy: 0.75\n",
      "At: 2355 [==========>] Loss 0.04802835259288085  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.12448219673823038  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.1450478836705191  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.14742685767173624  - accuracy: 0.8125\n",
      "At: 2359 [==========>] Loss 0.1732910229461277  - accuracy: 0.78125\n",
      "At: 2360 [==========>] Loss 0.15375363021913324  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.17061426077242034  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.08532292466859917  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.1865058421897906  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.07103366875999664  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.07392517595066381  - accuracy: 0.9375\n",
      "At: 2366 [==========>] Loss 0.16801133444855626  - accuracy: 0.6875\n",
      "At: 2367 [==========>] Loss 0.12147729260828882  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.128112922188091  - accuracy: 0.8125\n",
      "At: 2369 [==========>] Loss 0.08158925479250081  - accuracy: 0.875\n",
      "At: 2370 [==========>] Loss 0.07523724609009871  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.1135656249730539  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.1463919888341021  - accuracy: 0.75\n",
      "At: 2373 [==========>] Loss 0.11986166313226254  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.09015490116091161  - accuracy: 0.90625\n",
      "At: 2375 [==========>] Loss 0.056143875662829426  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.09857925815887318  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.09344145005931839  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.10678188638954936  - accuracy: 0.8125\n",
      "At: 2379 [==========>] Loss 0.15780479950536735  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.09763931021865052  - accuracy: 0.875\n",
      "At: 2381 [==========>] Loss 0.08423067833836188  - accuracy: 0.9375\n",
      "At: 2382 [==========>] Loss 0.09554930325329364  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1332848267286353  - accuracy: 0.84375\n",
      "At: 2384 [==========>] Loss 0.10427046438874739  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.1336113727954935  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.09856387013294404  - accuracy: 0.90625\n",
      "At: 2387 [==========>] Loss 0.07032185359979735  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.11928866712227681  - accuracy: 0.8125\n",
      "At: 2389 [==========>] Loss 0.04173584476065906  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.06334925965745901  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.1793567879795549  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.1534562903155182  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.11075520943016977  - accuracy: 0.84375\n",
      "At: 2394 [==========>] Loss 0.07603402641077758  - accuracy: 0.875\n",
      "At: 2395 [==========>] Loss 0.08008196235745307  - accuracy: 0.9375\n",
      "At: 2396 [==========>] Loss 0.08696674603692678  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.11031908234658033  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.13470973167052652  - accuracy: 0.71875\n",
      "At: 2399 [==========>] Loss 0.1578929886838204  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.1104933258281568  - accuracy: 0.84375\n",
      "At: 2401 [==========>] Loss 0.08638050261232333  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.10353165891634053  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.19836450006696804  - accuracy: 0.6875\n",
      "At: 2404 [==========>] Loss 0.1554603343029709  - accuracy: 0.875\n",
      "At: 2405 [==========>] Loss 0.07626040117712664  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.130853524382911  - accuracy: 0.875\n",
      "At: 2407 [==========>] Loss 0.11887236205225571  - accuracy: 0.8125\n",
      "At: 2408 [==========>] Loss 0.11156540766818585  - accuracy: 0.84375\n",
      "At: 2409 [==========>] Loss 0.14856646638713128  - accuracy: 0.84375\n",
      "At: 2410 [==========>] Loss 0.1532394301624066  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.09361814947306271  - accuracy: 0.9375\n",
      "At: 2412 [==========>] Loss 0.06676136998990967  - accuracy: 0.9375\n",
      "At: 2413 [==========>] Loss 0.10991568888090363  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.06854568426942637  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.10279727047266589  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.08445334767036854  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.16917323137596998  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.11391522583821737  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.12046260213002684  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.12737838401751625  - accuracy: 0.78125\n",
      "At: 2421 [==========>] Loss 0.08421832515096199  - accuracy: 0.90625\n",
      "At: 2422 [==========>] Loss 0.13303552178260045  - accuracy: 0.8125\n",
      "At: 2423 [==========>] Loss 0.14209531551548676  - accuracy: 0.78125\n",
      "At: 2424 [==========>] Loss 0.08993838053759745  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.08375659854274703  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.2423099187517857  - accuracy: 0.625\n",
      "At: 2427 [==========>] Loss 0.13579445509664353  - accuracy: 0.78125\n",
      "At: 2428 [==========>] Loss 0.09685814975030624  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.11103302747451232  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.14382340092303297  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.15903876525720947  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.07237431849770139  - accuracy: 0.90625\n",
      "At: 2433 [==========>] Loss 0.06533330438333715  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.04891018176892947  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.16896826132531123  - accuracy: 0.78125\n",
      "At: 2436 [==========>] Loss 0.09782064812961021  - accuracy: 0.875\n",
      "At: 2437 [==========>] Loss 0.1884109037594392  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.11347138775496643  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.14384385205284622  - accuracy: 0.6875\n",
      "At: 2440 [==========>] Loss 0.10465732431150374  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.12804080957185449  - accuracy: 0.78125\n",
      "At: 2442 [==========>] Loss 0.15047026700064375  - accuracy: 0.78125\n",
      "At: 2443 [==========>] Loss 0.11013906285893044  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.09598179783197248  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.06950319672942046  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.1883099604953607  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.15767901280517535  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.12232968540380373  - accuracy: 0.8125\n",
      "At: 2449 [==========>] Loss 0.10477002779645907  - accuracy: 0.875\n",
      "At: 2450 [==========>] Loss 0.07907615681990812  - accuracy: 0.84375\n",
      "At: 2451 [==========>] Loss 0.0446493943348341  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.13094472260221368  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.15636364639582986  - accuracy: 0.78125\n",
      "At: 2454 [==========>] Loss 0.15702480819936038  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1323933871839602  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.14245830978622454  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.1556493998486447  - accuracy: 0.8125\n",
      "At: 2458 [==========>] Loss 0.08601107219536935  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.1511685673187684  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.0738747768598216  - accuracy: 0.9375\n",
      "At: 2461 [==========>] Loss 0.08704189352004747  - accuracy: 0.90625\n",
      "At: 2462 [==========>] Loss 0.1558559825846309  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.12629846163607747  - accuracy: 0.78125\n",
      "At: 2464 [==========>] Loss 0.14381031425050417  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.12949133562394072  - accuracy: 0.75\n",
      "At: 2466 [==========>] Loss 0.07815815758597885  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.10164750982870965  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.1114911031593728  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.16313703274863478  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.11192663535022615  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.12203695456376333  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.10636685854742252  - accuracy: 0.84375\n",
      "At: 2473 [==========>] Loss 0.12642050494641505  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.08789101576281322  - accuracy: 0.875\n",
      "At: 2475 [==========>] Loss 0.1241265697606423  - accuracy: 0.8125\n",
      "At: 2476 [==========>] Loss 0.06819339968380315  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.16107735184148103  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.09850960662409551  - accuracy: 0.875\n",
      "At: 2479 [==========>] Loss 0.09266523230202267  - accuracy: 0.84375\n",
      "At: 2480 [==========>] Loss 0.13719971876337372  - accuracy: 0.75\n",
      "At: 2481 [==========>] Loss 0.07877383194776232  - accuracy: 0.875\n",
      "At: 2482 [==========>] Loss 0.17171034486146997  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.11413280886651961  - accuracy: 0.78125\n",
      "At: 2484 [==========>] Loss 0.0693319791422485  - accuracy: 0.90625\n",
      "At: 2485 [==========>] Loss 0.11863180572435213  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.12232869469119317  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.1098922342530051  - accuracy: 0.875\n",
      "At: 2488 [==========>] Loss 0.14783011550893072  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.18485189226678195  - accuracy: 0.78125\n",
      "At: 2490 [==========>] Loss 0.09962965469085126  - accuracy: 0.90625\n",
      "At: 2491 [==========>] Loss 0.1113888167838959  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.13011221833814113  - accuracy: 0.8125\n",
      "At: 2493 [==========>] Loss 0.08842232906305775  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.09387094421876327  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.10195875218398412  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.07159184291186703  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.22352387162710816  - accuracy: 0.65625\n",
      "At: 2498 [==========>] Loss 0.12222291175277493  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.0745158481444605  - accuracy: 0.9375\n",
      "At: 2500 [==========>] Loss 0.18926589741616878  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.15742964593751194  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.10445490659256504  - accuracy: 0.875\n",
      "At: 2503 [==========>] Loss 0.1273158125735071  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.16936386336910353  - accuracy: 0.71875\n",
      "At: 2505 [==========>] Loss 0.14589915558247446  - accuracy: 0.78125\n",
      "At: 2506 [==========>] Loss 0.09742065064487854  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.15897561573211783  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.15644326790445726  - accuracy: 0.71875\n",
      "At: 2509 [==========>] Loss 0.1490133234809752  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.10714033493393568  - accuracy: 0.90625\n",
      "At: 2511 [==========>] Loss 0.1530609616082991  - accuracy: 0.84375\n",
      "At: 2512 [==========>] Loss 0.0964706067983076  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.1480688792943223  - accuracy: 0.75\n",
      "At: 2514 [==========>] Loss 0.1425449831573862  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.18434963281512273  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.19402542230304182  - accuracy: 0.6875\n",
      "At: 2517 [==========>] Loss 0.12687667057066648  - accuracy: 0.78125\n",
      "At: 2518 [==========>] Loss 0.15218627483629787  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.12298848857576691  - accuracy: 0.78125\n",
      "At: 2520 [==========>] Loss 0.15078755723480577  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.09254487696588501  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.20254392407313437  - accuracy: 0.6875\n",
      "At: 2523 [==========>] Loss 0.12190325498401133  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.16541846311264063  - accuracy: 0.78125\n",
      "At: 2525 [==========>] Loss 0.06669490925381602  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.11109273541173093  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.14973945991221002  - accuracy: 0.75\n",
      "At: 2528 [==========>] Loss 0.0855096736620365  - accuracy: 0.90625\n",
      "At: 2529 [==========>] Loss 0.15156138429624189  - accuracy: 0.71875\n",
      "At: 2530 [==========>] Loss 0.142945891058734  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.05510638797478107  - accuracy: 0.9375\n",
      "At: 2532 [==========>] Loss 0.10163713844779709  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.11297364143906019  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.06053352753580276  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.09109844508684775  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.1481603164597996  - accuracy: 0.78125\n",
      "At: 2537 [==========>] Loss 0.10228373234155384  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.14747337808614913  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.07740434473275103  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.11633592822932051  - accuracy: 0.84375\n",
      "At: 2541 [==========>] Loss 0.0704496931709995  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.0648320847314593  - accuracy: 0.96875\n",
      "At: 2543 [==========>] Loss 0.17593620166918686  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.14339163575756364  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.07011233832607819  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.1450278970397462  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.12617029033345784  - accuracy: 0.78125\n",
      "At: 2548 [==========>] Loss 0.09383373983701615  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.10381413376717134  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.1472466512133841  - accuracy: 0.75\n",
      "At: 2551 [==========>] Loss 0.1348947721986271  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.12670595201484033  - accuracy: 0.8125\n",
      "At: 2553 [==========>] Loss 0.08380586648458747  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.15719235498204137  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.16594532378300164  - accuracy: 0.8125\n",
      "At: 2556 [==========>] Loss 0.08961993116783866  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.12291053011243053  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.09154880533181452  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.08317141540842743  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.19036954762561692  - accuracy: 0.71875\n",
      "At: 2561 [==========>] Loss 0.09163106870821935  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.11198514754632197  - accuracy: 0.8125\n",
      "At: 2563 [==========>] Loss 0.12221719834746715  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09004038764592207  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.1123018147333468  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.18789429361094412  - accuracy: 0.78125\n",
      "At: 2567 [==========>] Loss 0.10672329457865343  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.10394563827234987  - accuracy: 0.84375\n",
      "At: 2569 [==========>] Loss 0.06359627615529169  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.18623339711933057  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.07969015504753446  - accuracy: 0.9375\n",
      "At: 2572 [==========>] Loss 0.20016486454068472  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.19761847353116566  - accuracy: 0.6875\n",
      "At: 2574 [==========>] Loss 0.14044097716911222  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.06436384476599132  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.11647662712492013  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.20657809728677556  - accuracy: 0.71875\n",
      "At: 2578 [==========>] Loss 0.18938223047977387  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.1844464539301634  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.15557959529568954  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.07225381955414292  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.18874577243193574  - accuracy: 0.78125\n",
      "At: 2583 [==========>] Loss 0.07077993503372378  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.17329434214712475  - accuracy: 0.78125\n",
      "At: 2585 [==========>] Loss 0.09447164668163624  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.0911643145938938  - accuracy: 0.875\n",
      "At: 2587 [==========>] Loss 0.1602035699778491  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.11921071352837813  - accuracy: 0.875\n",
      "At: 2589 [==========>] Loss 0.11888269880975605  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.21971991277348218  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.12513400976712968  - accuracy: 0.8125\n",
      "At: 2592 [==========>] Loss 0.12225191425555805  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.10370517418871954  - accuracy: 0.8125\n",
      "At: 2594 [==========>] Loss 0.08770310626051461  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.09781418134935635  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.12101468911385266  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.10232098400833237  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.13085364428391796  - accuracy: 0.875\n",
      "At: 2599 [==========>] Loss 0.14215651251658198  - accuracy: 0.75\n",
      "At: 2600 [==========>] Loss 0.11606691202668765  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.14706237625395208  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.07563519247608527  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.16140570462619208  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.11212521538585764  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.1430509883781319  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.11363149191992752  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.13886019742667396  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.07282187759139885  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.08621770170237697  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.16271896604470829  - accuracy: 0.75\n",
      "At: 2611 [==========>] Loss 0.1440332437547941  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.08874450818563226  - accuracy: 0.90625\n",
      "At: 2613 [==========>] Loss 0.1172010545532871  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.10976241219965618  - accuracy: 0.90625\n",
      "At: 2615 [==========>] Loss 0.10032603095587335  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.06254232488131228  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.09792529292563412  - accuracy: 0.875\n",
      "At: 2618 [==========>] Loss 0.09229018098901595  - accuracy: 0.8125\n",
      "At: 2619 [==========>] Loss 0.11477567619884042  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.11736670927237342  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.1148439087425891  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.14042550816986393  - accuracy: 0.75\n",
      "At: 2623 [==========>] Loss 0.10086183610009672  - accuracy: 0.84375\n",
      "At: 2624 [==========>] Loss 0.1380384645390746  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.05122641523643001  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.05934283655154105  - accuracy: 0.96875\n",
      "At: 2627 [==========>] Loss 0.15427037726916157  - accuracy: 0.71875\n",
      "At: 2628 [==========>] Loss 0.0910452600896188  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.07833733029928991  - accuracy: 0.90625\n",
      "At: 2630 [==========>] Loss 0.08658455488509201  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.11676888415270734  - accuracy: 0.8125\n",
      "At: 2632 [==========>] Loss 0.11266841521300906  - accuracy: 0.84375\n",
      "At: 2633 [==========>] Loss 0.08810274277892027  - accuracy: 0.9375\n",
      "At: 2634 [==========>] Loss 0.0557272377991134  - accuracy: 0.96875\n",
      "At: 2635 [==========>] Loss 0.22084617469426693  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.12934641862392146  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.14614250152229824  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.08431920018801409  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.06936428812565687  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.1497242582131096  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.1439119383298151  - accuracy: 0.84375\n",
      "At: 2642 [==========>] Loss 0.17308203501852315  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.11331395004753192  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.15798981507532495  - accuracy: 0.8125\n",
      "At: 2645 [==========>] Loss 0.08403993820361531  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.14288700873954924  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.11139158557200265  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.14426765099423894  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.12606220154840558  - accuracy: 0.78125\n",
      "At: 2650 [==========>] Loss 0.13014934212495066  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.17475953628763324  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.09234656023770946  - accuracy: 0.875\n",
      "At: 2653 [==========>] Loss 0.12317132937504105  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.13399369666173944  - accuracy: 0.78125\n",
      "At: 2655 [==========>] Loss 0.22650903751378523  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.026167511865622918  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.11697834451729064  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.09636617559962102  - accuracy: 0.84375\n",
      "At: 2659 [==========>] Loss 0.05896034605315247  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.08706692497327308  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.06634180600074807  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.06780453555657055  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.08738553911585936  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.105694617197954  - accuracy: 0.84375\n",
      "At: 2665 [==========>] Loss 0.115413540002446  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.14299003848437997  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.14810646935386773  - accuracy: 0.8125\n",
      "At: 2668 [==========>] Loss 0.14385246185889877  - accuracy: 0.71875\n",
      "At: 2669 [==========>] Loss 0.15159265314479292  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.12119293378951933  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.08164700811466832  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.09823384783962139  - accuracy: 0.875\n",
      "At: 2673 [==========>] Loss 0.12439671644401157  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.10737445230861402  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.13615781054986337  - accuracy: 0.84375\n",
      "At: 2676 [==========>] Loss 0.12197309839212866  - accuracy: 0.8125\n",
      "At: 2677 [==========>] Loss 0.11462605871230042  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.06600838223157215  - accuracy: 0.90625\n",
      "At: 2679 [==========>] Loss 0.07385285610966089  - accuracy: 0.9375\n",
      "At: 2680 [==========>] Loss 0.0823410961622791  - accuracy: 0.84375\n",
      "At: 2681 [==========>] Loss 0.1080141937594437  - accuracy: 0.875\n",
      "At: 2682 [==========>] Loss 0.1566877942731337  - accuracy: 0.75\n",
      "At: 2683 [==========>] Loss 0.18232704898027915  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09088464570639973  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.10057706694100554  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.07110394188745545  - accuracy: 0.875\n",
      "At: 2687 [==========>] Loss 0.14537750666181348  - accuracy: 0.8125\n",
      "At: 2688 [==========>] Loss 0.13917240260385835  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.12398292330071918  - accuracy: 0.875\n",
      "At: 2690 [==========>] Loss 0.12061141025046791  - accuracy: 0.84375\n",
      "Epochs  4 / 10\n",
      "At: 1 [==========>] Loss 0.13106075236753278  - accuracy: 0.78125\n",
      "At: 2 [==========>] Loss 0.1796585165175432  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.17704195650369903  - accuracy: 0.8125\n",
      "At: 4 [==========>] Loss 0.13735739336737518  - accuracy: 0.78125\n",
      "At: 5 [==========>] Loss 0.07928063681723131  - accuracy: 0.96875\n",
      "At: 6 [==========>] Loss 0.15750471353121023  - accuracy: 0.8125\n",
      "At: 7 [==========>] Loss 0.19106967716765214  - accuracy: 0.6875\n",
      "At: 8 [==========>] Loss 0.21341037622929757  - accuracy: 0.75\n",
      "At: 9 [==========>] Loss 0.28975455695938546  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.1924287824653093  - accuracy: 0.78125\n",
      "At: 11 [==========>] Loss 0.1646872387992373  - accuracy: 0.71875\n",
      "At: 12 [==========>] Loss 0.15566712363108892  - accuracy: 0.8125\n",
      "At: 13 [==========>] Loss 0.1533428984316713  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.06187789882405456  - accuracy: 0.96875\n",
      "At: 15 [==========>] Loss 0.16616328208953712  - accuracy: 0.75\n",
      "At: 16 [==========>] Loss 0.17283594005900538  - accuracy: 0.8125\n",
      "At: 17 [==========>] Loss 0.20952661751697618  - accuracy: 0.75\n",
      "At: 18 [==========>] Loss 0.2459723003673664  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.14056466669293852  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.1168498350925363  - accuracy: 0.84375\n",
      "At: 21 [==========>] Loss 0.1814521097005257  - accuracy: 0.78125\n",
      "At: 22 [==========>] Loss 0.1742759383578938  - accuracy: 0.75\n",
      "At: 23 [==========>] Loss 0.07249899544136591  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.24868722113053457  - accuracy: 0.71875\n",
      "At: 25 [==========>] Loss 0.19991646343586317  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.17168627054420704  - accuracy: 0.75\n",
      "At: 27 [==========>] Loss 0.20459878468584047  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.17229293991218786  - accuracy: 0.8125\n",
      "At: 29 [==========>] Loss 0.15522442613405651  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.22460891542888464  - accuracy: 0.6875\n",
      "At: 31 [==========>] Loss 0.19620504916410153  - accuracy: 0.78125\n",
      "At: 32 [==========>] Loss 0.1955757449985738  - accuracy: 0.78125\n",
      "At: 33 [==========>] Loss 0.15433501282656842  - accuracy: 0.8125\n",
      "At: 34 [==========>] Loss 0.19527883427359813  - accuracy: 0.78125\n",
      "At: 35 [==========>] Loss 0.1769814539465453  - accuracy: 0.71875\n",
      "At: 36 [==========>] Loss 0.18494973261788333  - accuracy: 0.75\n",
      "At: 37 [==========>] Loss 0.18365464618562924  - accuracy: 0.78125\n",
      "At: 38 [==========>] Loss 0.1978924911236799  - accuracy: 0.6875\n",
      "At: 39 [==========>] Loss 0.14598580420334978  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.1995454042324983  - accuracy: 0.71875\n",
      "At: 41 [==========>] Loss 0.06647043496683182  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.17383127755975486  - accuracy: 0.78125\n",
      "At: 43 [==========>] Loss 0.15788571322313813  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.1624553150839107  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.08526807829001594  - accuracy: 0.875\n",
      "At: 46 [==========>] Loss 0.1515906182691728  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.20650427118452694  - accuracy: 0.71875\n",
      "At: 48 [==========>] Loss 0.1484788254891089  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.11509365975759181  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.18248559010775917  - accuracy: 0.8125\n",
      "At: 51 [==========>] Loss 0.23078987124882938  - accuracy: 0.65625\n",
      "At: 52 [==========>] Loss 0.24212519254818293  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.17748150631125717  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.16610257714928958  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.19958175982798226  - accuracy: 0.6875\n",
      "At: 56 [==========>] Loss 0.1593304553647717  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.133000543733789  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.17823673627639403  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.21364188600286899  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.19503994842748656  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.20313430210094843  - accuracy: 0.6875\n",
      "At: 62 [==========>] Loss 0.18260356099055244  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.21176244559582058  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.18464349281060197  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.23871151576023716  - accuracy: 0.71875\n",
      "At: 66 [==========>] Loss 0.1883151988388746  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.22552905435346624  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.12117565182756232  - accuracy: 0.8125\n",
      "At: 69 [==========>] Loss 0.1238345604873807  - accuracy: 0.875\n",
      "At: 70 [==========>] Loss 0.133856348284811  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.15692080879060932  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.14933508898013176  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.13432940359341908  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.1619654905763753  - accuracy: 0.78125\n",
      "At: 75 [==========>] Loss 0.22655324290671752  - accuracy: 0.65625\n",
      "At: 76 [==========>] Loss 0.2298877621209751  - accuracy: 0.6875\n",
      "At: 77 [==========>] Loss 0.2028588241465911  - accuracy: 0.75\n",
      "At: 78 [==========>] Loss 0.13180978373452143  - accuracy: 0.78125\n",
      "At: 79 [==========>] Loss 0.20525730978348447  - accuracy: 0.75\n",
      "At: 80 [==========>] Loss 0.20685867888002563  - accuracy: 0.75\n",
      "At: 81 [==========>] Loss 0.15942884732016738  - accuracy: 0.75\n",
      "At: 82 [==========>] Loss 0.17622419435805725  - accuracy: 0.71875\n",
      "At: 83 [==========>] Loss 0.09791795096205017  - accuracy: 0.90625\n",
      "At: 84 [==========>] Loss 0.1850672920546431  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.17244663502313173  - accuracy: 0.78125\n",
      "At: 86 [==========>] Loss 0.13782883805060853  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.1521358166962007  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.2969510780422118  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.22299247902768288  - accuracy: 0.71875\n",
      "At: 90 [==========>] Loss 0.20613563776766952  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.15896235608873427  - accuracy: 0.75\n",
      "At: 92 [==========>] Loss 0.08509663130320248  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.14086154371237597  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.1516897469607572  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.16364539764649472  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.12155725319634061  - accuracy: 0.875\n",
      "At: 97 [==========>] Loss 0.09772792265418653  - accuracy: 0.875\n",
      "At: 98 [==========>] Loss 0.24638783511481133  - accuracy: 0.6875\n",
      "At: 99 [==========>] Loss 0.14008204593447807  - accuracy: 0.8125\n",
      "At: 100 [==========>] Loss 0.13547011035793172  - accuracy: 0.84375\n",
      "At: 101 [==========>] Loss 0.09937100908172243  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.17187350955288289  - accuracy: 0.8125\n",
      "At: 103 [==========>] Loss 0.12025872926402134  - accuracy: 0.84375\n",
      "At: 104 [==========>] Loss 0.10456288866861765  - accuracy: 0.875\n",
      "At: 105 [==========>] Loss 0.11944455198866233  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.18979802719633088  - accuracy: 0.78125\n",
      "At: 107 [==========>] Loss 0.14519571480950338  - accuracy: 0.78125\n",
      "At: 108 [==========>] Loss 0.19136970012198817  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.11222581292322469  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.16110759666666002  - accuracy: 0.75\n",
      "At: 111 [==========>] Loss 0.08755710301526629  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.17835087682931408  - accuracy: 0.75\n",
      "At: 113 [==========>] Loss 0.14894744923180855  - accuracy: 0.8125\n",
      "At: 114 [==========>] Loss 0.15565880842738822  - accuracy: 0.84375\n",
      "At: 115 [==========>] Loss 0.14351994817907082  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.19276008362911984  - accuracy: 0.8125\n",
      "At: 117 [==========>] Loss 0.15013869633700078  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.24554526040783303  - accuracy: 0.6875\n",
      "At: 119 [==========>] Loss 0.17677043388157646  - accuracy: 0.6875\n",
      "At: 120 [==========>] Loss 0.17338488022433152  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.15797676128352345  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.2054695904415188  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.20643316493484604  - accuracy: 0.71875\n",
      "At: 124 [==========>] Loss 0.18410572092878733  - accuracy: 0.75\n",
      "At: 125 [==========>] Loss 0.1204521158501444  - accuracy: 0.875\n",
      "At: 126 [==========>] Loss 0.2609827138412536  - accuracy: 0.6875\n",
      "At: 127 [==========>] Loss 0.18997845190800608  - accuracy: 0.6875\n",
      "At: 128 [==========>] Loss 0.23971804992824508  - accuracy: 0.625\n",
      "At: 129 [==========>] Loss 0.0792185419971759  - accuracy: 0.90625\n",
      "At: 130 [==========>] Loss 0.21907460605223683  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.16903557711335507  - accuracy: 0.71875\n",
      "At: 132 [==========>] Loss 0.22071880721846826  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.22601643241199354  - accuracy: 0.78125\n",
      "At: 134 [==========>] Loss 0.19251950513791086  - accuracy: 0.75\n",
      "At: 135 [==========>] Loss 0.22406510211555097  - accuracy: 0.6875\n",
      "At: 136 [==========>] Loss 0.15918747938712674  - accuracy: 0.8125\n",
      "At: 137 [==========>] Loss 0.08880870037404671  - accuracy: 0.84375\n",
      "At: 138 [==========>] Loss 0.14841085706902035  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.1413513789412609  - accuracy: 0.78125\n",
      "At: 140 [==========>] Loss 0.13520906069822208  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.23647677029950342  - accuracy: 0.65625\n",
      "At: 142 [==========>] Loss 0.13677769812378793  - accuracy: 0.84375\n",
      "At: 143 [==========>] Loss 0.17622582037777004  - accuracy: 0.71875\n",
      "At: 144 [==========>] Loss 0.10920128379676713  - accuracy: 0.875\n",
      "At: 145 [==========>] Loss 0.08839756508245734  - accuracy: 0.90625\n",
      "At: 146 [==========>] Loss 0.14441630302174463  - accuracy: 0.75\n",
      "At: 147 [==========>] Loss 0.19389435088530668  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.1520153717345885  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.17765669447086616  - accuracy: 0.8125\n",
      "At: 150 [==========>] Loss 0.15177325209402293  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.15937311609772004  - accuracy: 0.84375\n",
      "At: 152 [==========>] Loss 0.207005333653623  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.13875963290547555  - accuracy: 0.78125\n",
      "At: 154 [==========>] Loss 0.20012033111308816  - accuracy: 0.71875\n",
      "At: 155 [==========>] Loss 0.20171615838266915  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.13024423003356927  - accuracy: 0.8125\n",
      "At: 157 [==========>] Loss 0.20371333668725822  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.16302988752582412  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.13940558914239437  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.1714811001022907  - accuracy: 0.75\n",
      "At: 161 [==========>] Loss 0.06850841524222737  - accuracy: 0.9375\n",
      "At: 162 [==========>] Loss 0.20647933729174528  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.19945494905796213  - accuracy: 0.71875\n",
      "At: 164 [==========>] Loss 0.1325796168714565  - accuracy: 0.8125\n",
      "At: 165 [==========>] Loss 0.20232702485774107  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.16704668591211602  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.13286522472063503  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.16080402028704258  - accuracy: 0.71875\n",
      "At: 169 [==========>] Loss 0.12278973977872537  - accuracy: 0.875\n",
      "At: 170 [==========>] Loss 0.125787549436943  - accuracy: 0.875\n",
      "At: 171 [==========>] Loss 0.203558407626521  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.12238207401914698  - accuracy: 0.8125\n",
      "At: 173 [==========>] Loss 0.23176607815957245  - accuracy: 0.6875\n",
      "At: 174 [==========>] Loss 0.17338653464536002  - accuracy: 0.78125\n",
      "At: 175 [==========>] Loss 0.10578962408660644  - accuracy: 0.90625\n",
      "At: 176 [==========>] Loss 0.18866814451322567  - accuracy: 0.6875\n",
      "At: 177 [==========>] Loss 0.10917975948788947  - accuracy: 0.8125\n",
      "At: 178 [==========>] Loss 0.1722539418666588  - accuracy: 0.78125\n",
      "At: 179 [==========>] Loss 0.12999140735323175  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.13465689672525133  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.06101878077520237  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.12444685906865499  - accuracy: 0.78125\n",
      "At: 183 [==========>] Loss 0.16126983711862747  - accuracy: 0.78125\n",
      "At: 184 [==========>] Loss 0.15372148580658562  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.12522448507904452  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.12769305191965155  - accuracy: 0.8125\n",
      "At: 187 [==========>] Loss 0.171700250653843  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.1619083689375923  - accuracy: 0.75\n",
      "At: 189 [==========>] Loss 0.18762446232169305  - accuracy: 0.78125\n",
      "At: 190 [==========>] Loss 0.08312725141165361  - accuracy: 0.875\n",
      "At: 191 [==========>] Loss 0.3065589444841762  - accuracy: 0.65625\n",
      "At: 192 [==========>] Loss 0.12356286906566576  - accuracy: 0.875\n",
      "At: 193 [==========>] Loss 0.18560594528248806  - accuracy: 0.75\n",
      "At: 194 [==========>] Loss 0.18498569247050828  - accuracy: 0.75\n",
      "At: 195 [==========>] Loss 0.14903410261414934  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.15626668445544675  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.15545721478696684  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.15042104930618666  - accuracy: 0.8125\n",
      "At: 199 [==========>] Loss 0.08931338799620639  - accuracy: 0.90625\n",
      "At: 200 [==========>] Loss 0.19729946532046794  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.11998850651343385  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.10127798957731372  - accuracy: 0.90625\n",
      "At: 203 [==========>] Loss 0.19301229052589258  - accuracy: 0.71875\n",
      "At: 204 [==========>] Loss 0.1987282256814465  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.1008854647355973  - accuracy: 0.84375\n",
      "At: 206 [==========>] Loss 0.09211016775590086  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.10203987521997462  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.2553164019759211  - accuracy: 0.65625\n",
      "At: 209 [==========>] Loss 0.1894044878868464  - accuracy: 0.78125\n",
      "At: 210 [==========>] Loss 0.1145981863882077  - accuracy: 0.8125\n",
      "At: 211 [==========>] Loss 0.12206523294899504  - accuracy: 0.84375\n",
      "At: 212 [==========>] Loss 0.1912292683999626  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.19096892700902468  - accuracy: 0.71875\n",
      "At: 214 [==========>] Loss 0.1658381149291589  - accuracy: 0.75\n",
      "At: 215 [==========>] Loss 0.07577117639742872  - accuracy: 0.90625\n",
      "At: 216 [==========>] Loss 0.17791527802427437  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.2072889417186368  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.12624719449190008  - accuracy: 0.84375\n",
      "At: 219 [==========>] Loss 0.20410245899685964  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.21032263964282527  - accuracy: 0.6875\n",
      "At: 221 [==========>] Loss 0.12776174164061196  - accuracy: 0.84375\n",
      "At: 222 [==========>] Loss 0.15736341285373778  - accuracy: 0.78125\n",
      "At: 223 [==========>] Loss 0.17670173456520336  - accuracy: 0.8125\n",
      "At: 224 [==========>] Loss 0.18336036181466925  - accuracy: 0.75\n",
      "At: 225 [==========>] Loss 0.1293142279747755  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.11952679415102527  - accuracy: 0.84375\n",
      "At: 227 [==========>] Loss 0.1770954739797837  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.16724479646944856  - accuracy: 0.78125\n",
      "At: 229 [==========>] Loss 0.18557581148224014  - accuracy: 0.71875\n",
      "At: 230 [==========>] Loss 0.1705224642104217  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.23316749468371623  - accuracy: 0.65625\n",
      "At: 232 [==========>] Loss 0.2008764661863106  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.17954879466261942  - accuracy: 0.78125\n",
      "At: 234 [==========>] Loss 0.11565696794336883  - accuracy: 0.8125\n",
      "At: 235 [==========>] Loss 0.23397711033862173  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.19570097039914064  - accuracy: 0.78125\n",
      "At: 237 [==========>] Loss 0.11127798829042196  - accuracy: 0.875\n",
      "At: 238 [==========>] Loss 0.1071818204893176  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.16065619081170945  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.18933351344926314  - accuracy: 0.75\n",
      "At: 241 [==========>] Loss 0.19377992733218413  - accuracy: 0.71875\n",
      "At: 242 [==========>] Loss 0.12696709414457333  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.11365118673902873  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.16138611882556256  - accuracy: 0.75\n",
      "At: 245 [==========>] Loss 0.17345472694187708  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.12908482344540828  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.11751541093518059  - accuracy: 0.8125\n",
      "At: 248 [==========>] Loss 0.09926875298105699  - accuracy: 0.90625\n",
      "At: 249 [==========>] Loss 0.09426667658436791  - accuracy: 0.9375\n",
      "At: 250 [==========>] Loss 0.17482258658668742  - accuracy: 0.78125\n",
      "At: 251 [==========>] Loss 0.16389046789247497  - accuracy: 0.75\n",
      "At: 252 [==========>] Loss 0.0972500466379683  - accuracy: 0.90625\n",
      "At: 253 [==========>] Loss 0.16222338840049869  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.1028530081915968  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.13263375117228862  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.177787355207455  - accuracy: 0.75\n",
      "At: 257 [==========>] Loss 0.08013855368957348  - accuracy: 0.9375\n",
      "At: 258 [==========>] Loss 0.14502421687437939  - accuracy: 0.875\n",
      "At: 259 [==========>] Loss 0.12555826884433124  - accuracy: 0.875\n",
      "At: 260 [==========>] Loss 0.13090629108235802  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.0679546269023612  - accuracy: 0.96875\n",
      "At: 262 [==========>] Loss 0.12404821745216779  - accuracy: 0.90625\n",
      "At: 263 [==========>] Loss 0.1004027360910344  - accuracy: 0.84375\n",
      "At: 264 [==========>] Loss 0.10332809756439713  - accuracy: 0.875\n",
      "At: 265 [==========>] Loss 0.15660100879582722  - accuracy: 0.78125\n",
      "At: 266 [==========>] Loss 0.23348756176579827  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.13578961366099218  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.19337347399847274  - accuracy: 0.71875\n",
      "At: 269 [==========>] Loss 0.1331828478907065  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.24322634734552623  - accuracy: 0.65625\n",
      "At: 271 [==========>] Loss 0.1284456396127065  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.09199402277603402  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.1858047258901335  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.19592368951958855  - accuracy: 0.65625\n",
      "At: 275 [==========>] Loss 0.056919861147179014  - accuracy: 0.96875\n",
      "At: 276 [==========>] Loss 0.2184767638318611  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.11405917504765388  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.07617077499532057  - accuracy: 0.90625\n",
      "At: 279 [==========>] Loss 0.15500414264487758  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.1674558157000013  - accuracy: 0.78125\n",
      "At: 281 [==========>] Loss 0.14815517414955848  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.2086754335875855  - accuracy: 0.71875\n",
      "At: 283 [==========>] Loss 0.11481198089145589  - accuracy: 0.84375\n",
      "At: 284 [==========>] Loss 0.09839458486382795  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.12372987112810802  - accuracy: 0.875\n",
      "At: 286 [==========>] Loss 0.044641254655055265  - accuracy: 0.9375\n",
      "At: 287 [==========>] Loss 0.1591310933964777  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.10916777029898928  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.10847791801338103  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.11891882030429689  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.13456908316625177  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.12719130110716714  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.18237022500273056  - accuracy: 0.71875\n",
      "At: 294 [==========>] Loss 0.16984581548061123  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.11432664616355392  - accuracy: 0.75\n",
      "At: 296 [==========>] Loss 0.0910012176470507  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.12981607994188887  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.12405346865758914  - accuracy: 0.8125\n",
      "At: 299 [==========>] Loss 0.1760405728979671  - accuracy: 0.78125\n",
      "At: 300 [==========>] Loss 0.15603237033860345  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.17399175482300286  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.1421901991029113  - accuracy: 0.78125\n",
      "At: 303 [==========>] Loss 0.08347035536828726  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.14833318766138814  - accuracy: 0.8125\n",
      "At: 305 [==========>] Loss 0.21962099973452828  - accuracy: 0.71875\n",
      "At: 306 [==========>] Loss 0.12311406464346358  - accuracy: 0.78125\n",
      "At: 307 [==========>] Loss 0.22328747715142877  - accuracy: 0.6875\n",
      "At: 308 [==========>] Loss 0.13534251770430403  - accuracy: 0.8125\n",
      "At: 309 [==========>] Loss 0.12662616551062408  - accuracy: 0.84375\n",
      "At: 310 [==========>] Loss 0.18171304518048748  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.09586315841651276  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.1517310911975578  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.08633263197910021  - accuracy: 0.90625\n",
      "At: 314 [==========>] Loss 0.19612945872640472  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.13067378216224795  - accuracy: 0.90625\n",
      "At: 316 [==========>] Loss 0.15106148062074315  - accuracy: 0.78125\n",
      "At: 317 [==========>] Loss 0.23206809585314328  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.1727999177298342  - accuracy: 0.75\n",
      "At: 319 [==========>] Loss 0.1010683741021707  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.20145665569530224  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.22343304299468944  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.06545445670024451  - accuracy: 0.96875\n",
      "At: 323 [==========>] Loss 0.10701827412834819  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.1371441528098717  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.0981657310941989  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.17993750524910768  - accuracy: 0.78125\n",
      "At: 327 [==========>] Loss 0.10833997366020526  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.1178401919914232  - accuracy: 0.90625\n",
      "At: 329 [==========>] Loss 0.0985025010388508  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.18440721266672006  - accuracy: 0.71875\n",
      "At: 331 [==========>] Loss 0.1823444457555921  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.2367228240189081  - accuracy: 0.65625\n",
      "At: 333 [==========>] Loss 0.16731851968568334  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.08509864362865217  - accuracy: 0.9375\n",
      "At: 335 [==========>] Loss 0.12646861364001583  - accuracy: 0.84375\n",
      "At: 336 [==========>] Loss 0.12654380709267354  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.19373343444884014  - accuracy: 0.6875\n",
      "At: 338 [==========>] Loss 0.11842956132547469  - accuracy: 0.84375\n",
      "At: 339 [==========>] Loss 0.12744402093305116  - accuracy: 0.84375\n",
      "At: 340 [==========>] Loss 0.11951973729810605  - accuracy: 0.875\n",
      "At: 341 [==========>] Loss 0.11484356196912363  - accuracy: 0.8125\n",
      "At: 342 [==========>] Loss 0.11311834189828167  - accuracy: 0.84375\n",
      "At: 343 [==========>] Loss 0.23131187398948916  - accuracy: 0.71875\n",
      "At: 344 [==========>] Loss 0.13621590595655492  - accuracy: 0.84375\n",
      "At: 345 [==========>] Loss 0.16600363529297013  - accuracy: 0.75\n",
      "At: 346 [==========>] Loss 0.18859306066487247  - accuracy: 0.75\n",
      "At: 347 [==========>] Loss 0.08190062793231426  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.12554614647537876  - accuracy: 0.84375\n",
      "At: 349 [==========>] Loss 0.13554386608363528  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.10709235659031058  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.2431836985752055  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.13680272905874263  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.1519075362415072  - accuracy: 0.75\n",
      "At: 354 [==========>] Loss 0.19625306650190572  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.11458319697644608  - accuracy: 0.84375\n",
      "At: 356 [==========>] Loss 0.19388753306937206  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.10431584028885489  - accuracy: 0.875\n",
      "At: 358 [==========>] Loss 0.1467325199188214  - accuracy: 0.78125\n",
      "At: 359 [==========>] Loss 0.11324166699319574  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.14451281910828423  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.06972696639203516  - accuracy: 0.9375\n",
      "At: 362 [==========>] Loss 0.17305913402125334  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.10400694061439504  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.18338353696072818  - accuracy: 0.78125\n",
      "At: 365 [==========>] Loss 0.11566632150523319  - accuracy: 0.90625\n",
      "At: 366 [==========>] Loss 0.21293141456489517  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.16075657271238286  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.20237370170690644  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.15381536626388173  - accuracy: 0.78125\n",
      "At: 370 [==========>] Loss 0.16890958626017433  - accuracy: 0.78125\n",
      "At: 371 [==========>] Loss 0.05461707179673715  - accuracy: 0.9375\n",
      "At: 372 [==========>] Loss 0.13001327259344134  - accuracy: 0.78125\n",
      "At: 373 [==========>] Loss 0.22918760095333807  - accuracy: 0.65625\n",
      "At: 374 [==========>] Loss 0.09324605191561136  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.09810345243525784  - accuracy: 0.84375\n",
      "At: 376 [==========>] Loss 0.09456061283412495  - accuracy: 0.84375\n",
      "At: 377 [==========>] Loss 0.14062640878404603  - accuracy: 0.78125\n",
      "At: 378 [==========>] Loss 0.12709957366597105  - accuracy: 0.875\n",
      "At: 379 [==========>] Loss 0.10726130637651929  - accuracy: 0.90625\n",
      "At: 380 [==========>] Loss 0.18992067112348965  - accuracy: 0.71875\n",
      "At: 381 [==========>] Loss 0.16374484191629599  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.10106780625826073  - accuracy: 0.875\n",
      "At: 383 [==========>] Loss 0.16135695533329386  - accuracy: 0.78125\n",
      "At: 384 [==========>] Loss 0.15899804326334138  - accuracy: 0.8125\n",
      "At: 385 [==========>] Loss 0.0954046867478018  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.20978718894227963  - accuracy: 0.75\n",
      "At: 387 [==========>] Loss 0.09572202254214356  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.17624095862551187  - accuracy: 0.78125\n",
      "At: 389 [==========>] Loss 0.1638729724343947  - accuracy: 0.84375\n",
      "At: 390 [==========>] Loss 0.0805924227359198  - accuracy: 0.9375\n",
      "At: 391 [==========>] Loss 0.10234703809976452  - accuracy: 0.875\n",
      "At: 392 [==========>] Loss 0.11602383437702929  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.23406291625186912  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.11336722243127635  - accuracy: 0.875\n",
      "At: 395 [==========>] Loss 0.1409116855213017  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.14638932012830844  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.11189998744777513  - accuracy: 0.875\n",
      "At: 398 [==========>] Loss 0.17149855504818023  - accuracy: 0.75\n",
      "At: 399 [==========>] Loss 0.20705139984818338  - accuracy: 0.75\n",
      "At: 400 [==========>] Loss 0.15441003346447357  - accuracy: 0.75\n",
      "At: 401 [==========>] Loss 0.13695692092926645  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.08868429234507846  - accuracy: 0.90625\n",
      "At: 403 [==========>] Loss 0.08821234012391885  - accuracy: 0.9375\n",
      "At: 404 [==========>] Loss 0.11173497188781951  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.1747341167519179  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.1281326859335553  - accuracy: 0.90625\n",
      "At: 407 [==========>] Loss 0.20914192416732738  - accuracy: 0.71875\n",
      "At: 408 [==========>] Loss 0.24748156999233056  - accuracy: 0.65625\n",
      "At: 409 [==========>] Loss 0.19399381713932567  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.11717611168190835  - accuracy: 0.84375\n",
      "At: 411 [==========>] Loss 0.0686680011593523  - accuracy: 0.90625\n",
      "At: 412 [==========>] Loss 0.19095745429835825  - accuracy: 0.6875\n",
      "At: 413 [==========>] Loss 0.11697212020823951  - accuracy: 0.90625\n",
      "At: 414 [==========>] Loss 0.12972312503691327  - accuracy: 0.8125\n",
      "At: 415 [==========>] Loss 0.11202809840446587  - accuracy: 0.84375\n",
      "At: 416 [==========>] Loss 0.19217695634252618  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.12271677261591096  - accuracy: 0.84375\n",
      "At: 418 [==========>] Loss 0.13065296687632055  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.10398905430548498  - accuracy: 0.84375\n",
      "At: 420 [==========>] Loss 0.15913812011221334  - accuracy: 0.78125\n",
      "At: 421 [==========>] Loss 0.11884900507573912  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.14207403130347163  - accuracy: 0.8125\n",
      "At: 423 [==========>] Loss 0.15648752703904362  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.18570724349387036  - accuracy: 0.71875\n",
      "At: 425 [==========>] Loss 0.19820892897676787  - accuracy: 0.78125\n",
      "At: 426 [==========>] Loss 0.1053222601992016  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.16813575837447775  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.2152647232081589  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.15670214223031806  - accuracy: 0.75\n",
      "At: 430 [==========>] Loss 0.09978213649912504  - accuracy: 0.90625\n",
      "At: 431 [==========>] Loss 0.11882068597634166  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.15142298837531848  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.06106670588870329  - accuracy: 0.9375\n",
      "At: 434 [==========>] Loss 0.10389705694069006  - accuracy: 0.90625\n",
      "At: 435 [==========>] Loss 0.20028220258739926  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.18335437888705394  - accuracy: 0.6875\n",
      "At: 437 [==========>] Loss 0.13394078654481234  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.14793382234549746  - accuracy: 0.6875\n",
      "At: 439 [==========>] Loss 0.12933405192380037  - accuracy: 0.84375\n",
      "At: 440 [==========>] Loss 0.08102263158123532  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.19758957252211815  - accuracy: 0.71875\n",
      "At: 442 [==========>] Loss 0.1439006706451739  - accuracy: 0.8125\n",
      "At: 443 [==========>] Loss 0.12925076751743642  - accuracy: 0.84375\n",
      "At: 444 [==========>] Loss 0.15538648580521672  - accuracy: 0.78125\n",
      "At: 445 [==========>] Loss 0.16288354871665167  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.19234989709223851  - accuracy: 0.6875\n",
      "At: 447 [==========>] Loss 0.16542737062755636  - accuracy: 0.75\n",
      "At: 448 [==========>] Loss 0.15608883863177997  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.11640366263975804  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.12219288229599296  - accuracy: 0.78125\n",
      "At: 451 [==========>] Loss 0.12942243870601441  - accuracy: 0.8125\n",
      "At: 452 [==========>] Loss 0.15133978934749412  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.15003610856317232  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.23791230006615874  - accuracy: 0.625\n",
      "At: 455 [==========>] Loss 0.15041472492126112  - accuracy: 0.84375\n",
      "At: 456 [==========>] Loss 0.1469464800550989  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.1370473943125432  - accuracy: 0.75\n",
      "At: 458 [==========>] Loss 0.0756976762095574  - accuracy: 0.875\n",
      "At: 459 [==========>] Loss 0.2218481950200361  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.0845262097281425  - accuracy: 0.90625\n",
      "At: 461 [==========>] Loss 0.22203278870846566  - accuracy: 0.625\n",
      "At: 462 [==========>] Loss 0.15235660590028688  - accuracy: 0.8125\n",
      "At: 463 [==========>] Loss 0.13460819552205014  - accuracy: 0.84375\n",
      "At: 464 [==========>] Loss 0.1635983343778601  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.19805840274914444  - accuracy: 0.71875\n",
      "At: 466 [==========>] Loss 0.09095276674902354  - accuracy: 0.875\n",
      "At: 467 [==========>] Loss 0.1773406762590835  - accuracy: 0.71875\n",
      "At: 468 [==========>] Loss 0.06489752338572917  - accuracy: 0.96875\n",
      "At: 469 [==========>] Loss 0.11044413360773796  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.10294559012713614  - accuracy: 0.90625\n",
      "At: 471 [==========>] Loss 0.15621472197548128  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.1117185565257751  - accuracy: 0.8125\n",
      "At: 473 [==========>] Loss 0.1670971363025481  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.19757060281176014  - accuracy: 0.71875\n",
      "At: 475 [==========>] Loss 0.13967481605379622  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.17135029090683535  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.1390133192708213  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.12423226654467999  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.2029731043952483  - accuracy: 0.78125\n",
      "At: 480 [==========>] Loss 0.17310934854040538  - accuracy: 0.78125\n",
      "At: 481 [==========>] Loss 0.112942323227231  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.07778140753810209  - accuracy: 0.9375\n",
      "At: 483 [==========>] Loss 0.12170012117525206  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.09051934664885308  - accuracy: 0.90625\n",
      "At: 485 [==========>] Loss 0.12332635001816887  - accuracy: 0.78125\n",
      "At: 486 [==========>] Loss 0.16846839114309622  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.1616474838834058  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.11392015540500292  - accuracy: 0.875\n",
      "At: 489 [==========>] Loss 0.16841407164553607  - accuracy: 0.8125\n",
      "At: 490 [==========>] Loss 0.14802430632349967  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.15281099149589866  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.20891905603579825  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.15535036735620214  - accuracy: 0.75\n",
      "At: 494 [==========>] Loss 0.1303960817375864  - accuracy: 0.875\n",
      "At: 495 [==========>] Loss 0.12724022528632525  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.14575037563798166  - accuracy: 0.8125\n",
      "At: 497 [==========>] Loss 0.14739922395409136  - accuracy: 0.84375\n",
      "At: 498 [==========>] Loss 0.09314397504990646  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.17237322577066144  - accuracy: 0.75\n",
      "At: 500 [==========>] Loss 0.15420191942588055  - accuracy: 0.75\n",
      "At: 501 [==========>] Loss 0.16014210438535223  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.10288462206219565  - accuracy: 0.90625\n",
      "At: 503 [==========>] Loss 0.10593342358503197  - accuracy: 0.875\n",
      "At: 504 [==========>] Loss 0.11207014394787784  - accuracy: 0.875\n",
      "At: 505 [==========>] Loss 0.18397999203458187  - accuracy: 0.78125\n",
      "At: 506 [==========>] Loss 0.2934102573241021  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.12463974849919776  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.10144098153283176  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.13256231398444035  - accuracy: 0.8125\n",
      "At: 510 [==========>] Loss 0.14086496368180446  - accuracy: 0.8125\n",
      "At: 511 [==========>] Loss 0.13147614862878224  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.18148688386912462  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.23109619948826016  - accuracy: 0.6875\n",
      "At: 514 [==========>] Loss 0.1553218762363939  - accuracy: 0.71875\n",
      "At: 515 [==========>] Loss 0.14029323239461783  - accuracy: 0.8125\n",
      "At: 516 [==========>] Loss 0.16613839712344258  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.11508719496440473  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.16765203340138707  - accuracy: 0.75\n",
      "At: 519 [==========>] Loss 0.12989582876133537  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.13024254846450387  - accuracy: 0.78125\n",
      "At: 521 [==========>] Loss 0.1640520520745668  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.14294370776764745  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.14780629918794097  - accuracy: 0.8125\n",
      "At: 524 [==========>] Loss 0.09011513960142885  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.18202856239107337  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.12819087419406627  - accuracy: 0.8125\n",
      "At: 527 [==========>] Loss 0.21998033854010748  - accuracy: 0.6875\n",
      "At: 528 [==========>] Loss 0.20724580583199573  - accuracy: 0.6875\n",
      "At: 529 [==========>] Loss 0.09266903362172307  - accuracy: 0.90625\n",
      "At: 530 [==========>] Loss 0.17091480661544134  - accuracy: 0.8125\n",
      "At: 531 [==========>] Loss 0.1491639842925176  - accuracy: 0.75\n",
      "At: 532 [==========>] Loss 0.11641167234938259  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.08393657390442216  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.1415185192239631  - accuracy: 0.84375\n",
      "At: 535 [==========>] Loss 0.09349812769983062  - accuracy: 0.875\n",
      "At: 536 [==========>] Loss 0.14120124841191434  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.0799761000086212  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.11951304115322553  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.09591305778456179  - accuracy: 0.90625\n",
      "At: 540 [==========>] Loss 0.2957199753782318  - accuracy: 0.5625\n",
      "At: 541 [==========>] Loss 0.1929669581241621  - accuracy: 0.75\n",
      "At: 542 [==========>] Loss 0.12720403026674196  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.1287341747277536  - accuracy: 0.8125\n",
      "At: 544 [==========>] Loss 0.21839080372768926  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.08042305487704866  - accuracy: 0.9375\n",
      "At: 546 [==========>] Loss 0.16543356394017547  - accuracy: 0.75\n",
      "At: 547 [==========>] Loss 0.09461691573772377  - accuracy: 0.90625\n",
      "At: 548 [==========>] Loss 0.09849766230337849  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.08234719993158221  - accuracy: 0.9375\n",
      "At: 550 [==========>] Loss 0.09086439800069104  - accuracy: 0.9375\n",
      "At: 551 [==========>] Loss 0.12085572158321216  - accuracy: 0.78125\n",
      "At: 552 [==========>] Loss 0.17351453065554906  - accuracy: 0.6875\n",
      "At: 553 [==========>] Loss 0.1380555814102704  - accuracy: 0.78125\n",
      "At: 554 [==========>] Loss 0.06624721514952685  - accuracy: 0.90625\n",
      "At: 555 [==========>] Loss 0.12645733957896124  - accuracy: 0.90625\n",
      "At: 556 [==========>] Loss 0.17356997146967013  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.12642452104285207  - accuracy: 0.84375\n",
      "At: 558 [==========>] Loss 0.09876036229415888  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.20057302261601262  - accuracy: 0.71875\n",
      "At: 560 [==========>] Loss 0.1021213152676616  - accuracy: 0.875\n",
      "At: 561 [==========>] Loss 0.12022613909294223  - accuracy: 0.875\n",
      "At: 562 [==========>] Loss 0.08949932516455497  - accuracy: 0.875\n",
      "At: 563 [==========>] Loss 0.11460954204279167  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.15062781528408722  - accuracy: 0.8125\n",
      "At: 565 [==========>] Loss 0.12137669735777047  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.1501650252851338  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.14468982461756602  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.20447072085993145  - accuracy: 0.625\n",
      "At: 569 [==========>] Loss 0.23289383763998847  - accuracy: 0.71875\n",
      "At: 570 [==========>] Loss 0.09282033728104358  - accuracy: 0.875\n",
      "At: 571 [==========>] Loss 0.11158396045212458  - accuracy: 0.875\n",
      "At: 572 [==========>] Loss 0.09204483475514452  - accuracy: 0.875\n",
      "At: 573 [==========>] Loss 0.09262566420026658  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.12377226996315259  - accuracy: 0.8125\n",
      "At: 575 [==========>] Loss 0.11143802045718557  - accuracy: 0.84375\n",
      "At: 576 [==========>] Loss 0.11036754208558663  - accuracy: 0.84375\n",
      "At: 577 [==========>] Loss 0.17050952877934542  - accuracy: 0.8125\n",
      "At: 578 [==========>] Loss 0.17630490168137253  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.11865534395641089  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.141832663055534  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.13809471842201448  - accuracy: 0.84375\n",
      "At: 582 [==========>] Loss 0.11667344002798759  - accuracy: 0.84375\n",
      "At: 583 [==========>] Loss 0.15919077168376689  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.10634724374263041  - accuracy: 0.875\n",
      "At: 585 [==========>] Loss 0.14538977501093864  - accuracy: 0.75\n",
      "At: 586 [==========>] Loss 0.0881545831565478  - accuracy: 0.84375\n",
      "At: 587 [==========>] Loss 0.1253330071141885  - accuracy: 0.84375\n",
      "At: 588 [==========>] Loss 0.13748241954611307  - accuracy: 0.8125\n",
      "At: 589 [==========>] Loss 0.15957163791548779  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.08685288106104658  - accuracy: 0.875\n",
      "At: 591 [==========>] Loss 0.14122172540214598  - accuracy: 0.875\n",
      "At: 592 [==========>] Loss 0.10568373824495436  - accuracy: 0.90625\n",
      "At: 593 [==========>] Loss 0.16507337304697223  - accuracy: 0.78125\n",
      "At: 594 [==========>] Loss 0.14857505197997833  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.1402624423596447  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.0974676814069672  - accuracy: 0.90625\n",
      "At: 597 [==========>] Loss 0.1999664850183987  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.13827137403342815  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.12820155665581062  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.09506172227905683  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.1288948772596416  - accuracy: 0.8125\n",
      "At: 602 [==========>] Loss 0.12778737207302188  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.10619891897897701  - accuracy: 0.84375\n",
      "At: 604 [==========>] Loss 0.21575833496125282  - accuracy: 0.71875\n",
      "At: 605 [==========>] Loss 0.09769967503610705  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.1689443714201412  - accuracy: 0.78125\n",
      "At: 607 [==========>] Loss 0.18139733597645624  - accuracy: 0.78125\n",
      "At: 608 [==========>] Loss 0.11668295738586157  - accuracy: 0.875\n",
      "At: 609 [==========>] Loss 0.13734870647905592  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.11434685820785523  - accuracy: 0.8125\n",
      "At: 611 [==========>] Loss 0.10427094782763459  - accuracy: 0.875\n",
      "At: 612 [==========>] Loss 0.1570201719990897  - accuracy: 0.75\n",
      "At: 613 [==========>] Loss 0.14862721859224426  - accuracy: 0.875\n",
      "At: 614 [==========>] Loss 0.14360494909655525  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.18912319899806818  - accuracy: 0.71875\n",
      "At: 616 [==========>] Loss 0.1780714312672235  - accuracy: 0.75\n",
      "At: 617 [==========>] Loss 0.11001239529621114  - accuracy: 0.84375\n",
      "At: 618 [==========>] Loss 0.18048158364364944  - accuracy: 0.71875\n",
      "At: 619 [==========>] Loss 0.17460918555444482  - accuracy: 0.75\n",
      "At: 620 [==========>] Loss 0.1484675105836186  - accuracy: 0.8125\n",
      "At: 621 [==========>] Loss 0.08348073635392433  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.1630884152546798  - accuracy: 0.78125\n",
      "At: 623 [==========>] Loss 0.1299435527215444  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.10866230865752208  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.1315617600647264  - accuracy: 0.8125\n",
      "At: 626 [==========>] Loss 0.11664227687381595  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.0925787851885477  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.08862598343082119  - accuracy: 0.875\n",
      "At: 629 [==========>] Loss 0.1833925192677733  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.2104231924252415  - accuracy: 0.65625\n",
      "At: 631 [==========>] Loss 0.14368970688659105  - accuracy: 0.84375\n",
      "At: 632 [==========>] Loss 0.14417738087007304  - accuracy: 0.78125\n",
      "At: 633 [==========>] Loss 0.15500508098445392  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.13631756029692696  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.104460958969112  - accuracy: 0.90625\n",
      "At: 636 [==========>] Loss 0.14036271265196615  - accuracy: 0.78125\n",
      "At: 637 [==========>] Loss 0.1350389852733322  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.09443233568047724  - accuracy: 0.9375\n",
      "At: 639 [==========>] Loss 0.13472332599505538  - accuracy: 0.84375\n",
      "At: 640 [==========>] Loss 0.17857270275397044  - accuracy: 0.8125\n",
      "At: 641 [==========>] Loss 0.13602118134523697  - accuracy: 0.8125\n",
      "At: 642 [==========>] Loss 0.19059805306683303  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.13288709800196638  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.07039761886183038  - accuracy: 0.9375\n",
      "At: 645 [==========>] Loss 0.12412351728126236  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.1224856514177643  - accuracy: 0.78125\n",
      "At: 647 [==========>] Loss 0.1483102786056752  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.1846682003861045  - accuracy: 0.6875\n",
      "At: 649 [==========>] Loss 0.16142743246861968  - accuracy: 0.75\n",
      "At: 650 [==========>] Loss 0.09949438998248983  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.18263700997361249  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.10679973497150219  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.10879503423399547  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.07309535036271765  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.11319206522330315  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.13720923730031204  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.1445208351424686  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.10840439438512958  - accuracy: 0.90625\n",
      "At: 659 [==========>] Loss 0.14434940749749686  - accuracy: 0.875\n",
      "At: 660 [==========>] Loss 0.11218216029418619  - accuracy: 0.875\n",
      "At: 661 [==========>] Loss 0.1318315599950508  - accuracy: 0.875\n",
      "At: 662 [==========>] Loss 0.1186923233600666  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.07856327847625227  - accuracy: 0.9375\n",
      "At: 664 [==========>] Loss 0.130718511439293  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.18360402379976287  - accuracy: 0.75\n",
      "At: 666 [==========>] Loss 0.14967552985625787  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.1328540408411442  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.14664471789315808  - accuracy: 0.84375\n",
      "At: 669 [==========>] Loss 0.15749585752282896  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.2001833486295842  - accuracy: 0.75\n",
      "At: 671 [==========>] Loss 0.11107663428969243  - accuracy: 0.84375\n",
      "At: 672 [==========>] Loss 0.10212822663836367  - accuracy: 0.875\n",
      "At: 673 [==========>] Loss 0.05558515412864801  - accuracy: 0.96875\n",
      "At: 674 [==========>] Loss 0.1555628684871886  - accuracy: 0.75\n",
      "At: 675 [==========>] Loss 0.11893444435489321  - accuracy: 0.8125\n",
      "At: 676 [==========>] Loss 0.14939685081675924  - accuracy: 0.84375\n",
      "At: 677 [==========>] Loss 0.14282730226393206  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.1392711347873281  - accuracy: 0.78125\n",
      "At: 679 [==========>] Loss 0.08093589852157047  - accuracy: 0.90625\n",
      "At: 680 [==========>] Loss 0.10006619598288512  - accuracy: 0.875\n",
      "At: 681 [==========>] Loss 0.14812135609266308  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.1369637483584305  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.17411886457210274  - accuracy: 0.71875\n",
      "At: 684 [==========>] Loss 0.10836753343076072  - accuracy: 0.75\n",
      "At: 685 [==========>] Loss 0.15541362218488147  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.07498809743923562  - accuracy: 0.9375\n",
      "At: 687 [==========>] Loss 0.08614417225140114  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.08632466067689977  - accuracy: 0.9375\n",
      "At: 689 [==========>] Loss 0.15440757386947576  - accuracy: 0.78125\n",
      "At: 690 [==========>] Loss 0.14812127296167826  - accuracy: 0.78125\n",
      "At: 691 [==========>] Loss 0.09696311839067945  - accuracy: 0.90625\n",
      "At: 692 [==========>] Loss 0.1103536307466968  - accuracy: 0.84375\n",
      "At: 693 [==========>] Loss 0.138420816762688  - accuracy: 0.84375\n",
      "At: 694 [==========>] Loss 0.18026893389793994  - accuracy: 0.71875\n",
      "At: 695 [==========>] Loss 0.1678373799310301  - accuracy: 0.75\n",
      "At: 696 [==========>] Loss 0.1662386475621105  - accuracy: 0.78125\n",
      "At: 697 [==========>] Loss 0.1723027355197107  - accuracy: 0.8125\n",
      "At: 698 [==========>] Loss 0.1062344110903706  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.14056222038322058  - accuracy: 0.8125\n",
      "At: 700 [==========>] Loss 0.07856895674954673  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.14309557818964375  - accuracy: 0.71875\n",
      "At: 702 [==========>] Loss 0.11696464897151644  - accuracy: 0.84375\n",
      "At: 703 [==========>] Loss 0.20903720267740405  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.1370703759369852  - accuracy: 0.78125\n",
      "At: 705 [==========>] Loss 0.2057758589043047  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.16032798418994298  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.07524918239812897  - accuracy: 0.90625\n",
      "At: 708 [==========>] Loss 0.12436718238515226  - accuracy: 0.875\n",
      "At: 709 [==========>] Loss 0.19803018310854845  - accuracy: 0.71875\n",
      "At: 710 [==========>] Loss 0.14398816920685348  - accuracy: 0.8125\n",
      "At: 711 [==========>] Loss 0.2127842953547457  - accuracy: 0.75\n",
      "At: 712 [==========>] Loss 0.15738424324873723  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.16776702461314522  - accuracy: 0.78125\n",
      "At: 714 [==========>] Loss 0.21687931477232675  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.10532760521659802  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.10486011977251872  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.10271877757045036  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.16225572788291065  - accuracy: 0.8125\n",
      "At: 719 [==========>] Loss 0.08632457691610176  - accuracy: 0.90625\n",
      "At: 720 [==========>] Loss 0.1218605267965108  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.1107831220938731  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.14649708966922806  - accuracy: 0.84375\n",
      "At: 723 [==========>] Loss 0.1582945544574849  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.08386795294369667  - accuracy: 0.9375\n",
      "At: 725 [==========>] Loss 0.14718584823885872  - accuracy: 0.8125\n",
      "At: 726 [==========>] Loss 0.20549793822404347  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.20494252115820172  - accuracy: 0.75\n",
      "At: 728 [==========>] Loss 0.13073537110644495  - accuracy: 0.8125\n",
      "At: 729 [==========>] Loss 0.1896350820743872  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.16072047928261685  - accuracy: 0.8125\n",
      "At: 731 [==========>] Loss 0.10465088944905439  - accuracy: 0.9375\n",
      "At: 732 [==========>] Loss 0.17753337679348452  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.08714328910881738  - accuracy: 0.90625\n",
      "At: 734 [==========>] Loss 0.17497237508934696  - accuracy: 0.78125\n",
      "At: 735 [==========>] Loss 0.11166201926259989  - accuracy: 0.875\n",
      "At: 736 [==========>] Loss 0.13580456253943474  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.16874823614226653  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.07324703238500412  - accuracy: 0.90625\n",
      "At: 739 [==========>] Loss 0.09562700234878557  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.14979280689054922  - accuracy: 0.71875\n",
      "At: 741 [==========>] Loss 0.08609455786227097  - accuracy: 0.90625\n",
      "At: 742 [==========>] Loss 0.1572869803547426  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.13859074775609398  - accuracy: 0.875\n",
      "At: 744 [==========>] Loss 0.14299903459360422  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.1861664550535005  - accuracy: 0.71875\n",
      "At: 746 [==========>] Loss 0.13882191489173065  - accuracy: 0.84375\n",
      "At: 747 [==========>] Loss 0.14035233314930567  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.16245047482499403  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.15105939323227005  - accuracy: 0.8125\n",
      "At: 750 [==========>] Loss 0.14739993223831566  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.2010436154173158  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.10600453685099018  - accuracy: 0.84375\n",
      "At: 753 [==========>] Loss 0.1794463760991274  - accuracy: 0.65625\n",
      "At: 754 [==========>] Loss 0.1041433557346949  - accuracy: 0.875\n",
      "At: 755 [==========>] Loss 0.0802321269435664  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.20580724255903474  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.07367124418571526  - accuracy: 0.9375\n",
      "At: 758 [==========>] Loss 0.0860390607944102  - accuracy: 0.90625\n",
      "At: 759 [==========>] Loss 0.10423675318435534  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.1650279586946593  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.10956305010094833  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.118759775481041  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.15471711596178023  - accuracy: 0.75\n",
      "At: 764 [==========>] Loss 0.09571418356501227  - accuracy: 0.875\n",
      "At: 765 [==========>] Loss 0.18003054665696916  - accuracy: 0.71875\n",
      "At: 766 [==========>] Loss 0.1328412702385748  - accuracy: 0.8125\n",
      "At: 767 [==========>] Loss 0.13012953127766175  - accuracy: 0.75\n",
      "At: 768 [==========>] Loss 0.1350871875090774  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.13814514725800228  - accuracy: 0.8125\n",
      "At: 770 [==========>] Loss 0.1228093667568047  - accuracy: 0.78125\n",
      "At: 771 [==========>] Loss 0.18623271285661797  - accuracy: 0.78125\n",
      "At: 772 [==========>] Loss 0.14196567802519328  - accuracy: 0.8125\n",
      "At: 773 [==========>] Loss 0.07926217590300697  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.1342002565450211  - accuracy: 0.78125\n",
      "At: 775 [==========>] Loss 0.15430128703212342  - accuracy: 0.8125\n",
      "At: 776 [==========>] Loss 0.16451971329500115  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.08237594601700302  - accuracy: 0.9375\n",
      "At: 778 [==========>] Loss 0.14180715329733581  - accuracy: 0.84375\n",
      "At: 779 [==========>] Loss 0.13858017185342963  - accuracy: 0.75\n",
      "At: 780 [==========>] Loss 0.07528020545543657  - accuracy: 0.84375\n",
      "At: 781 [==========>] Loss 0.1219506741957542  - accuracy: 0.875\n",
      "At: 782 [==========>] Loss 0.15436818765620303  - accuracy: 0.75\n",
      "At: 783 [==========>] Loss 0.18081238890321502  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.1613847935287106  - accuracy: 0.78125\n",
      "At: 785 [==========>] Loss 0.19675699084628218  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.1449405463918974  - accuracy: 0.84375\n",
      "At: 787 [==========>] Loss 0.12630341456105362  - accuracy: 0.84375\n",
      "At: 788 [==========>] Loss 0.08397143470209076  - accuracy: 0.96875\n",
      "At: 789 [==========>] Loss 0.1380369881473173  - accuracy: 0.8125\n",
      "At: 790 [==========>] Loss 0.14661858392767038  - accuracy: 0.78125\n",
      "At: 791 [==========>] Loss 0.16239651820004164  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.15690410525194964  - accuracy: 0.8125\n",
      "At: 793 [==========>] Loss 0.14948961863272953  - accuracy: 0.75\n",
      "At: 794 [==========>] Loss 0.11025809260818785  - accuracy: 0.84375\n",
      "At: 795 [==========>] Loss 0.1015541517141898  - accuracy: 0.875\n",
      "At: 796 [==========>] Loss 0.13488812765757027  - accuracy: 0.875\n",
      "At: 797 [==========>] Loss 0.17619239712080362  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.15074668271139513  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.0931410979563373  - accuracy: 0.84375\n",
      "At: 800 [==========>] Loss 0.15342894227711776  - accuracy: 0.75\n",
      "At: 801 [==========>] Loss 0.1334694487087178  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.17550346494431301  - accuracy: 0.78125\n",
      "At: 803 [==========>] Loss 0.1358551307123487  - accuracy: 0.8125\n",
      "At: 804 [==========>] Loss 0.13610130374988694  - accuracy: 0.71875\n",
      "At: 805 [==========>] Loss 0.1510734151532307  - accuracy: 0.8125\n",
      "At: 806 [==========>] Loss 0.12266975731992624  - accuracy: 0.8125\n",
      "At: 807 [==========>] Loss 0.07875894743502376  - accuracy: 0.9375\n",
      "At: 808 [==========>] Loss 0.11987660999902772  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.09287161620785142  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.18567031019596913  - accuracy: 0.75\n",
      "At: 811 [==========>] Loss 0.12734066220478446  - accuracy: 0.8125\n",
      "At: 812 [==========>] Loss 0.14040720874490953  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.200501326181883  - accuracy: 0.71875\n",
      "At: 814 [==========>] Loss 0.18586461539212876  - accuracy: 0.71875\n",
      "At: 815 [==========>] Loss 0.15153259038168027  - accuracy: 0.78125\n",
      "At: 816 [==========>] Loss 0.13813808648421977  - accuracy: 0.84375\n",
      "At: 817 [==========>] Loss 0.06974371320937905  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.10764401106822916  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.13141968339846016  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.1252235110331085  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.16205385991776705  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.12323327856643837  - accuracy: 0.8125\n",
      "At: 823 [==========>] Loss 0.16081907249672525  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.13132882643466276  - accuracy: 0.8125\n",
      "At: 825 [==========>] Loss 0.2037482899013348  - accuracy: 0.65625\n",
      "At: 826 [==========>] Loss 0.10802591038021171  - accuracy: 0.84375\n",
      "At: 827 [==========>] Loss 0.07699913161947337  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.05548465742051313  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.08150224473993081  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.09059146217490668  - accuracy: 0.875\n",
      "At: 831 [==========>] Loss 0.07524911475432125  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.08890685216379651  - accuracy: 0.84375\n",
      "At: 833 [==========>] Loss 0.189919301945717  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.1122915474331166  - accuracy: 0.78125\n",
      "At: 835 [==========>] Loss 0.08507128308624057  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.11083387494814337  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.1369908850702609  - accuracy: 0.78125\n",
      "At: 838 [==========>] Loss 0.09391037926298587  - accuracy: 0.90625\n",
      "At: 839 [==========>] Loss 0.1412612616382983  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.10821724060138888  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.07074270510584352  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.09229637267960239  - accuracy: 0.875\n",
      "At: 843 [==========>] Loss 0.18592528760349053  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.12012589744926996  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.13188360566020157  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.11481851960853512  - accuracy: 0.84375\n",
      "At: 847 [==========>] Loss 0.07964588054249835  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.1576521773336641  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.1626133414491393  - accuracy: 0.75\n",
      "At: 850 [==========>] Loss 0.0749223680873763  - accuracy: 0.9375\n",
      "At: 851 [==========>] Loss 0.08586783631308478  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.13763131793178413  - accuracy: 0.8125\n",
      "At: 853 [==========>] Loss 0.16022651068097737  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.21079890960265657  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.08437199620271006  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.08361945554664377  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.07424316171466383  - accuracy: 0.9375\n",
      "At: 858 [==========>] Loss 0.2648200816663068  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.11014877935681429  - accuracy: 0.78125\n",
      "At: 860 [==========>] Loss 0.11069164974530044  - accuracy: 0.875\n",
      "At: 861 [==========>] Loss 0.10686278816195723  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.1080278508238001  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.17384090386211104  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.11274015904654641  - accuracy: 0.8125\n",
      "At: 865 [==========>] Loss 0.21364951106108387  - accuracy: 0.71875\n",
      "At: 866 [==========>] Loss 0.16043496693145268  - accuracy: 0.8125\n",
      "At: 867 [==========>] Loss 0.08751651141823395  - accuracy: 0.90625\n",
      "At: 868 [==========>] Loss 0.18045675142856699  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.17844126466359245  - accuracy: 0.6875\n",
      "At: 870 [==========>] Loss 0.11628731891286287  - accuracy: 0.84375\n",
      "At: 871 [==========>] Loss 0.0779789103933841  - accuracy: 0.875\n",
      "At: 872 [==========>] Loss 0.09314570964472438  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.1745817624712386  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.16311816452134967  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.10134106780076771  - accuracy: 0.90625\n",
      "At: 876 [==========>] Loss 0.11269702989766797  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.1569086568871292  - accuracy: 0.78125\n",
      "At: 878 [==========>] Loss 0.06003646401470776  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.16275524097235405  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.1102726196276805  - accuracy: 0.90625\n",
      "At: 881 [==========>] Loss 0.17277748179393815  - accuracy: 0.6875\n",
      "At: 882 [==========>] Loss 0.11994641686792751  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.1465604448952729  - accuracy: 0.8125\n",
      "At: 884 [==========>] Loss 0.13335746455659991  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.10510519267813367  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.09862477614318674  - accuracy: 0.90625\n",
      "At: 887 [==========>] Loss 0.11749304261518739  - accuracy: 0.84375\n",
      "At: 888 [==========>] Loss 0.17987205945505108  - accuracy: 0.71875\n",
      "At: 889 [==========>] Loss 0.11810701642419416  - accuracy: 0.75\n",
      "At: 890 [==========>] Loss 0.12907764551894546  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.1187776798107038  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.12980639219846038  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.1217787600864539  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.14507453998589348  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.10972314433839828  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.10849871714036474  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.13172113879093872  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.16388021977088713  - accuracy: 0.75\n",
      "At: 899 [==========>] Loss 0.09295454992954227  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.1554905064386276  - accuracy: 0.71875\n",
      "At: 901 [==========>] Loss 0.16034115334482707  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.09879686233255659  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.1556774869045085  - accuracy: 0.71875\n",
      "At: 904 [==========>] Loss 0.1029255544167694  - accuracy: 0.84375\n",
      "At: 905 [==========>] Loss 0.08955322338538171  - accuracy: 0.9375\n",
      "At: 906 [==========>] Loss 0.06789581847089726  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.14908804704248407  - accuracy: 0.75\n",
      "At: 908 [==========>] Loss 0.11371938052839112  - accuracy: 0.84375\n",
      "At: 909 [==========>] Loss 0.10840984689146893  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.1170537426546657  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.1324477037593107  - accuracy: 0.875\n",
      "At: 912 [==========>] Loss 0.12495054392898834  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.11073308468839599  - accuracy: 0.875\n",
      "At: 914 [==========>] Loss 0.14411219532316052  - accuracy: 0.78125\n",
      "At: 915 [==========>] Loss 0.1485496714283611  - accuracy: 0.8125\n",
      "At: 916 [==========>] Loss 0.18026979070756838  - accuracy: 0.71875\n",
      "At: 917 [==========>] Loss 0.1654251206069034  - accuracy: 0.78125\n",
      "At: 918 [==========>] Loss 0.16109115678170027  - accuracy: 0.6875\n",
      "At: 919 [==========>] Loss 0.10477867734697101  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.11725719235930734  - accuracy: 0.875\n",
      "At: 921 [==========>] Loss 0.15313656429892225  - accuracy: 0.71875\n",
      "At: 922 [==========>] Loss 0.16108333489876836  - accuracy: 0.75\n",
      "At: 923 [==========>] Loss 0.10312754707974822  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.1794153136658415  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.17120874937052086  - accuracy: 0.75\n",
      "At: 926 [==========>] Loss 0.16314793544267336  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.12614380688719545  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.11819176396483594  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.15968472062491507  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.13325728387488756  - accuracy: 0.8125\n",
      "At: 931 [==========>] Loss 0.15443364967537052  - accuracy: 0.8125\n",
      "At: 932 [==========>] Loss 0.10427650436956543  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.10199236846379972  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.15030156016086058  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.06984157314256523  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.13621478757288516  - accuracy: 0.84375\n",
      "At: 937 [==========>] Loss 0.13882829953603762  - accuracy: 0.84375\n",
      "At: 938 [==========>] Loss 0.128547501095523  - accuracy: 0.8125\n",
      "At: 939 [==========>] Loss 0.10849989116172917  - accuracy: 0.8125\n",
      "At: 940 [==========>] Loss 0.21397803007195307  - accuracy: 0.75\n",
      "At: 941 [==========>] Loss 0.09359050749955473  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.16229763092319496  - accuracy: 0.78125\n",
      "At: 943 [==========>] Loss 0.08439590290352622  - accuracy: 0.90625\n",
      "At: 944 [==========>] Loss 0.09821945359760874  - accuracy: 0.90625\n",
      "At: 945 [==========>] Loss 0.09919565986923994  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.1103698669139264  - accuracy: 0.875\n",
      "At: 947 [==========>] Loss 0.14803393788258695  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.1743644337488239  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.100827794394935  - accuracy: 0.84375\n",
      "At: 950 [==========>] Loss 0.08589007981607144  - accuracy: 0.9375\n",
      "At: 951 [==========>] Loss 0.11008139457456206  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.09352611310461047  - accuracy: 0.84375\n",
      "At: 953 [==========>] Loss 0.06987650712381517  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.12574404331452327  - accuracy: 0.8125\n",
      "At: 955 [==========>] Loss 0.14744357339509745  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.0713735133364224  - accuracy: 0.96875\n",
      "At: 957 [==========>] Loss 0.10276396726847509  - accuracy: 0.90625\n",
      "At: 958 [==========>] Loss 0.08474739352640952  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.12412800848787137  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.143736047097874  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.13224043825550227  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.08648201271559816  - accuracy: 0.875\n",
      "At: 963 [==========>] Loss 0.10133943660549218  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.16574223314431122  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.11417509899487102  - accuracy: 0.90625\n",
      "At: 966 [==========>] Loss 0.16793192204339275  - accuracy: 0.78125\n",
      "At: 967 [==========>] Loss 0.1232491514946556  - accuracy: 0.78125\n",
      "At: 968 [==========>] Loss 0.15219777197891188  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.14275839810449711  - accuracy: 0.8125\n",
      "At: 970 [==========>] Loss 0.10334829434747284  - accuracy: 0.8125\n",
      "At: 971 [==========>] Loss 0.11446289399288914  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.07638815474296515  - accuracy: 0.875\n",
      "At: 973 [==========>] Loss 0.11051531586523253  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.07843413473091311  - accuracy: 0.9375\n",
      "At: 975 [==========>] Loss 0.13385914754668699  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.11445709240913252  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.07882181306876931  - accuracy: 0.90625\n",
      "At: 978 [==========>] Loss 0.1853430477640327  - accuracy: 0.78125\n",
      "At: 979 [==========>] Loss 0.10419267938339372  - accuracy: 0.90625\n",
      "At: 980 [==========>] Loss 0.15447966739065372  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.1802130785448718  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.07353585584599852  - accuracy: 0.875\n",
      "At: 983 [==========>] Loss 0.13838601653078747  - accuracy: 0.8125\n",
      "At: 984 [==========>] Loss 0.0844312220719016  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.14517764030333863  - accuracy: 0.84375\n",
      "At: 986 [==========>] Loss 0.13794960767078673  - accuracy: 0.75\n",
      "At: 987 [==========>] Loss 0.13926686966366164  - accuracy: 0.8125\n",
      "At: 988 [==========>] Loss 0.11653723566510918  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.14950352411055834  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.1351538077393799  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.13646585986243592  - accuracy: 0.8125\n",
      "At: 992 [==========>] Loss 0.24669436172374068  - accuracy: 0.625\n",
      "At: 993 [==========>] Loss 0.11978041303201589  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.14644006219775124  - accuracy: 0.78125\n",
      "At: 995 [==========>] Loss 0.15581741070074157  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.0507644265962779  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.14402732639631877  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.11521348809190599  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.1359541999198689  - accuracy: 0.8125\n",
      "At: 1000 [==========>] Loss 0.21101489197854834  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.1263595469927018  - accuracy: 0.8125\n",
      "At: 1002 [==========>] Loss 0.2197056450495532  - accuracy: 0.75\n",
      "At: 1003 [==========>] Loss 0.13948325379420945  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.13055115305167778  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.08697442402138661  - accuracy: 0.9375\n",
      "At: 1006 [==========>] Loss 0.10755088019975753  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.1303891497085185  - accuracy: 0.84375\n",
      "At: 1008 [==========>] Loss 0.1587028704077004  - accuracy: 0.78125\n",
      "At: 1009 [==========>] Loss 0.105084348916606  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.13360075376796526  - accuracy: 0.8125\n",
      "At: 1011 [==========>] Loss 0.16972222991594532  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.0846142654107761  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.08936237668981059  - accuracy: 0.8125\n",
      "At: 1014 [==========>] Loss 0.09068782709988359  - accuracy: 0.9375\n",
      "At: 1015 [==========>] Loss 0.18682995046779688  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.14702519767507777  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.16280036967313566  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.17693849687794622  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.1808441323009251  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.14705136800053972  - accuracy: 0.8125\n",
      "At: 1021 [==========>] Loss 0.13566462809583593  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.1023954889600033  - accuracy: 0.90625\n",
      "At: 1023 [==========>] Loss 0.16329071264664213  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.2094332295970759  - accuracy: 0.6875\n",
      "At: 1025 [==========>] Loss 0.2227617130346336  - accuracy: 0.6875\n",
      "At: 1026 [==========>] Loss 0.11125162231177874  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.12404361626073765  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.21747878795499004  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.07995125366398283  - accuracy: 0.90625\n",
      "At: 1030 [==========>] Loss 0.1015016487662429  - accuracy: 0.875\n",
      "At: 1031 [==========>] Loss 0.19694620339105318  - accuracy: 0.71875\n",
      "At: 1032 [==========>] Loss 0.1258908816742406  - accuracy: 0.8125\n",
      "At: 1033 [==========>] Loss 0.13481941964971952  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.07757293421590712  - accuracy: 0.96875\n",
      "At: 1035 [==========>] Loss 0.08946479458406904  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.15292451027425252  - accuracy: 0.84375\n",
      "At: 1037 [==========>] Loss 0.13272796400408482  - accuracy: 0.84375\n",
      "At: 1038 [==========>] Loss 0.09854481067916715  - accuracy: 0.90625\n",
      "At: 1039 [==========>] Loss 0.08974304459703814  - accuracy: 0.9375\n",
      "At: 1040 [==========>] Loss 0.11883243279060526  - accuracy: 0.8125\n",
      "At: 1041 [==========>] Loss 0.1283646797216606  - accuracy: 0.84375\n",
      "At: 1042 [==========>] Loss 0.09056033092897056  - accuracy: 0.875\n",
      "At: 1043 [==========>] Loss 0.22350632395278466  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.12313302233027199  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.15630903451808853  - accuracy: 0.78125\n",
      "At: 1046 [==========>] Loss 0.16668251198480824  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.1365621062097425  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.1820020992577126  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.13411593927398271  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.14041369716593893  - accuracy: 0.78125\n",
      "At: 1051 [==========>] Loss 0.05997179046518276  - accuracy: 0.9375\n",
      "At: 1052 [==========>] Loss 0.11205779740930681  - accuracy: 0.875\n",
      "At: 1053 [==========>] Loss 0.09472426971103909  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.07931497356667341  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.19479905017505955  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.11712990680096685  - accuracy: 0.84375\n",
      "At: 1057 [==========>] Loss 0.10906049388659406  - accuracy: 0.84375\n",
      "At: 1058 [==========>] Loss 0.03802601570918884  - accuracy: 0.96875\n",
      "At: 1059 [==========>] Loss 0.07626637621712218  - accuracy: 0.90625\n",
      "At: 1060 [==========>] Loss 0.11987098995187223  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.09443335876631492  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.17751767329568846  - accuracy: 0.75\n",
      "At: 1063 [==========>] Loss 0.14626789269428175  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.13662404170002576  - accuracy: 0.84375\n",
      "At: 1065 [==========>] Loss 0.0920414118039731  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.08965913899914552  - accuracy: 0.875\n",
      "At: 1067 [==========>] Loss 0.11577392631141334  - accuracy: 0.78125\n",
      "At: 1068 [==========>] Loss 0.09193607753860047  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.10100422308942766  - accuracy: 0.90625\n",
      "At: 1070 [==========>] Loss 0.13984466889605085  - accuracy: 0.84375\n",
      "At: 1071 [==========>] Loss 0.09343514280965332  - accuracy: 0.875\n",
      "At: 1072 [==========>] Loss 0.13726595649149936  - accuracy: 0.78125\n",
      "At: 1073 [==========>] Loss 0.1529886038260591  - accuracy: 0.84375\n",
      "At: 1074 [==========>] Loss 0.17874362746529404  - accuracy: 0.75\n",
      "At: 1075 [==========>] Loss 0.09012623585504374  - accuracy: 0.875\n",
      "At: 1076 [==========>] Loss 0.1422166060997094  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.08661193564573308  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.10381943950897587  - accuracy: 0.84375\n",
      "At: 1079 [==========>] Loss 0.1324386040306853  - accuracy: 0.78125\n",
      "At: 1080 [==========>] Loss 0.13548965092681656  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.11285691341836222  - accuracy: 0.78125\n",
      "At: 1082 [==========>] Loss 0.10083910158925857  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.09035145435315509  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.09045988171278663  - accuracy: 0.9375\n",
      "At: 1085 [==========>] Loss 0.12057477463744377  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.12733253936205335  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.1332676282858039  - accuracy: 0.84375\n",
      "At: 1088 [==========>] Loss 0.1608846710211461  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.09709358175736563  - accuracy: 0.90625\n",
      "At: 1090 [==========>] Loss 0.09922509320701853  - accuracy: 0.8125\n",
      "At: 1091 [==========>] Loss 0.21871345656491464  - accuracy: 0.71875\n",
      "At: 1092 [==========>] Loss 0.1291353356542111  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.13224058352007378  - accuracy: 0.84375\n",
      "At: 1094 [==========>] Loss 0.14810229222290994  - accuracy: 0.78125\n",
      "At: 1095 [==========>] Loss 0.13530246241767646  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.11681422909249391  - accuracy: 0.8125\n",
      "At: 1097 [==========>] Loss 0.08234985246348053  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.12729755079195443  - accuracy: 0.8125\n",
      "At: 1099 [==========>] Loss 0.1251266050738444  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.08514022978849878  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.09619833021897116  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.12943784645938314  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.1258864431954445  - accuracy: 0.8125\n",
      "At: 1104 [==========>] Loss 0.07550048137059359  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.10597527489005812  - accuracy: 0.84375\n",
      "At: 1106 [==========>] Loss 0.06096829285044862  - accuracy: 0.90625\n",
      "At: 1107 [==========>] Loss 0.21518984588547246  - accuracy: 0.65625\n",
      "At: 1108 [==========>] Loss 0.08096846145140957  - accuracy: 0.875\n",
      "At: 1109 [==========>] Loss 0.04555805560182313  - accuracy: 0.96875\n",
      "At: 1110 [==========>] Loss 0.11469009582368254  - accuracy: 0.84375\n",
      "At: 1111 [==========>] Loss 0.17999122619952881  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.1781568651857234  - accuracy: 0.78125\n",
      "At: 1113 [==========>] Loss 0.14088098905621546  - accuracy: 0.78125\n",
      "At: 1114 [==========>] Loss 0.09824555295762696  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.12135291116672683  - accuracy: 0.875\n",
      "At: 1116 [==========>] Loss 0.10452206169809376  - accuracy: 0.90625\n",
      "At: 1117 [==========>] Loss 0.060521321708621206  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.12697293270657245  - accuracy: 0.75\n",
      "At: 1119 [==========>] Loss 0.13475526810410016  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.07256248999012782  - accuracy: 0.96875\n",
      "At: 1121 [==========>] Loss 0.08719876230098994  - accuracy: 0.875\n",
      "At: 1122 [==========>] Loss 0.07166438378662048  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.11100306078745091  - accuracy: 0.84375\n",
      "At: 1124 [==========>] Loss 0.1255323891051086  - accuracy: 0.8125\n",
      "At: 1125 [==========>] Loss 0.18202854408415925  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.1183386526496891  - accuracy: 0.8125\n",
      "At: 1127 [==========>] Loss 0.13474759888881307  - accuracy: 0.8125\n",
      "At: 1128 [==========>] Loss 0.0700933714717015  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.17983361310261148  - accuracy: 0.75\n",
      "At: 1130 [==========>] Loss 0.11430032869818571  - accuracy: 0.90625\n",
      "At: 1131 [==========>] Loss 0.1116304358254805  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.09665183055462295  - accuracy: 0.875\n",
      "At: 1133 [==========>] Loss 0.14268715230589923  - accuracy: 0.8125\n",
      "At: 1134 [==========>] Loss 0.11427529078440131  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.13337960378974484  - accuracy: 0.84375\n",
      "At: 1136 [==========>] Loss 0.16897832581048589  - accuracy: 0.75\n",
      "At: 1137 [==========>] Loss 0.09992782475776626  - accuracy: 0.875\n",
      "At: 1138 [==========>] Loss 0.12208159504545567  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.055482345556967426  - accuracy: 0.9375\n",
      "At: 1140 [==========>] Loss 0.15646068276680153  - accuracy: 0.8125\n",
      "At: 1141 [==========>] Loss 0.13742869083561654  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.1512659101544863  - accuracy: 0.84375\n",
      "At: 1143 [==========>] Loss 0.06956438472640766  - accuracy: 0.875\n",
      "At: 1144 [==========>] Loss 0.11138686881022158  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.13110648771457545  - accuracy: 0.875\n",
      "At: 1146 [==========>] Loss 0.11709327675472259  - accuracy: 0.84375\n",
      "At: 1147 [==========>] Loss 0.19922064338436218  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.09151136652739143  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.10274279449945188  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.13310325044220903  - accuracy: 0.84375\n",
      "At: 1151 [==========>] Loss 0.1802031699410686  - accuracy: 0.71875\n",
      "At: 1152 [==========>] Loss 0.10717370640083529  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.16083095514052795  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.08437677328050547  - accuracy: 0.9375\n",
      "At: 1155 [==========>] Loss 0.1013911618710152  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.17025744814428964  - accuracy: 0.6875\n",
      "At: 1157 [==========>] Loss 0.13010985126533237  - accuracy: 0.8125\n",
      "At: 1158 [==========>] Loss 0.15722023829945003  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.09911213574119168  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.09618935491590072  - accuracy: 0.84375\n",
      "At: 1161 [==========>] Loss 0.07808207933419672  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.10373253795253647  - accuracy: 0.875\n",
      "At: 1163 [==========>] Loss 0.15819551221382494  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.09335196506025611  - accuracy: 0.84375\n",
      "At: 1165 [==========>] Loss 0.15078806262111344  - accuracy: 0.75\n",
      "At: 1166 [==========>] Loss 0.07032254969644801  - accuracy: 0.96875\n",
      "At: 1167 [==========>] Loss 0.13165023699181994  - accuracy: 0.8125\n",
      "At: 1168 [==========>] Loss 0.10757631392060191  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.12232165518719477  - accuracy: 0.8125\n",
      "At: 1170 [==========>] Loss 0.1417318803686642  - accuracy: 0.8125\n",
      "At: 1171 [==========>] Loss 0.05532891654984482  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.10923644302404559  - accuracy: 0.875\n",
      "At: 1173 [==========>] Loss 0.13337177290961666  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.19047780307981083  - accuracy: 0.71875\n",
      "At: 1175 [==========>] Loss 0.13919341500619536  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.11486805930450586  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.08299638305220537  - accuracy: 0.90625\n",
      "At: 1178 [==========>] Loss 0.1599407671678631  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.12371256470722813  - accuracy: 0.84375\n",
      "At: 1180 [==========>] Loss 0.19888655452780224  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.08458874121303347  - accuracy: 0.84375\n",
      "At: 1182 [==========>] Loss 0.11812595887568939  - accuracy: 0.8125\n",
      "At: 1183 [==========>] Loss 0.15983958722776795  - accuracy: 0.8125\n",
      "At: 1184 [==========>] Loss 0.15264781559808385  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.09057930751930982  - accuracy: 0.90625\n",
      "At: 1186 [==========>] Loss 0.14799703765675348  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.11555919811194795  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.08265006581392931  - accuracy: 0.90625\n",
      "At: 1189 [==========>] Loss 0.14789816312144355  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.10358762086873474  - accuracy: 0.90625\n",
      "At: 1191 [==========>] Loss 0.17397786021899284  - accuracy: 0.78125\n",
      "At: 1192 [==========>] Loss 0.10580544211440297  - accuracy: 0.8125\n",
      "At: 1193 [==========>] Loss 0.12937431521093154  - accuracy: 0.78125\n",
      "At: 1194 [==========>] Loss 0.14453966024153375  - accuracy: 0.78125\n",
      "At: 1195 [==========>] Loss 0.12518355605514822  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.11603576105723065  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.08523859224897729  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.11791342024872839  - accuracy: 0.8125\n",
      "At: 1199 [==========>] Loss 0.1835988623610585  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.08246268508327928  - accuracy: 0.90625\n",
      "At: 1201 [==========>] Loss 0.11044623024011667  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.16651098911834902  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.1267016197215546  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.09357273827878917  - accuracy: 0.84375\n",
      "At: 1205 [==========>] Loss 0.0707985890563358  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.12571233038761864  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.1625635002208338  - accuracy: 0.8125\n",
      "At: 1208 [==========>] Loss 0.09619155261861699  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.12162422967286723  - accuracy: 0.84375\n",
      "At: 1210 [==========>] Loss 0.1623066033020483  - accuracy: 0.8125\n",
      "At: 1211 [==========>] Loss 0.1889554882743724  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.10832644567478941  - accuracy: 0.875\n",
      "At: 1213 [==========>] Loss 0.16970438153696968  - accuracy: 0.78125\n",
      "At: 1214 [==========>] Loss 0.15872081754829653  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.12513396664940807  - accuracy: 0.8125\n",
      "At: 1216 [==========>] Loss 0.09093163230657418  - accuracy: 0.90625\n",
      "At: 1217 [==========>] Loss 0.08632188727576642  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.08886576491163325  - accuracy: 0.9375\n",
      "At: 1219 [==========>] Loss 0.14937581442324804  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.13014340848070222  - accuracy: 0.78125\n",
      "At: 1221 [==========>] Loss 0.0781687629294411  - accuracy: 0.90625\n",
      "At: 1222 [==========>] Loss 0.19391991181297888  - accuracy: 0.625\n",
      "At: 1223 [==========>] Loss 0.09424602950693066  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08596899029119014  - accuracy: 0.875\n",
      "At: 1225 [==========>] Loss 0.07008729086684584  - accuracy: 0.96875\n",
      "At: 1226 [==========>] Loss 0.09496071945318058  - accuracy: 0.90625\n",
      "At: 1227 [==========>] Loss 0.1748295136310393  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.16829523198817387  - accuracy: 0.71875\n",
      "At: 1229 [==========>] Loss 0.10721536110837797  - accuracy: 0.8125\n",
      "At: 1230 [==========>] Loss 0.19808251372260338  - accuracy: 0.71875\n",
      "At: 1231 [==========>] Loss 0.1423347828730023  - accuracy: 0.75\n",
      "At: 1232 [==========>] Loss 0.07653239660515722  - accuracy: 0.96875\n",
      "At: 1233 [==========>] Loss 0.09573171973585473  - accuracy: 0.84375\n",
      "At: 1234 [==========>] Loss 0.12578859922366292  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.07091295786589365  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.11544066930649566  - accuracy: 0.875\n",
      "At: 1237 [==========>] Loss 0.08414471893533068  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.10385507106326436  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.15351243663230357  - accuracy: 0.78125\n",
      "At: 1240 [==========>] Loss 0.10230231992897733  - accuracy: 0.90625\n",
      "At: 1241 [==========>] Loss 0.08763165945330256  - accuracy: 0.875\n",
      "At: 1242 [==========>] Loss 0.13701857971908285  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.14831290953812581  - accuracy: 0.78125\n",
      "At: 1244 [==========>] Loss 0.14960300246201597  - accuracy: 0.75\n",
      "At: 1245 [==========>] Loss 0.10382581382517556  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.0698918044037957  - accuracy: 0.9375\n",
      "At: 1247 [==========>] Loss 0.1676717456114188  - accuracy: 0.8125\n",
      "At: 1248 [==========>] Loss 0.09529350740861711  - accuracy: 0.90625\n",
      "At: 1249 [==========>] Loss 0.13441666069783892  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.1183039673459676  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.11601161587492376  - accuracy: 0.84375\n",
      "At: 1252 [==========>] Loss 0.08292666203571437  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.14431193938509873  - accuracy: 0.8125\n",
      "At: 1254 [==========>] Loss 0.16759513830736322  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.08615121384511507  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.1201112516972418  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.11884086426232571  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.0790664394411919  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.14965009648951605  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.0958100316641112  - accuracy: 0.84375\n",
      "At: 1261 [==========>] Loss 0.09725215835360215  - accuracy: 0.875\n",
      "At: 1262 [==========>] Loss 0.15715055659593952  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.10839729356529539  - accuracy: 0.875\n",
      "At: 1264 [==========>] Loss 0.07134812105487837  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.1269702678422223  - accuracy: 0.78125\n",
      "At: 1266 [==========>] Loss 0.09831394641144259  - accuracy: 0.84375\n",
      "At: 1267 [==========>] Loss 0.10775745082442488  - accuracy: 0.84375\n",
      "At: 1268 [==========>] Loss 0.13081678798183505  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.10822018266962044  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.10786420717585235  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.16040143211606633  - accuracy: 0.75\n",
      "At: 1272 [==========>] Loss 0.05097465457717578  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.21677947924497848  - accuracy: 0.75\n",
      "At: 1274 [==========>] Loss 0.11567408705200687  - accuracy: 0.8125\n",
      "At: 1275 [==========>] Loss 0.09944986344604342  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.11159621525037933  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.107055753564632  - accuracy: 0.78125\n",
      "At: 1278 [==========>] Loss 0.15077131126769916  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.09887036303657895  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.10346998698790956  - accuracy: 0.78125\n",
      "At: 1281 [==========>] Loss 0.15964887424493326  - accuracy: 0.6875\n",
      "At: 1282 [==========>] Loss 0.135798967788813  - accuracy: 0.78125\n",
      "At: 1283 [==========>] Loss 0.1261535625779405  - accuracy: 0.8125\n",
      "At: 1284 [==========>] Loss 0.1655293190967715  - accuracy: 0.84375\n",
      "At: 1285 [==========>] Loss 0.08500342165669278  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.10810315240225911  - accuracy: 0.84375\n",
      "At: 1287 [==========>] Loss 0.13841598709252645  - accuracy: 0.8125\n",
      "At: 1288 [==========>] Loss 0.1694196194346343  - accuracy: 0.6875\n",
      "At: 1289 [==========>] Loss 0.1245239349212738  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.13360246363056139  - accuracy: 0.8125\n",
      "At: 1291 [==========>] Loss 0.16422547692384237  - accuracy: 0.8125\n",
      "At: 1292 [==========>] Loss 0.10451037931586463  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.16277129094487702  - accuracy: 0.71875\n",
      "At: 1294 [==========>] Loss 0.11771213506940113  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.16292347454626038  - accuracy: 0.8125\n",
      "At: 1296 [==========>] Loss 0.13996432996216948  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.13708051045524094  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.0917717787421268  - accuracy: 0.875\n",
      "At: 1299 [==========>] Loss 0.1344363845169444  - accuracy: 0.84375\n",
      "At: 1300 [==========>] Loss 0.14871319467754207  - accuracy: 0.75\n",
      "At: 1301 [==========>] Loss 0.12825292693915408  - accuracy: 0.8125\n",
      "At: 1302 [==========>] Loss 0.0942144100327574  - accuracy: 0.84375\n",
      "At: 1303 [==========>] Loss 0.08953611975567333  - accuracy: 0.90625\n",
      "At: 1304 [==========>] Loss 0.12912723637275692  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.127348880981291  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.06251316152727186  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.16232664900933522  - accuracy: 0.71875\n",
      "At: 1308 [==========>] Loss 0.08289681653139622  - accuracy: 0.875\n",
      "At: 1309 [==========>] Loss 0.14385782893829252  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.1482851616693483  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.14736511112330927  - accuracy: 0.84375\n",
      "At: 1312 [==========>] Loss 0.10312700005905052  - accuracy: 0.875\n",
      "At: 1313 [==========>] Loss 0.15447796556376558  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.072373865616958  - accuracy: 0.84375\n",
      "At: 1315 [==========>] Loss 0.15341867246958554  - accuracy: 0.6875\n",
      "At: 1316 [==========>] Loss 0.1274239066628743  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.1064801043826284  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.09798918931316489  - accuracy: 0.875\n",
      "At: 1319 [==========>] Loss 0.11754726173785086  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.1643559685466711  - accuracy: 0.75\n",
      "At: 1321 [==========>] Loss 0.08695222983265657  - accuracy: 0.8125\n",
      "At: 1322 [==========>] Loss 0.14129890440907655  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.06503727375227553  - accuracy: 0.96875\n",
      "At: 1324 [==========>] Loss 0.1435524244347573  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.0998352797330303  - accuracy: 0.84375\n",
      "At: 1326 [==========>] Loss 0.08211323855665319  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.1528724928297417  - accuracy: 0.75\n",
      "At: 1328 [==========>] Loss 0.11109616965875657  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.05967391776068  - accuracy: 0.90625\n",
      "At: 1330 [==========>] Loss 0.12597977607583727  - accuracy: 0.78125\n",
      "At: 1331 [==========>] Loss 0.1955381400279778  - accuracy: 0.65625\n",
      "At: 1332 [==========>] Loss 0.10863983067159733  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.14212735514838964  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.10388920973612342  - accuracy: 0.90625\n",
      "At: 1335 [==========>] Loss 0.11431482258315477  - accuracy: 0.84375\n",
      "At: 1336 [==========>] Loss 0.11616806027226693  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.13678555790703584  - accuracy: 0.875\n",
      "At: 1338 [==========>] Loss 0.15626847070575808  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.10421082703874042  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.16240230654834764  - accuracy: 0.75\n",
      "At: 1341 [==========>] Loss 0.12073349089376906  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.14062980244491374  - accuracy: 0.78125\n",
      "At: 1343 [==========>] Loss 0.17685913842752193  - accuracy: 0.71875\n",
      "At: 1344 [==========>] Loss 0.16112479537987978  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.10469136850540059  - accuracy: 0.875\n",
      "At: 1346 [==========>] Loss 0.08242857162724254  - accuracy: 0.9375\n",
      "At: 1347 [==========>] Loss 0.09707571118560153  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.11198888003875591  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.155616916848891  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.11828612082053638  - accuracy: 0.875\n",
      "At: 1351 [==========>] Loss 0.08895059561389149  - accuracy: 0.875\n",
      "At: 1352 [==========>] Loss 0.08566258994582526  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.18474132035347463  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.18480962975285326  - accuracy: 0.71875\n",
      "At: 1355 [==========>] Loss 0.07344341924801694  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.07944165201432062  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.11983168569424775  - accuracy: 0.875\n",
      "At: 1358 [==========>] Loss 0.12584040795769638  - accuracy: 0.75\n",
      "At: 1359 [==========>] Loss 0.07824670740100384  - accuracy: 0.90625\n",
      "At: 1360 [==========>] Loss 0.15143752043362158  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.11749245839208097  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.11567692796944178  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.11053431874418593  - accuracy: 0.90625\n",
      "At: 1364 [==========>] Loss 0.1374315637054629  - accuracy: 0.78125\n",
      "At: 1365 [==========>] Loss 0.14152498211657935  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.16878711303619376  - accuracy: 0.71875\n",
      "At: 1367 [==========>] Loss 0.10184766803938627  - accuracy: 0.90625\n",
      "At: 1368 [==========>] Loss 0.14571638997158284  - accuracy: 0.78125\n",
      "At: 1369 [==========>] Loss 0.09030335531283176  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.11344657196711275  - accuracy: 0.875\n",
      "At: 1371 [==========>] Loss 0.2125764027238968  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.1112052450814341  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.13888153168286843  - accuracy: 0.8125\n",
      "At: 1374 [==========>] Loss 0.12971869791451418  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.11001953398146214  - accuracy: 0.875\n",
      "At: 1376 [==========>] Loss 0.08149463364193242  - accuracy: 0.90625\n",
      "At: 1377 [==========>] Loss 0.14016587594349086  - accuracy: 0.84375\n",
      "At: 1378 [==========>] Loss 0.12489316692609234  - accuracy: 0.8125\n",
      "At: 1379 [==========>] Loss 0.16744892117766919  - accuracy: 0.71875\n",
      "At: 1380 [==========>] Loss 0.1355820962536774  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.09020951849126807  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.14762912329771585  - accuracy: 0.71875\n",
      "At: 1383 [==========>] Loss 0.10790621557015506  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.0980396015440355  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.17990572909799712  - accuracy: 0.75\n",
      "At: 1386 [==========>] Loss 0.19776066689107702  - accuracy: 0.6875\n",
      "At: 1387 [==========>] Loss 0.07808984029890716  - accuracy: 0.875\n",
      "At: 1388 [==========>] Loss 0.15654491299897477  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.10706198021564262  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.14851081970400973  - accuracy: 0.84375\n",
      "At: 1391 [==========>] Loss 0.10163786902964478  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.06853861283403495  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.13525570886974456  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.07688881120268762  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.2202831843430234  - accuracy: 0.6875\n",
      "At: 1396 [==========>] Loss 0.048864364532267626  - accuracy: 0.96875\n",
      "At: 1397 [==========>] Loss 0.12831468824104356  - accuracy: 0.8125\n",
      "At: 1398 [==========>] Loss 0.12351658072438293  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.10659337838305288  - accuracy: 0.90625\n",
      "At: 1400 [==========>] Loss 0.1519722307550379  - accuracy: 0.75\n",
      "At: 1401 [==========>] Loss 0.11085988711031883  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.17703189272593037  - accuracy: 0.75\n",
      "At: 1403 [==========>] Loss 0.11854738431264097  - accuracy: 0.8125\n",
      "At: 1404 [==========>] Loss 0.11259000214051763  - accuracy: 0.875\n",
      "At: 1405 [==========>] Loss 0.08442717325200425  - accuracy: 0.875\n",
      "At: 1406 [==========>] Loss 0.15421168565720075  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.12605086158101586  - accuracy: 0.875\n",
      "At: 1408 [==========>] Loss 0.12706494025841508  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.024707607016972184  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.11735787687057991  - accuracy: 0.84375\n",
      "At: 1411 [==========>] Loss 0.13455190013996113  - accuracy: 0.84375\n",
      "At: 1412 [==========>] Loss 0.1436448259787047  - accuracy: 0.75\n",
      "At: 1413 [==========>] Loss 0.10689403722298982  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.1811758915107688  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.07826270681710004  - accuracy: 0.90625\n",
      "At: 1416 [==========>] Loss 0.17445225191349456  - accuracy: 0.75\n",
      "At: 1417 [==========>] Loss 0.11949630289020474  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.15671283092953742  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.13234972405513085  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.1120291355956895  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.10014591672182468  - accuracy: 0.84375\n",
      "At: 1422 [==========>] Loss 0.13897751965561891  - accuracy: 0.875\n",
      "At: 1423 [==========>] Loss 0.12682991750053968  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.11316844794099704  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.08233545832167696  - accuracy: 0.9375\n",
      "At: 1426 [==========>] Loss 0.11957895233618718  - accuracy: 0.875\n",
      "At: 1427 [==========>] Loss 0.12081760841119807  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.10846772050515777  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.1496547460422187  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.0725637662091334  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.10755081340554278  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.09388913970532564  - accuracy: 0.875\n",
      "At: 1433 [==========>] Loss 0.10660106198302367  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.1588240119114593  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.1319521322949334  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.05793278975641072  - accuracy: 0.96875\n",
      "At: 1437 [==========>] Loss 0.1407740452056609  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.17896791041313606  - accuracy: 0.75\n",
      "At: 1439 [==========>] Loss 0.11180820358364976  - accuracy: 0.84375\n",
      "At: 1440 [==========>] Loss 0.1210096281874383  - accuracy: 0.78125\n",
      "At: 1441 [==========>] Loss 0.0671305989849494  - accuracy: 0.96875\n",
      "At: 1442 [==========>] Loss 0.12748595726832257  - accuracy: 0.84375\n",
      "At: 1443 [==========>] Loss 0.13019797855576654  - accuracy: 0.84375\n",
      "At: 1444 [==========>] Loss 0.12588393013542504  - accuracy: 0.8125\n",
      "At: 1445 [==========>] Loss 0.1824311904331585  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.1849733396093762  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.1685540647599294  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.056587019133165085  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.14722449412242805  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.1379430773135424  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.13835494719363345  - accuracy: 0.78125\n",
      "At: 1452 [==========>] Loss 0.0840308231340148  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.05202108234195203  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.1576130391557904  - accuracy: 0.75\n",
      "At: 1455 [==========>] Loss 0.1039953278428601  - accuracy: 0.875\n",
      "At: 1456 [==========>] Loss 0.1004270571660544  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.08390457251782137  - accuracy: 0.9375\n",
      "At: 1458 [==========>] Loss 0.1447446351038665  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.12458515326938921  - accuracy: 0.90625\n",
      "At: 1460 [==========>] Loss 0.16405673040305838  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.12153830021809095  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.176953485678237  - accuracy: 0.78125\n",
      "At: 1463 [==========>] Loss 0.08051272311914359  - accuracy: 0.9375\n",
      "At: 1464 [==========>] Loss 0.15629103114835083  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.10090073033737569  - accuracy: 0.875\n",
      "At: 1466 [==========>] Loss 0.09034886471511766  - accuracy: 0.875\n",
      "At: 1467 [==========>] Loss 0.18869489429768171  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.19538466294144582  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.1697762790205088  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.13057738799075186  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.1609357940622334  - accuracy: 0.75\n",
      "At: 1472 [==========>] Loss 0.10394048343830216  - accuracy: 0.90625\n",
      "At: 1473 [==========>] Loss 0.12434605737105421  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.21774951312367866  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.1288548443649824  - accuracy: 0.75\n",
      "At: 1476 [==========>] Loss 0.10542445995237942  - accuracy: 0.875\n",
      "At: 1477 [==========>] Loss 0.09127100895770064  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.10747843950598389  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.14293245590704082  - accuracy: 0.8125\n",
      "At: 1480 [==========>] Loss 0.08142725121777697  - accuracy: 0.90625\n",
      "At: 1481 [==========>] Loss 0.14148281094429557  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.0932625807574622  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.22067693372783997  - accuracy: 0.6875\n",
      "At: 1484 [==========>] Loss 0.14473878378928362  - accuracy: 0.8125\n",
      "At: 1485 [==========>] Loss 0.17856523955150969  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.10632836120863143  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.07620812546640918  - accuracy: 0.90625\n",
      "At: 1488 [==========>] Loss 0.14234174195835123  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.23153953192671706  - accuracy: 0.65625\n",
      "At: 1490 [==========>] Loss 0.10107099095129923  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.14814190646570655  - accuracy: 0.8125\n",
      "At: 1492 [==========>] Loss 0.14668899979169472  - accuracy: 0.78125\n",
      "At: 1493 [==========>] Loss 0.14819090214182468  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.16369270526054097  - accuracy: 0.71875\n",
      "At: 1495 [==========>] Loss 0.11867766725201184  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.09514616393924478  - accuracy: 0.84375\n",
      "At: 1497 [==========>] Loss 0.16018117942254945  - accuracy: 0.8125\n",
      "At: 1498 [==========>] Loss 0.15955907731275504  - accuracy: 0.78125\n",
      "At: 1499 [==========>] Loss 0.10728238763874785  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.07188025381782528  - accuracy: 0.875\n",
      "At: 1501 [==========>] Loss 0.10740237442314968  - accuracy: 0.84375\n",
      "At: 1502 [==========>] Loss 0.12755800184069677  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.14033352044983113  - accuracy: 0.75\n",
      "At: 1504 [==========>] Loss 0.13798556036248064  - accuracy: 0.84375\n",
      "At: 1505 [==========>] Loss 0.11677393231978497  - accuracy: 0.84375\n",
      "At: 1506 [==========>] Loss 0.15387374648978683  - accuracy: 0.8125\n",
      "At: 1507 [==========>] Loss 0.1349436957021925  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.21298877220731696  - accuracy: 0.65625\n",
      "At: 1509 [==========>] Loss 0.06324568537749925  - accuracy: 0.9375\n",
      "At: 1510 [==========>] Loss 0.10974097511932114  - accuracy: 0.84375\n",
      "At: 1511 [==========>] Loss 0.1367586717935522  - accuracy: 0.8125\n",
      "At: 1512 [==========>] Loss 0.09467452993724235  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.1510980994605784  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.1440810809485713  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.15287095764217623  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.12510238170067595  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.13369899356480333  - accuracy: 0.875\n",
      "At: 1518 [==========>] Loss 0.11706526952126936  - accuracy: 0.8125\n",
      "At: 1519 [==========>] Loss 0.13019680035274003  - accuracy: 0.84375\n",
      "At: 1520 [==========>] Loss 0.10117822161784187  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.0818362036504153  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.17405473295373555  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.10135193436772387  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.14781682580701733  - accuracy: 0.84375\n",
      "At: 1525 [==========>] Loss 0.10623942955081003  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.0991162323163225  - accuracy: 0.875\n",
      "At: 1527 [==========>] Loss 0.13741189322975428  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.15383249122112674  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.07723654003155822  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.05103935151390357  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.12300689212962243  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.1812254536720549  - accuracy: 0.78125\n",
      "At: 1533 [==========>] Loss 0.16798257878597084  - accuracy: 0.75\n",
      "At: 1534 [==========>] Loss 0.10124963875194867  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.13306528297087233  - accuracy: 0.8125\n",
      "At: 1536 [==========>] Loss 0.15250021744409845  - accuracy: 0.78125\n",
      "At: 1537 [==========>] Loss 0.09968848445219974  - accuracy: 0.90625\n",
      "At: 1538 [==========>] Loss 0.1284456581301881  - accuracy: 0.8125\n",
      "At: 1539 [==========>] Loss 0.09029507926251504  - accuracy: 0.84375\n",
      "At: 1540 [==========>] Loss 0.17617712380877548  - accuracy: 0.71875\n",
      "At: 1541 [==========>] Loss 0.11261596445861749  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.06982069723471276  - accuracy: 0.90625\n",
      "At: 1543 [==========>] Loss 0.12276952279418203  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.14758986362342058  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.24123441916276234  - accuracy: 0.6875\n",
      "At: 1546 [==========>] Loss 0.13315267317860927  - accuracy: 0.78125\n",
      "At: 1547 [==========>] Loss 0.13571899077294747  - accuracy: 0.84375\n",
      "At: 1548 [==========>] Loss 0.14834997666691968  - accuracy: 0.78125\n",
      "At: 1549 [==========>] Loss 0.1393600307804473  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.05902226703779104  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.1606089106241354  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.10889986353941378  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.08679714960310314  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.14792182746867127  - accuracy: 0.78125\n",
      "At: 1555 [==========>] Loss 0.1221818534629991  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.18172708888311703  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.08755103211831317  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.13688438167010297  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.08816638325042075  - accuracy: 0.875\n",
      "At: 1560 [==========>] Loss 0.1327345223878193  - accuracy: 0.78125\n",
      "At: 1561 [==========>] Loss 0.1431135800771502  - accuracy: 0.8125\n",
      "At: 1562 [==========>] Loss 0.07054454401875496  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.10005648059299381  - accuracy: 0.9375\n",
      "At: 1564 [==========>] Loss 0.11891438581619347  - accuracy: 0.8125\n",
      "At: 1565 [==========>] Loss 0.1295662244959096  - accuracy: 0.78125\n",
      "At: 1566 [==========>] Loss 0.15251505458967063  - accuracy: 0.84375\n",
      "At: 1567 [==========>] Loss 0.14122855744588014  - accuracy: 0.8125\n",
      "At: 1568 [==========>] Loss 0.10218487965500545  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.11171751098856902  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.08005199511891695  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.14718524482881495  - accuracy: 0.875\n",
      "At: 1572 [==========>] Loss 0.12762734228848682  - accuracy: 0.875\n",
      "At: 1573 [==========>] Loss 0.038426402456218  - accuracy: 0.96875\n",
      "At: 1574 [==========>] Loss 0.12284942272057191  - accuracy: 0.875\n",
      "At: 1575 [==========>] Loss 0.09612393484952922  - accuracy: 0.90625\n",
      "At: 1576 [==========>] Loss 0.16158163348585833  - accuracy: 0.78125\n",
      "At: 1577 [==========>] Loss 0.07720502239328281  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.05121930449059619  - accuracy: 0.9375\n",
      "At: 1579 [==========>] Loss 0.09242861224794643  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.10663760370409044  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.09166335177481334  - accuracy: 0.875\n",
      "At: 1582 [==========>] Loss 0.15037861739916264  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.10391573933524875  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.11685336498971739  - accuracy: 0.90625\n",
      "At: 1585 [==========>] Loss 0.12128168610426408  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.1386803896153247  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.09864037431978262  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.11840967724397816  - accuracy: 0.84375\n",
      "At: 1589 [==========>] Loss 0.17038413272016634  - accuracy: 0.75\n",
      "At: 1590 [==========>] Loss 0.1528405509455748  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.08657575057028563  - accuracy: 0.9375\n",
      "At: 1592 [==========>] Loss 0.05944062667151617  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.16243296315767808  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.09643411972917404  - accuracy: 0.84375\n",
      "At: 1595 [==========>] Loss 0.10827952840707883  - accuracy: 0.875\n",
      "At: 1596 [==========>] Loss 0.17092860414538608  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.1345836108635829  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.15768105104086755  - accuracy: 0.84375\n",
      "At: 1599 [==========>] Loss 0.21779864374462315  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.13802694449600983  - accuracy: 0.875\n",
      "At: 1601 [==========>] Loss 0.07143765327449897  - accuracy: 0.96875\n",
      "At: 1602 [==========>] Loss 0.09207641499306735  - accuracy: 0.875\n",
      "At: 1603 [==========>] Loss 0.18686973869624773  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.24199141654982095  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.07849689985076246  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.11774376886829793  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.18204332892962077  - accuracy: 0.65625\n",
      "At: 1608 [==========>] Loss 0.14625934928962325  - accuracy: 0.84375\n",
      "At: 1609 [==========>] Loss 0.15455531514711257  - accuracy: 0.8125\n",
      "At: 1610 [==========>] Loss 0.17782365063362637  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.06434638100642996  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.07177160802268001  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.1400772777901952  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.13157747218499463  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.10617237190294998  - accuracy: 0.84375\n",
      "At: 1616 [==========>] Loss 0.1092887115510988  - accuracy: 0.875\n",
      "At: 1617 [==========>] Loss 0.10383498449405094  - accuracy: 0.90625\n",
      "At: 1618 [==========>] Loss 0.12121791291178852  - accuracy: 0.8125\n",
      "At: 1619 [==========>] Loss 0.18758204889632538  - accuracy: 0.71875\n",
      "At: 1620 [==========>] Loss 0.10988566366527709  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.11642652320011426  - accuracy: 0.84375\n",
      "At: 1622 [==========>] Loss 0.1728987693657302  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.09404043688224274  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.17092520819189227  - accuracy: 0.71875\n",
      "At: 1625 [==========>] Loss 0.13837775839581673  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.11289022954584009  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.10213906389213509  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.13019793571424265  - accuracy: 0.8125\n",
      "At: 1629 [==========>] Loss 0.15987475134688728  - accuracy: 0.75\n",
      "At: 1630 [==========>] Loss 0.07693269513057988  - accuracy: 0.9375\n",
      "At: 1631 [==========>] Loss 0.11953687131774302  - accuracy: 0.84375\n",
      "At: 1632 [==========>] Loss 0.10287406819949427  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.06908289825594059  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.1127628887201694  - accuracy: 0.875\n",
      "At: 1635 [==========>] Loss 0.214332012133824  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.12267546748152164  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.08016687208512402  - accuracy: 0.84375\n",
      "At: 1638 [==========>] Loss 0.14556416373345976  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.12521557627075147  - accuracy: 0.90625\n",
      "At: 1640 [==========>] Loss 0.1147109150372968  - accuracy: 0.875\n",
      "At: 1641 [==========>] Loss 0.0797629553071878  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.08987568696660046  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.08691744012182652  - accuracy: 0.9375\n",
      "At: 1644 [==========>] Loss 0.10712113729427294  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.049881847486718076  - accuracy: 0.96875\n",
      "At: 1646 [==========>] Loss 0.12706627090121903  - accuracy: 0.875\n",
      "At: 1647 [==========>] Loss 0.17499182603278834  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.11201777558922646  - accuracy: 0.875\n",
      "At: 1649 [==========>] Loss 0.08796963887880148  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.07414121386939654  - accuracy: 0.9375\n",
      "At: 1651 [==========>] Loss 0.13410203281209368  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.08652728879499824  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.06564607120044243  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.06633806021810583  - accuracy: 0.90625\n",
      "At: 1655 [==========>] Loss 0.15805105068734304  - accuracy: 0.875\n",
      "At: 1656 [==========>] Loss 0.10987256699093075  - accuracy: 0.8125\n",
      "At: 1657 [==========>] Loss 0.1418390814046989  - accuracy: 0.71875\n",
      "At: 1658 [==========>] Loss 0.20380958692820053  - accuracy: 0.75\n",
      "At: 1659 [==========>] Loss 0.07819712683159954  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.03520944403193567  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.0877665759879465  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.11512603102734972  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16649520609885857  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.10200184270495624  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.09767843225556866  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.06872479962174656  - accuracy: 0.90625\n",
      "At: 1667 [==========>] Loss 0.1862105600925928  - accuracy: 0.71875\n",
      "At: 1668 [==========>] Loss 0.12969181075246466  - accuracy: 0.75\n",
      "At: 1669 [==========>] Loss 0.12216786724976239  - accuracy: 0.875\n",
      "At: 1670 [==========>] Loss 0.09195307488673246  - accuracy: 0.875\n",
      "At: 1671 [==========>] Loss 0.12325621470707016  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.13406446227705549  - accuracy: 0.75\n",
      "At: 1673 [==========>] Loss 0.06397888286254659  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.17494040982601422  - accuracy: 0.75\n",
      "At: 1675 [==========>] Loss 0.2068181893502588  - accuracy: 0.65625\n",
      "At: 1676 [==========>] Loss 0.18894515945242546  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.10643499121581515  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.2164784750836134  - accuracy: 0.71875\n",
      "At: 1679 [==========>] Loss 0.09584067359585416  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.16237152599488527  - accuracy: 0.78125\n",
      "At: 1681 [==========>] Loss 0.11564215737934  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.05598373568643915  - accuracy: 0.9375\n",
      "At: 1683 [==========>] Loss 0.17392120420756768  - accuracy: 0.8125\n",
      "At: 1684 [==========>] Loss 0.09814529134099104  - accuracy: 0.90625\n",
      "At: 1685 [==========>] Loss 0.09884704689896966  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.12172141079894247  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.17466906110061645  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.06381602154374345  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.1191210041157388  - accuracy: 0.8125\n",
      "At: 1690 [==========>] Loss 0.09531534536953495  - accuracy: 0.875\n",
      "At: 1691 [==========>] Loss 0.09310170930763348  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.14607028197958527  - accuracy: 0.8125\n",
      "At: 1693 [==========>] Loss 0.10384808022858344  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.08689504880330301  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.13952310777315735  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.14749382224225227  - accuracy: 0.84375\n",
      "At: 1697 [==========>] Loss 0.12618634110026145  - accuracy: 0.84375\n",
      "At: 1698 [==========>] Loss 0.07861789773591091  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.1259749277423426  - accuracy: 0.8125\n",
      "At: 1700 [==========>] Loss 0.12961037480951668  - accuracy: 0.84375\n",
      "At: 1701 [==========>] Loss 0.07828908739724955  - accuracy: 0.9375\n",
      "At: 1702 [==========>] Loss 0.0982267580506556  - accuracy: 0.78125\n",
      "At: 1703 [==========>] Loss 0.17647661729063444  - accuracy: 0.78125\n",
      "At: 1704 [==========>] Loss 0.10319049589158398  - accuracy: 0.84375\n",
      "At: 1705 [==========>] Loss 0.09863026131762337  - accuracy: 0.90625\n",
      "At: 1706 [==========>] Loss 0.1540349663167836  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.1815373344521685  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.08217725687923172  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.1877230534638335  - accuracy: 0.625\n",
      "At: 1710 [==========>] Loss 0.1509226372783892  - accuracy: 0.78125\n",
      "At: 1711 [==========>] Loss 0.08483377653116835  - accuracy: 0.9375\n",
      "At: 1712 [==========>] Loss 0.1135700428461499  - accuracy: 0.8125\n",
      "At: 1713 [==========>] Loss 0.0852109998791808  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.17267547264110567  - accuracy: 0.78125\n",
      "At: 1715 [==========>] Loss 0.1245646309325664  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06452346891317798  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08082517862260936  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.1331067515437684  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.10868602926110352  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.05674272969548348  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.16391918064059685  - accuracy: 0.8125\n",
      "At: 1722 [==========>] Loss 0.054648291105762754  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.1930715967576453  - accuracy: 0.71875\n",
      "At: 1724 [==========>] Loss 0.07153623145259526  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.15435165107630613  - accuracy: 0.84375\n",
      "At: 1726 [==========>] Loss 0.0820321073691662  - accuracy: 0.875\n",
      "At: 1727 [==========>] Loss 0.12588573219179927  - accuracy: 0.875\n",
      "At: 1728 [==========>] Loss 0.09177973097283007  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.18119596884993355  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.12082604576241981  - accuracy: 0.84375\n",
      "At: 1731 [==========>] Loss 0.08064017752708771  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.07728992551031295  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.1787717100864789  - accuracy: 0.78125\n",
      "At: 1734 [==========>] Loss 0.10117183696359364  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.1581483910185986  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.10693991711137246  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.1457112992454091  - accuracy: 0.875\n",
      "At: 1738 [==========>] Loss 0.13710514996665185  - accuracy: 0.84375\n",
      "At: 1739 [==========>] Loss 0.12500895180419155  - accuracy: 0.875\n",
      "At: 1740 [==========>] Loss 0.15016175928560793  - accuracy: 0.78125\n",
      "At: 1741 [==========>] Loss 0.15710759848580214  - accuracy: 0.71875\n",
      "At: 1742 [==========>] Loss 0.0343213614873308  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.15593951315440882  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.09194941451517949  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.12950795184324604  - accuracy: 0.8125\n",
      "At: 1746 [==========>] Loss 0.15113164116620645  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.11292676457522503  - accuracy: 0.84375\n",
      "At: 1748 [==========>] Loss 0.14270589489248514  - accuracy: 0.8125\n",
      "At: 1749 [==========>] Loss 0.1343645387790356  - accuracy: 0.8125\n",
      "At: 1750 [==========>] Loss 0.11426065219119541  - accuracy: 0.84375\n",
      "At: 1751 [==========>] Loss 0.16686737463017498  - accuracy: 0.75\n",
      "At: 1752 [==========>] Loss 0.1075444125683275  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.09829540079589322  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.12308870894935477  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.070833083656868  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.1604271826454114  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.16776046105194115  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.07064481537628502  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.10554051570908987  - accuracy: 0.8125\n",
      "At: 1760 [==========>] Loss 0.07237933525831697  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.11272296150220021  - accuracy: 0.8125\n",
      "At: 1762 [==========>] Loss 0.16856251987019594  - accuracy: 0.75\n",
      "At: 1763 [==========>] Loss 0.07337986079041342  - accuracy: 0.9375\n",
      "At: 1764 [==========>] Loss 0.14341026731850648  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.16324122891298215  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.0668839399870427  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.07419968200426837  - accuracy: 0.875\n",
      "At: 1768 [==========>] Loss 0.10188891955602011  - accuracy: 0.78125\n",
      "At: 1769 [==========>] Loss 0.06777332643946891  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.07259438965603618  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.13813466380818318  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.11885392153767757  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.12388199512634107  - accuracy: 0.84375\n",
      "At: 1774 [==========>] Loss 0.16946306475253956  - accuracy: 0.78125\n",
      "At: 1775 [==========>] Loss 0.09797198758934261  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.10117688728907659  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.1283545183129132  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.11080421389730488  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.09815406028938102  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.11108258454160212  - accuracy: 0.90625\n",
      "At: 1781 [==========>] Loss 0.1977308508130774  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.13044745604687605  - accuracy: 0.78125\n",
      "At: 1783 [==========>] Loss 0.14614660897851886  - accuracy: 0.78125\n",
      "At: 1784 [==========>] Loss 0.08543553637833258  - accuracy: 0.875\n",
      "At: 1785 [==========>] Loss 0.09989494870294113  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.11531455782284725  - accuracy: 0.875\n",
      "At: 1787 [==========>] Loss 0.1434752251536413  - accuracy: 0.78125\n",
      "At: 1788 [==========>] Loss 0.09242134576308009  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.0976994505953881  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.1809618554207  - accuracy: 0.71875\n",
      "At: 1791 [==========>] Loss 0.08227669913635591  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.12573582100704894  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.09167916008084549  - accuracy: 0.90625\n",
      "At: 1794 [==========>] Loss 0.1787609187064257  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.07803547275666123  - accuracy: 0.9375\n",
      "At: 1796 [==========>] Loss 0.11246372328901164  - accuracy: 0.84375\n",
      "At: 1797 [==========>] Loss 0.1026692146153557  - accuracy: 0.84375\n",
      "At: 1798 [==========>] Loss 0.12900127407473683  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.08511257770847246  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.10842534832861767  - accuracy: 0.875\n",
      "At: 1801 [==========>] Loss 0.19423957305869444  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.12382954921474051  - accuracy: 0.875\n",
      "At: 1803 [==========>] Loss 0.16725765253359343  - accuracy: 0.71875\n",
      "At: 1804 [==========>] Loss 0.1318401861468761  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.05529232340078349  - accuracy: 0.9375\n",
      "At: 1806 [==========>] Loss 0.1602525027331319  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.16029038997635053  - accuracy: 0.75\n",
      "At: 1808 [==========>] Loss 0.16789808868493372  - accuracy: 0.75\n",
      "At: 1809 [==========>] Loss 0.09136190472495521  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.12869711079647383  - accuracy: 0.75\n",
      "At: 1811 [==========>] Loss 0.14970342216296617  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.10549543868983569  - accuracy: 0.875\n",
      "At: 1813 [==========>] Loss 0.12820593606241037  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.13084844973525583  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.17235995366208395  - accuracy: 0.6875\n",
      "At: 1816 [==========>] Loss 0.03494477334195694  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.12980568975738946  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.11694511353299267  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.14971082494569873  - accuracy: 0.8125\n",
      "At: 1820 [==========>] Loss 0.09444129402761  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.11279369204227466  - accuracy: 0.8125\n",
      "At: 1822 [==========>] Loss 0.16343776141681665  - accuracy: 0.78125\n",
      "At: 1823 [==========>] Loss 0.1647428472390387  - accuracy: 0.6875\n",
      "At: 1824 [==========>] Loss 0.16061824127605817  - accuracy: 0.71875\n",
      "At: 1825 [==========>] Loss 0.14919070017166058  - accuracy: 0.84375\n",
      "At: 1826 [==========>] Loss 0.0676731351869677  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11207465897612302  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.1397638832369362  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.15297002994264983  - accuracy: 0.75\n",
      "At: 1830 [==========>] Loss 0.11366123539743057  - accuracy: 0.8125\n",
      "At: 1831 [==========>] Loss 0.11097427908940867  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.1493601959173524  - accuracy: 0.71875\n",
      "At: 1833 [==========>] Loss 0.13274380109796208  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.07453460492501783  - accuracy: 0.9375\n",
      "At: 1835 [==========>] Loss 0.158406083491277  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.11182027970696828  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.05877000961490048  - accuracy: 0.9375\n",
      "At: 1838 [==========>] Loss 0.10941884411543004  - accuracy: 0.875\n",
      "At: 1839 [==========>] Loss 0.07624189551972224  - accuracy: 0.875\n",
      "At: 1840 [==========>] Loss 0.1086523021551613  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.10117108096466157  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.12443606711182736  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.10758308974237318  - accuracy: 0.78125\n",
      "At: 1844 [==========>] Loss 0.0937360317000835  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.21325061520240876  - accuracy: 0.71875\n",
      "At: 1846 [==========>] Loss 0.13945670312525554  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.05972315309643879  - accuracy: 0.90625\n",
      "At: 1848 [==========>] Loss 0.05771411061454935  - accuracy: 0.875\n",
      "At: 1849 [==========>] Loss 0.16882324327279957  - accuracy: 0.8125\n",
      "At: 1850 [==========>] Loss 0.03373957138231627  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.16397333832719554  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.0780456052075019  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.08882564877211115  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.12757216379443287  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.15701137300263202  - accuracy: 0.71875\n",
      "At: 1856 [==========>] Loss 0.11724262526719997  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.14990533120736585  - accuracy: 0.84375\n",
      "At: 1858 [==========>] Loss 0.10319198390048398  - accuracy: 0.875\n",
      "At: 1859 [==========>] Loss 0.17254174361159677  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.1287114682105914  - accuracy: 0.84375\n",
      "At: 1861 [==========>] Loss 0.10531118466666742  - accuracy: 0.78125\n",
      "At: 1862 [==========>] Loss 0.17603688473560797  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.12151568848325767  - accuracy: 0.84375\n",
      "At: 1864 [==========>] Loss 0.11960962966930043  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.08698177337711097  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.1974664811531806  - accuracy: 0.625\n",
      "At: 1867 [==========>] Loss 0.11453268430428126  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.14315535887092168  - accuracy: 0.8125\n",
      "At: 1869 [==========>] Loss 0.18031738680537723  - accuracy: 0.78125\n",
      "At: 1870 [==========>] Loss 0.1285159144867813  - accuracy: 0.84375\n",
      "At: 1871 [==========>] Loss 0.12349437572114891  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.1081776018855608  - accuracy: 0.8125\n",
      "At: 1873 [==========>] Loss 0.08092670207516434  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.16602918557647003  - accuracy: 0.84375\n",
      "At: 1875 [==========>] Loss 0.09516978230715767  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.17051710856209665  - accuracy: 0.78125\n",
      "At: 1877 [==========>] Loss 0.09874221347397608  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.11557293083995007  - accuracy: 0.875\n",
      "At: 1879 [==========>] Loss 0.1465661311168281  - accuracy: 0.78125\n",
      "At: 1880 [==========>] Loss 0.09229296523238956  - accuracy: 0.9375\n",
      "At: 1881 [==========>] Loss 0.12432411442619908  - accuracy: 0.78125\n",
      "At: 1882 [==========>] Loss 0.10975725791203719  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.12758827143133578  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.10061305947258925  - accuracy: 0.8125\n",
      "At: 1885 [==========>] Loss 0.11415269458676502  - accuracy: 0.78125\n",
      "At: 1886 [==========>] Loss 0.13343639176089134  - accuracy: 0.75\n",
      "At: 1887 [==========>] Loss 0.0858783061913605  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.16519375950120993  - accuracy: 0.65625\n",
      "At: 1889 [==========>] Loss 0.09259677673847147  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.1605408052490373  - accuracy: 0.8125\n",
      "At: 1891 [==========>] Loss 0.059473876134629004  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.08229923050142578  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.09842734542276717  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.0908588746998457  - accuracy: 0.84375\n",
      "At: 1895 [==========>] Loss 0.06086413383698973  - accuracy: 0.9375\n",
      "At: 1896 [==========>] Loss 0.10534963743890341  - accuracy: 0.84375\n",
      "At: 1897 [==========>] Loss 0.06736622024187668  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.10381642320907584  - accuracy: 0.875\n",
      "At: 1899 [==========>] Loss 0.11079008412342686  - accuracy: 0.84375\n",
      "At: 1900 [==========>] Loss 0.11954089349482327  - accuracy: 0.90625\n",
      "At: 1901 [==========>] Loss 0.12448526051400162  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.1546639578542184  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.14084782910947438  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.07341050934110943  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.12141198183721894  - accuracy: 0.8125\n",
      "At: 1906 [==========>] Loss 0.10507778628707451  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.0810446300391623  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.0859469452377051  - accuracy: 0.875\n",
      "At: 1909 [==========>] Loss 0.10684801396549337  - accuracy: 0.8125\n",
      "At: 1910 [==========>] Loss 0.07576149713045872  - accuracy: 0.875\n",
      "At: 1911 [==========>] Loss 0.12597289161798791  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.1358900149792383  - accuracy: 0.78125\n",
      "At: 1913 [==========>] Loss 0.16898948759942953  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.09469101464640199  - accuracy: 0.84375\n",
      "At: 1915 [==========>] Loss 0.10916631101177263  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.14658048232845294  - accuracy: 0.84375\n",
      "At: 1917 [==========>] Loss 0.1528256885657686  - accuracy: 0.8125\n",
      "At: 1918 [==========>] Loss 0.15762669999724194  - accuracy: 0.78125\n",
      "At: 1919 [==========>] Loss 0.09517726337176957  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.11747166138226345  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.13917513380299112  - accuracy: 0.75\n",
      "At: 1922 [==========>] Loss 0.12745463344523372  - accuracy: 0.8125\n",
      "At: 1923 [==========>] Loss 0.19041722139487163  - accuracy: 0.625\n",
      "At: 1924 [==========>] Loss 0.1239002144920447  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.15563299238702297  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.09712766390615214  - accuracy: 0.875\n",
      "At: 1927 [==========>] Loss 0.10726354129692914  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.11987457826723291  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.17009284531644114  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.15878589399926385  - accuracy: 0.75\n",
      "At: 1931 [==========>] Loss 0.09187604454265495  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.1543080139793471  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.07451702077000377  - accuracy: 0.9375\n",
      "At: 1934 [==========>] Loss 0.14769454643434568  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.13976420143565715  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.1257976312564465  - accuracy: 0.78125\n",
      "At: 1937 [==========>] Loss 0.13188306003685185  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.19444628141779563  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.09810270157681902  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.1235832485355339  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.12153340443415436  - accuracy: 0.8125\n",
      "At: 1942 [==========>] Loss 0.1659391700423128  - accuracy: 0.75\n",
      "At: 1943 [==========>] Loss 0.14699783814968864  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.10289742050026295  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.16595551138220643  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.0809686924691808  - accuracy: 0.9375\n",
      "At: 1947 [==========>] Loss 0.09362216594941836  - accuracy: 0.84375\n",
      "At: 1948 [==========>] Loss 0.13304275178209685  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.07190477089528582  - accuracy: 0.875\n",
      "At: 1950 [==========>] Loss 0.14696342590114903  - accuracy: 0.71875\n",
      "At: 1951 [==========>] Loss 0.14060930885213285  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.0926892398616983  - accuracy: 0.84375\n",
      "At: 1953 [==========>] Loss 0.06828131436009867  - accuracy: 0.96875\n",
      "At: 1954 [==========>] Loss 0.1921640211478532  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.07124286186432258  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.10251285914382921  - accuracy: 0.90625\n",
      "At: 1957 [==========>] Loss 0.07465043055222852  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.06807455017182516  - accuracy: 0.90625\n",
      "At: 1959 [==========>] Loss 0.14256001274501456  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.06257885925611832  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.1599616970509702  - accuracy: 0.8125\n",
      "At: 1962 [==========>] Loss 0.21320088718675978  - accuracy: 0.65625\n",
      "At: 1963 [==========>] Loss 0.06383809790504422  - accuracy: 1.0\n",
      "At: 1964 [==========>] Loss 0.1759116687176397  - accuracy: 0.71875\n",
      "At: 1965 [==========>] Loss 0.14545853043916085  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.12172094440491779  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.12335059959334255  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.2172759629621533  - accuracy: 0.6875\n",
      "At: 1969 [==========>] Loss 0.15288009445145018  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.08196430354111882  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.1983841803035774  - accuracy: 0.75\n",
      "At: 1972 [==========>] Loss 0.07732600996321118  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.14738885503444463  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.14482429642825512  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.15298778000620006  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.06803460813206771  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.06416682243726995  - accuracy: 0.9375\n",
      "At: 1978 [==========>] Loss 0.11495402954769411  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.12056006556351924  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.1316209735923899  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.1635906300307581  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.07527692172337959  - accuracy: 0.875\n",
      "At: 1983 [==========>] Loss 0.15895252017646516  - accuracy: 0.78125\n",
      "At: 1984 [==========>] Loss 0.08638068552504351  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.17354800671324594  - accuracy: 0.71875\n",
      "At: 1986 [==========>] Loss 0.2152003739153652  - accuracy: 0.65625\n",
      "At: 1987 [==========>] Loss 0.10289396578448926  - accuracy: 0.8125\n",
      "At: 1988 [==========>] Loss 0.10754958035691903  - accuracy: 0.84375\n",
      "At: 1989 [==========>] Loss 0.09480940741748778  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.12436906986654409  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.13049094706161496  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.12175475882500492  - accuracy: 0.78125\n",
      "At: 1993 [==========>] Loss 0.1856515635678357  - accuracy: 0.6875\n",
      "At: 1994 [==========>] Loss 0.10801628869981465  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.19231196391382688  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.1379012354366593  - accuracy: 0.78125\n",
      "At: 1997 [==========>] Loss 0.17886335549868362  - accuracy: 0.75\n",
      "At: 1998 [==========>] Loss 0.14256964095799818  - accuracy: 0.8125\n",
      "At: 1999 [==========>] Loss 0.08084561830013354  - accuracy: 0.9375\n",
      "At: 2000 [==========>] Loss 0.11498135011947404  - accuracy: 0.84375\n",
      "At: 2001 [==========>] Loss 0.06844718147485176  - accuracy: 0.9375\n",
      "At: 2002 [==========>] Loss 0.0781672223779088  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.12282844221458024  - accuracy: 0.78125\n",
      "At: 2004 [==========>] Loss 0.15009889232889526  - accuracy: 0.8125\n",
      "At: 2005 [==========>] Loss 0.11599297466461406  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.11298904036620389  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.0829001436426667  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.13867716533884594  - accuracy: 0.78125\n",
      "At: 2009 [==========>] Loss 0.13645770390211098  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.12105976987973777  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.12411012596337559  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.099160397282171  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.08593652078433002  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.2649694405191701  - accuracy: 0.59375\n",
      "At: 2015 [==========>] Loss 0.04975661781490629  - accuracy: 0.96875\n",
      "At: 2016 [==========>] Loss 0.13754628369057184  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.0946367643776051  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.10029641662512595  - accuracy: 0.84375\n",
      "At: 2019 [==========>] Loss 0.13298266875666742  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.08141509206076473  - accuracy: 0.84375\n",
      "At: 2021 [==========>] Loss 0.11222069799953502  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.12638098500385853  - accuracy: 0.84375\n",
      "At: 2023 [==========>] Loss 0.08495200904598048  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.07716367746880153  - accuracy: 0.9375\n",
      "At: 2025 [==========>] Loss 0.16752063960225638  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.09231450303359094  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.12686367815749877  - accuracy: 0.84375\n",
      "At: 2028 [==========>] Loss 0.12885284956195198  - accuracy: 0.84375\n",
      "At: 2029 [==========>] Loss 0.10975470969960752  - accuracy: 0.875\n",
      "At: 2030 [==========>] Loss 0.1528695167624483  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.1436213894699367  - accuracy: 0.78125\n",
      "At: 2032 [==========>] Loss 0.14281890206767706  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.1504997096564894  - accuracy: 0.75\n",
      "At: 2034 [==========>] Loss 0.23045677523469893  - accuracy: 0.65625\n",
      "At: 2035 [==========>] Loss 0.11656166116506567  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09418769714728598  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.14173738862802596  - accuracy: 0.8125\n",
      "At: 2038 [==========>] Loss 0.10228291990019102  - accuracy: 0.875\n",
      "At: 2039 [==========>] Loss 0.08805346176473938  - accuracy: 0.8125\n",
      "At: 2040 [==========>] Loss 0.07341008374546085  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.05283653433212421  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.12607042226063353  - accuracy: 0.875\n",
      "At: 2043 [==========>] Loss 0.1037521142618085  - accuracy: 0.90625\n",
      "At: 2044 [==========>] Loss 0.08198612966546415  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.2065119973318057  - accuracy: 0.71875\n",
      "At: 2046 [==========>] Loss 0.06547116947759786  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08305971133207485  - accuracy: 0.84375\n",
      "At: 2048 [==========>] Loss 0.10187725231684208  - accuracy: 0.90625\n",
      "At: 2049 [==========>] Loss 0.16715348224846313  - accuracy: 0.71875\n",
      "At: 2050 [==========>] Loss 0.15465166405732814  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.17218084266061917  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.06415675013334655  - accuracy: 1.0\n",
      "At: 2053 [==========>] Loss 0.09779356525956677  - accuracy: 0.875\n",
      "At: 2054 [==========>] Loss 0.12426993445846049  - accuracy: 0.8125\n",
      "At: 2055 [==========>] Loss 0.0791807016705623  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.0980142039156359  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.12461375766946214  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.1440793298611983  - accuracy: 0.71875\n",
      "At: 2059 [==========>] Loss 0.20818927200976733  - accuracy: 0.65625\n",
      "At: 2060 [==========>] Loss 0.13072462254337705  - accuracy: 0.84375\n",
      "At: 2061 [==========>] Loss 0.15686940693544116  - accuracy: 0.84375\n",
      "At: 2062 [==========>] Loss 0.13519156929092446  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.10490278813557806  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.18305451101397088  - accuracy: 0.78125\n",
      "At: 2065 [==========>] Loss 0.047680187822639865  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.14519806594548126  - accuracy: 0.78125\n",
      "At: 2067 [==========>] Loss 0.09362783372654787  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.09848535982196677  - accuracy: 0.875\n",
      "At: 2069 [==========>] Loss 0.09742608949450512  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.12706349769131342  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.11023334022016035  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.061414903387607814  - accuracy: 0.96875\n",
      "At: 2073 [==========>] Loss 0.11243028469988967  - accuracy: 0.8125\n",
      "At: 2074 [==========>] Loss 0.1041373702017771  - accuracy: 0.84375\n",
      "At: 2075 [==========>] Loss 0.13215055706064394  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.12298853490587648  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.12795736410024613  - accuracy: 0.84375\n",
      "At: 2078 [==========>] Loss 0.09527681361607503  - accuracy: 0.8125\n",
      "At: 2079 [==========>] Loss 0.06813387025067322  - accuracy: 0.9375\n",
      "At: 2080 [==========>] Loss 0.1198312837272904  - accuracy: 0.84375\n",
      "At: 2081 [==========>] Loss 0.1439010725941437  - accuracy: 0.78125\n",
      "At: 2082 [==========>] Loss 0.11813432136787763  - accuracy: 0.875\n",
      "At: 2083 [==========>] Loss 0.19254109284040316  - accuracy: 0.6875\n",
      "At: 2084 [==========>] Loss 0.09515332262783827  - accuracy: 0.90625\n",
      "At: 2085 [==========>] Loss 0.08724509424754853  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.10410821596982399  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.1528660490688744  - accuracy: 0.8125\n",
      "At: 2088 [==========>] Loss 0.11038078836805089  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.14024960775783793  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.10211372169826763  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.13098031441675587  - accuracy: 0.8125\n",
      "At: 2092 [==========>] Loss 0.10205348402120751  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.15345664191911967  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.15404093068660635  - accuracy: 0.71875\n",
      "At: 2095 [==========>] Loss 0.12775623778086515  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.16131258870624055  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.1325691798768777  - accuracy: 0.875\n",
      "At: 2098 [==========>] Loss 0.1305771136887136  - accuracy: 0.78125\n",
      "At: 2099 [==========>] Loss 0.09285940889248992  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.050763518023462934  - accuracy: 0.96875\n",
      "At: 2101 [==========>] Loss 0.1481770822660653  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.08674803045584983  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.13959784857056537  - accuracy: 0.8125\n",
      "At: 2104 [==========>] Loss 0.10341934486815238  - accuracy: 0.90625\n",
      "At: 2105 [==========>] Loss 0.17886325630739158  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.1434725748023612  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.09552813434793522  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.14366348599773285  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.10797235297961341  - accuracy: 0.875\n",
      "At: 2110 [==========>] Loss 0.0678809475501772  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.11225172746728466  - accuracy: 0.875\n",
      "At: 2112 [==========>] Loss 0.1146661924721101  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.09825005401797446  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.11862034014989802  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.12116051103434737  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.08542411816938009  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.15333058306509595  - accuracy: 0.78125\n",
      "At: 2118 [==========>] Loss 0.12914539650920148  - accuracy: 0.78125\n",
      "At: 2119 [==========>] Loss 0.07311754647567262  - accuracy: 0.90625\n",
      "At: 2120 [==========>] Loss 0.15345722890561808  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.1350790248767136  - accuracy: 0.84375\n",
      "At: 2122 [==========>] Loss 0.12856491625557886  - accuracy: 0.84375\n",
      "At: 2123 [==========>] Loss 0.1827983970210943  - accuracy: 0.71875\n",
      "At: 2124 [==========>] Loss 0.12507805166751346  - accuracy: 0.84375\n",
      "At: 2125 [==========>] Loss 0.07706453702153968  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.05480956205210115  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.09964816335604193  - accuracy: 0.90625\n",
      "At: 2128 [==========>] Loss 0.09789742822880587  - accuracy: 0.875\n",
      "At: 2129 [==========>] Loss 0.17989613162980828  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.06942458682784879  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.11452419675791808  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.21722571338102536  - accuracy: 0.71875\n",
      "At: 2133 [==========>] Loss 0.17651328366140612  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.1369415239998593  - accuracy: 0.78125\n",
      "At: 2135 [==========>] Loss 0.09334706697895195  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.15399374377524605  - accuracy: 0.78125\n",
      "At: 2137 [==========>] Loss 0.12697840224783147  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.12286454931374183  - accuracy: 0.8125\n",
      "At: 2139 [==========>] Loss 0.17463652626659057  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.11045639007495892  - accuracy: 0.84375\n",
      "At: 2141 [==========>] Loss 0.11089790070089212  - accuracy: 0.90625\n",
      "At: 2142 [==========>] Loss 0.11972120296604333  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.10419064069373564  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.08190814407489799  - accuracy: 0.9375\n",
      "At: 2145 [==========>] Loss 0.11450072391674554  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.1679334894329313  - accuracy: 0.6875\n",
      "At: 2147 [==========>] Loss 0.13925396151159966  - accuracy: 0.78125\n",
      "At: 2148 [==========>] Loss 0.195421941924316  - accuracy: 0.78125\n",
      "At: 2149 [==========>] Loss 0.12777665815913594  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.11918236113238133  - accuracy: 0.875\n",
      "At: 2151 [==========>] Loss 0.09944088079793081  - accuracy: 0.875\n",
      "At: 2152 [==========>] Loss 0.27180422239529844  - accuracy: 0.625\n",
      "At: 2153 [==========>] Loss 0.15488281605989124  - accuracy: 0.71875\n",
      "At: 2154 [==========>] Loss 0.1457420805283659  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.11990366943313173  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.10193278049614407  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.12436745029582519  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.1298463668955963  - accuracy: 0.78125\n",
      "At: 2159 [==========>] Loss 0.08714956150752394  - accuracy: 0.875\n",
      "At: 2160 [==========>] Loss 0.15695699196428653  - accuracy: 0.75\n",
      "At: 2161 [==========>] Loss 0.08534300428474349  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.07125550164193571  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.11322288189687388  - accuracy: 0.8125\n",
      "At: 2164 [==========>] Loss 0.1767469858669443  - accuracy: 0.71875\n",
      "At: 2165 [==========>] Loss 0.09531163597008283  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.11174934970586485  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.0879495933374124  - accuracy: 0.875\n",
      "At: 2168 [==========>] Loss 0.08685978157428662  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.1257332680159417  - accuracy: 0.84375\n",
      "At: 2170 [==========>] Loss 0.12434739011317901  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.1274863801614522  - accuracy: 0.875\n",
      "At: 2172 [==========>] Loss 0.08693265871445677  - accuracy: 0.90625\n",
      "At: 2173 [==========>] Loss 0.15780026690987634  - accuracy: 0.75\n",
      "At: 2174 [==========>] Loss 0.1311450752932899  - accuracy: 0.78125\n",
      "At: 2175 [==========>] Loss 0.10917041197641136  - accuracy: 0.90625\n",
      "At: 2176 [==========>] Loss 0.1068490157131488  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.15076348870247316  - accuracy: 0.78125\n",
      "At: 2178 [==========>] Loss 0.10313546638375899  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.12433933713557889  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.10464994061221555  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.1952521878776904  - accuracy: 0.6875\n",
      "At: 2182 [==========>] Loss 0.0998534154211492  - accuracy: 0.875\n",
      "At: 2183 [==========>] Loss 0.24790849027763823  - accuracy: 0.625\n",
      "At: 2184 [==========>] Loss 0.073560142628846  - accuracy: 0.84375\n",
      "At: 2185 [==========>] Loss 0.11855591442919908  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.17197362679129155  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.17648615677934718  - accuracy: 0.78125\n",
      "At: 2188 [==========>] Loss 0.0680487998169188  - accuracy: 0.875\n",
      "At: 2189 [==========>] Loss 0.07824889358500066  - accuracy: 0.84375\n",
      "At: 2190 [==========>] Loss 0.10637633664496973  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.11419408820305682  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.10147118359885501  - accuracy: 0.8125\n",
      "At: 2193 [==========>] Loss 0.16775879640995742  - accuracy: 0.71875\n",
      "At: 2194 [==========>] Loss 0.12297581695275545  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.139574226350354  - accuracy: 0.78125\n",
      "At: 2196 [==========>] Loss 0.13985122792766572  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.09617730937028632  - accuracy: 0.84375\n",
      "At: 2198 [==========>] Loss 0.09328484743338278  - accuracy: 0.875\n",
      "At: 2199 [==========>] Loss 0.05571423665779543  - accuracy: 0.90625\n",
      "At: 2200 [==========>] Loss 0.04998644434258582  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.12059399247199268  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.08448084626877705  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.08629551505790012  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.14299922130131132  - accuracy: 0.71875\n",
      "At: 2205 [==========>] Loss 0.13745626990911672  - accuracy: 0.78125\n",
      "At: 2206 [==========>] Loss 0.0800728370434623  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.08368649752578937  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.13496472183961297  - accuracy: 0.875\n",
      "At: 2209 [==========>] Loss 0.21104607161087016  - accuracy: 0.6875\n",
      "At: 2210 [==========>] Loss 0.13016738196087935  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.12284162364504664  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.10404655469752194  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.11759442215925237  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.13276944443185715  - accuracy: 0.75\n",
      "At: 2215 [==========>] Loss 0.14906589116956376  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.12793377822624166  - accuracy: 0.8125\n",
      "At: 2217 [==========>] Loss 0.11672815168748678  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.1519866550174499  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.06496943416186815  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.1171941715484368  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.1699950624910956  - accuracy: 0.75\n",
      "At: 2222 [==========>] Loss 0.11085209942914094  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.13778956117177155  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.12614494080935015  - accuracy: 0.875\n",
      "At: 2225 [==========>] Loss 0.1374252388818542  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.13356433682127403  - accuracy: 0.84375\n",
      "At: 2227 [==========>] Loss 0.16374246019035726  - accuracy: 0.78125\n",
      "At: 2228 [==========>] Loss 0.07578226850234356  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.1609102928620477  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.12028852673379287  - accuracy: 0.8125\n",
      "At: 2231 [==========>] Loss 0.1457045204095112  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.1769979089368124  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.1644836098448088  - accuracy: 0.8125\n",
      "At: 2234 [==========>] Loss 0.12063773153607821  - accuracy: 0.8125\n",
      "At: 2235 [==========>] Loss 0.10602547806623433  - accuracy: 0.875\n",
      "At: 2236 [==========>] Loss 0.08808335507391457  - accuracy: 0.875\n",
      "At: 2237 [==========>] Loss 0.11789656786055178  - accuracy: 0.875\n",
      "At: 2238 [==========>] Loss 0.14836799854778598  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.1731164480832008  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.14361559990445877  - accuracy: 0.75\n",
      "At: 2241 [==========>] Loss 0.15407735318810112  - accuracy: 0.75\n",
      "At: 2242 [==========>] Loss 0.1833213222923408  - accuracy: 0.71875\n",
      "At: 2243 [==========>] Loss 0.08155050348655737  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.10542023345441096  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.08237669612371004  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.1597523619186254  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.11780202307369331  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.1819355368353986  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.10787619126577042  - accuracy: 0.875\n",
      "At: 2250 [==========>] Loss 0.06775772933563459  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.07640919232628346  - accuracy: 0.9375\n",
      "At: 2252 [==========>] Loss 0.13272915527543022  - accuracy: 0.84375\n",
      "At: 2253 [==========>] Loss 0.13024001276063119  - accuracy: 0.78125\n",
      "At: 2254 [==========>] Loss 0.12560271534555165  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.13337945984795976  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.17086795117082104  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.09361163573964444  - accuracy: 0.90625\n",
      "At: 2258 [==========>] Loss 0.12299684135338373  - accuracy: 0.84375\n",
      "At: 2259 [==========>] Loss 0.10199896061284025  - accuracy: 0.875\n",
      "At: 2260 [==========>] Loss 0.19447320302047705  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.08184672505492137  - accuracy: 0.96875\n",
      "At: 2262 [==========>] Loss 0.1855280089020599  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.1620643899025918  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.08971486690786673  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.08213143767986437  - accuracy: 0.90625\n",
      "At: 2266 [==========>] Loss 0.1272237043093312  - accuracy: 0.8125\n",
      "At: 2267 [==========>] Loss 0.06963876791262591  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.0851953061227941  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.033358541012142556  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.10123373778249724  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.10754564365772999  - accuracy: 0.875\n",
      "At: 2272 [==========>] Loss 0.07463849659607397  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.11358374905665672  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.09246025411595829  - accuracy: 0.84375\n",
      "At: 2275 [==========>] Loss 0.10088738578807219  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.09087088622828389  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.15711158332009828  - accuracy: 0.78125\n",
      "At: 2278 [==========>] Loss 0.09409696817168783  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.12574184982360512  - accuracy: 0.84375\n",
      "At: 2280 [==========>] Loss 0.15108469174506325  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.10967682202405594  - accuracy: 0.875\n",
      "At: 2282 [==========>] Loss 0.07869419956706587  - accuracy: 0.875\n",
      "At: 2283 [==========>] Loss 0.1452081703476827  - accuracy: 0.78125\n",
      "At: 2284 [==========>] Loss 0.10594174920257891  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.1173522613441252  - accuracy: 0.84375\n",
      "At: 2286 [==========>] Loss 0.14698709412457736  - accuracy: 0.78125\n",
      "At: 2287 [==========>] Loss 0.15335545338595225  - accuracy: 0.78125\n",
      "At: 2288 [==========>] Loss 0.09729515752860345  - accuracy: 0.875\n",
      "At: 2289 [==========>] Loss 0.13997809072841783  - accuracy: 0.8125\n",
      "At: 2290 [==========>] Loss 0.08374170017496058  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.12081640534584735  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.06867211893784791  - accuracy: 0.9375\n",
      "At: 2293 [==========>] Loss 0.05587652204675132  - accuracy: 0.90625\n",
      "At: 2294 [==========>] Loss 0.05209171994970806  - accuracy: 0.875\n",
      "At: 2295 [==========>] Loss 0.15019065484847827  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.1608148065023849  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.06979505860243192  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.0941175814055758  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.12472366535947842  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.12081219345993051  - accuracy: 0.75\n",
      "At: 2301 [==========>] Loss 0.19258665054735258  - accuracy: 0.6875\n",
      "At: 2302 [==========>] Loss 0.16232035410860007  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.07586778198261807  - accuracy: 0.875\n",
      "At: 2304 [==========>] Loss 0.08500482224169106  - accuracy: 0.90625\n",
      "At: 2305 [==========>] Loss 0.07865222349416033  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.12603385946243068  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.15353542764275474  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.1876309886309248  - accuracy: 0.71875\n",
      "At: 2309 [==========>] Loss 0.1252768251445578  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.12006794683587703  - accuracy: 0.84375\n",
      "At: 2311 [==========>] Loss 0.13232459107550815  - accuracy: 0.84375\n",
      "At: 2312 [==========>] Loss 0.11532547207475657  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.06993235241028047  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.10854971334527883  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.14771375468442946  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.1243598875434476  - accuracy: 0.84375\n",
      "At: 2317 [==========>] Loss 0.16828305169805163  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.16762475752803152  - accuracy: 0.78125\n",
      "At: 2319 [==========>] Loss 0.12188838165872697  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.08929916131450195  - accuracy: 0.90625\n",
      "At: 2321 [==========>] Loss 0.14792243664801977  - accuracy: 0.8125\n",
      "At: 2322 [==========>] Loss 0.20506413244377542  - accuracy: 0.6875\n",
      "At: 2323 [==========>] Loss 0.10690268441103716  - accuracy: 0.84375\n",
      "At: 2324 [==========>] Loss 0.14918748985783825  - accuracy: 0.8125\n",
      "At: 2325 [==========>] Loss 0.14800549279418676  - accuracy: 0.78125\n",
      "At: 2326 [==========>] Loss 0.07758045879046363  - accuracy: 0.84375\n",
      "At: 2327 [==========>] Loss 0.07494338675905651  - accuracy: 0.90625\n",
      "At: 2328 [==========>] Loss 0.12807356763517294  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.10409080140944468  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.1419554200562424  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.11937783723556941  - accuracy: 0.78125\n",
      "At: 2332 [==========>] Loss 0.09091207956812117  - accuracy: 0.875\n",
      "At: 2333 [==========>] Loss 0.08067890831396635  - accuracy: 0.9375\n",
      "At: 2334 [==========>] Loss 0.1871453598650037  - accuracy: 0.65625\n",
      "At: 2335 [==========>] Loss 0.13072300535662545  - accuracy: 0.78125\n",
      "At: 2336 [==========>] Loss 0.10778207530488446  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.16378934703445763  - accuracy: 0.75\n",
      "At: 2338 [==========>] Loss 0.12183468141850266  - accuracy: 0.78125\n",
      "At: 2339 [==========>] Loss 0.09398318364958445  - accuracy: 0.78125\n",
      "At: 2340 [==========>] Loss 0.163745895364638  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.14484864112887386  - accuracy: 0.71875\n",
      "At: 2342 [==========>] Loss 0.17171983770069724  - accuracy: 0.71875\n",
      "At: 2343 [==========>] Loss 0.083406554213393  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.2130823714320343  - accuracy: 0.75\n",
      "At: 2345 [==========>] Loss 0.1835604196831968  - accuracy: 0.75\n",
      "At: 2346 [==========>] Loss 0.07302634646183923  - accuracy: 0.96875\n",
      "At: 2347 [==========>] Loss 0.12166936366527015  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.07775894697298091  - accuracy: 0.875\n",
      "At: 2349 [==========>] Loss 0.08540316559491287  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.12348927054372778  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.0800641477094945  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.09743667148584098  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.0734367417982019  - accuracy: 0.90625\n",
      "At: 2354 [==========>] Loss 0.14284327982919387  - accuracy: 0.78125\n",
      "At: 2355 [==========>] Loss 0.04572773117449741  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.13592514854166565  - accuracy: 0.78125\n",
      "At: 2357 [==========>] Loss 0.1399842437026379  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.16392406463728487  - accuracy: 0.75\n",
      "At: 2359 [==========>] Loss 0.16401327296024207  - accuracy: 0.6875\n",
      "At: 2360 [==========>] Loss 0.13716087863882565  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.1577007250897268  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.05365331346467105  - accuracy: 0.96875\n",
      "At: 2363 [==========>] Loss 0.17308464282187874  - accuracy: 0.78125\n",
      "At: 2364 [==========>] Loss 0.08758865004412367  - accuracy: 0.875\n",
      "At: 2365 [==========>] Loss 0.09708515928215537  - accuracy: 0.84375\n",
      "At: 2366 [==========>] Loss 0.15670718300162928  - accuracy: 0.78125\n",
      "At: 2367 [==========>] Loss 0.08965882025378145  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.11329117582858701  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.09227018108670434  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.07577543671424344  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.09498186327616534  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.13886331141832767  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.1024207263417301  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.11475992793406213  - accuracy: 0.8125\n",
      "At: 2375 [==========>] Loss 0.05196734284746499  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.10823693355846785  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.08965617559694103  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.12480150405886595  - accuracy: 0.8125\n",
      "At: 2379 [==========>] Loss 0.17160969256741038  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.0726717703639706  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.07472097257110044  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.10908251383928169  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.1362726520386183  - accuracy: 0.84375\n",
      "At: 2384 [==========>] Loss 0.11270329527906495  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.13678125255420231  - accuracy: 0.78125\n",
      "At: 2386 [==========>] Loss 0.1126266120072719  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.07847495536944364  - accuracy: 0.875\n",
      "At: 2388 [==========>] Loss 0.11730945286658105  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.03958366926966135  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.06356650549243403  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.18993622771981936  - accuracy: 0.75\n",
      "At: 2392 [==========>] Loss 0.12969553101075185  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.08891440870393622  - accuracy: 0.84375\n",
      "At: 2394 [==========>] Loss 0.07172827939225676  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.0874001566923501  - accuracy: 0.9375\n",
      "At: 2396 [==========>] Loss 0.0905833627114718  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.0939339766733915  - accuracy: 0.84375\n",
      "At: 2398 [==========>] Loss 0.13294584736019557  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.12927074850774725  - accuracy: 0.875\n",
      "At: 2400 [==========>] Loss 0.10642178630816682  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.11480778229652153  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.10314620184104266  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.18581357790311542  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.12267790301292628  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.05983884078701606  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.1593716616190184  - accuracy: 0.75\n",
      "At: 2407 [==========>] Loss 0.11462278361616723  - accuracy: 0.8125\n",
      "At: 2408 [==========>] Loss 0.09732638769886758  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.14139793332702538  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.15463317641526272  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.10591421123038808  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.09368834112681046  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.11126376221800634  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.0702059673499054  - accuracy: 0.9375\n",
      "At: 2415 [==========>] Loss 0.07829816700136044  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.10043092102230425  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.1615270519414126  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.09891289506315668  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.14468017334553324  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.12858170447341255  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.10949258418784114  - accuracy: 0.84375\n",
      "At: 2422 [==========>] Loss 0.14398228738455432  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.13337574351485715  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.09050106320050526  - accuracy: 0.90625\n",
      "At: 2425 [==========>] Loss 0.07818103327784948  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.19954628324448292  - accuracy: 0.71875\n",
      "At: 2427 [==========>] Loss 0.1521485372306122  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.08361738290346626  - accuracy: 0.9375\n",
      "At: 2429 [==========>] Loss 0.09650946070880255  - accuracy: 0.90625\n",
      "At: 2430 [==========>] Loss 0.15593397366078388  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.1676021725582346  - accuracy: 0.75\n",
      "At: 2432 [==========>] Loss 0.07851106972830137  - accuracy: 0.875\n",
      "At: 2433 [==========>] Loss 0.05768252256682743  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.03864434538033627  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.14901311790950084  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.11707877162856493  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.17199663214584504  - accuracy: 0.78125\n",
      "At: 2438 [==========>] Loss 0.1101479048147055  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.17877923930036752  - accuracy: 0.71875\n",
      "At: 2440 [==========>] Loss 0.09241246696762623  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.11343993557517539  - accuracy: 0.875\n",
      "At: 2442 [==========>] Loss 0.12720367248531994  - accuracy: 0.875\n",
      "At: 2443 [==========>] Loss 0.11923844598962689  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.10499389444292126  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.06591274102473364  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.19179970344680203  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.18472198599764283  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.10978870720019963  - accuracy: 0.875\n",
      "At: 2449 [==========>] Loss 0.09029005513725652  - accuracy: 0.84375\n",
      "At: 2450 [==========>] Loss 0.07015875530423848  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.038257124772239304  - accuracy: 0.96875\n",
      "At: 2452 [==========>] Loss 0.14618035725191403  - accuracy: 0.8125\n",
      "At: 2453 [==========>] Loss 0.1411973205437927  - accuracy: 0.75\n",
      "At: 2454 [==========>] Loss 0.15821289698514612  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.1340157026816604  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.1323114617556823  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.16675404086671533  - accuracy: 0.8125\n",
      "At: 2458 [==========>] Loss 0.06665576382207074  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.1295605840445566  - accuracy: 0.84375\n",
      "At: 2460 [==========>] Loss 0.08642797467438951  - accuracy: 0.84375\n",
      "At: 2461 [==========>] Loss 0.05702087167667501  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.1340889927722328  - accuracy: 0.78125\n",
      "At: 2463 [==========>] Loss 0.09934781909709406  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.13934697128219242  - accuracy: 0.8125\n",
      "At: 2465 [==========>] Loss 0.13698104121493215  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.092459790047187  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.10851225966370474  - accuracy: 0.84375\n",
      "At: 2468 [==========>] Loss 0.10590529569212118  - accuracy: 0.8125\n",
      "At: 2469 [==========>] Loss 0.12032329745791313  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.10639793767899383  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.11553441518250972  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.09670539364152216  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.10961007930573582  - accuracy: 0.875\n",
      "At: 2474 [==========>] Loss 0.07822812115634983  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10844623829511793  - accuracy: 0.875\n",
      "At: 2476 [==========>] Loss 0.06504832173576394  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.152317675096845  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.11875122935787077  - accuracy: 0.8125\n",
      "At: 2479 [==========>] Loss 0.08522442810607136  - accuracy: 0.84375\n",
      "At: 2480 [==========>] Loss 0.12730896163724584  - accuracy: 0.75\n",
      "At: 2481 [==========>] Loss 0.056981640992279425  - accuracy: 0.96875\n",
      "At: 2482 [==========>] Loss 0.15893903415917468  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.09776653408415543  - accuracy: 0.875\n",
      "At: 2484 [==========>] Loss 0.08902024793917176  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.09643437541196771  - accuracy: 0.875\n",
      "At: 2486 [==========>] Loss 0.13576044493502193  - accuracy: 0.8125\n",
      "At: 2487 [==========>] Loss 0.1429449149678122  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.166516251125531  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.2060093255355215  - accuracy: 0.6875\n",
      "At: 2490 [==========>] Loss 0.13538360658535503  - accuracy: 0.8125\n",
      "At: 2491 [==========>] Loss 0.11234481935946848  - accuracy: 0.84375\n",
      "At: 2492 [==========>] Loss 0.11029357202322027  - accuracy: 0.84375\n",
      "At: 2493 [==========>] Loss 0.089790466112502  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.09739108257091943  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.0853768587969894  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.0712340071512515  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.19838944397254296  - accuracy: 0.625\n",
      "At: 2498 [==========>] Loss 0.12491948307922708  - accuracy: 0.8125\n",
      "At: 2499 [==========>] Loss 0.062214340650191255  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.16262583538415293  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.1540312136329549  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.11189349043714272  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.11823000883267756  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.14594397357412436  - accuracy: 0.8125\n",
      "At: 2505 [==========>] Loss 0.13032994988284988  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.120905782539132  - accuracy: 0.84375\n",
      "At: 2507 [==========>] Loss 0.15073548875259835  - accuracy: 0.75\n",
      "At: 2508 [==========>] Loss 0.12669530837601556  - accuracy: 0.84375\n",
      "At: 2509 [==========>] Loss 0.16361664629875367  - accuracy: 0.75\n",
      "At: 2510 [==========>] Loss 0.10984428014574947  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14569492661897332  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.10652842268359515  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.15219025438309539  - accuracy: 0.8125\n",
      "At: 2514 [==========>] Loss 0.1429913891883588  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.1880678258739839  - accuracy: 0.75\n",
      "At: 2516 [==========>] Loss 0.19757885879423348  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.12280298681034423  - accuracy: 0.75\n",
      "At: 2518 [==========>] Loss 0.12366249040321438  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.10047910092088366  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.1431053673593453  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.12018542647654859  - accuracy: 0.78125\n",
      "At: 2522 [==========>] Loss 0.23842391919450093  - accuracy: 0.71875\n",
      "At: 2523 [==========>] Loss 0.11523007716497691  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.180469380511177  - accuracy: 0.65625\n",
      "At: 2525 [==========>] Loss 0.07210312510144215  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.10285700231775327  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.11049520920702001  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.09693455419610936  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.16219130453634878  - accuracy: 0.75\n",
      "At: 2530 [==========>] Loss 0.14568706690454897  - accuracy: 0.78125\n",
      "At: 2531 [==========>] Loss 0.03730897182203985  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.15106760624712526  - accuracy: 0.78125\n",
      "At: 2533 [==========>] Loss 0.10223536501251004  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.05629017686830308  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.05964305306447133  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.17993036998063341  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.10759844553284309  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.1390874839295902  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.08763242946801472  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.13270342825303968  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.0632362382086822  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.0560433180523264  - accuracy: 0.96875\n",
      "At: 2543 [==========>] Loss 0.1744988598311089  - accuracy: 0.75\n",
      "At: 2544 [==========>] Loss 0.13614009295811863  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.049405045587006285  - accuracy: 0.96875\n",
      "At: 2546 [==========>] Loss 0.14771163417400754  - accuracy: 0.8125\n",
      "At: 2547 [==========>] Loss 0.1153629498832615  - accuracy: 0.84375\n",
      "At: 2548 [==========>] Loss 0.09928939353013705  - accuracy: 0.84375\n",
      "At: 2549 [==========>] Loss 0.0915821796858423  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.13547201707083797  - accuracy: 0.8125\n",
      "At: 2551 [==========>] Loss 0.12437069422889577  - accuracy: 0.84375\n",
      "At: 2552 [==========>] Loss 0.10720140969336336  - accuracy: 0.84375\n",
      "At: 2553 [==========>] Loss 0.07828442048917103  - accuracy: 0.84375\n",
      "At: 2554 [==========>] Loss 0.16510086054100115  - accuracy: 0.75\n",
      "At: 2555 [==========>] Loss 0.197882112369401  - accuracy: 0.65625\n",
      "At: 2556 [==========>] Loss 0.08577677674170708  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.15958063125037592  - accuracy: 0.78125\n",
      "At: 2558 [==========>] Loss 0.08337249780120054  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.09462377615369143  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.18516076182366942  - accuracy: 0.75\n",
      "At: 2561 [==========>] Loss 0.10322395518467531  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.09740021823955508  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.12075193654425292  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09040380582017783  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.12100717641049531  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.18173647805148518  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.11923587899693758  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.09784113427158889  - accuracy: 0.84375\n",
      "At: 2569 [==========>] Loss 0.07597165490588043  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.1905518950481355  - accuracy: 0.75\n",
      "At: 2571 [==========>] Loss 0.087560026895532  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.163110342069513  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.16525643425916103  - accuracy: 0.75\n",
      "At: 2574 [==========>] Loss 0.13477980487924024  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.05171553111217861  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.11961300691513155  - accuracy: 0.8125\n",
      "At: 2577 [==========>] Loss 0.2281816525470555  - accuracy: 0.59375\n",
      "At: 2578 [==========>] Loss 0.18559481064446798  - accuracy: 0.71875\n",
      "At: 2579 [==========>] Loss 0.2036482354239536  - accuracy: 0.625\n",
      "At: 2580 [==========>] Loss 0.14327568368635735  - accuracy: 0.8125\n",
      "At: 2581 [==========>] Loss 0.07066952638653885  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.1815289446045543  - accuracy: 0.78125\n",
      "At: 2583 [==========>] Loss 0.07283999116504636  - accuracy: 0.90625\n",
      "At: 2584 [==========>] Loss 0.18458110845843306  - accuracy: 0.71875\n",
      "At: 2585 [==========>] Loss 0.081453785555651  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.10372944772396642  - accuracy: 0.8125\n",
      "At: 2587 [==========>] Loss 0.16122840747041772  - accuracy: 0.75\n",
      "At: 2588 [==========>] Loss 0.12708927861479796  - accuracy: 0.8125\n",
      "At: 2589 [==========>] Loss 0.14006508022799863  - accuracy: 0.78125\n",
      "At: 2590 [==========>] Loss 0.18431789084272657  - accuracy: 0.78125\n",
      "At: 2591 [==========>] Loss 0.12076930528835189  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.09439832625832581  - accuracy: 0.84375\n",
      "At: 2593 [==========>] Loss 0.09400431598784104  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.1007931327779443  - accuracy: 0.8125\n",
      "At: 2595 [==========>] Loss 0.0842270153551771  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.09689764995475902  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.06843478985342705  - accuracy: 0.875\n",
      "At: 2598 [==========>] Loss 0.12786967841364222  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.1584996006504229  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.14602249457355337  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.1240593402454197  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.07992696830147182  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.13765717754968637  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.11408832544788652  - accuracy: 0.84375\n",
      "At: 2605 [==========>] Loss 0.1220401310303873  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.13496332538837746  - accuracy: 0.8125\n",
      "At: 2607 [==========>] Loss 0.1356060993927008  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.06993159450618623  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.10580724102734354  - accuracy: 0.84375\n",
      "At: 2610 [==========>] Loss 0.12117407950825898  - accuracy: 0.78125\n",
      "At: 2611 [==========>] Loss 0.1541106277766938  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.09103925155925541  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.08084756694479188  - accuracy: 0.90625\n",
      "At: 2614 [==========>] Loss 0.12387023436875173  - accuracy: 0.84375\n",
      "At: 2615 [==========>] Loss 0.07495683465363694  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.06823296069467333  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.08688508143438659  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.0702058642433664  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.10182382248224475  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.11707247923749968  - accuracy: 0.84375\n",
      "At: 2621 [==========>] Loss 0.12658997014118173  - accuracy: 0.8125\n",
      "At: 2622 [==========>] Loss 0.11134258092000952  - accuracy: 0.875\n",
      "At: 2623 [==========>] Loss 0.09477093632133557  - accuracy: 0.875\n",
      "At: 2624 [==========>] Loss 0.11638813415673335  - accuracy: 0.84375\n",
      "At: 2625 [==========>] Loss 0.046459990751184115  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.047923645058096584  - accuracy: 0.9375\n",
      "At: 2627 [==========>] Loss 0.14239126562197693  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.08426561734663461  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.09185152124320971  - accuracy: 0.90625\n",
      "At: 2630 [==========>] Loss 0.09900413626764448  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.1355345512082951  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.12142626671541862  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.10566743353216129  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.07239288481035966  - accuracy: 0.875\n",
      "At: 2635 [==========>] Loss 0.1937271888615218  - accuracy: 0.75\n",
      "At: 2636 [==========>] Loss 0.11426915732971032  - accuracy: 0.84375\n",
      "At: 2637 [==========>] Loss 0.12756340051662488  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.0816005534589502  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.06414764639862311  - accuracy: 0.90625\n",
      "At: 2640 [==========>] Loss 0.11991783467809364  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.15119487976717766  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.1482248666077707  - accuracy: 0.78125\n",
      "At: 2643 [==========>] Loss 0.08460262195383063  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.1509971120709932  - accuracy: 0.84375\n",
      "At: 2645 [==========>] Loss 0.07787042306045998  - accuracy: 0.90625\n",
      "At: 2646 [==========>] Loss 0.1579662300782888  - accuracy: 0.75\n",
      "At: 2647 [==========>] Loss 0.10739341797139054  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.13593436958621824  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.12058475310937107  - accuracy: 0.875\n",
      "At: 2650 [==========>] Loss 0.09496187856614913  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.1866313465772017  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.08657086718384986  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.11511296434498039  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11317797214485051  - accuracy: 0.875\n",
      "At: 2655 [==========>] Loss 0.24662494637735835  - accuracy: 0.6875\n",
      "At: 2656 [==========>] Loss 0.018457138814493165  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.09392517998726274  - accuracy: 0.90625\n",
      "At: 2658 [==========>] Loss 0.09350829096441322  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.05070157332316454  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.09540548230348217  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.07183052888014645  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.06940654340435702  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.0844918505091907  - accuracy: 0.9375\n",
      "At: 2664 [==========>] Loss 0.0727240381505952  - accuracy: 0.96875\n",
      "At: 2665 [==========>] Loss 0.09964286031695928  - accuracy: 0.875\n",
      "At: 2666 [==========>] Loss 0.12322863256733461  - accuracy: 0.84375\n",
      "At: 2667 [==========>] Loss 0.1312066143540348  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.12634123480944787  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.13811705825540377  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.08957854579093708  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.06714002812672495  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.08989383387677931  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.1447547179375148  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.08851876046224147  - accuracy: 0.90625\n",
      "At: 2675 [==========>] Loss 0.14219982783151772  - accuracy: 0.78125\n",
      "At: 2676 [==========>] Loss 0.11626337359123129  - accuracy: 0.84375\n",
      "At: 2677 [==========>] Loss 0.12557546258252317  - accuracy: 0.78125\n",
      "At: 2678 [==========>] Loss 0.06096438560237603  - accuracy: 0.875\n",
      "At: 2679 [==========>] Loss 0.07509966725075273  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.09668663926458956  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.09842549518581517  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.17077658712018884  - accuracy: 0.8125\n",
      "At: 2683 [==========>] Loss 0.20678521873109573  - accuracy: 0.6875\n",
      "At: 2684 [==========>] Loss 0.10292060111165317  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.08477797212015364  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.06313087694107701  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.12339313028517795  - accuracy: 0.875\n",
      "At: 2688 [==========>] Loss 0.16978957729639643  - accuracy: 0.75\n",
      "At: 2689 [==========>] Loss 0.11253277598853344  - accuracy: 0.875\n",
      "At: 2690 [==========>] Loss 0.1181141820139549  - accuracy: 0.8125\n",
      "Epochs  5 / 10\n",
      "At: 1 [==========>] Loss 0.21439564066309252  - accuracy: 0.6875\n",
      "At: 2 [==========>] Loss 0.24836150924757766  - accuracy: 0.625\n",
      "At: 3 [==========>] Loss 0.1671548374197836  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.18014187990542896  - accuracy: 0.78125\n",
      "At: 5 [==========>] Loss 0.09611349835976866  - accuracy: 0.9375\n",
      "At: 6 [==========>] Loss 0.13741592703446656  - accuracy: 0.8125\n",
      "At: 7 [==========>] Loss 0.20730422590520198  - accuracy: 0.75\n",
      "At: 8 [==========>] Loss 0.25741409254318653  - accuracy: 0.75\n",
      "At: 9 [==========>] Loss 0.32429552482398205  - accuracy: 0.53125\n",
      "At: 10 [==========>] Loss 0.28538745933415066  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.27000373779066794  - accuracy: 0.625\n",
      "At: 12 [==========>] Loss 0.2119068402495878  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.20682064354406493  - accuracy: 0.78125\n",
      "At: 14 [==========>] Loss 0.09083804259112821  - accuracy: 0.90625\n",
      "At: 15 [==========>] Loss 0.17437136106438045  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.2052472400270661  - accuracy: 0.78125\n",
      "At: 17 [==========>] Loss 0.24195165910644464  - accuracy: 0.71875\n",
      "At: 18 [==========>] Loss 0.22065463205388317  - accuracy: 0.75\n",
      "At: 19 [==========>] Loss 0.21435661779650714  - accuracy: 0.78125\n",
      "At: 20 [==========>] Loss 0.14210941583001424  - accuracy: 0.78125\n",
      "At: 21 [==========>] Loss 0.27874642078884765  - accuracy: 0.65625\n",
      "At: 22 [==========>] Loss 0.2026741158263606  - accuracy: 0.75\n",
      "At: 23 [==========>] Loss 0.08236837423194573  - accuracy: 0.90625\n",
      "At: 24 [==========>] Loss 0.2762350363046501  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.2566738165079115  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.2685411209608385  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.2642274408108579  - accuracy: 0.6875\n",
      "At: 28 [==========>] Loss 0.18391340263532216  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.17207171222255432  - accuracy: 0.78125\n",
      "At: 30 [==========>] Loss 0.21144980351391277  - accuracy: 0.75\n",
      "At: 31 [==========>] Loss 0.2863615434695016  - accuracy: 0.6875\n",
      "At: 32 [==========>] Loss 0.2289123301823091  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.1356995887849688  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.17288927073338684  - accuracy: 0.84375\n",
      "At: 35 [==========>] Loss 0.20729400083622626  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.19365678239910972  - accuracy: 0.71875\n",
      "At: 37 [==========>] Loss 0.2698816829409159  - accuracy: 0.6875\n",
      "At: 38 [==========>] Loss 0.2680894610508431  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.22072234751005082  - accuracy: 0.75\n",
      "At: 40 [==========>] Loss 0.2971699240337279  - accuracy: 0.5625\n",
      "At: 41 [==========>] Loss 0.10306865089824443  - accuracy: 0.90625\n",
      "At: 42 [==========>] Loss 0.18892078402170243  - accuracy: 0.71875\n",
      "At: 43 [==========>] Loss 0.198447959920542  - accuracy: 0.78125\n",
      "At: 44 [==========>] Loss 0.20953688064693446  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.10415948233274795  - accuracy: 0.875\n",
      "At: 46 [==========>] Loss 0.18795200288579467  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.24460043245055244  - accuracy: 0.75\n",
      "At: 48 [==========>] Loss 0.16793494812698445  - accuracy: 0.78125\n",
      "At: 49 [==========>] Loss 0.12047700337584522  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.26470768022407765  - accuracy: 0.6875\n",
      "At: 51 [==========>] Loss 0.24854525296938768  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.28069682713210264  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.14281853034948663  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.19599174158659377  - accuracy: 0.75\n",
      "At: 55 [==========>] Loss 0.22206455064547742  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.1609055766360605  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.15558618639445362  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.23390628104190636  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.24608562130853556  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.23312473201936515  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.2838928712842588  - accuracy: 0.6875\n",
      "At: 62 [==========>] Loss 0.23480299641776847  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.19445938312281796  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.21094084227139087  - accuracy: 0.75\n",
      "At: 65 [==========>] Loss 0.30035369422339986  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.27122793187863026  - accuracy: 0.6875\n",
      "At: 67 [==========>] Loss 0.2511326503451342  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.1587779564769777  - accuracy: 0.8125\n",
      "At: 69 [==========>] Loss 0.17284685049037823  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.2172183835957363  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.167893149876692  - accuracy: 0.8125\n",
      "At: 72 [==========>] Loss 0.1752314945249326  - accuracy: 0.84375\n",
      "At: 73 [==========>] Loss 0.16123211899112755  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.25017455212707534  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.2602502702006486  - accuracy: 0.6875\n",
      "At: 76 [==========>] Loss 0.3161278633667917  - accuracy: 0.625\n",
      "At: 77 [==========>] Loss 0.2693672296301628  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.18581395495805997  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.18615105036001212  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.25650038132350184  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.2006754364626075  - accuracy: 0.8125\n",
      "At: 82 [==========>] Loss 0.2667397989830386  - accuracy: 0.625\n",
      "At: 83 [==========>] Loss 0.22808793225349386  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.2253506117482206  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.2373476191861419  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.15194659508453312  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.17996315815383135  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.3744767562410637  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.205526060973153  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.2524374330358214  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.18822957280900798  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.10214255884119687  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.15666174785113013  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.17964733616188167  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.20774705335907281  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.13705745692531096  - accuracy: 0.84375\n",
      "At: 97 [==========>] Loss 0.11453154315503303  - accuracy: 0.875\n",
      "At: 98 [==========>] Loss 0.30072761424547373  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.15579976566319895  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.12842708178408133  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.15299476715468321  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.1916474039866614  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.1705258715225857  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.1611420827416002  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.21245335265147186  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.2676807748590916  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.24969014303988285  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.2720520811029091  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.1133826669927719  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.31427483441834614  - accuracy: 0.625\n",
      "At: 111 [==========>] Loss 0.12225675387205763  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.21789941216458453  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.20723242495642918  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.17782165965377705  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.22292390794059114  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.23061803912923923  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.17194982776077467  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.2824442166165002  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.10476247859012341  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.21539297330794877  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.17280129561840818  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.172410858856627  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.22501891266880958  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.2749940816104549  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.20048731935138137  - accuracy: 0.75\n",
      "At: 126 [==========>] Loss 0.2680108442599125  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.2374840002309076  - accuracy: 0.71875\n",
      "At: 128 [==========>] Loss 0.27187747079585006  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.1578303612093716  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.1971833577173971  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.2157389810309322  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.2693326510888271  - accuracy: 0.71875\n",
      "At: 133 [==========>] Loss 0.24707163784199748  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.15306590973291367  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.19784841265207215  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.1922687100605866  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.095294579552887  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.20144700893641668  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.12635587610841933  - accuracy: 0.84375\n",
      "At: 140 [==========>] Loss 0.15240396594932298  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.3448504306293598  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.20729157586118613  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.18573667778296363  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.14739783851802307  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.12902203935455545  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.1758880673214872  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.23271160721376632  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.15078864369842407  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.20739265085000913  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.16974623869288352  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.19293549370825586  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.20988989358618534  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.22370819873952857  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.18741008990100194  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.19893403523108666  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1660303005068856  - accuracy: 0.78125\n",
      "At: 157 [==========>] Loss 0.27657814241067924  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.16040183403638483  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.14450663382924164  - accuracy: 0.78125\n",
      "At: 160 [==========>] Loss 0.13680773705123983  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.13960694485712907  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.20049489092471012  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.16753461719973134  - accuracy: 0.78125\n",
      "At: 164 [==========>] Loss 0.19412223310741775  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.25141000802588886  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.21199598327902475  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.13105421113016905  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.22608168154244174  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.18860951406356477  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.18060873530801652  - accuracy: 0.78125\n",
      "At: 171 [==========>] Loss 0.2744034986392263  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.2370287455733151  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.2688980057208946  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.20443296336994582  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.1830118332482658  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.1911271362877216  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.12887981840640386  - accuracy: 0.84375\n",
      "At: 178 [==========>] Loss 0.24543194674172406  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.18779051527719337  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.14220305639428044  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.061479226927650295  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.22512889638475742  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.17602653716725067  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.2217906643765421  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.1160888165077316  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.1685506436873861  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.17182060202172547  - accuracy: 0.78125\n",
      "At: 188 [==========>] Loss 0.22357512166413257  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.24326808869578176  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.14660490765861042  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.35369568487373026  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.1510349883346377  - accuracy: 0.78125\n",
      "At: 193 [==========>] Loss 0.26613554345285284  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.20573063278373288  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.1730187434314327  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.17352865878663631  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.200238317662173  - accuracy: 0.78125\n",
      "At: 198 [==========>] Loss 0.13302540022016826  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.12844344043355374  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.20751095268431732  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.15716288922394842  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.12547164148451936  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.15635722288257276  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.2327564069829085  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.18449251181112963  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.08913284790070516  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.11319875723018691  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.2649043942138127  - accuracy: 0.6875\n",
      "At: 209 [==========>] Loss 0.22309969241979  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.12245606239616351  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.167201388945098  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.23991442574656605  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.1886666731306869  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.2448092709691321  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.12626284617055203  - accuracy: 0.875\n",
      "At: 216 [==========>] Loss 0.20797147333190125  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.25868770701723565  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.17193303956284872  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.22942466144796353  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.22107603934841102  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.15154269333779247  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.0699256003903867  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.320333246461  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.21514985219739097  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.13981529355643993  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.20031768134837785  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.21314777989678424  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.20381311117061557  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.21809886860540115  - accuracy: 0.75\n",
      "At: 230 [==========>] Loss 0.1837249023043773  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.2437037815804043  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.2539470047439801  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.21942175668718072  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.1173244303363359  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.2420389564194577  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.20284901023972568  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.14265935176623984  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.14084050837652587  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.1349261029286607  - accuracy: 0.84375\n",
      "At: 240 [==========>] Loss 0.2826436623674159  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.14852709752027402  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.2065369027487346  - accuracy: 0.75\n",
      "At: 243 [==========>] Loss 0.1235358129746053  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.15553593794001264  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.14737270473929714  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.14355618379410748  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.20187043953135755  - accuracy: 0.75\n",
      "At: 248 [==========>] Loss 0.13052236537418194  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.09310199301710606  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.2180760264462268  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.24289662139905877  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.13198965311296887  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.2078682488443899  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.0891617061565183  - accuracy: 0.875\n",
      "At: 255 [==========>] Loss 0.15539959695899147  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.22003036041358875  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.1283330183529792  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.17136408717220833  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.1568287296532137  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.14278991569931632  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.09242911818477678  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.20409052320400306  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.127012085514716  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.14068502289866247  - accuracy: 0.8125\n",
      "At: 265 [==========>] Loss 0.23604425843669413  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.30847978807310583  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.17732106261712766  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.267111136986679  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.14782925376085915  - accuracy: 0.875\n",
      "At: 270 [==========>] Loss 0.30589727048505244  - accuracy: 0.59375\n",
      "At: 271 [==========>] Loss 0.2206883268590349  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.10857733498170452  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.19615066965274106  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.1992035201521543  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.0863219805776862  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.2356009034046003  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.12247723121847418  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.14143530827016362  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.17723968871685367  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.15720919588278814  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.11424024688863871  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.2518257715234421  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.15185480528019646  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.09450631632856313  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.18595111748953008  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.10027366003989743  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.19453941773667585  - accuracy: 0.78125\n",
      "At: 288 [==========>] Loss 0.1475842775345813  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.1308551023014456  - accuracy: 0.84375\n",
      "At: 290 [==========>] Loss 0.11627795263519958  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.10953805192772312  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.22445638760507247  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.21425212038490113  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.20843216340128834  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.19031193145752792  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.14417205067616545  - accuracy: 0.78125\n",
      "At: 297 [==========>] Loss 0.1613225212745295  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.1663687102345588  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.22201649664722675  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.20332898352103604  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.17768955012018728  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.10473650133664014  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.107780003115936  - accuracy: 0.90625\n",
      "At: 304 [==========>] Loss 0.24556986028151823  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.2733303237492439  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.1154821910080917  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.2618157008453579  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.20903845930306791  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.08556864416037643  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.2528353997514736  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.05666408904024685  - accuracy: 0.9375\n",
      "At: 312 [==========>] Loss 0.16076294343486847  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.14390974712047416  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.2261838421896829  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.13880428128213712  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.2212484392612031  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.31266260413445224  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.16581972429033498  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.11256908142600587  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.21888932950145368  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.28252382935201364  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.11449265436845935  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.09478126850133743  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.20702282697273094  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.11580330469049649  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.17838605611465347  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.11902488658556741  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.1424051594679298  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.11283086535896233  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.14354118386131345  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.2202688343646682  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.279068728953773  - accuracy: 0.625\n",
      "At: 333 [==========>] Loss 0.1772096022703574  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.09887150384240034  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.14804633131909933  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.15212697222199964  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.2200563495185469  - accuracy: 0.71875\n",
      "At: 338 [==========>] Loss 0.15258668083592547  - accuracy: 0.84375\n",
      "At: 339 [==========>] Loss 0.1980296696628625  - accuracy: 0.75\n",
      "At: 340 [==========>] Loss 0.14237418966768114  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.10434710486951354  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.2007313901062479  - accuracy: 0.75\n",
      "At: 343 [==========>] Loss 0.29249401022406457  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.21534235968870727  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.2452793374823851  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.16466081411582847  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.13034388133550784  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.13815959419876916  - accuracy: 0.78125\n",
      "At: 349 [==========>] Loss 0.17607064568332806  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.12970569537149218  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.2488131692683755  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.12373451653008367  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.17174544719697754  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.21999518133329832  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.1221592751211709  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.2093708689057086  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.13288267161893075  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.1302297129487452  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.09450949025598707  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.17622025955069195  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.08690319070642494  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.16188156384874108  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.11051263283862281  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.2257453611576315  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.12037060868306709  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.22136651645642705  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.20679703748547393  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.17379773701905707  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.1607717932362509  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.18026777684601358  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.11560278821328467  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.1263008392090918  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.23479675868256683  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.10016482825756695  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1697523947550976  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.0691845852815736  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.2583125004991598  - accuracy: 0.6875\n",
      "At: 378 [==========>] Loss 0.20150599544706188  - accuracy: 0.71875\n",
      "At: 379 [==========>] Loss 0.20185005699144973  - accuracy: 0.71875\n",
      "At: 380 [==========>] Loss 0.1355195167742948  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.20621588005052197  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.10433333276420928  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.18854447810257696  - accuracy: 0.78125\n",
      "At: 384 [==========>] Loss 0.17351766235222538  - accuracy: 0.75\n",
      "At: 385 [==========>] Loss 0.16088033423401868  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.2364550199388571  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.08544190848125077  - accuracy: 0.875\n",
      "At: 388 [==========>] Loss 0.23609826149868468  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.2553608025998952  - accuracy: 0.71875\n",
      "At: 390 [==========>] Loss 0.126863294247578  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.1458421475662494  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.16408027044632328  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.26653764493022963  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.1020980115891294  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.18661645776677796  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.18194589691224644  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.15065755732876665  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.23064659656786116  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.2312562536768011  - accuracy: 0.6875\n",
      "At: 400 [==========>] Loss 0.15624434949958044  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.14696726348082645  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.11917013202987176  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.03819250008239075  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.11786973871108933  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.2220355776134565  - accuracy: 0.71875\n",
      "At: 406 [==========>] Loss 0.17802690467636884  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.17429199579603544  - accuracy: 0.78125\n",
      "At: 408 [==========>] Loss 0.24115607827675262  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.25125815917174393  - accuracy: 0.65625\n",
      "At: 410 [==========>] Loss 0.1306660147170451  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10511314890483398  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.22113596918718414  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.1432661249101684  - accuracy: 0.84375\n",
      "At: 414 [==========>] Loss 0.19362590200166646  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.14451763729724254  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.21003645037600172  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.17461740958940952  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.12901931651478193  - accuracy: 0.875\n",
      "At: 419 [==========>] Loss 0.16444296813219808  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.17200479274935693  - accuracy: 0.71875\n",
      "At: 421 [==========>] Loss 0.14800799362325712  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.13620078975487226  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.12713775328595084  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.2545062649677601  - accuracy: 0.6875\n",
      "At: 425 [==========>] Loss 0.23350006250798552  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.15663744166896254  - accuracy: 0.8125\n",
      "At: 427 [==========>] Loss 0.2016786051714102  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.2426028507004241  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.17686424803928258  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.1450278709542568  - accuracy: 0.78125\n",
      "At: 431 [==========>] Loss 0.16193466330344122  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.14216731623319534  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.14178779144441067  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.161989631675248  - accuracy: 0.78125\n",
      "At: 435 [==========>] Loss 0.23155405359790635  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.15251875919525976  - accuracy: 0.8125\n",
      "At: 437 [==========>] Loss 0.1700094801845728  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.16578897554061214  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.11578351593397129  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.0981229189357964  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.2056950063409827  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.21085436294428989  - accuracy: 0.6875\n",
      "At: 443 [==========>] Loss 0.15160329523168078  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.19818838661539698  - accuracy: 0.8125\n",
      "At: 445 [==========>] Loss 0.15997506680705076  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.2623119755764544  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.15739199344581328  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.19913636116416453  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.12235140188863826  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.14992397259406257  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.18995078940051596  - accuracy: 0.78125\n",
      "At: 452 [==========>] Loss 0.1588951114576665  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.17872903536723553  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.23847274255674542  - accuracy: 0.65625\n",
      "At: 455 [==========>] Loss 0.17189259792974348  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.15087240605537122  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.16614714615283016  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.12482064823768113  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.19846316024712873  - accuracy: 0.71875\n",
      "At: 460 [==========>] Loss 0.13922748876722324  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.23467197591515812  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.14607897011317114  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.1421524837220227  - accuracy: 0.78125\n",
      "At: 464 [==========>] Loss 0.18139404157095995  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.17607318744206585  - accuracy: 0.71875\n",
      "At: 466 [==========>] Loss 0.1089487266298048  - accuracy: 0.8125\n",
      "At: 467 [==========>] Loss 0.1760407700743541  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.12728081873161606  - accuracy: 0.84375\n",
      "At: 469 [==========>] Loss 0.15629302503667505  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.1434623230831099  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.21933654694113658  - accuracy: 0.71875\n",
      "At: 472 [==========>] Loss 0.12739896051134686  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.16521619526824355  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.12506289999619902  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.1549573200482372  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.1827228177498325  - accuracy: 0.75\n",
      "At: 477 [==========>] Loss 0.13927671201290634  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.11459876323585506  - accuracy: 0.875\n",
      "At: 479 [==========>] Loss 0.1947511473944023  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.19901917811967715  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.14113221381604424  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.1260649110379607  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.13115758930449548  - accuracy: 0.84375\n",
      "At: 484 [==========>] Loss 0.12728370379755152  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.12203644018199711  - accuracy: 0.8125\n",
      "At: 486 [==========>] Loss 0.21408962436966128  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.17434844907086192  - accuracy: 0.75\n",
      "At: 488 [==========>] Loss 0.1688955429331352  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.1974097749804305  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.1533416481981121  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.1816348945419251  - accuracy: 0.71875\n",
      "At: 492 [==========>] Loss 0.27059762759708234  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.1171109235007137  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.20129562115280764  - accuracy: 0.75\n",
      "At: 495 [==========>] Loss 0.16879362008201143  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.1932019830606134  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.1773925072511443  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.07700975734425934  - accuracy: 0.9375\n",
      "At: 499 [==========>] Loss 0.1690043610720118  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.1653950430601276  - accuracy: 0.78125\n",
      "At: 501 [==========>] Loss 0.13788648323228442  - accuracy: 0.75\n",
      "At: 502 [==========>] Loss 0.13054264131941526  - accuracy: 0.8125\n",
      "At: 503 [==========>] Loss 0.14299050937678784  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.10580544007550614  - accuracy: 0.84375\n",
      "At: 505 [==========>] Loss 0.1821742952455632  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.28469417998914187  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.1352885607879399  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.12199560595352657  - accuracy: 0.90625\n",
      "At: 509 [==========>] Loss 0.21418960807674278  - accuracy: 0.71875\n",
      "At: 510 [==========>] Loss 0.18980081785059108  - accuracy: 0.75\n",
      "At: 511 [==========>] Loss 0.11918370540493947  - accuracy: 0.875\n",
      "At: 512 [==========>] Loss 0.1677880293582773  - accuracy: 0.8125\n",
      "At: 513 [==========>] Loss 0.2337634952092707  - accuracy: 0.6875\n",
      "At: 514 [==========>] Loss 0.222940409640117  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.10899195163844874  - accuracy: 0.8125\n",
      "At: 516 [==========>] Loss 0.20490405273606005  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.13785445647228042  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.16723710995913973  - accuracy: 0.84375\n",
      "At: 519 [==========>] Loss 0.1315208150784869  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.14153734996564737  - accuracy: 0.78125\n",
      "At: 521 [==========>] Loss 0.18395342132677073  - accuracy: 0.75\n",
      "At: 522 [==========>] Loss 0.15502553670848537  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.19516887720077888  - accuracy: 0.6875\n",
      "At: 524 [==========>] Loss 0.12390967095505132  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.18407811012933986  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.20779360381342685  - accuracy: 0.78125\n",
      "At: 527 [==========>] Loss 0.2985526648991518  - accuracy: 0.5625\n",
      "At: 528 [==========>] Loss 0.19422573104477245  - accuracy: 0.78125\n",
      "At: 529 [==========>] Loss 0.13608920577543276  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.26500651904789757  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.16665510355080207  - accuracy: 0.75\n",
      "At: 532 [==========>] Loss 0.10868638904069597  - accuracy: 0.84375\n",
      "At: 533 [==========>] Loss 0.1012815153870209  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.18941035687297944  - accuracy: 0.71875\n",
      "At: 535 [==========>] Loss 0.17462115925024335  - accuracy: 0.75\n",
      "At: 536 [==========>] Loss 0.18281853606694642  - accuracy: 0.78125\n",
      "At: 537 [==========>] Loss 0.13221618277463676  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.14558779641639585  - accuracy: 0.8125\n",
      "At: 539 [==========>] Loss 0.09584500552649589  - accuracy: 0.84375\n",
      "At: 540 [==========>] Loss 0.25529429595797903  - accuracy: 0.59375\n",
      "At: 541 [==========>] Loss 0.181411903331174  - accuracy: 0.71875\n",
      "At: 542 [==========>] Loss 0.14331107120372732  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.1788522895824653  - accuracy: 0.75\n",
      "At: 544 [==========>] Loss 0.26477492189502627  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.10935018598719146  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.13737055001732693  - accuracy: 0.8125\n",
      "At: 547 [==========>] Loss 0.14450751504185855  - accuracy: 0.8125\n",
      "At: 548 [==========>] Loss 0.11384336217387353  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.1350458089429161  - accuracy: 0.8125\n",
      "At: 550 [==========>] Loss 0.10688640859330056  - accuracy: 0.90625\n",
      "At: 551 [==========>] Loss 0.13614956996327487  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.1697148745783253  - accuracy: 0.8125\n",
      "At: 553 [==========>] Loss 0.1301871818419555  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.11570841022254824  - accuracy: 0.875\n",
      "At: 555 [==========>] Loss 0.17425090262244558  - accuracy: 0.8125\n",
      "At: 556 [==========>] Loss 0.1669491324674937  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.16016100873537564  - accuracy: 0.8125\n",
      "At: 558 [==========>] Loss 0.15537607063350511  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.23726593272620172  - accuracy: 0.6875\n",
      "At: 560 [==========>] Loss 0.15549256681085616  - accuracy: 0.8125\n",
      "At: 561 [==========>] Loss 0.15132481081539437  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.08767302291073652  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.13183205658824626  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.20039729686096847  - accuracy: 0.71875\n",
      "At: 565 [==========>] Loss 0.13110644349870712  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.16227878326352618  - accuracy: 0.78125\n",
      "At: 567 [==========>] Loss 0.19461758529263384  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.1982240052694324  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.1603510091087602  - accuracy: 0.6875\n",
      "At: 570 [==========>] Loss 0.08842691887184212  - accuracy: 0.90625\n",
      "At: 571 [==========>] Loss 0.14707491025525798  - accuracy: 0.78125\n",
      "At: 572 [==========>] Loss 0.13439987269861625  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.08631925208416827  - accuracy: 0.90625\n",
      "At: 574 [==========>] Loss 0.15661768859093617  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.16055635979292385  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.06881798284905044  - accuracy: 0.96875\n",
      "At: 577 [==========>] Loss 0.1654265902236124  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.1895245473546233  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.14567582538770768  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.1350179683204242  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.15061780715745549  - accuracy: 0.78125\n",
      "At: 582 [==========>] Loss 0.19245988490885296  - accuracy: 0.75\n",
      "At: 583 [==========>] Loss 0.2019477781904514  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.12073318140497791  - accuracy: 0.875\n",
      "At: 585 [==========>] Loss 0.1865549012708761  - accuracy: 0.6875\n",
      "At: 586 [==========>] Loss 0.06331487392609579  - accuracy: 0.9375\n",
      "At: 587 [==========>] Loss 0.1824616329628026  - accuracy: 0.75\n",
      "At: 588 [==========>] Loss 0.17633382590247834  - accuracy: 0.8125\n",
      "At: 589 [==========>] Loss 0.16596467675410354  - accuracy: 0.78125\n",
      "At: 590 [==========>] Loss 0.06631146897168427  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.13626686260115584  - accuracy: 0.90625\n",
      "At: 592 [==========>] Loss 0.12035192479477021  - accuracy: 0.8125\n",
      "At: 593 [==========>] Loss 0.23138982796462904  - accuracy: 0.65625\n",
      "At: 594 [==========>] Loss 0.14894430432255268  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.11389633085362055  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.15041883471815637  - accuracy: 0.78125\n",
      "At: 597 [==========>] Loss 0.2322509327762728  - accuracy: 0.65625\n",
      "At: 598 [==========>] Loss 0.13693668841407192  - accuracy: 0.84375\n",
      "At: 599 [==========>] Loss 0.1582295340108788  - accuracy: 0.84375\n",
      "At: 600 [==========>] Loss 0.11441634194230575  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.14209926964799766  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.13996587197726915  - accuracy: 0.78125\n",
      "At: 603 [==========>] Loss 0.18943934809572802  - accuracy: 0.75\n",
      "At: 604 [==========>] Loss 0.23888692527046035  - accuracy: 0.625\n",
      "At: 605 [==========>] Loss 0.11424991519199426  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.1697430231499819  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.13554320324937713  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.16496374764403682  - accuracy: 0.75\n",
      "At: 609 [==========>] Loss 0.11823728062542121  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.17973893248327955  - accuracy: 0.6875\n",
      "At: 611 [==========>] Loss 0.13809255022330758  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.14842112835750781  - accuracy: 0.78125\n",
      "At: 613 [==========>] Loss 0.18299186936750522  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.09813325037537085  - accuracy: 0.875\n",
      "At: 615 [==========>] Loss 0.16025479092351108  - accuracy: 0.78125\n",
      "At: 616 [==========>] Loss 0.18244963499830247  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.15103770914551667  - accuracy: 0.78125\n",
      "At: 618 [==========>] Loss 0.23421878578394578  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.1585074170709774  - accuracy: 0.8125\n",
      "At: 620 [==========>] Loss 0.1963004361763484  - accuracy: 0.75\n",
      "At: 621 [==========>] Loss 0.07216882131900929  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.13812847016492408  - accuracy: 0.78125\n",
      "At: 623 [==========>] Loss 0.16061586513286286  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.11230679896084808  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.1518895117229123  - accuracy: 0.8125\n",
      "At: 626 [==========>] Loss 0.1815263425785028  - accuracy: 0.6875\n",
      "At: 627 [==========>] Loss 0.15826865648812005  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.12948909420377838  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.21349509402840497  - accuracy: 0.65625\n",
      "At: 630 [==========>] Loss 0.2566322558076132  - accuracy: 0.59375\n",
      "At: 631 [==========>] Loss 0.16303221128412987  - accuracy: 0.8125\n",
      "At: 632 [==========>] Loss 0.16845059056753925  - accuracy: 0.78125\n",
      "At: 633 [==========>] Loss 0.1895023160825495  - accuracy: 0.78125\n",
      "At: 634 [==========>] Loss 0.15852209252750687  - accuracy: 0.71875\n",
      "At: 635 [==========>] Loss 0.12244966520179223  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.14946665546532462  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.14984399424628123  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.1417830877697611  - accuracy: 0.8125\n",
      "At: 639 [==========>] Loss 0.1469028390880166  - accuracy: 0.84375\n",
      "At: 640 [==========>] Loss 0.20659766138979924  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.14442015263852684  - accuracy: 0.78125\n",
      "At: 642 [==========>] Loss 0.1734730746460763  - accuracy: 0.75\n",
      "At: 643 [==========>] Loss 0.13881512122534054  - accuracy: 0.84375\n",
      "At: 644 [==========>] Loss 0.08673390548031176  - accuracy: 0.90625\n",
      "At: 645 [==========>] Loss 0.1386566770231275  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.14658836028261696  - accuracy: 0.8125\n",
      "At: 647 [==========>] Loss 0.16639270483539273  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.12613360843319701  - accuracy: 0.875\n",
      "At: 649 [==========>] Loss 0.19299781219260154  - accuracy: 0.71875\n",
      "At: 650 [==========>] Loss 0.09217957919615591  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.2037606627650395  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.09534968633733382  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.12185412212186857  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.08503082328812556  - accuracy: 0.90625\n",
      "At: 655 [==========>] Loss 0.15110894852689793  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.12239501338987024  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.16218670383728173  - accuracy: 0.78125\n",
      "At: 658 [==========>] Loss 0.1452311368356004  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.16515831560211885  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.1362499071923221  - accuracy: 0.8125\n",
      "At: 661 [==========>] Loss 0.20602131012764677  - accuracy: 0.71875\n",
      "At: 662 [==========>] Loss 0.14025196125290268  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.10845322137249311  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.13169251187221512  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.19577129126410264  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.22834059050689495  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.16799654267600772  - accuracy: 0.78125\n",
      "At: 668 [==========>] Loss 0.15425634470972233  - accuracy: 0.78125\n",
      "At: 669 [==========>] Loss 0.1460785609092045  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.24965160345189216  - accuracy: 0.625\n",
      "At: 671 [==========>] Loss 0.10163277739552154  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.11258973059257  - accuracy: 0.875\n",
      "At: 673 [==========>] Loss 0.08044862105984169  - accuracy: 0.96875\n",
      "At: 674 [==========>] Loss 0.13339799865748084  - accuracy: 0.78125\n",
      "At: 675 [==========>] Loss 0.100099856941147  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.17445304503678344  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.16441477357662065  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.14583086443790555  - accuracy: 0.78125\n",
      "At: 679 [==========>] Loss 0.12448373047098328  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.09732773657391519  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.13937820862501013  - accuracy: 0.875\n",
      "At: 682 [==========>] Loss 0.15100136111369752  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.13120138164110787  - accuracy: 0.84375\n",
      "At: 684 [==========>] Loss 0.10458858651587144  - accuracy: 0.90625\n",
      "At: 685 [==========>] Loss 0.13978618164787576  - accuracy: 0.875\n",
      "At: 686 [==========>] Loss 0.13168158608589747  - accuracy: 0.78125\n",
      "At: 687 [==========>] Loss 0.05852898747165151  - accuracy: 0.96875\n",
      "At: 688 [==========>] Loss 0.08944135916768109  - accuracy: 0.90625\n",
      "At: 689 [==========>] Loss 0.14041907178753482  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.1257056610865176  - accuracy: 0.875\n",
      "At: 691 [==========>] Loss 0.11111090179633623  - accuracy: 0.90625\n",
      "At: 692 [==========>] Loss 0.0885925134395312  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.16352520978671115  - accuracy: 0.84375\n",
      "At: 694 [==========>] Loss 0.14978488999847972  - accuracy: 0.8125\n",
      "At: 695 [==========>] Loss 0.1404171491112589  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.12388531962272906  - accuracy: 0.8125\n",
      "At: 697 [==========>] Loss 0.18669132168212305  - accuracy: 0.75\n",
      "At: 698 [==========>] Loss 0.10727472933686708  - accuracy: 0.90625\n",
      "At: 699 [==========>] Loss 0.12142791665806149  - accuracy: 0.84375\n",
      "At: 700 [==========>] Loss 0.11395031440871611  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.16640398796620287  - accuracy: 0.78125\n",
      "At: 702 [==========>] Loss 0.09481380457434986  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.1907689675192594  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.1765120917528658  - accuracy: 0.71875\n",
      "At: 705 [==========>] Loss 0.21496206729109174  - accuracy: 0.65625\n",
      "At: 706 [==========>] Loss 0.12397565085039856  - accuracy: 0.84375\n",
      "At: 707 [==========>] Loss 0.11104207473173866  - accuracy: 0.90625\n",
      "At: 708 [==========>] Loss 0.16240467512205178  - accuracy: 0.8125\n",
      "At: 709 [==========>] Loss 0.16731683837392883  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.15039577025176173  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.23621782568344973  - accuracy: 0.65625\n",
      "At: 712 [==========>] Loss 0.16853549963051667  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.22065284713456595  - accuracy: 0.65625\n",
      "At: 714 [==========>] Loss 0.22533539382412351  - accuracy: 0.59375\n",
      "At: 715 [==========>] Loss 0.11526272368310023  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.13620260594108266  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.09192868992367442  - accuracy: 0.84375\n",
      "At: 718 [==========>] Loss 0.1659159768429256  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.1272494230321427  - accuracy: 0.78125\n",
      "At: 720 [==========>] Loss 0.15164881006539718  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.14467973194516048  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.1691181157470351  - accuracy: 0.78125\n",
      "At: 723 [==========>] Loss 0.13507156583807553  - accuracy: 0.875\n",
      "At: 724 [==========>] Loss 0.09228364149047905  - accuracy: 0.90625\n",
      "At: 725 [==========>] Loss 0.1558456207056149  - accuracy: 0.84375\n",
      "At: 726 [==========>] Loss 0.20209867533073522  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.1615458412451608  - accuracy: 0.75\n",
      "At: 728 [==========>] Loss 0.1686789711494847  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.16592743793180287  - accuracy: 0.6875\n",
      "At: 730 [==========>] Loss 0.21626189366431342  - accuracy: 0.71875\n",
      "At: 731 [==========>] Loss 0.1485677695599703  - accuracy: 0.84375\n",
      "At: 732 [==========>] Loss 0.14976805002802418  - accuracy: 0.84375\n",
      "At: 733 [==========>] Loss 0.10839892004704638  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.21276847976947227  - accuracy: 0.6875\n",
      "At: 735 [==========>] Loss 0.11770358954815817  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.09171824575926857  - accuracy: 0.90625\n",
      "At: 737 [==========>] Loss 0.16537513763906944  - accuracy: 0.84375\n",
      "At: 738 [==========>] Loss 0.10702762719952467  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.059014186646391624  - accuracy: 0.96875\n",
      "At: 740 [==========>] Loss 0.14845921001098927  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.12087777705358044  - accuracy: 0.84375\n",
      "At: 742 [==========>] Loss 0.15794952528372835  - accuracy: 0.78125\n",
      "At: 743 [==========>] Loss 0.14183596954726066  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.13486484459169867  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.17243210803037226  - accuracy: 0.78125\n",
      "At: 746 [==========>] Loss 0.12280422571256862  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.141469210768849  - accuracy: 0.875\n",
      "At: 748 [==========>] Loss 0.1565054942635203  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.197929561734302  - accuracy: 0.65625\n",
      "At: 750 [==========>] Loss 0.13020948480594424  - accuracy: 0.8125\n",
      "At: 751 [==========>] Loss 0.17433841117772675  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.08335708420412394  - accuracy: 0.9375\n",
      "At: 753 [==========>] Loss 0.20241816788641925  - accuracy: 0.6875\n",
      "At: 754 [==========>] Loss 0.14976926139557328  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.11211556037674972  - accuracy: 0.8125\n",
      "At: 756 [==========>] Loss 0.18395159882527312  - accuracy: 0.75\n",
      "At: 757 [==========>] Loss 0.0772773570947456  - accuracy: 0.90625\n",
      "At: 758 [==========>] Loss 0.1509328780692213  - accuracy: 0.78125\n",
      "At: 759 [==========>] Loss 0.07492014171194938  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.17465531953263844  - accuracy: 0.75\n",
      "At: 761 [==========>] Loss 0.09612094701344648  - accuracy: 0.84375\n",
      "At: 762 [==========>] Loss 0.12648604182941342  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.11950679196349001  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.12678811805253237  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.1772904147759185  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.13845967436279172  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.14082622654935673  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.17437270189901993  - accuracy: 0.71875\n",
      "At: 769 [==========>] Loss 0.1446507064307684  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.14568403522724147  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.17926196079962897  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.13933964918465375  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.08632050608365857  - accuracy: 0.84375\n",
      "At: 774 [==========>] Loss 0.16261752157268913  - accuracy: 0.78125\n",
      "At: 775 [==========>] Loss 0.20541167229510704  - accuracy: 0.6875\n",
      "At: 776 [==========>] Loss 0.20885878346003808  - accuracy: 0.71875\n",
      "At: 777 [==========>] Loss 0.09539621790336172  - accuracy: 0.84375\n",
      "At: 778 [==========>] Loss 0.15232394815159933  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.14186593918265886  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.07383126156654135  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.15916108987453165  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.17551555528559937  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.19493050388110894  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.1960057658844398  - accuracy: 0.65625\n",
      "At: 785 [==========>] Loss 0.25979392928839135  - accuracy: 0.65625\n",
      "At: 786 [==========>] Loss 0.13915610817035676  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.18017664722175136  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.08518199343581753  - accuracy: 0.875\n",
      "At: 789 [==========>] Loss 0.1731491900358469  - accuracy: 0.75\n",
      "At: 790 [==========>] Loss 0.14252181932487215  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.17292148892013748  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.17370443075470615  - accuracy: 0.78125\n",
      "At: 793 [==========>] Loss 0.11655530588839456  - accuracy: 0.84375\n",
      "At: 794 [==========>] Loss 0.12281758367056901  - accuracy: 0.8125\n",
      "At: 795 [==========>] Loss 0.11020475756773318  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.17391642005818791  - accuracy: 0.75\n",
      "At: 797 [==========>] Loss 0.16482238946510136  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.1775624206747139  - accuracy: 0.75\n",
      "At: 799 [==========>] Loss 0.07707489518962918  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.12887901240414776  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.125406263379377  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.24396402450639676  - accuracy: 0.6875\n",
      "At: 803 [==========>] Loss 0.15140220880048133  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.14690178850039443  - accuracy: 0.78125\n",
      "At: 805 [==========>] Loss 0.1405070000725071  - accuracy: 0.78125\n",
      "At: 806 [==========>] Loss 0.10876812544213707  - accuracy: 0.8125\n",
      "At: 807 [==========>] Loss 0.12867542305578153  - accuracy: 0.875\n",
      "At: 808 [==========>] Loss 0.13342545612557946  - accuracy: 0.78125\n",
      "At: 809 [==========>] Loss 0.10999697435520595  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.15608169172689484  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.14975763432361947  - accuracy: 0.8125\n",
      "At: 812 [==========>] Loss 0.12983345480419545  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.2037848779471077  - accuracy: 0.75\n",
      "At: 814 [==========>] Loss 0.1873535936025667  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.13948373049339696  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.17289817760633897  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.07058280626829389  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.13731861179085658  - accuracy: 0.8125\n",
      "At: 819 [==========>] Loss 0.10041936536374335  - accuracy: 0.90625\n",
      "At: 820 [==========>] Loss 0.13699888187614184  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.16263611570217051  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.16338671149127032  - accuracy: 0.78125\n",
      "At: 823 [==========>] Loss 0.14134132061108814  - accuracy: 0.84375\n",
      "At: 824 [==========>] Loss 0.15367074225138322  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.18694746062609816  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.144960617195716  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.09508440389344382  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.05291484337929002  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.10853526727341659  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.09899942513719834  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.09754407918057335  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.09449815124593641  - accuracy: 0.9375\n",
      "At: 833 [==========>] Loss 0.1971258259433623  - accuracy: 0.6875\n",
      "At: 834 [==========>] Loss 0.16672942379373237  - accuracy: 0.71875\n",
      "At: 835 [==========>] Loss 0.09639106737707412  - accuracy: 0.875\n",
      "At: 836 [==========>] Loss 0.08920354277043119  - accuracy: 0.9375\n",
      "At: 837 [==========>] Loss 0.0988268472402449  - accuracy: 0.875\n",
      "At: 838 [==========>] Loss 0.12389261301279988  - accuracy: 0.84375\n",
      "At: 839 [==========>] Loss 0.11911548047693324  - accuracy: 0.78125\n",
      "At: 840 [==========>] Loss 0.12172402938367563  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.08504969018303929  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.07304542871054795  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.1861299255184346  - accuracy: 0.78125\n",
      "At: 844 [==========>] Loss 0.13690771888255024  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.14101481534718305  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.13881155275667642  - accuracy: 0.78125\n",
      "At: 847 [==========>] Loss 0.0821158004173685  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.15321430802282016  - accuracy: 0.84375\n",
      "At: 849 [==========>] Loss 0.14193370581449105  - accuracy: 0.84375\n",
      "At: 850 [==========>] Loss 0.12208976576361551  - accuracy: 0.84375\n",
      "At: 851 [==========>] Loss 0.0956909317714984  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.13330299772561152  - accuracy: 0.8125\n",
      "At: 853 [==========>] Loss 0.15989719388138707  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.20831864171966413  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.07895148528076285  - accuracy: 0.84375\n",
      "At: 856 [==========>] Loss 0.07687195594562415  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.06647224464730075  - accuracy: 0.96875\n",
      "At: 858 [==========>] Loss 0.2972646410135477  - accuracy: 0.59375\n",
      "At: 859 [==========>] Loss 0.10919708052188784  - accuracy: 0.90625\n",
      "At: 860 [==========>] Loss 0.13462456995717959  - accuracy: 0.8125\n",
      "At: 861 [==========>] Loss 0.11587325760896117  - accuracy: 0.8125\n",
      "At: 862 [==========>] Loss 0.08724229133600622  - accuracy: 0.9375\n",
      "At: 863 [==========>] Loss 0.1446973000853588  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.1878751398099427  - accuracy: 0.6875\n",
      "At: 865 [==========>] Loss 0.23585587714478504  - accuracy: 0.625\n",
      "At: 866 [==========>] Loss 0.16728663243721806  - accuracy: 0.75\n",
      "At: 867 [==========>] Loss 0.09637551866196091  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.16411787273233108  - accuracy: 0.78125\n",
      "At: 869 [==========>] Loss 0.1973512582831476  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.1455659393792353  - accuracy: 0.75\n",
      "At: 871 [==========>] Loss 0.08048373462629069  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.07654574176674117  - accuracy: 0.90625\n",
      "At: 873 [==========>] Loss 0.14878533274482705  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.19291821752542704  - accuracy: 0.71875\n",
      "At: 875 [==========>] Loss 0.11382769213529967  - accuracy: 0.84375\n",
      "At: 876 [==========>] Loss 0.13405066474008057  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.18079386496843136  - accuracy: 0.75\n",
      "At: 878 [==========>] Loss 0.04540409649169799  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.13913432335680992  - accuracy: 0.84375\n",
      "At: 880 [==========>] Loss 0.15416209459997787  - accuracy: 0.8125\n",
      "At: 881 [==========>] Loss 0.1639820761603475  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.10426157305283953  - accuracy: 0.90625\n",
      "At: 883 [==========>] Loss 0.14745450419977518  - accuracy: 0.78125\n",
      "At: 884 [==========>] Loss 0.12980945900962337  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.10515714958926244  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.09041678676787818  - accuracy: 0.875\n",
      "At: 887 [==========>] Loss 0.16109542121765874  - accuracy: 0.8125\n",
      "At: 888 [==========>] Loss 0.16413843533656391  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.0892478555615829  - accuracy: 0.90625\n",
      "At: 890 [==========>] Loss 0.11559672585190578  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.10550028503237788  - accuracy: 0.84375\n",
      "At: 892 [==========>] Loss 0.11443359289861213  - accuracy: 0.84375\n",
      "At: 893 [==========>] Loss 0.16131473679382585  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.13181459539801943  - accuracy: 0.78125\n",
      "At: 895 [==========>] Loss 0.10005200536317427  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.1432125565695625  - accuracy: 0.8125\n",
      "At: 897 [==========>] Loss 0.16046390782758435  - accuracy: 0.75\n",
      "At: 898 [==========>] Loss 0.1636309832455588  - accuracy: 0.75\n",
      "At: 899 [==========>] Loss 0.07712398217805536  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.1662413553200696  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.14414819359775538  - accuracy: 0.78125\n",
      "At: 902 [==========>] Loss 0.12830121412127393  - accuracy: 0.875\n",
      "At: 903 [==========>] Loss 0.15379602462508532  - accuracy: 0.78125\n",
      "At: 904 [==========>] Loss 0.09891156714585941  - accuracy: 0.90625\n",
      "At: 905 [==========>] Loss 0.10012200132025634  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.09874698701755191  - accuracy: 0.84375\n",
      "At: 907 [==========>] Loss 0.11178830017518723  - accuracy: 0.875\n",
      "At: 908 [==========>] Loss 0.12442801320778045  - accuracy: 0.8125\n",
      "At: 909 [==========>] Loss 0.08878422476952731  - accuracy: 0.84375\n",
      "At: 910 [==========>] Loss 0.10477982653147261  - accuracy: 0.84375\n",
      "At: 911 [==========>] Loss 0.15139018906992033  - accuracy: 0.75\n",
      "At: 912 [==========>] Loss 0.13023206709695467  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.11580074694225136  - accuracy: 0.875\n",
      "At: 914 [==========>] Loss 0.12340696049705253  - accuracy: 0.75\n",
      "At: 915 [==========>] Loss 0.17146780740807951  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.1800254582522784  - accuracy: 0.6875\n",
      "At: 917 [==========>] Loss 0.18784929177916937  - accuracy: 0.71875\n",
      "At: 918 [==========>] Loss 0.1675646772819398  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.14699900976727434  - accuracy: 0.78125\n",
      "At: 920 [==========>] Loss 0.09468840737753431  - accuracy: 0.875\n",
      "At: 921 [==========>] Loss 0.12895923167346146  - accuracy: 0.8125\n",
      "At: 922 [==========>] Loss 0.15228072397283726  - accuracy: 0.78125\n",
      "At: 923 [==========>] Loss 0.11435566964341828  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.16109868329244384  - accuracy: 0.8125\n",
      "At: 925 [==========>] Loss 0.1340861414226431  - accuracy: 0.78125\n",
      "At: 926 [==========>] Loss 0.13823246829580155  - accuracy: 0.875\n",
      "At: 927 [==========>] Loss 0.09665564575882865  - accuracy: 0.875\n",
      "At: 928 [==========>] Loss 0.11999381905512824  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.12404522321888163  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.1156150377081644  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.17921683290697468  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.08992250628206944  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.0930223645920095  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.1469089253552755  - accuracy: 0.78125\n",
      "At: 935 [==========>] Loss 0.05433377068092942  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.17624461102619965  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.18081302621137935  - accuracy: 0.78125\n",
      "At: 938 [==========>] Loss 0.1253618959555306  - accuracy: 0.78125\n",
      "At: 939 [==========>] Loss 0.14168624887012005  - accuracy: 0.8125\n",
      "At: 940 [==========>] Loss 0.2340515869241998  - accuracy: 0.71875\n",
      "At: 941 [==========>] Loss 0.11293558797411304  - accuracy: 0.875\n",
      "At: 942 [==========>] Loss 0.14449239433423366  - accuracy: 0.8125\n",
      "At: 943 [==========>] Loss 0.12235950654117152  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.09142491713180886  - accuracy: 0.875\n",
      "At: 945 [==========>] Loss 0.1002782196493728  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.13401960176129674  - accuracy: 0.78125\n",
      "At: 947 [==========>] Loss 0.1794932386374854  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.16819860766663763  - accuracy: 0.75\n",
      "At: 949 [==========>] Loss 0.0658418397036901  - accuracy: 0.9375\n",
      "At: 950 [==========>] Loss 0.1338108217975958  - accuracy: 0.75\n",
      "At: 951 [==========>] Loss 0.1100909736367936  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.08561565198283173  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.04572306840526656  - accuracy: 0.96875\n",
      "At: 954 [==========>] Loss 0.10992088566062488  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.14166721762613765  - accuracy: 0.8125\n",
      "At: 956 [==========>] Loss 0.08590350900614455  - accuracy: 0.90625\n",
      "At: 957 [==========>] Loss 0.15315924478809656  - accuracy: 0.78125\n",
      "At: 958 [==========>] Loss 0.07456307984229714  - accuracy: 0.9375\n",
      "At: 959 [==========>] Loss 0.13419417157046137  - accuracy: 0.78125\n",
      "At: 960 [==========>] Loss 0.11881300481702915  - accuracy: 0.8125\n",
      "At: 961 [==========>] Loss 0.10698178037591129  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.09030669771257385  - accuracy: 0.84375\n",
      "At: 963 [==========>] Loss 0.09357221552710898  - accuracy: 0.9375\n",
      "At: 964 [==========>] Loss 0.19901732746239306  - accuracy: 0.65625\n",
      "At: 965 [==========>] Loss 0.1288094458091548  - accuracy: 0.84375\n",
      "At: 966 [==========>] Loss 0.13877267724117046  - accuracy: 0.8125\n",
      "At: 967 [==========>] Loss 0.12488686689310839  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.15367805159245154  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.130766939979836  - accuracy: 0.875\n",
      "At: 970 [==========>] Loss 0.09422133355650793  - accuracy: 0.8125\n",
      "At: 971 [==========>] Loss 0.10503862037941598  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.07174504183192296  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.12738592292029266  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.06955796284906007  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.14151417688992168  - accuracy: 0.875\n",
      "At: 976 [==========>] Loss 0.10634573216136059  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.10847831132557292  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.20709539496117946  - accuracy: 0.71875\n",
      "At: 979 [==========>] Loss 0.11313205861879579  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.17930765174548557  - accuracy: 0.71875\n",
      "At: 981 [==========>] Loss 0.1699170318950346  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.07826718827197368  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.08540973861438779  - accuracy: 0.90625\n",
      "At: 984 [==========>] Loss 0.10904395751072983  - accuracy: 0.875\n",
      "At: 985 [==========>] Loss 0.17214290998314935  - accuracy: 0.71875\n",
      "At: 986 [==========>] Loss 0.12721042783526365  - accuracy: 0.84375\n",
      "At: 987 [==========>] Loss 0.1443207377672164  - accuracy: 0.84375\n",
      "At: 988 [==========>] Loss 0.1334341401840277  - accuracy: 0.78125\n",
      "At: 989 [==========>] Loss 0.1245100973067016  - accuracy: 0.78125\n",
      "At: 990 [==========>] Loss 0.11927439945792856  - accuracy: 0.8125\n",
      "At: 991 [==========>] Loss 0.14344494061567914  - accuracy: 0.78125\n",
      "At: 992 [==========>] Loss 0.20256545341490123  - accuracy: 0.75\n",
      "At: 993 [==========>] Loss 0.12622371510940053  - accuracy: 0.875\n",
      "At: 994 [==========>] Loss 0.14803071720139552  - accuracy: 0.78125\n",
      "At: 995 [==========>] Loss 0.1625008923206568  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.07496794237948777  - accuracy: 0.90625\n",
      "At: 997 [==========>] Loss 0.11495142961416788  - accuracy: 0.875\n",
      "At: 998 [==========>] Loss 0.164343574349199  - accuracy: 0.75\n",
      "At: 999 [==========>] Loss 0.14386480023401077  - accuracy: 0.84375\n",
      "At: 1000 [==========>] Loss 0.166214261477587  - accuracy: 0.75\n",
      "At: 1001 [==========>] Loss 0.1762561049814011  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.2306767132319703  - accuracy: 0.71875\n",
      "At: 1003 [==========>] Loss 0.15805521884147605  - accuracy: 0.78125\n",
      "At: 1004 [==========>] Loss 0.12356191096787161  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.10003106417077293  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.10689081609191767  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.13821973558599557  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.17812182251718345  - accuracy: 0.75\n",
      "At: 1009 [==========>] Loss 0.10790042951202687  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.15185580994175193  - accuracy: 0.84375\n",
      "At: 1011 [==========>] Loss 0.1555074740923153  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.11570405226629048  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.08060688554839897  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.0909595577381428  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.16743054403185909  - accuracy: 0.71875\n",
      "At: 1016 [==========>] Loss 0.15060528523365135  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.15073377193042986  - accuracy: 0.8125\n",
      "At: 1018 [==========>] Loss 0.1462055972197893  - accuracy: 0.8125\n",
      "At: 1019 [==========>] Loss 0.15600896861509184  - accuracy: 0.84375\n",
      "At: 1020 [==========>] Loss 0.14473776829621293  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.1153474462781978  - accuracy: 0.84375\n",
      "At: 1022 [==========>] Loss 0.1601095063048083  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.18808686977536335  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.17607654685844001  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.17549187431261504  - accuracy: 0.78125\n",
      "At: 1026 [==========>] Loss 0.0911385167019567  - accuracy: 0.90625\n",
      "At: 1027 [==========>] Loss 0.1260468579054747  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.21153428313317968  - accuracy: 0.65625\n",
      "At: 1029 [==========>] Loss 0.09916853160953393  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.097837190814185  - accuracy: 0.875\n",
      "At: 1031 [==========>] Loss 0.1819202423490071  - accuracy: 0.78125\n",
      "At: 1032 [==========>] Loss 0.17827431800849253  - accuracy: 0.71875\n",
      "At: 1033 [==========>] Loss 0.12329380001362483  - accuracy: 0.84375\n",
      "At: 1034 [==========>] Loss 0.10482572362053723  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.09071560313121863  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.15731125053892853  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.1543978721214363  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.0891004059757472  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.08983747244216399  - accuracy: 0.9375\n",
      "At: 1040 [==========>] Loss 0.11419391005841172  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.14716979685338907  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.11421948352287901  - accuracy: 0.84375\n",
      "At: 1043 [==========>] Loss 0.20542975200664015  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.12964138195353775  - accuracy: 0.875\n",
      "At: 1045 [==========>] Loss 0.1408588197254642  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.19785738713502524  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.11424080751474372  - accuracy: 0.90625\n",
      "At: 1048 [==========>] Loss 0.17490564741515563  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.12665353588010755  - accuracy: 0.84375\n",
      "At: 1050 [==========>] Loss 0.15485475889343986  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.10409525324613095  - accuracy: 0.90625\n",
      "At: 1052 [==========>] Loss 0.07860830513332677  - accuracy: 0.90625\n",
      "At: 1053 [==========>] Loss 0.07902827532727211  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.14612284447802304  - accuracy: 0.8125\n",
      "At: 1055 [==========>] Loss 0.17202329996695992  - accuracy: 0.71875\n",
      "At: 1056 [==========>] Loss 0.1264222697005095  - accuracy: 0.875\n",
      "At: 1057 [==========>] Loss 0.14828407613555364  - accuracy: 0.8125\n",
      "At: 1058 [==========>] Loss 0.06823283683871834  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.09845702837153346  - accuracy: 0.84375\n",
      "At: 1060 [==========>] Loss 0.1024301807140221  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.08532475621151989  - accuracy: 0.90625\n",
      "At: 1062 [==========>] Loss 0.16086052663915637  - accuracy: 0.78125\n",
      "At: 1063 [==========>] Loss 0.1163197591743833  - accuracy: 0.78125\n",
      "At: 1064 [==========>] Loss 0.13901979689217894  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.10375578716847508  - accuracy: 0.84375\n",
      "At: 1066 [==========>] Loss 0.0663462137747397  - accuracy: 0.96875\n",
      "At: 1067 [==========>] Loss 0.10476867543838671  - accuracy: 0.875\n",
      "At: 1068 [==========>] Loss 0.10209360240069054  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.1340662177852914  - accuracy: 0.78125\n",
      "At: 1070 [==========>] Loss 0.12177957102135882  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.12413023332838402  - accuracy: 0.78125\n",
      "At: 1072 [==========>] Loss 0.09837375838942818  - accuracy: 0.90625\n",
      "At: 1073 [==========>] Loss 0.16789356343474798  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.18110607067695447  - accuracy: 0.75\n",
      "At: 1075 [==========>] Loss 0.12084184706249236  - accuracy: 0.8125\n",
      "At: 1076 [==========>] Loss 0.1189937140825083  - accuracy: 0.90625\n",
      "At: 1077 [==========>] Loss 0.06798319842774357  - accuracy: 0.9375\n",
      "At: 1078 [==========>] Loss 0.0660010102538105  - accuracy: 0.9375\n",
      "At: 1079 [==========>] Loss 0.11664830474974358  - accuracy: 0.84375\n",
      "At: 1080 [==========>] Loss 0.13091507100011912  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.11748587060847175  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.09089245362016751  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.11596294625994874  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.11199441469002865  - accuracy: 0.90625\n",
      "At: 1085 [==========>] Loss 0.11288305971251184  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.1218436003102089  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.1207093709670013  - accuracy: 0.90625\n",
      "At: 1088 [==========>] Loss 0.16654712714286637  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.05653271648343981  - accuracy: 0.9375\n",
      "At: 1090 [==========>] Loss 0.07077983692530776  - accuracy: 0.9375\n",
      "At: 1091 [==========>] Loss 0.19518577295760844  - accuracy: 0.6875\n",
      "At: 1092 [==========>] Loss 0.10876598995335679  - accuracy: 0.875\n",
      "At: 1093 [==========>] Loss 0.144809705915868  - accuracy: 0.84375\n",
      "At: 1094 [==========>] Loss 0.13489218439316752  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.16580646999304022  - accuracy: 0.8125\n",
      "At: 1096 [==========>] Loss 0.08676885624600496  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.06852703573243552  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.15487429365020602  - accuracy: 0.8125\n",
      "At: 1099 [==========>] Loss 0.16239221920313426  - accuracy: 0.78125\n",
      "At: 1100 [==========>] Loss 0.09810545464020241  - accuracy: 0.875\n",
      "At: 1101 [==========>] Loss 0.12708265592203966  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.12515231162804125  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.09339034404325476  - accuracy: 0.875\n",
      "At: 1104 [==========>] Loss 0.050997259633100415  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.06196848238162826  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.08633828336093712  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.19176634219465213  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.097603488909906  - accuracy: 0.84375\n",
      "At: 1109 [==========>] Loss 0.06754629216729943  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.09433151549348587  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.15714712332697128  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.16257625746406962  - accuracy: 0.71875\n",
      "At: 1113 [==========>] Loss 0.14777988324350394  - accuracy: 0.84375\n",
      "At: 1114 [==========>] Loss 0.07973217267802911  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.1240441743187526  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.09702773318662355  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.05974843678894581  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.10614837981444536  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.11975915955631147  - accuracy: 0.78125\n",
      "At: 1120 [==========>] Loss 0.08919442791881824  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.10150232683871262  - accuracy: 0.84375\n",
      "At: 1122 [==========>] Loss 0.07928642961129424  - accuracy: 0.9375\n",
      "At: 1123 [==========>] Loss 0.15224213473665316  - accuracy: 0.75\n",
      "At: 1124 [==========>] Loss 0.14106296990790335  - accuracy: 0.78125\n",
      "At: 1125 [==========>] Loss 0.158956100081147  - accuracy: 0.78125\n",
      "At: 1126 [==========>] Loss 0.08912807056685079  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.13679427490289398  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.08234269067643063  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.15159195081058807  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.11552838911995492  - accuracy: 0.90625\n",
      "At: 1131 [==========>] Loss 0.09655672068950528  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.12678179732722805  - accuracy: 0.75\n",
      "At: 1133 [==========>] Loss 0.09632032961683797  - accuracy: 0.84375\n",
      "At: 1134 [==========>] Loss 0.10519149782110332  - accuracy: 0.75\n",
      "At: 1135 [==========>] Loss 0.10695001628844482  - accuracy: 0.84375\n",
      "At: 1136 [==========>] Loss 0.12592299963866949  - accuracy: 0.84375\n",
      "At: 1137 [==========>] Loss 0.10321252217569696  - accuracy: 0.84375\n",
      "At: 1138 [==========>] Loss 0.10791413319358051  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.10398870056272885  - accuracy: 0.84375\n",
      "At: 1140 [==========>] Loss 0.14675651149379454  - accuracy: 0.71875\n",
      "At: 1141 [==========>] Loss 0.15807397855437216  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.1338004055656774  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.06408052665156097  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.1223833487493716  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.1374953666098091  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.09546468872903904  - accuracy: 0.875\n",
      "At: 1147 [==========>] Loss 0.17884167171059526  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.08115986925716456  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.1277181497370083  - accuracy: 0.84375\n",
      "At: 1150 [==========>] Loss 0.12133553081690639  - accuracy: 0.8125\n",
      "At: 1151 [==========>] Loss 0.16302616546007123  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.12564668587654426  - accuracy: 0.8125\n",
      "At: 1153 [==========>] Loss 0.17047322582199398  - accuracy: 0.8125\n",
      "At: 1154 [==========>] Loss 0.08380853385435734  - accuracy: 0.90625\n",
      "At: 1155 [==========>] Loss 0.11517518232221685  - accuracy: 0.875\n",
      "At: 1156 [==========>] Loss 0.1714302220550733  - accuracy: 0.75\n",
      "At: 1157 [==========>] Loss 0.14996782735090824  - accuracy: 0.78125\n",
      "At: 1158 [==========>] Loss 0.1428013747017883  - accuracy: 0.84375\n",
      "At: 1159 [==========>] Loss 0.12502472444060841  - accuracy: 0.8125\n",
      "At: 1160 [==========>] Loss 0.08602802977697067  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.08872672262367635  - accuracy: 0.9375\n",
      "At: 1162 [==========>] Loss 0.16366033876775854  - accuracy: 0.75\n",
      "At: 1163 [==========>] Loss 0.17663064612345714  - accuracy: 0.71875\n",
      "At: 1164 [==========>] Loss 0.09411725745188296  - accuracy: 0.84375\n",
      "At: 1165 [==========>] Loss 0.17617509216260335  - accuracy: 0.75\n",
      "At: 1166 [==========>] Loss 0.07721212467186288  - accuracy: 0.90625\n",
      "At: 1167 [==========>] Loss 0.16137016727738088  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.14153611653905837  - accuracy: 0.78125\n",
      "At: 1169 [==========>] Loss 0.08234569158620175  - accuracy: 0.90625\n",
      "At: 1170 [==========>] Loss 0.18658504993714356  - accuracy: 0.6875\n",
      "At: 1171 [==========>] Loss 0.06816196217782763  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.13750305773195648  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.09975624633459348  - accuracy: 0.875\n",
      "At: 1174 [==========>] Loss 0.16658173635346213  - accuracy: 0.78125\n",
      "At: 1175 [==========>] Loss 0.15098717739851208  - accuracy: 0.78125\n",
      "At: 1176 [==========>] Loss 0.11954871230591435  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.08879271233551925  - accuracy: 0.90625\n",
      "At: 1178 [==========>] Loss 0.16359195088828116  - accuracy: 0.78125\n",
      "At: 1179 [==========>] Loss 0.12852629158532802  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.1882969489330867  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.09120794256469891  - accuracy: 0.90625\n",
      "At: 1182 [==========>] Loss 0.09905007666139823  - accuracy: 0.8125\n",
      "At: 1183 [==========>] Loss 0.14443289381548752  - accuracy: 0.84375\n",
      "At: 1184 [==========>] Loss 0.18865155471166806  - accuracy: 0.71875\n",
      "At: 1185 [==========>] Loss 0.09961619847654198  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.16064463448004618  - accuracy: 0.71875\n",
      "At: 1187 [==========>] Loss 0.11097211641091359  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.066910081112206  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.12652773821883542  - accuracy: 0.875\n",
      "At: 1190 [==========>] Loss 0.10633907450177874  - accuracy: 0.90625\n",
      "At: 1191 [==========>] Loss 0.14420365937267335  - accuracy: 0.78125\n",
      "At: 1192 [==========>] Loss 0.07320036905870295  - accuracy: 0.9375\n",
      "At: 1193 [==========>] Loss 0.13535673844866153  - accuracy: 0.78125\n",
      "At: 1194 [==========>] Loss 0.11602899946069457  - accuracy: 0.875\n",
      "At: 1195 [==========>] Loss 0.1004117498161676  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.11965855528276655  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.10778118901329489  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.08043877780740685  - accuracy: 0.96875\n",
      "At: 1199 [==========>] Loss 0.19353183396597354  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.09085150823141355  - accuracy: 0.875\n",
      "At: 1201 [==========>] Loss 0.1283875242465096  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.1540820368184326  - accuracy: 0.8125\n",
      "At: 1203 [==========>] Loss 0.14708896394546223  - accuracy: 0.8125\n",
      "At: 1204 [==========>] Loss 0.07432333077157398  - accuracy: 0.90625\n",
      "At: 1205 [==========>] Loss 0.06272873469671557  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.08924936225358215  - accuracy: 0.90625\n",
      "At: 1207 [==========>] Loss 0.15429208026898023  - accuracy: 0.8125\n",
      "At: 1208 [==========>] Loss 0.12592960423478022  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.07866763226175989  - accuracy: 0.90625\n",
      "At: 1210 [==========>] Loss 0.12445636143162908  - accuracy: 0.875\n",
      "At: 1211 [==========>] Loss 0.15531552842086693  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.10555775045907578  - accuracy: 0.8125\n",
      "At: 1213 [==========>] Loss 0.17927090276700275  - accuracy: 0.75\n",
      "At: 1214 [==========>] Loss 0.1537606372220579  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.15057422602345125  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.12036682612875302  - accuracy: 0.8125\n",
      "At: 1217 [==========>] Loss 0.04290027688232605  - accuracy: 0.96875\n",
      "At: 1218 [==========>] Loss 0.12462089342204491  - accuracy: 0.8125\n",
      "At: 1219 [==========>] Loss 0.11297949099497423  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.10280098555562059  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.09727729050482509  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.1407497517437938  - accuracy: 0.75\n",
      "At: 1223 [==========>] Loss 0.07513075030928973  - accuracy: 0.90625\n",
      "At: 1224 [==========>] Loss 0.07127561550461817  - accuracy: 0.9375\n",
      "At: 1225 [==========>] Loss 0.09212941193612233  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.09376391547000643  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.13742728950241956  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.16876881104643604  - accuracy: 0.65625\n",
      "At: 1229 [==========>] Loss 0.11155159599475345  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.18191298360382685  - accuracy: 0.75\n",
      "At: 1231 [==========>] Loss 0.17043879581131138  - accuracy: 0.71875\n",
      "At: 1232 [==========>] Loss 0.08143197575414049  - accuracy: 0.90625\n",
      "At: 1233 [==========>] Loss 0.10181337340045107  - accuracy: 0.8125\n",
      "At: 1234 [==========>] Loss 0.11648943659173333  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.08298725925138109  - accuracy: 0.875\n",
      "At: 1236 [==========>] Loss 0.16537557243061812  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.08077315852035277  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.12380024722006051  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.15035893326865662  - accuracy: 0.8125\n",
      "At: 1240 [==========>] Loss 0.1129484968725108  - accuracy: 0.90625\n",
      "At: 1241 [==========>] Loss 0.11684243489002129  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.13249950689542256  - accuracy: 0.84375\n",
      "At: 1243 [==========>] Loss 0.11598970457830146  - accuracy: 0.8125\n",
      "At: 1244 [==========>] Loss 0.15478726569600926  - accuracy: 0.71875\n",
      "At: 1245 [==========>] Loss 0.11632023401492814  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.08897860402267269  - accuracy: 0.84375\n",
      "At: 1247 [==========>] Loss 0.15923216434846502  - accuracy: 0.8125\n",
      "At: 1248 [==========>] Loss 0.07850506581745707  - accuracy: 0.875\n",
      "At: 1249 [==========>] Loss 0.10477419565612202  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.11243877360692456  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.11993736662003678  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.08214005051006316  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.10909370453388995  - accuracy: 0.84375\n",
      "At: 1254 [==========>] Loss 0.14309742240264667  - accuracy: 0.78125\n",
      "At: 1255 [==========>] Loss 0.12145745021643736  - accuracy: 0.84375\n",
      "At: 1256 [==========>] Loss 0.1258382054391655  - accuracy: 0.875\n",
      "At: 1257 [==========>] Loss 0.11969469566909804  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.06803335888611658  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.1236018657414783  - accuracy: 0.78125\n",
      "At: 1260 [==========>] Loss 0.12948005045750582  - accuracy: 0.78125\n",
      "At: 1261 [==========>] Loss 0.11031053609921915  - accuracy: 0.84375\n",
      "At: 1262 [==========>] Loss 0.1443457293648617  - accuracy: 0.75\n",
      "At: 1263 [==========>] Loss 0.13218545422624953  - accuracy: 0.8125\n",
      "At: 1264 [==========>] Loss 0.09874128644253448  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.13517571004497095  - accuracy: 0.8125\n",
      "At: 1266 [==========>] Loss 0.11847692596150508  - accuracy: 0.78125\n",
      "At: 1267 [==========>] Loss 0.12112917169262576  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.1342875815434288  - accuracy: 0.8125\n",
      "At: 1269 [==========>] Loss 0.12336136391715563  - accuracy: 0.875\n",
      "At: 1270 [==========>] Loss 0.13856840585103447  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.12304586061214283  - accuracy: 0.84375\n",
      "At: 1272 [==========>] Loss 0.05270828437059409  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.19771409962007347  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.10702126688731853  - accuracy: 0.84375\n",
      "At: 1275 [==========>] Loss 0.07584919064893  - accuracy: 0.90625\n",
      "At: 1276 [==========>] Loss 0.10509562791150376  - accuracy: 0.8125\n",
      "At: 1277 [==========>] Loss 0.07392969280599385  - accuracy: 0.90625\n",
      "At: 1278 [==========>] Loss 0.1284024796584649  - accuracy: 0.84375\n",
      "At: 1279 [==========>] Loss 0.09882901342895764  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.12761126263142375  - accuracy: 0.8125\n",
      "At: 1281 [==========>] Loss 0.16246748177420847  - accuracy: 0.75\n",
      "At: 1282 [==========>] Loss 0.10876052763193775  - accuracy: 0.875\n",
      "At: 1283 [==========>] Loss 0.15257524640942657  - accuracy: 0.78125\n",
      "At: 1284 [==========>] Loss 0.1765479989200125  - accuracy: 0.75\n",
      "At: 1285 [==========>] Loss 0.11043601043935666  - accuracy: 0.8125\n",
      "At: 1286 [==========>] Loss 0.09862278723910126  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.11431213641082917  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.1550604665431621  - accuracy: 0.8125\n",
      "At: 1289 [==========>] Loss 0.12240428191563253  - accuracy: 0.84375\n",
      "At: 1290 [==========>] Loss 0.14565991655234134  - accuracy: 0.78125\n",
      "At: 1291 [==========>] Loss 0.15729273591850038  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.11601895201789551  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.1676658842065752  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.1261642523059595  - accuracy: 0.84375\n",
      "At: 1295 [==========>] Loss 0.16116970542446907  - accuracy: 0.78125\n",
      "At: 1296 [==========>] Loss 0.11897786753053195  - accuracy: 0.84375\n",
      "At: 1297 [==========>] Loss 0.10505592077181629  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.10075709794568603  - accuracy: 0.8125\n",
      "At: 1299 [==========>] Loss 0.15475177546418414  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.12924326349935367  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.10984471788634087  - accuracy: 0.90625\n",
      "At: 1302 [==========>] Loss 0.06414084456488071  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.1086408393218688  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.09728899034767889  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.11087146971567464  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.0739263568136527  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.13893867652667374  - accuracy: 0.8125\n",
      "At: 1308 [==========>] Loss 0.06867590955286247  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.14070052553461787  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.19023120872539717  - accuracy: 0.71875\n",
      "At: 1311 [==========>] Loss 0.14582371241527708  - accuracy: 0.8125\n",
      "At: 1312 [==========>] Loss 0.08612449588993422  - accuracy: 0.875\n",
      "At: 1313 [==========>] Loss 0.15978046783284228  - accuracy: 0.78125\n",
      "At: 1314 [==========>] Loss 0.05534612662882248  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.13408849271602596  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.145656260849772  - accuracy: 0.75\n",
      "At: 1317 [==========>] Loss 0.09216817044833298  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.14319857201109565  - accuracy: 0.78125\n",
      "At: 1319 [==========>] Loss 0.12942313711206263  - accuracy: 0.78125\n",
      "At: 1320 [==========>] Loss 0.1507126815185868  - accuracy: 0.75\n",
      "At: 1321 [==========>] Loss 0.07715682158845996  - accuracy: 0.90625\n",
      "At: 1322 [==========>] Loss 0.13924251074119606  - accuracy: 0.875\n",
      "At: 1323 [==========>] Loss 0.10690328578803793  - accuracy: 0.84375\n",
      "At: 1324 [==========>] Loss 0.1194254763757445  - accuracy: 0.875\n",
      "At: 1325 [==========>] Loss 0.10520392515661278  - accuracy: 0.90625\n",
      "At: 1326 [==========>] Loss 0.09018894370152902  - accuracy: 0.875\n",
      "At: 1327 [==========>] Loss 0.15151850312606902  - accuracy: 0.78125\n",
      "At: 1328 [==========>] Loss 0.12076086848835496  - accuracy: 0.78125\n",
      "At: 1329 [==========>] Loss 0.07416048069979095  - accuracy: 0.84375\n",
      "At: 1330 [==========>] Loss 0.10919006477582754  - accuracy: 0.90625\n",
      "At: 1331 [==========>] Loss 0.15174440614941948  - accuracy: 0.8125\n",
      "At: 1332 [==========>] Loss 0.10995082180885245  - accuracy: 0.84375\n",
      "At: 1333 [==========>] Loss 0.1423226891558992  - accuracy: 0.78125\n",
      "At: 1334 [==========>] Loss 0.07210626566148533  - accuracy: 0.9375\n",
      "At: 1335 [==========>] Loss 0.11528418389040178  - accuracy: 0.8125\n",
      "At: 1336 [==========>] Loss 0.09031002288457282  - accuracy: 0.875\n",
      "At: 1337 [==========>] Loss 0.14365875620739366  - accuracy: 0.875\n",
      "At: 1338 [==========>] Loss 0.11231215051100188  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.09138859990301512  - accuracy: 0.90625\n",
      "At: 1340 [==========>] Loss 0.12556701763640585  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.07895110417408187  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.13324110515219276  - accuracy: 0.78125\n",
      "At: 1343 [==========>] Loss 0.19070645891498045  - accuracy: 0.78125\n",
      "At: 1344 [==========>] Loss 0.1590242447919761  - accuracy: 0.78125\n",
      "At: 1345 [==========>] Loss 0.1256497500065473  - accuracy: 0.8125\n",
      "At: 1346 [==========>] Loss 0.12316978955746091  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.13044171350391098  - accuracy: 0.8125\n",
      "At: 1348 [==========>] Loss 0.11381048247143231  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.13079539034728982  - accuracy: 0.84375\n",
      "At: 1350 [==========>] Loss 0.1500246969696892  - accuracy: 0.75\n",
      "At: 1351 [==========>] Loss 0.10733487502700927  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.09016853914493653  - accuracy: 0.875\n",
      "At: 1353 [==========>] Loss 0.17490901917530877  - accuracy: 0.65625\n",
      "At: 1354 [==========>] Loss 0.17192459669478538  - accuracy: 0.84375\n",
      "At: 1355 [==========>] Loss 0.07414858697645095  - accuracy: 0.84375\n",
      "At: 1356 [==========>] Loss 0.10173401450845866  - accuracy: 0.90625\n",
      "At: 1357 [==========>] Loss 0.12728603019947043  - accuracy: 0.875\n",
      "At: 1358 [==========>] Loss 0.1264033084518082  - accuracy: 0.84375\n",
      "At: 1359 [==========>] Loss 0.06328640736401034  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.17765182951056374  - accuracy: 0.78125\n",
      "At: 1361 [==========>] Loss 0.10889657353773323  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.13670831922453636  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.1269682201678246  - accuracy: 0.8125\n",
      "At: 1364 [==========>] Loss 0.1228175215907599  - accuracy: 0.8125\n",
      "At: 1365 [==========>] Loss 0.1253038426295729  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.10622967556601645  - accuracy: 0.875\n",
      "At: 1367 [==========>] Loss 0.11417981373034394  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.1583411869361212  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.10251685741469771  - accuracy: 0.84375\n",
      "At: 1370 [==========>] Loss 0.12924513301553847  - accuracy: 0.8125\n",
      "At: 1371 [==========>] Loss 0.19505611314669247  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.09566678710477389  - accuracy: 0.90625\n",
      "At: 1373 [==========>] Loss 0.14484624502696997  - accuracy: 0.84375\n",
      "At: 1374 [==========>] Loss 0.11867258223794269  - accuracy: 0.84375\n",
      "At: 1375 [==========>] Loss 0.14813687527001426  - accuracy: 0.75\n",
      "At: 1376 [==========>] Loss 0.08931544894099688  - accuracy: 0.90625\n",
      "At: 1377 [==========>] Loss 0.1623175462439645  - accuracy: 0.8125\n",
      "At: 1378 [==========>] Loss 0.1067501814876941  - accuracy: 0.875\n",
      "At: 1379 [==========>] Loss 0.15407725420419027  - accuracy: 0.78125\n",
      "At: 1380 [==========>] Loss 0.1368851756472081  - accuracy: 0.78125\n",
      "At: 1381 [==========>] Loss 0.09245625953637113  - accuracy: 0.84375\n",
      "At: 1382 [==========>] Loss 0.1306449407631185  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.08964871099546352  - accuracy: 0.90625\n",
      "At: 1384 [==========>] Loss 0.11767059519676902  - accuracy: 0.78125\n",
      "At: 1385 [==========>] Loss 0.1804888179496302  - accuracy: 0.75\n",
      "At: 1386 [==========>] Loss 0.1751314707247345  - accuracy: 0.75\n",
      "At: 1387 [==========>] Loss 0.054257771672840495  - accuracy: 0.9375\n",
      "At: 1388 [==========>] Loss 0.18236602251729642  - accuracy: 0.71875\n",
      "At: 1389 [==========>] Loss 0.10795537421414023  - accuracy: 0.78125\n",
      "At: 1390 [==========>] Loss 0.13521773869676676  - accuracy: 0.8125\n",
      "At: 1391 [==========>] Loss 0.09511617925135563  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.06636099964570397  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.12069497186377881  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.10156378643177888  - accuracy: 0.84375\n",
      "At: 1395 [==========>] Loss 0.19971443391942068  - accuracy: 0.71875\n",
      "At: 1396 [==========>] Loss 0.05806703912440077  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.09646912626489411  - accuracy: 0.875\n",
      "At: 1398 [==========>] Loss 0.10806728156247758  - accuracy: 0.875\n",
      "At: 1399 [==========>] Loss 0.12039886564533209  - accuracy: 0.84375\n",
      "At: 1400 [==========>] Loss 0.15186027272093644  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.09841995518897192  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.1629184775496538  - accuracy: 0.8125\n",
      "At: 1403 [==========>] Loss 0.12933290758372457  - accuracy: 0.8125\n",
      "At: 1404 [==========>] Loss 0.12577924415671243  - accuracy: 0.75\n",
      "At: 1405 [==========>] Loss 0.07158028140796924  - accuracy: 0.9375\n",
      "At: 1406 [==========>] Loss 0.1377211708684693  - accuracy: 0.78125\n",
      "At: 1407 [==========>] Loss 0.09147952775876209  - accuracy: 0.90625\n",
      "At: 1408 [==========>] Loss 0.1281477366143205  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.03586587575181479  - accuracy: 0.96875\n",
      "At: 1410 [==========>] Loss 0.09970227787887019  - accuracy: 0.84375\n",
      "At: 1411 [==========>] Loss 0.15155410416382473  - accuracy: 0.78125\n",
      "At: 1412 [==========>] Loss 0.11427508166760811  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.0695626938768899  - accuracy: 0.9375\n",
      "At: 1414 [==========>] Loss 0.17028348296265983  - accuracy: 0.75\n",
      "At: 1415 [==========>] Loss 0.089004336961624  - accuracy: 0.84375\n",
      "At: 1416 [==========>] Loss 0.12839540117687223  - accuracy: 0.84375\n",
      "At: 1417 [==========>] Loss 0.09052334974673368  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.13524465368716268  - accuracy: 0.875\n",
      "At: 1419 [==========>] Loss 0.11438310214748604  - accuracy: 0.875\n",
      "At: 1420 [==========>] Loss 0.07809762112290308  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.082845318922973  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.12894244649249073  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.12571311686930126  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.1525323170449277  - accuracy: 0.8125\n",
      "At: 1425 [==========>] Loss 0.0826352592501364  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.1144244406196417  - accuracy: 0.875\n",
      "At: 1427 [==========>] Loss 0.08229354738644527  - accuracy: 0.90625\n",
      "At: 1428 [==========>] Loss 0.11311726310474422  - accuracy: 0.84375\n",
      "At: 1429 [==========>] Loss 0.14299971253108457  - accuracy: 0.78125\n",
      "At: 1430 [==========>] Loss 0.07261420164992549  - accuracy: 0.9375\n",
      "At: 1431 [==========>] Loss 0.1264763623746341  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.11505192849773187  - accuracy: 0.8125\n",
      "At: 1433 [==========>] Loss 0.12596947181722074  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.15669102944811886  - accuracy: 0.75\n",
      "At: 1435 [==========>] Loss 0.13749063577236448  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.06165076443751115  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.15413940465824244  - accuracy: 0.78125\n",
      "At: 1438 [==========>] Loss 0.1226779209040527  - accuracy: 0.84375\n",
      "At: 1439 [==========>] Loss 0.1330539170906118  - accuracy: 0.78125\n",
      "At: 1440 [==========>] Loss 0.11839578022318792  - accuracy: 0.78125\n",
      "At: 1441 [==========>] Loss 0.0934825505322988  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.13714503792543287  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.14192217130855098  - accuracy: 0.75\n",
      "At: 1444 [==========>] Loss 0.13248230101227623  - accuracy: 0.84375\n",
      "At: 1445 [==========>] Loss 0.2049884227990822  - accuracy: 0.6875\n",
      "At: 1446 [==========>] Loss 0.20086019033012553  - accuracy: 0.65625\n",
      "At: 1447 [==========>] Loss 0.191647199556242  - accuracy: 0.71875\n",
      "At: 1448 [==========>] Loss 0.0688211150583995  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.11954869493915415  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.1337978235139963  - accuracy: 0.78125\n",
      "At: 1451 [==========>] Loss 0.11991372727970337  - accuracy: 0.8125\n",
      "At: 1452 [==========>] Loss 0.09828486895145652  - accuracy: 0.84375\n",
      "At: 1453 [==========>] Loss 0.06794279272099321  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.17726625465509338  - accuracy: 0.75\n",
      "At: 1455 [==========>] Loss 0.12869433134618932  - accuracy: 0.8125\n",
      "At: 1456 [==========>] Loss 0.12069049145387059  - accuracy: 0.8125\n",
      "At: 1457 [==========>] Loss 0.087718414119249  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.15485583256932184  - accuracy: 0.75\n",
      "At: 1459 [==========>] Loss 0.1251814102483913  - accuracy: 0.8125\n",
      "At: 1460 [==========>] Loss 0.17816942354794063  - accuracy: 0.71875\n",
      "At: 1461 [==========>] Loss 0.12444141129857265  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.19213352204891715  - accuracy: 0.71875\n",
      "At: 1463 [==========>] Loss 0.07265729447549263  - accuracy: 0.9375\n",
      "At: 1464 [==========>] Loss 0.20197718840121628  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.12277188244944456  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.0801757178706977  - accuracy: 0.875\n",
      "At: 1467 [==========>] Loss 0.1807050610314444  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.16258870781445717  - accuracy: 0.8125\n",
      "At: 1469 [==========>] Loss 0.16845917895724194  - accuracy: 0.75\n",
      "At: 1470 [==========>] Loss 0.12405604398510973  - accuracy: 0.84375\n",
      "At: 1471 [==========>] Loss 0.11172691959204213  - accuracy: 0.84375\n",
      "At: 1472 [==========>] Loss 0.07871709517689059  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.12675080786546658  - accuracy: 0.8125\n",
      "At: 1474 [==========>] Loss 0.19172189544528728  - accuracy: 0.75\n",
      "At: 1475 [==========>] Loss 0.15920384724216602  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.12262722336124854  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.10385442023231231  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.09003615125481162  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.12833638192959373  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.10913440649443737  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.11573348277012135  - accuracy: 0.8125\n",
      "At: 1482 [==========>] Loss 0.09458368873892327  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.20879882419988277  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.13170331807155033  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.13753102093537733  - accuracy: 0.8125\n",
      "At: 1486 [==========>] Loss 0.08663402564128032  - accuracy: 0.9375\n",
      "At: 1487 [==========>] Loss 0.08893536329773877  - accuracy: 0.875\n",
      "At: 1488 [==========>] Loss 0.14018011784482556  - accuracy: 0.78125\n",
      "At: 1489 [==========>] Loss 0.19435861681835084  - accuracy: 0.6875\n",
      "At: 1490 [==========>] Loss 0.09786570999106045  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.1639570960698241  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.13103488434688237  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.1762831165330751  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.1449526951392879  - accuracy: 0.75\n",
      "At: 1495 [==========>] Loss 0.10953012418671174  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.09962399506328598  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.1715118947185233  - accuracy: 0.8125\n",
      "At: 1498 [==========>] Loss 0.14038950773692033  - accuracy: 0.84375\n",
      "At: 1499 [==========>] Loss 0.11689610639950541  - accuracy: 0.8125\n",
      "At: 1500 [==========>] Loss 0.08891013506842112  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.09889987415465235  - accuracy: 0.84375\n",
      "At: 1502 [==========>] Loss 0.17342189911498423  - accuracy: 0.75\n",
      "At: 1503 [==========>] Loss 0.1468845109939802  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.11630698732410918  - accuracy: 0.875\n",
      "At: 1505 [==========>] Loss 0.12614606591369085  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.1479998771073589  - accuracy: 0.84375\n",
      "At: 1507 [==========>] Loss 0.12104734291631881  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.23111492706097364  - accuracy: 0.65625\n",
      "At: 1509 [==========>] Loss 0.09648736611290719  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.10500311322562224  - accuracy: 0.90625\n",
      "At: 1511 [==========>] Loss 0.1051681076468487  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.08849573120598983  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.1627845236641022  - accuracy: 0.78125\n",
      "At: 1514 [==========>] Loss 0.16804492874804677  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.14234463055035873  - accuracy: 0.78125\n",
      "At: 1516 [==========>] Loss 0.11609966699332577  - accuracy: 0.875\n",
      "At: 1517 [==========>] Loss 0.17504907698954125  - accuracy: 0.75\n",
      "At: 1518 [==========>] Loss 0.11285968947488922  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.12947735766032892  - accuracy: 0.875\n",
      "At: 1520 [==========>] Loss 0.11756420095982167  - accuracy: 0.875\n",
      "At: 1521 [==========>] Loss 0.08779454747614376  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.19755209170297677  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.08339070145166368  - accuracy: 0.90625\n",
      "At: 1524 [==========>] Loss 0.15645275323644325  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.07514539751935502  - accuracy: 0.90625\n",
      "At: 1526 [==========>] Loss 0.0973272565324029  - accuracy: 0.90625\n",
      "At: 1527 [==========>] Loss 0.1486744491800997  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.16654039905671522  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.05786117570275116  - accuracy: 0.9375\n",
      "At: 1530 [==========>] Loss 0.045201937433421734  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.11537669846030643  - accuracy: 0.84375\n",
      "At: 1532 [==========>] Loss 0.19492164204194454  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.15199137381673133  - accuracy: 0.84375\n",
      "At: 1534 [==========>] Loss 0.0942901226533952  - accuracy: 0.875\n",
      "At: 1535 [==========>] Loss 0.12519001814467362  - accuracy: 0.90625\n",
      "At: 1536 [==========>] Loss 0.16848548325438895  - accuracy: 0.78125\n",
      "At: 1537 [==========>] Loss 0.09637995762216969  - accuracy: 0.9375\n",
      "At: 1538 [==========>] Loss 0.1455793428012946  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.09062026867375217  - accuracy: 0.90625\n",
      "At: 1540 [==========>] Loss 0.1619052926170262  - accuracy: 0.71875\n",
      "At: 1541 [==========>] Loss 0.10801182388176214  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.09732308721317168  - accuracy: 0.84375\n",
      "At: 1543 [==========>] Loss 0.11434897074735129  - accuracy: 0.90625\n",
      "At: 1544 [==========>] Loss 0.14461998780127264  - accuracy: 0.84375\n",
      "At: 1545 [==========>] Loss 0.20688351495229357  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.11564845984139314  - accuracy: 0.78125\n",
      "At: 1547 [==========>] Loss 0.1419858707179324  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.12457887236861453  - accuracy: 0.875\n",
      "At: 1549 [==========>] Loss 0.13999803595975308  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.06666916061999563  - accuracy: 0.875\n",
      "At: 1551 [==========>] Loss 0.17927490112652872  - accuracy: 0.6875\n",
      "At: 1552 [==========>] Loss 0.09206700607821361  - accuracy: 0.875\n",
      "At: 1553 [==========>] Loss 0.08365259088229487  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.14371835953462497  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.14499931504142385  - accuracy: 0.75\n",
      "At: 1556 [==========>] Loss 0.16941151759365464  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.0795767544853281  - accuracy: 0.90625\n",
      "At: 1558 [==========>] Loss 0.11693715401906649  - accuracy: 0.875\n",
      "At: 1559 [==========>] Loss 0.07448137560292957  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.10966405452881468  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.13426381192971681  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.08058392687292204  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.10668896453056155  - accuracy: 0.875\n",
      "At: 1564 [==========>] Loss 0.09966325301858227  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.12077296435343655  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.15984828051684918  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.1363214504358584  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.07803933252361964  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.12296363173746333  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.09251656855088608  - accuracy: 0.875\n",
      "At: 1571 [==========>] Loss 0.15313429269703627  - accuracy: 0.78125\n",
      "At: 1572 [==========>] Loss 0.12024608885244537  - accuracy: 0.8125\n",
      "At: 1573 [==========>] Loss 0.037242548109412923  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.11834876056527753  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.13626512199232532  - accuracy: 0.8125\n",
      "At: 1576 [==========>] Loss 0.1469613679847462  - accuracy: 0.8125\n",
      "At: 1577 [==========>] Loss 0.06645153144492807  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.09671212227999872  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.11848480728963223  - accuracy: 0.84375\n",
      "At: 1580 [==========>] Loss 0.12279852041770493  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.11301918881668838  - accuracy: 0.90625\n",
      "At: 1582 [==========>] Loss 0.18512175334291792  - accuracy: 0.78125\n",
      "At: 1583 [==========>] Loss 0.08836050464917164  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.13801128051658487  - accuracy: 0.8125\n",
      "At: 1585 [==========>] Loss 0.12726793391464178  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.13750379812817434  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.10610909056268356  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.14034031216750154  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.17806979876581333  - accuracy: 0.6875\n",
      "At: 1590 [==========>] Loss 0.11871067016756384  - accuracy: 0.875\n",
      "At: 1591 [==========>] Loss 0.10562693779834678  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.06687311664657672  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.18027182648385626  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.07640312966500157  - accuracy: 0.9375\n",
      "At: 1595 [==========>] Loss 0.1569497015894465  - accuracy: 0.75\n",
      "At: 1596 [==========>] Loss 0.16777563160334813  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.15685090134143617  - accuracy: 0.78125\n",
      "At: 1598 [==========>] Loss 0.17726633507356446  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.22204377400688136  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.12799480375616307  - accuracy: 0.875\n",
      "At: 1601 [==========>] Loss 0.08491135545692627  - accuracy: 0.84375\n",
      "At: 1602 [==========>] Loss 0.12375003231784842  - accuracy: 0.84375\n",
      "At: 1603 [==========>] Loss 0.1827291135880223  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.20915542725861166  - accuracy: 0.71875\n",
      "At: 1605 [==========>] Loss 0.08875653836320638  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.11763478831965442  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.1965518101475779  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.13079619513994134  - accuracy: 0.84375\n",
      "At: 1609 [==========>] Loss 0.1364499552613082  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.18484907879898252  - accuracy: 0.75\n",
      "At: 1611 [==========>] Loss 0.09181114990518183  - accuracy: 0.84375\n",
      "At: 1612 [==========>] Loss 0.06678277433616314  - accuracy: 0.9375\n",
      "At: 1613 [==========>] Loss 0.1474671673881239  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.1173945553201186  - accuracy: 0.8125\n",
      "At: 1615 [==========>] Loss 0.07650241971865382  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.13109754597887455  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.09317886050699775  - accuracy: 0.90625\n",
      "At: 1618 [==========>] Loss 0.12027851582497685  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.20695126860562263  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.10500770838911959  - accuracy: 0.84375\n",
      "At: 1621 [==========>] Loss 0.12922841104931168  - accuracy: 0.84375\n",
      "At: 1622 [==========>] Loss 0.18778732651288235  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.08518252528057732  - accuracy: 0.875\n",
      "At: 1624 [==========>] Loss 0.1483076801379345  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.1441522748717233  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.11674130137624467  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.09477292759082093  - accuracy: 0.90625\n",
      "At: 1628 [==========>] Loss 0.1799965180300971  - accuracy: 0.6875\n",
      "At: 1629 [==========>] Loss 0.1486401491130931  - accuracy: 0.8125\n",
      "At: 1630 [==========>] Loss 0.09690257088370323  - accuracy: 0.90625\n",
      "At: 1631 [==========>] Loss 0.1211049947393133  - accuracy: 0.8125\n",
      "At: 1632 [==========>] Loss 0.07563698572172167  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08147476660354898  - accuracy: 0.875\n",
      "At: 1634 [==========>] Loss 0.10412481938473675  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.22438925374584998  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.1012619285411101  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.07902495986600241  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.14978184069624162  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.15357860691890274  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.11327994618677659  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.08220005599502084  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.1173914089308771  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.10458256007611241  - accuracy: 0.84375\n",
      "At: 1644 [==========>] Loss 0.1295321165419132  - accuracy: 0.8125\n",
      "At: 1645 [==========>] Loss 0.047695161045533924  - accuracy: 0.9375\n",
      "At: 1646 [==========>] Loss 0.11832448511093817  - accuracy: 0.875\n",
      "At: 1647 [==========>] Loss 0.18602862751867832  - accuracy: 0.6875\n",
      "At: 1648 [==========>] Loss 0.1400018745117062  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.08883746778871576  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.07901175159851975  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.1307393759929136  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.07038236539576812  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.09193396267481012  - accuracy: 0.90625\n",
      "At: 1654 [==========>] Loss 0.05025900091209487  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.15642331660039027  - accuracy: 0.84375\n",
      "At: 1656 [==========>] Loss 0.11989593130575138  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.1391763244380644  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.19631227510314164  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.09649965718509079  - accuracy: 0.90625\n",
      "At: 1660 [==========>] Loss 0.030566928251041722  - accuracy: 1.0\n",
      "At: 1661 [==========>] Loss 0.07762382737551628  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.09955931432398123  - accuracy: 0.90625\n",
      "At: 1663 [==========>] Loss 0.16598138656883493  - accuracy: 0.71875\n",
      "At: 1664 [==========>] Loss 0.1121612825057206  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.11874107951618901  - accuracy: 0.8125\n",
      "At: 1666 [==========>] Loss 0.10906839316264641  - accuracy: 0.8125\n",
      "At: 1667 [==========>] Loss 0.1467458652361353  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.16486315486203618  - accuracy: 0.78125\n",
      "At: 1669 [==========>] Loss 0.12477177393620459  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.05971970973705662  - accuracy: 0.875\n",
      "At: 1671 [==========>] Loss 0.12171567279028797  - accuracy: 0.84375\n",
      "At: 1672 [==========>] Loss 0.13909600597020022  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.06721323053229053  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.16218465923007636  - accuracy: 0.8125\n",
      "At: 1675 [==========>] Loss 0.1851733762956163  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.18407778193458713  - accuracy: 0.75\n",
      "At: 1677 [==========>] Loss 0.10399097589612522  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.1688473774524102  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.08958995484959399  - accuracy: 0.90625\n",
      "At: 1680 [==========>] Loss 0.1750038251949606  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.12416692452867845  - accuracy: 0.8125\n",
      "At: 1682 [==========>] Loss 0.09140162112428779  - accuracy: 0.90625\n",
      "At: 1683 [==========>] Loss 0.1801149387955946  - accuracy: 0.8125\n",
      "At: 1684 [==========>] Loss 0.11712074497436374  - accuracy: 0.8125\n",
      "At: 1685 [==========>] Loss 0.08548092619596656  - accuracy: 0.90625\n",
      "At: 1686 [==========>] Loss 0.10866353640317086  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.19965868726865785  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.053263483742248326  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.12811883582140954  - accuracy: 0.8125\n",
      "At: 1690 [==========>] Loss 0.11702952537129481  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.1072966422926766  - accuracy: 0.8125\n",
      "At: 1692 [==========>] Loss 0.16895937928645202  - accuracy: 0.8125\n",
      "At: 1693 [==========>] Loss 0.08986583562902958  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.09366933153797849  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.11965453495500693  - accuracy: 0.8125\n",
      "At: 1696 [==========>] Loss 0.17501920541377963  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.10881066685434407  - accuracy: 0.8125\n",
      "At: 1698 [==========>] Loss 0.09495765352932888  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.11467160853108782  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.1223319879180597  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.10570380662949015  - accuracy: 0.84375\n",
      "At: 1702 [==========>] Loss 0.08215663814558984  - accuracy: 0.9375\n",
      "At: 1703 [==========>] Loss 0.17924606061615905  - accuracy: 0.71875\n",
      "At: 1704 [==========>] Loss 0.08033505495925714  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.11631181528569254  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.17521287103159883  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.16876907753420645  - accuracy: 0.8125\n",
      "At: 1708 [==========>] Loss 0.07563937130276392  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.16728396085864625  - accuracy: 0.71875\n",
      "At: 1710 [==========>] Loss 0.13847203257973786  - accuracy: 0.84375\n",
      "At: 1711 [==========>] Loss 0.07859294156738214  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.09590912636016456  - accuracy: 0.90625\n",
      "At: 1713 [==========>] Loss 0.11013194005016341  - accuracy: 0.78125\n",
      "At: 1714 [==========>] Loss 0.1550881761083613  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.12246748730917106  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06495911059434603  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.08305561334780526  - accuracy: 0.9375\n",
      "At: 1718 [==========>] Loss 0.14450967319814012  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.1173826397017201  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.0636424568008687  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.17440614892901318  - accuracy: 0.6875\n",
      "At: 1722 [==========>] Loss 0.07376565150104146  - accuracy: 0.875\n",
      "At: 1723 [==========>] Loss 0.18520167671717125  - accuracy: 0.71875\n",
      "At: 1724 [==========>] Loss 0.09020561857761392  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.13021902242046907  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.10360237901648332  - accuracy: 0.90625\n",
      "At: 1727 [==========>] Loss 0.1326773088021475  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.09955861776435337  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.16258639940085817  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.12982193491017585  - accuracy: 0.90625\n",
      "At: 1731 [==========>] Loss 0.09346219696249727  - accuracy: 0.84375\n",
      "At: 1732 [==========>] Loss 0.06166430732030251  - accuracy: 0.9375\n",
      "At: 1733 [==========>] Loss 0.16210881668061344  - accuracy: 0.71875\n",
      "At: 1734 [==========>] Loss 0.08315853163397904  - accuracy: 0.875\n",
      "At: 1735 [==========>] Loss 0.1536547695490938  - accuracy: 0.84375\n",
      "At: 1736 [==========>] Loss 0.09850877829987068  - accuracy: 0.8125\n",
      "At: 1737 [==========>] Loss 0.1425875724920843  - accuracy: 0.84375\n",
      "At: 1738 [==========>] Loss 0.10496555834795754  - accuracy: 0.84375\n",
      "At: 1739 [==========>] Loss 0.1218035024037063  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.13749777112852662  - accuracy: 0.8125\n",
      "At: 1741 [==========>] Loss 0.14845172912573412  - accuracy: 0.75\n",
      "At: 1742 [==========>] Loss 0.0547733640964356  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.14838437960466003  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.08324719635126329  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.11789568154089214  - accuracy: 0.84375\n",
      "At: 1746 [==========>] Loss 0.17184870513759293  - accuracy: 0.75\n",
      "At: 1747 [==========>] Loss 0.14526354780484016  - accuracy: 0.75\n",
      "At: 1748 [==========>] Loss 0.11883151003269435  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.10144540848561229  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.09694306823986362  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.19612405631049393  - accuracy: 0.71875\n",
      "At: 1752 [==========>] Loss 0.09284020537714953  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.07767327761738525  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.10066042951161959  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.06488189061643476  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.15327965844667624  - accuracy: 0.75\n",
      "At: 1757 [==========>] Loss 0.1491465975154655  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.06610235906844023  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.08344587506119742  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.08907944666900039  - accuracy: 0.875\n",
      "At: 1761 [==========>] Loss 0.11477365738341512  - accuracy: 0.8125\n",
      "At: 1762 [==========>] Loss 0.15863030207996381  - accuracy: 0.75\n",
      "At: 1763 [==========>] Loss 0.10733378811844249  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.15813508352714586  - accuracy: 0.71875\n",
      "At: 1765 [==========>] Loss 0.11944769589767407  - accuracy: 0.84375\n",
      "At: 1766 [==========>] Loss 0.060621450174472194  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.07144869610725862  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.08607771304079515  - accuracy: 0.875\n",
      "At: 1769 [==========>] Loss 0.06256940466865274  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.04100085781218181  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.17327694335186933  - accuracy: 0.71875\n",
      "At: 1772 [==========>] Loss 0.12798923043643207  - accuracy: 0.78125\n",
      "At: 1773 [==========>] Loss 0.09662602951789914  - accuracy: 0.90625\n",
      "At: 1774 [==========>] Loss 0.15296005603676466  - accuracy: 0.8125\n",
      "At: 1775 [==========>] Loss 0.11225075312292537  - accuracy: 0.8125\n",
      "At: 1776 [==========>] Loss 0.1129857118679044  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.10202133432455185  - accuracy: 0.8125\n",
      "At: 1778 [==========>] Loss 0.10879984389552508  - accuracy: 0.84375\n",
      "At: 1779 [==========>] Loss 0.09663977941589758  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.11006713469825063  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.18260471922704008  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.09548042982747493  - accuracy: 0.8125\n",
      "At: 1783 [==========>] Loss 0.1289867531369379  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.06522588663820023  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.07363588606854717  - accuracy: 0.90625\n",
      "At: 1786 [==========>] Loss 0.10982387651459705  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.14493291952659318  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.08603894101345913  - accuracy: 0.875\n",
      "At: 1789 [==========>] Loss 0.10942581298952891  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.1708117953972218  - accuracy: 0.6875\n",
      "At: 1791 [==========>] Loss 0.05426844503109912  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.1055179138012771  - accuracy: 0.90625\n",
      "At: 1793 [==========>] Loss 0.08762977469701799  - accuracy: 0.90625\n",
      "At: 1794 [==========>] Loss 0.13447566482710782  - accuracy: 0.84375\n",
      "At: 1795 [==========>] Loss 0.09120759719105137  - accuracy: 0.84375\n",
      "At: 1796 [==========>] Loss 0.13793042188819127  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.12648774852864  - accuracy: 0.8125\n",
      "At: 1798 [==========>] Loss 0.1477718911389642  - accuracy: 0.78125\n",
      "At: 1799 [==========>] Loss 0.0706086225543858  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.12492271212191541  - accuracy: 0.78125\n",
      "At: 1801 [==========>] Loss 0.16535717336310296  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.12160509537959825  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.16123682188640628  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.141351963163153  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.036719162993588926  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.1486957634417924  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.18014150936968884  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.1667725107402419  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.10154675676975591  - accuracy: 0.84375\n",
      "At: 1810 [==========>] Loss 0.16467803568569517  - accuracy: 0.71875\n",
      "At: 1811 [==========>] Loss 0.1363656999046644  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.09625536737132227  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.13110145002134418  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.11642869991900648  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.17089269342853763  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.04824641212336733  - accuracy: 0.9375\n",
      "At: 1817 [==========>] Loss 0.12216141446477205  - accuracy: 0.8125\n",
      "At: 1818 [==========>] Loss 0.17032328121152313  - accuracy: 0.71875\n",
      "At: 1819 [==========>] Loss 0.16720703718705215  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.09710428095574103  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.10349315124166417  - accuracy: 0.90625\n",
      "At: 1822 [==========>] Loss 0.14854416480838256  - accuracy: 0.84375\n",
      "At: 1823 [==========>] Loss 0.16438267306281892  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.17039014321592627  - accuracy: 0.71875\n",
      "At: 1825 [==========>] Loss 0.1031047296887387  - accuracy: 0.90625\n",
      "At: 1826 [==========>] Loss 0.04648763009237861  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.09636448443278875  - accuracy: 0.90625\n",
      "At: 1828 [==========>] Loss 0.15738335654028485  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.14208416426498893  - accuracy: 0.84375\n",
      "At: 1830 [==========>] Loss 0.1337896574526439  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.11849446454161369  - accuracy: 0.875\n",
      "At: 1832 [==========>] Loss 0.165803186428482  - accuracy: 0.71875\n",
      "At: 1833 [==========>] Loss 0.09869804135276489  - accuracy: 0.875\n",
      "At: 1834 [==========>] Loss 0.08683944832388735  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.16046834534461724  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.10595500569277805  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.042313470967851693  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.08736653187146783  - accuracy: 0.90625\n",
      "At: 1839 [==========>] Loss 0.09021585324711925  - accuracy: 0.875\n",
      "At: 1840 [==========>] Loss 0.14982074912423893  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.10083994034212093  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.12086290486063941  - accuracy: 0.8125\n",
      "At: 1843 [==========>] Loss 0.09589057064916157  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.12205010126115909  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.15871192925468364  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.13229774143643042  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.06135490386160713  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.052332817288597386  - accuracy: 0.96875\n",
      "At: 1849 [==========>] Loss 0.19121222996795545  - accuracy: 0.78125\n",
      "At: 1850 [==========>] Loss 0.030435555703057858  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.14236116570931157  - accuracy: 0.8125\n",
      "At: 1852 [==========>] Loss 0.08266422126972749  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.11803562470098482  - accuracy: 0.875\n",
      "At: 1854 [==========>] Loss 0.12419441960108725  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.15874617131373286  - accuracy: 0.8125\n",
      "At: 1856 [==========>] Loss 0.11527154012543438  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.15261149864147314  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.10942369899965676  - accuracy: 0.90625\n",
      "At: 1859 [==========>] Loss 0.13544105240816234  - accuracy: 0.875\n",
      "At: 1860 [==========>] Loss 0.12372946023854506  - accuracy: 0.84375\n",
      "At: 1861 [==========>] Loss 0.11110300880499745  - accuracy: 0.78125\n",
      "At: 1862 [==========>] Loss 0.17615715101175472  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.1531110112565396  - accuracy: 0.8125\n",
      "At: 1864 [==========>] Loss 0.16274428756560544  - accuracy: 0.71875\n",
      "At: 1865 [==========>] Loss 0.09845965260940533  - accuracy: 0.8125\n",
      "At: 1866 [==========>] Loss 0.20011676415891516  - accuracy: 0.65625\n",
      "At: 1867 [==========>] Loss 0.1040388444246087  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.17471985587825845  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.2123260228722586  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.1091793335509989  - accuracy: 0.84375\n",
      "At: 1871 [==========>] Loss 0.11388417249371217  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.13824177315705627  - accuracy: 0.8125\n",
      "At: 1873 [==========>] Loss 0.0997574179498531  - accuracy: 0.78125\n",
      "At: 1874 [==========>] Loss 0.126484655884237  - accuracy: 0.875\n",
      "At: 1875 [==========>] Loss 0.10123144791392966  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.22186260935092933  - accuracy: 0.6875\n",
      "At: 1877 [==========>] Loss 0.0968824665249577  - accuracy: 0.90625\n",
      "At: 1878 [==========>] Loss 0.1142814397640214  - accuracy: 0.78125\n",
      "At: 1879 [==========>] Loss 0.12815083390106438  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.08591299442310085  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.0873368670123369  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.11322223607581208  - accuracy: 0.84375\n",
      "At: 1883 [==========>] Loss 0.15201876359008748  - accuracy: 0.78125\n",
      "At: 1884 [==========>] Loss 0.09548231754039971  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.08785533757692687  - accuracy: 0.90625\n",
      "At: 1886 [==========>] Loss 0.12094074268793893  - accuracy: 0.84375\n",
      "At: 1887 [==========>] Loss 0.0671959304621225  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.14030672955763762  - accuracy: 0.78125\n",
      "At: 1889 [==========>] Loss 0.09336732963294142  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.17688906516821712  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.056683525283929614  - accuracy: 0.96875\n",
      "At: 1892 [==========>] Loss 0.08432036227153433  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.07189265961054579  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.08273763204382853  - accuracy: 0.9375\n",
      "At: 1895 [==========>] Loss 0.06501464649576304  - accuracy: 0.9375\n",
      "At: 1896 [==========>] Loss 0.13160064966373325  - accuracy: 0.8125\n",
      "At: 1897 [==========>] Loss 0.08027615693242299  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.11721099640086345  - accuracy: 0.875\n",
      "At: 1899 [==========>] Loss 0.094979673201086  - accuracy: 0.84375\n",
      "At: 1900 [==========>] Loss 0.10625678854392567  - accuracy: 0.875\n",
      "At: 1901 [==========>] Loss 0.09381014575436139  - accuracy: 0.90625\n",
      "At: 1902 [==========>] Loss 0.1541664667032529  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.12398140414771872  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.057580141344352256  - accuracy: 0.9375\n",
      "At: 1905 [==========>] Loss 0.15258240555369867  - accuracy: 0.6875\n",
      "At: 1906 [==========>] Loss 0.10337727611143097  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.08003094531048507  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.09938406865761863  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.11510406368620359  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.05457581742793145  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.11069901775681402  - accuracy: 0.84375\n",
      "At: 1912 [==========>] Loss 0.11019728894790802  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.1595612101643128  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.08682330354414433  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.10148208480778989  - accuracy: 0.90625\n",
      "At: 1916 [==========>] Loss 0.1447864304571146  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.16115393916965062  - accuracy: 0.8125\n",
      "At: 1918 [==========>] Loss 0.13832125471657172  - accuracy: 0.8125\n",
      "At: 1919 [==========>] Loss 0.10042745323762933  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.0920593832741859  - accuracy: 0.9375\n",
      "At: 1921 [==========>] Loss 0.13909558202727784  - accuracy: 0.78125\n",
      "At: 1922 [==========>] Loss 0.14810884967055704  - accuracy: 0.75\n",
      "At: 1923 [==========>] Loss 0.21745565464448255  - accuracy: 0.6875\n",
      "At: 1924 [==========>] Loss 0.10878047708043899  - accuracy: 0.875\n",
      "At: 1925 [==========>] Loss 0.1733499005353628  - accuracy: 0.8125\n",
      "At: 1926 [==========>] Loss 0.10347443783100446  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.09808273755459396  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.12587547056602974  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.18763276391649827  - accuracy: 0.75\n",
      "At: 1930 [==========>] Loss 0.1730695014944673  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.11556066412929375  - accuracy: 0.8125\n",
      "At: 1932 [==========>] Loss 0.15366251799468272  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.08467659528587251  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.15262719131709104  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.12633843505649742  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.08716830195535369  - accuracy: 0.90625\n",
      "At: 1937 [==========>] Loss 0.15019118920674973  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.17537425622080122  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.07717294002693317  - accuracy: 0.90625\n",
      "At: 1940 [==========>] Loss 0.11478514831484887  - accuracy: 0.84375\n",
      "At: 1941 [==========>] Loss 0.1056317534208839  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.1529608538327606  - accuracy: 0.8125\n",
      "At: 1943 [==========>] Loss 0.14739051616189994  - accuracy: 0.84375\n",
      "At: 1944 [==========>] Loss 0.12604214940989736  - accuracy: 0.8125\n",
      "At: 1945 [==========>] Loss 0.16978728759846612  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.0770892361122626  - accuracy: 0.9375\n",
      "At: 1947 [==========>] Loss 0.1144003511825465  - accuracy: 0.875\n",
      "At: 1948 [==========>] Loss 0.11927165502888654  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.08008869323044465  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.1484752747469853  - accuracy: 0.875\n",
      "At: 1951 [==========>] Loss 0.13136801246869612  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.08841144842180218  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.06685838244985705  - accuracy: 0.96875\n",
      "At: 1954 [==========>] Loss 0.17391645466762945  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.08194673715853136  - accuracy: 0.875\n",
      "At: 1956 [==========>] Loss 0.12514824601329078  - accuracy: 0.8125\n",
      "At: 1957 [==========>] Loss 0.07160195682256992  - accuracy: 0.875\n",
      "At: 1958 [==========>] Loss 0.11309162996871001  - accuracy: 0.8125\n",
      "At: 1959 [==========>] Loss 0.12376143137866547  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.057411572091948075  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.18180129088858804  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.17909407588957804  - accuracy: 0.75\n",
      "At: 1963 [==========>] Loss 0.05670776194921481  - accuracy: 0.9375\n",
      "At: 1964 [==========>] Loss 0.1881315813470297  - accuracy: 0.71875\n",
      "At: 1965 [==========>] Loss 0.13834185215929548  - accuracy: 0.8125\n",
      "At: 1966 [==========>] Loss 0.09176362137727867  - accuracy: 0.875\n",
      "At: 1967 [==========>] Loss 0.1565694416926611  - accuracy: 0.6875\n",
      "At: 1968 [==========>] Loss 0.18972047979452872  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.16889108835316904  - accuracy: 0.75\n",
      "At: 1970 [==========>] Loss 0.10823302198515031  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.2138752940475651  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.10008658020339237  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.15949461921060898  - accuracy: 0.71875\n",
      "At: 1974 [==========>] Loss 0.11456154059551349  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.1777066484750816  - accuracy: 0.71875\n",
      "At: 1976 [==========>] Loss 0.07751490642361782  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.09926821335154953  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.12569810585364632  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.12633430893693628  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.13124417930059157  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.16276012057021988  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.0738492225569358  - accuracy: 0.875\n",
      "At: 1983 [==========>] Loss 0.13098451924838994  - accuracy: 0.875\n",
      "At: 1984 [==========>] Loss 0.1003013855840251  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.13336093024795886  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.19610039630244994  - accuracy: 0.6875\n",
      "At: 1987 [==========>] Loss 0.11223304902001902  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.0983200513886624  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.10618655400863336  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.10115520854938469  - accuracy: 0.84375\n",
      "At: 1991 [==========>] Loss 0.12563486919101008  - accuracy: 0.78125\n",
      "At: 1992 [==========>] Loss 0.09939506529804346  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.1651963360220166  - accuracy: 0.75\n",
      "At: 1994 [==========>] Loss 0.119384132976043  - accuracy: 0.78125\n",
      "At: 1995 [==========>] Loss 0.17684087129949533  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.09508546302499277  - accuracy: 0.875\n",
      "At: 1997 [==========>] Loss 0.20131462600299144  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.14553814331658277  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.09976671922463716  - accuracy: 0.875\n",
      "At: 2000 [==========>] Loss 0.12523429716855122  - accuracy: 0.78125\n",
      "At: 2001 [==========>] Loss 0.07548831017393814  - accuracy: 0.90625\n",
      "At: 2002 [==========>] Loss 0.07439824858418634  - accuracy: 0.90625\n",
      "At: 2003 [==========>] Loss 0.14795307376452596  - accuracy: 0.75\n",
      "At: 2004 [==========>] Loss 0.15775983681590972  - accuracy: 0.71875\n",
      "At: 2005 [==========>] Loss 0.12519289887510127  - accuracy: 0.8125\n",
      "At: 2006 [==========>] Loss 0.12243323253055056  - accuracy: 0.84375\n",
      "At: 2007 [==========>] Loss 0.10643106368290409  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.13249301333972607  - accuracy: 0.8125\n",
      "At: 2009 [==========>] Loss 0.14582054090708185  - accuracy: 0.78125\n",
      "At: 2010 [==========>] Loss 0.10573433937285791  - accuracy: 0.875\n",
      "At: 2011 [==========>] Loss 0.12144108526010872  - accuracy: 0.84375\n",
      "At: 2012 [==========>] Loss 0.1318789086008093  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09334456400706477  - accuracy: 0.84375\n",
      "At: 2014 [==========>] Loss 0.21509882837705357  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.06540786172775712  - accuracy: 0.875\n",
      "At: 2016 [==========>] Loss 0.11554852834959982  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.0776660899849988  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.08111836943327652  - accuracy: 0.90625\n",
      "At: 2019 [==========>] Loss 0.12495110958402744  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.08142474448889431  - accuracy: 0.90625\n",
      "At: 2021 [==========>] Loss 0.10717149661382494  - accuracy: 0.84375\n",
      "At: 2022 [==========>] Loss 0.12848945286099195  - accuracy: 0.71875\n",
      "At: 2023 [==========>] Loss 0.0753000626793824  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.07361115659625792  - accuracy: 0.9375\n",
      "At: 2025 [==========>] Loss 0.1676943493102949  - accuracy: 0.71875\n",
      "At: 2026 [==========>] Loss 0.09218630617794034  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.14673846072703353  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.11430950350332361  - accuracy: 0.78125\n",
      "At: 2029 [==========>] Loss 0.11653208249009002  - accuracy: 0.84375\n",
      "At: 2030 [==========>] Loss 0.15580041180707282  - accuracy: 0.78125\n",
      "At: 2031 [==========>] Loss 0.169582028218502  - accuracy: 0.6875\n",
      "At: 2032 [==========>] Loss 0.14047016351399272  - accuracy: 0.84375\n",
      "At: 2033 [==========>] Loss 0.1362541394737632  - accuracy: 0.8125\n",
      "At: 2034 [==========>] Loss 0.22633826516071137  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.11804602664879503  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09819849568805103  - accuracy: 0.90625\n",
      "At: 2037 [==========>] Loss 0.11083741831187877  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.09942659289716699  - accuracy: 0.84375\n",
      "At: 2039 [==========>] Loss 0.07817824002959975  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.10242733367964302  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.061105640441481854  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.1113537447890639  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.10027415524079508  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.09555010515254955  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.20822523482130706  - accuracy: 0.75\n",
      "At: 2046 [==========>] Loss 0.06117252154158309  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08302616674671354  - accuracy: 0.875\n",
      "At: 2048 [==========>] Loss 0.10231732275205568  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.15431348880222392  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.16765223711109697  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1944477773746785  - accuracy: 0.71875\n",
      "At: 2052 [==========>] Loss 0.06405882898271478  - accuracy: 0.9375\n",
      "At: 2053 [==========>] Loss 0.11091412392015751  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.10916596020071252  - accuracy: 0.84375\n",
      "At: 2055 [==========>] Loss 0.06634005436503942  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.11689349401415836  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.1363249290936518  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.14333520996508303  - accuracy: 0.8125\n",
      "At: 2059 [==========>] Loss 0.17838993322865757  - accuracy: 0.75\n",
      "At: 2060 [==========>] Loss 0.11044582514351238  - accuracy: 0.84375\n",
      "At: 2061 [==========>] Loss 0.14396462809949467  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.14159370774749616  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.0750885072358213  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.19607659024311302  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.052354457550221736  - accuracy: 0.9375\n",
      "At: 2066 [==========>] Loss 0.16016586541514916  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.0947814177340851  - accuracy: 0.84375\n",
      "At: 2068 [==========>] Loss 0.08515140325629547  - accuracy: 0.84375\n",
      "At: 2069 [==========>] Loss 0.09334984396995316  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.13770018884852292  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.14857380007099252  - accuracy: 0.75\n",
      "At: 2072 [==========>] Loss 0.06301864804236214  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.09140749763065427  - accuracy: 0.875\n",
      "At: 2074 [==========>] Loss 0.06604667910200337  - accuracy: 0.9375\n",
      "At: 2075 [==========>] Loss 0.08731742235045578  - accuracy: 0.90625\n",
      "At: 2076 [==========>] Loss 0.14424751584442608  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.16443543165653857  - accuracy: 0.78125\n",
      "At: 2078 [==========>] Loss 0.11732035863598525  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07050334317255912  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.11170530349770771  - accuracy: 0.90625\n",
      "At: 2081 [==========>] Loss 0.15547125522308583  - accuracy: 0.75\n",
      "At: 2082 [==========>] Loss 0.12876409255254562  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.1764921688086652  - accuracy: 0.78125\n",
      "At: 2084 [==========>] Loss 0.10487927629952049  - accuracy: 0.84375\n",
      "At: 2085 [==========>] Loss 0.09059415366021696  - accuracy: 0.9375\n",
      "At: 2086 [==========>] Loss 0.08799329306529897  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.18624655097568532  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.08690751026668904  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.15454597084842037  - accuracy: 0.8125\n",
      "At: 2090 [==========>] Loss 0.09189988673492824  - accuracy: 0.8125\n",
      "At: 2091 [==========>] Loss 0.11621460871763029  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.08637667396743545  - accuracy: 0.8125\n",
      "At: 2093 [==========>] Loss 0.15916228284383532  - accuracy: 0.8125\n",
      "At: 2094 [==========>] Loss 0.10020664960397904  - accuracy: 0.875\n",
      "At: 2095 [==========>] Loss 0.08151786961482393  - accuracy: 0.9375\n",
      "At: 2096 [==========>] Loss 0.1500297707157098  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.11913865131013265  - accuracy: 0.875\n",
      "At: 2098 [==========>] Loss 0.11918365361388869  - accuracy: 0.75\n",
      "At: 2099 [==========>] Loss 0.1083632088465989  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.04876080158154037  - accuracy: 0.96875\n",
      "At: 2101 [==========>] Loss 0.1483730052081596  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.07183328655479558  - accuracy: 0.875\n",
      "At: 2103 [==========>] Loss 0.1445759855051609  - accuracy: 0.78125\n",
      "At: 2104 [==========>] Loss 0.1162450906307427  - accuracy: 0.78125\n",
      "At: 2105 [==========>] Loss 0.170783629973068  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.13846446303897264  - accuracy: 0.8125\n",
      "At: 2107 [==========>] Loss 0.0917209916087581  - accuracy: 0.84375\n",
      "At: 2108 [==========>] Loss 0.14730903312352794  - accuracy: 0.78125\n",
      "At: 2109 [==========>] Loss 0.11484416299168071  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.048718958235967225  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.09607968410031748  - accuracy: 0.90625\n",
      "At: 2112 [==========>] Loss 0.11440292737290043  - accuracy: 0.84375\n",
      "At: 2113 [==========>] Loss 0.0782541124522615  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.15324750935272805  - accuracy: 0.8125\n",
      "At: 2115 [==========>] Loss 0.10498183952772439  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.12753140411369646  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.1270857112337655  - accuracy: 0.84375\n",
      "At: 2118 [==========>] Loss 0.1545493433331294  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.09260308401315681  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.12392367472205908  - accuracy: 0.8125\n",
      "At: 2121 [==========>] Loss 0.14098349015228628  - accuracy: 0.84375\n",
      "At: 2122 [==========>] Loss 0.12232810938470486  - accuracy: 0.84375\n",
      "At: 2123 [==========>] Loss 0.19691222651303464  - accuracy: 0.71875\n",
      "At: 2124 [==========>] Loss 0.1291847523603289  - accuracy: 0.78125\n",
      "At: 2125 [==========>] Loss 0.10152720055017728  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.059244291301974734  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.10819705568247302  - accuracy: 0.90625\n",
      "At: 2128 [==========>] Loss 0.08811207090513279  - accuracy: 0.90625\n",
      "At: 2129 [==========>] Loss 0.13862488383384589  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.08274866073761256  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.07910856825239782  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.21431858994275266  - accuracy: 0.75\n",
      "At: 2133 [==========>] Loss 0.17852583616845172  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.13399301626011537  - accuracy: 0.75\n",
      "At: 2135 [==========>] Loss 0.10245019682339374  - accuracy: 0.84375\n",
      "At: 2136 [==========>] Loss 0.13091532230734187  - accuracy: 0.78125\n",
      "At: 2137 [==========>] Loss 0.10631797725430006  - accuracy: 0.875\n",
      "At: 2138 [==========>] Loss 0.1261495039149313  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.17065327853447143  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.1094950624426049  - accuracy: 0.8125\n",
      "At: 2141 [==========>] Loss 0.09171340254500052  - accuracy: 0.90625\n",
      "At: 2142 [==========>] Loss 0.11077570744351485  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.10546356899627474  - accuracy: 0.84375\n",
      "At: 2144 [==========>] Loss 0.09065154784425025  - accuracy: 0.84375\n",
      "At: 2145 [==========>] Loss 0.10760092166980612  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.1434573814467328  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.12355754206148545  - accuracy: 0.84375\n",
      "At: 2148 [==========>] Loss 0.20370765450055972  - accuracy: 0.71875\n",
      "At: 2149 [==========>] Loss 0.11305146605067509  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.08712111278029588  - accuracy: 0.9375\n",
      "At: 2151 [==========>] Loss 0.09830715882350408  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.23851313722285056  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.16678381288365823  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.17749118883532367  - accuracy: 0.75\n",
      "At: 2155 [==========>] Loss 0.11647640210578161  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.11250793093959002  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.11209570131446742  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.15169110849808856  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.07640576823865376  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.13313896033353914  - accuracy: 0.75\n",
      "At: 2161 [==========>] Loss 0.0949611276416674  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.09853261117251172  - accuracy: 0.8125\n",
      "At: 2163 [==========>] Loss 0.13580410835366014  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.17200893370257148  - accuracy: 0.71875\n",
      "At: 2165 [==========>] Loss 0.09293378441506214  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.10433286768164629  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.10187944732987098  - accuracy: 0.875\n",
      "At: 2168 [==========>] Loss 0.09635360959668648  - accuracy: 0.875\n",
      "At: 2169 [==========>] Loss 0.101290263441321  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11655535649459295  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.14863417834171355  - accuracy: 0.8125\n",
      "At: 2172 [==========>] Loss 0.09957883621930663  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.13566579367435466  - accuracy: 0.8125\n",
      "At: 2174 [==========>] Loss 0.11093870318180774  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.11174784427010882  - accuracy: 0.90625\n",
      "At: 2176 [==========>] Loss 0.11008523966547698  - accuracy: 0.875\n",
      "At: 2177 [==========>] Loss 0.14992489910624848  - accuracy: 0.78125\n",
      "At: 2178 [==========>] Loss 0.09604741061263088  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.12529181599580488  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.1079344287871222  - accuracy: 0.78125\n",
      "At: 2181 [==========>] Loss 0.18464963734779744  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.08119549593478836  - accuracy: 0.90625\n",
      "At: 2183 [==========>] Loss 0.2066161683343831  - accuracy: 0.65625\n",
      "At: 2184 [==========>] Loss 0.0770791573164622  - accuracy: 0.96875\n",
      "At: 2185 [==========>] Loss 0.10093435042742183  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.13592999647859016  - accuracy: 0.84375\n",
      "At: 2187 [==========>] Loss 0.1826217560812066  - accuracy: 0.8125\n",
      "At: 2188 [==========>] Loss 0.06377438312621483  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.08485740765460148  - accuracy: 0.875\n",
      "At: 2190 [==========>] Loss 0.11692923274006083  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.09852342851265808  - accuracy: 0.84375\n",
      "At: 2192 [==========>] Loss 0.0980470809999568  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.14185953325762102  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.10488158672963338  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.19550579100794868  - accuracy: 0.65625\n",
      "At: 2196 [==========>] Loss 0.14332256374528068  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.08556408788971338  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.08145371065713865  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.05732353170207807  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.0562007507162748  - accuracy: 0.96875\n",
      "At: 2201 [==========>] Loss 0.10787424917132193  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.08008903657834168  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.08388215461522311  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.09988030473675472  - accuracy: 0.90625\n",
      "At: 2205 [==========>] Loss 0.10692051569847502  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.09809048431623407  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.08662393367197473  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.13995487713635962  - accuracy: 0.84375\n",
      "At: 2209 [==========>] Loss 0.172327049777395  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.11769087022753555  - accuracy: 0.875\n",
      "At: 2211 [==========>] Loss 0.1584956515146944  - accuracy: 0.78125\n",
      "At: 2212 [==========>] Loss 0.09125544804593888  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.12492586027061436  - accuracy: 0.78125\n",
      "At: 2214 [==========>] Loss 0.13024292993936867  - accuracy: 0.8125\n",
      "At: 2215 [==========>] Loss 0.11997429471334428  - accuracy: 0.875\n",
      "At: 2216 [==========>] Loss 0.1283105984443379  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.12142828363143114  - accuracy: 0.84375\n",
      "At: 2218 [==========>] Loss 0.15560284207293956  - accuracy: 0.84375\n",
      "At: 2219 [==========>] Loss 0.07381530544899098  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.1183891666227019  - accuracy: 0.8125\n",
      "At: 2221 [==========>] Loss 0.1611903769221536  - accuracy: 0.8125\n",
      "At: 2222 [==========>] Loss 0.11575908051570344  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.14582278278832012  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.13249298663300982  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.10571229161777164  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.1092811590991048  - accuracy: 0.875\n",
      "At: 2227 [==========>] Loss 0.17723844918516413  - accuracy: 0.78125\n",
      "At: 2228 [==========>] Loss 0.06851172891544452  - accuracy: 0.9375\n",
      "At: 2229 [==========>] Loss 0.14714688135549836  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.12247602057062229  - accuracy: 0.84375\n",
      "At: 2231 [==========>] Loss 0.15682232910388166  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.14845559160749416  - accuracy: 0.8125\n",
      "At: 2233 [==========>] Loss 0.15374852109606554  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.1290084068667247  - accuracy: 0.84375\n",
      "At: 2235 [==========>] Loss 0.11539926432683695  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.07393597440142362  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.11712542746649085  - accuracy: 0.78125\n",
      "At: 2238 [==========>] Loss 0.1167325780334831  - accuracy: 0.875\n",
      "At: 2239 [==========>] Loss 0.19852257149001507  - accuracy: 0.6875\n",
      "At: 2240 [==========>] Loss 0.15868427164185295  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.1291957730702882  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.1640456371320924  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.08060980600563822  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.09721508596727357  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.08107994390989652  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.13924270392426177  - accuracy: 0.84375\n",
      "At: 2247 [==========>] Loss 0.1019776318152232  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.16960673699974546  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.08287664940558498  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.09947574452656911  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.06810116759654541  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.11795007261646474  - accuracy: 0.84375\n",
      "At: 2253 [==========>] Loss 0.13408541416056322  - accuracy: 0.78125\n",
      "At: 2254 [==========>] Loss 0.11891575963669272  - accuracy: 0.84375\n",
      "At: 2255 [==========>] Loss 0.14267691328858842  - accuracy: 0.78125\n",
      "At: 2256 [==========>] Loss 0.163634623195615  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.0912826665215403  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.15469757498092884  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.14295992197589052  - accuracy: 0.78125\n",
      "At: 2260 [==========>] Loss 0.149638940725894  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.10065483040426024  - accuracy: 0.875\n",
      "At: 2262 [==========>] Loss 0.14539727041621203  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.13459003782597023  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.09567671384773525  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.0926640372061173  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.12177740648709603  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.06981542084810563  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.07977687483389106  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.0463927934323244  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.09554927363905058  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.17591398640528128  - accuracy: 0.6875\n",
      "At: 2272 [==========>] Loss 0.07695352447802148  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.11365664558355347  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.08599363074058688  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.10150145749925432  - accuracy: 0.84375\n",
      "At: 2276 [==========>] Loss 0.08700320577753362  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.13596988200703625  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.08038141759638182  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.15418969538097776  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.15189588628025807  - accuracy: 0.8125\n",
      "At: 2281 [==========>] Loss 0.0970796495517578  - accuracy: 0.90625\n",
      "At: 2282 [==========>] Loss 0.056580481659939244  - accuracy: 0.96875\n",
      "At: 2283 [==========>] Loss 0.16626837889327964  - accuracy: 0.75\n",
      "At: 2284 [==========>] Loss 0.11813730323728236  - accuracy: 0.78125\n",
      "At: 2285 [==========>] Loss 0.09478371149951237  - accuracy: 0.875\n",
      "At: 2286 [==========>] Loss 0.12736977968146326  - accuracy: 0.78125\n",
      "At: 2287 [==========>] Loss 0.1434703215692092  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.09638453398241797  - accuracy: 0.875\n",
      "At: 2289 [==========>] Loss 0.15816127147914757  - accuracy: 0.6875\n",
      "At: 2290 [==========>] Loss 0.04694656150914664  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.1388351077079606  - accuracy: 0.78125\n",
      "At: 2292 [==========>] Loss 0.0809300361880955  - accuracy: 0.84375\n",
      "At: 2293 [==========>] Loss 0.057306860075017514  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.05734612397001565  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.13853590674742572  - accuracy: 0.8125\n",
      "At: 2296 [==========>] Loss 0.183160982921781  - accuracy: 0.8125\n",
      "At: 2297 [==========>] Loss 0.07234661240195092  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.10786337525991746  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.1057356870369835  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.11834120770702988  - accuracy: 0.84375\n",
      "At: 2301 [==========>] Loss 0.21214702776777805  - accuracy: 0.65625\n",
      "At: 2302 [==========>] Loss 0.17196079317375906  - accuracy: 0.78125\n",
      "At: 2303 [==========>] Loss 0.04657119190804859  - accuracy: 0.96875\n",
      "At: 2304 [==========>] Loss 0.11125707833419955  - accuracy: 0.8125\n",
      "At: 2305 [==========>] Loss 0.09402761382836272  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.10821253657884278  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.15558472508823543  - accuracy: 0.8125\n",
      "At: 2308 [==========>] Loss 0.18015282759564266  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.13296300418375806  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.1182556231219069  - accuracy: 0.84375\n",
      "At: 2311 [==========>] Loss 0.14105076234659297  - accuracy: 0.75\n",
      "At: 2312 [==========>] Loss 0.09719791974081246  - accuracy: 0.90625\n",
      "At: 2313 [==========>] Loss 0.08372538110882985  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.11193494080661973  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.15125010064663708  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.11846011876343826  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.18371999421768356  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.16316988055030174  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.11676151649059349  - accuracy: 0.90625\n",
      "At: 2320 [==========>] Loss 0.10296232351374597  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.13659566980154378  - accuracy: 0.84375\n",
      "At: 2322 [==========>] Loss 0.1762397697963804  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.13596405106606227  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.17087003629293662  - accuracy: 0.78125\n",
      "At: 2325 [==========>] Loss 0.1343075211565653  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.06738258730914522  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.06375844056728022  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12723771325312797  - accuracy: 0.84375\n",
      "At: 2329 [==========>] Loss 0.10747072376698377  - accuracy: 0.8125\n",
      "At: 2330 [==========>] Loss 0.14408697189812716  - accuracy: 0.78125\n",
      "At: 2331 [==========>] Loss 0.10976889626639126  - accuracy: 0.84375\n",
      "At: 2332 [==========>] Loss 0.11701248865967304  - accuracy: 0.75\n",
      "At: 2333 [==========>] Loss 0.09447232343509915  - accuracy: 0.90625\n",
      "At: 2334 [==========>] Loss 0.17175498655828308  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.1206448545906551  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.08160590079947144  - accuracy: 0.90625\n",
      "At: 2337 [==========>] Loss 0.13002173895851102  - accuracy: 0.8125\n",
      "At: 2338 [==========>] Loss 0.12136540465497213  - accuracy: 0.78125\n",
      "At: 2339 [==========>] Loss 0.09126213312430702  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.13237916412904516  - accuracy: 0.875\n",
      "At: 2341 [==========>] Loss 0.13485236764578615  - accuracy: 0.8125\n",
      "At: 2342 [==========>] Loss 0.15458771392223136  - accuracy: 0.8125\n",
      "At: 2343 [==========>] Loss 0.08812777821073989  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.18653496158269597  - accuracy: 0.65625\n",
      "At: 2345 [==========>] Loss 0.1539714338409937  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.07171671823937673  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.12145793115260052  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.058682151724303744  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.11577373722841386  - accuracy: 0.78125\n",
      "At: 2350 [==========>] Loss 0.11400997377431145  - accuracy: 0.8125\n",
      "At: 2351 [==========>] Loss 0.08004030749184843  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.09785463459213684  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.10425774425100191  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.1621577799065308  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.056542839899189096  - accuracy: 0.90625\n",
      "At: 2356 [==========>] Loss 0.13290473559602015  - accuracy: 0.84375\n",
      "At: 2357 [==========>] Loss 0.1348168210993107  - accuracy: 0.8125\n",
      "At: 2358 [==========>] Loss 0.15491243920614536  - accuracy: 0.8125\n",
      "At: 2359 [==========>] Loss 0.18604179192108228  - accuracy: 0.6875\n",
      "At: 2360 [==========>] Loss 0.12621206114155537  - accuracy: 0.875\n",
      "At: 2361 [==========>] Loss 0.15769855584787826  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.07554160547029934  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.1635346379507665  - accuracy: 0.78125\n",
      "At: 2364 [==========>] Loss 0.08233315447373735  - accuracy: 0.875\n",
      "At: 2365 [==========>] Loss 0.08130187079663286  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.15696929019345846  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.08177198562432214  - accuracy: 0.90625\n",
      "At: 2368 [==========>] Loss 0.1069467021786473  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.11536179854404849  - accuracy: 0.8125\n",
      "At: 2370 [==========>] Loss 0.08646602182989893  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.09790375482263282  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.1094269932921525  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.107600176872689  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.09677287754646081  - accuracy: 0.8125\n",
      "At: 2375 [==========>] Loss 0.045610719158740626  - accuracy: 0.96875\n",
      "At: 2376 [==========>] Loss 0.10075195173470786  - accuracy: 0.875\n",
      "At: 2377 [==========>] Loss 0.09667980864179113  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.12656380688895671  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.15954132435868068  - accuracy: 0.71875\n",
      "At: 2380 [==========>] Loss 0.05809761448243342  - accuracy: 0.9375\n",
      "At: 2381 [==========>] Loss 0.08701718006082895  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.09720591250968802  - accuracy: 0.84375\n",
      "At: 2383 [==========>] Loss 0.12485509258503641  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.1169336788217607  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.12401708628255026  - accuracy: 0.8125\n",
      "At: 2386 [==========>] Loss 0.08941424024086525  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.07690691476195438  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.12578442019481487  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.03975048290107906  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.06473979013943557  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.16265698241667834  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.1316168327585885  - accuracy: 0.84375\n",
      "At: 2393 [==========>] Loss 0.08687467128542446  - accuracy: 0.90625\n",
      "At: 2394 [==========>] Loss 0.06679817778695549  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.08531957812318805  - accuracy: 0.90625\n",
      "At: 2396 [==========>] Loss 0.07633036553916683  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.08276923586437286  - accuracy: 0.9375\n",
      "At: 2398 [==========>] Loss 0.11332632273720653  - accuracy: 0.8125\n",
      "At: 2399 [==========>] Loss 0.15394778584581795  - accuracy: 0.78125\n",
      "At: 2400 [==========>] Loss 0.10545993319146772  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.08417758199757702  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.08158045180774412  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.2060373040454005  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.1318722952739624  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.05968718013385376  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.13864515052902637  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.13570347646282083  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.09815383623101662  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.1473354835107346  - accuracy: 0.875\n",
      "At: 2410 [==========>] Loss 0.1459577527384569  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.11036077401214874  - accuracy: 0.84375\n",
      "At: 2412 [==========>] Loss 0.07781092894777536  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.09037119459901959  - accuracy: 0.875\n",
      "At: 2414 [==========>] Loss 0.08036785434524403  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.09972993409959377  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.09774950713559566  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.15804076334364664  - accuracy: 0.75\n",
      "At: 2418 [==========>] Loss 0.12241951511098145  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.13063637817055804  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.1548263692759362  - accuracy: 0.75\n",
      "At: 2421 [==========>] Loss 0.0924912383942842  - accuracy: 0.90625\n",
      "At: 2422 [==========>] Loss 0.13581358296723656  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.1244755263671851  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.09624239125294715  - accuracy: 0.90625\n",
      "At: 2425 [==========>] Loss 0.08610845629916337  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.15242685907439116  - accuracy: 0.8125\n",
      "At: 2427 [==========>] Loss 0.13180857646293176  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.08364961984223127  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.10943203499867547  - accuracy: 0.875\n",
      "At: 2430 [==========>] Loss 0.14697162591566043  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.1664485317549016  - accuracy: 0.8125\n",
      "At: 2432 [==========>] Loss 0.07015427921732553  - accuracy: 0.90625\n",
      "At: 2433 [==========>] Loss 0.048809958094533985  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.041711070513083606  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.14225986228591767  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.09560944216946132  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.19837858212777232  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.12891566923041453  - accuracy: 0.8125\n",
      "At: 2439 [==========>] Loss 0.1415056435188398  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.08561565622435668  - accuracy: 0.90625\n",
      "At: 2441 [==========>] Loss 0.11875668988273533  - accuracy: 0.8125\n",
      "At: 2442 [==========>] Loss 0.13684723944217106  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.10695940712651036  - accuracy: 0.84375\n",
      "At: 2444 [==========>] Loss 0.07626707395486669  - accuracy: 0.90625\n",
      "At: 2445 [==========>] Loss 0.08205986284235692  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.15865124166651037  - accuracy: 0.84375\n",
      "At: 2447 [==========>] Loss 0.1841326111706277  - accuracy: 0.71875\n",
      "At: 2448 [==========>] Loss 0.11098209726541343  - accuracy: 0.84375\n",
      "At: 2449 [==========>] Loss 0.07183050940567626  - accuracy: 0.9375\n",
      "At: 2450 [==========>] Loss 0.07062055288258444  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.053133027607311326  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.1297977257769935  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.11298040555795358  - accuracy: 0.875\n",
      "At: 2454 [==========>] Loss 0.15834814935496577  - accuracy: 0.78125\n",
      "At: 2455 [==========>] Loss 0.1435450167981015  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.14012693635105872  - accuracy: 0.8125\n",
      "At: 2457 [==========>] Loss 0.1645257948133525  - accuracy: 0.8125\n",
      "At: 2458 [==========>] Loss 0.07315513294189965  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.1399023842328514  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.0715063905765631  - accuracy: 0.96875\n",
      "At: 2461 [==========>] Loss 0.05932804056906123  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.19300229745601916  - accuracy: 0.6875\n",
      "At: 2463 [==========>] Loss 0.0876871052647461  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.19495933984491598  - accuracy: 0.6875\n",
      "At: 2465 [==========>] Loss 0.14283643260291667  - accuracy: 0.71875\n",
      "At: 2466 [==========>] Loss 0.08159685418179978  - accuracy: 0.9375\n",
      "At: 2467 [==========>] Loss 0.09322164032886385  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.09176313003889203  - accuracy: 0.875\n",
      "At: 2469 [==========>] Loss 0.14297574455975337  - accuracy: 0.8125\n",
      "At: 2470 [==========>] Loss 0.10940333867606276  - accuracy: 0.875\n",
      "At: 2471 [==========>] Loss 0.10605176266584063  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.1011237890812321  - accuracy: 0.90625\n",
      "At: 2473 [==========>] Loss 0.12269915977221764  - accuracy: 0.875\n",
      "At: 2474 [==========>] Loss 0.060163669222078  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.10677966804286575  - accuracy: 0.84375\n",
      "At: 2476 [==========>] Loss 0.08987402570546404  - accuracy: 0.84375\n",
      "At: 2477 [==========>] Loss 0.15206984509101637  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.1082680663471372  - accuracy: 0.78125\n",
      "At: 2479 [==========>] Loss 0.06914628476327994  - accuracy: 0.90625\n",
      "At: 2480 [==========>] Loss 0.13208716936365963  - accuracy: 0.78125\n",
      "At: 2481 [==========>] Loss 0.0590483739388861  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.14102992066202244  - accuracy: 0.84375\n",
      "At: 2483 [==========>] Loss 0.12102742451754714  - accuracy: 0.8125\n",
      "At: 2484 [==========>] Loss 0.08900655619157385  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.10568300182647311  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.1219468491922046  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.1220981365654985  - accuracy: 0.84375\n",
      "At: 2488 [==========>] Loss 0.18570269852060137  - accuracy: 0.71875\n",
      "At: 2489 [==========>] Loss 0.17514903005313237  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.09162369476183603  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.1205520014187123  - accuracy: 0.84375\n",
      "At: 2492 [==========>] Loss 0.11961358649364989  - accuracy: 0.84375\n",
      "At: 2493 [==========>] Loss 0.09118112668379763  - accuracy: 0.90625\n",
      "At: 2494 [==========>] Loss 0.09977437799700839  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.11220285902565015  - accuracy: 0.84375\n",
      "At: 2496 [==========>] Loss 0.06565203882600701  - accuracy: 0.90625\n",
      "At: 2497 [==========>] Loss 0.21079937423205736  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.0754438096353362  - accuracy: 0.90625\n",
      "At: 2499 [==========>] Loss 0.06245174273606251  - accuracy: 0.9375\n",
      "At: 2500 [==========>] Loss 0.19456179230452766  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.1441795399862731  - accuracy: 0.78125\n",
      "At: 2502 [==========>] Loss 0.1205249346541016  - accuracy: 0.8125\n",
      "At: 2503 [==========>] Loss 0.1270129015104783  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.1679261451548219  - accuracy: 0.75\n",
      "At: 2505 [==========>] Loss 0.11030681413264473  - accuracy: 0.84375\n",
      "At: 2506 [==========>] Loss 0.08722414228622824  - accuracy: 0.9375\n",
      "At: 2507 [==========>] Loss 0.12528179126025096  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.1053880394741416  - accuracy: 0.84375\n",
      "At: 2509 [==========>] Loss 0.14230697234404396  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.10136456118084818  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.1605247879722822  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.11423437645775011  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.16779604341763998  - accuracy: 0.75\n",
      "At: 2514 [==========>] Loss 0.11433262048126402  - accuracy: 0.875\n",
      "At: 2515 [==========>] Loss 0.19112212239563336  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.21148638936608272  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.119065295241111  - accuracy: 0.78125\n",
      "At: 2518 [==========>] Loss 0.1277488788009082  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.10275302586287761  - accuracy: 0.875\n",
      "At: 2520 [==========>] Loss 0.14001870989239162  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.09540621807344854  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.2208045987603139  - accuracy: 0.71875\n",
      "At: 2523 [==========>] Loss 0.11188231543320334  - accuracy: 0.84375\n",
      "At: 2524 [==========>] Loss 0.15890350787147978  - accuracy: 0.78125\n",
      "At: 2525 [==========>] Loss 0.0686035676595806  - accuracy: 0.9375\n",
      "At: 2526 [==========>] Loss 0.1248957439893705  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.13542109543717673  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.08424195722911235  - accuracy: 0.875\n",
      "At: 2529 [==========>] Loss 0.16183894920740838  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.14194543428545853  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.0419886665037853  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.11802758117878819  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.08749921794969437  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.06622135682191395  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.08401550547883065  - accuracy: 0.90625\n",
      "At: 2536 [==========>] Loss 0.15400271298282733  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.10883861480236046  - accuracy: 0.875\n",
      "At: 2538 [==========>] Loss 0.19022850236683797  - accuracy: 0.65625\n",
      "At: 2539 [==========>] Loss 0.08662727200100132  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.1271517285655773  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.05406086531006726  - accuracy: 0.9375\n",
      "At: 2542 [==========>] Loss 0.07318968843977577  - accuracy: 0.90625\n",
      "At: 2543 [==========>] Loss 0.1583744767031186  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.12632220228858138  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.052617497717265004  - accuracy: 0.96875\n",
      "At: 2546 [==========>] Loss 0.13327727465609301  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.09061439457913029  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.08642263368088718  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.11120577675884077  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.13373503558725197  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.13785239899413315  - accuracy: 0.78125\n",
      "At: 2552 [==========>] Loss 0.094662446423181  - accuracy: 0.90625\n",
      "At: 2553 [==========>] Loss 0.062106537544840155  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.1470280162898052  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.1865216441733656  - accuracy: 0.78125\n",
      "At: 2556 [==========>] Loss 0.09424659849654987  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.1406993174142045  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.09554677055449125  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.10385271305110724  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.19041665630498528  - accuracy: 0.6875\n",
      "At: 2561 [==========>] Loss 0.0970478082311399  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.10030474983661941  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.09581382045849227  - accuracy: 0.84375\n",
      "At: 2564 [==========>] Loss 0.09710323064163395  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.12365175965597638  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.15714400058495026  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.11599450341720827  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.10625482645390555  - accuracy: 0.78125\n",
      "At: 2569 [==========>] Loss 0.07033314443539927  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.1768628321191124  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.08525616919891575  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.17561750586186284  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.17221261246925512  - accuracy: 0.78125\n",
      "At: 2574 [==========>] Loss 0.1395410771399374  - accuracy: 0.78125\n",
      "At: 2575 [==========>] Loss 0.056127756153927275  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.1188413208294587  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.2059937723923792  - accuracy: 0.75\n",
      "At: 2578 [==========>] Loss 0.17606634007714494  - accuracy: 0.75\n",
      "At: 2579 [==========>] Loss 0.1818240209926158  - accuracy: 0.75\n",
      "At: 2580 [==========>] Loss 0.15931003620933137  - accuracy: 0.75\n",
      "At: 2581 [==========>] Loss 0.09930025359687375  - accuracy: 0.875\n",
      "At: 2582 [==========>] Loss 0.2133222659777475  - accuracy: 0.6875\n",
      "At: 2583 [==========>] Loss 0.07630950470259032  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.17805563747635475  - accuracy: 0.71875\n",
      "At: 2585 [==========>] Loss 0.10100444775957422  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.09380405719896848  - accuracy: 0.8125\n",
      "At: 2587 [==========>] Loss 0.1632278994059843  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.12569898241547442  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.09346084355378034  - accuracy: 0.84375\n",
      "At: 2590 [==========>] Loss 0.19589145855166062  - accuracy: 0.75\n",
      "At: 2591 [==========>] Loss 0.1131014611742898  - accuracy: 0.875\n",
      "At: 2592 [==========>] Loss 0.08487625800059992  - accuracy: 0.84375\n",
      "At: 2593 [==========>] Loss 0.07403741767418189  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.08791932212081655  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.0865994993448243  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.11203703840865192  - accuracy: 0.84375\n",
      "At: 2597 [==========>] Loss 0.08962427462893915  - accuracy: 0.84375\n",
      "At: 2598 [==========>] Loss 0.1223688507174305  - accuracy: 0.84375\n",
      "At: 2599 [==========>] Loss 0.11259647389884357  - accuracy: 0.90625\n",
      "At: 2600 [==========>] Loss 0.12171957009968892  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.15200531357910713  - accuracy: 0.78125\n",
      "At: 2602 [==========>] Loss 0.07764917619825334  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.12149740349114257  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.09734222729537828  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.11160287792602697  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.130621675002869  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.12662618268811604  - accuracy: 0.875\n",
      "At: 2608 [==========>] Loss 0.058558661679140415  - accuracy: 0.90625\n",
      "At: 2609 [==========>] Loss 0.07915020433456525  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.1499493192218755  - accuracy: 0.78125\n",
      "At: 2611 [==========>] Loss 0.1598686009976905  - accuracy: 0.75\n",
      "At: 2612 [==========>] Loss 0.09514380225414559  - accuracy: 0.84375\n",
      "At: 2613 [==========>] Loss 0.06686298849606741  - accuracy: 0.90625\n",
      "At: 2614 [==========>] Loss 0.10178031947568938  - accuracy: 0.90625\n",
      "At: 2615 [==========>] Loss 0.09392343853597118  - accuracy: 0.84375\n",
      "At: 2616 [==========>] Loss 0.05904786775373115  - accuracy: 0.90625\n",
      "At: 2617 [==========>] Loss 0.0947865099860154  - accuracy: 0.875\n",
      "At: 2618 [==========>] Loss 0.10903539694670844  - accuracy: 0.8125\n",
      "At: 2619 [==========>] Loss 0.09317896552887064  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.12542107949112408  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.11567386601962863  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.12800740012516043  - accuracy: 0.84375\n",
      "At: 2623 [==========>] Loss 0.08983762467548687  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.1383912760542672  - accuracy: 0.78125\n",
      "At: 2625 [==========>] Loss 0.06155028308484099  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.054666112933732317  - accuracy: 0.96875\n",
      "At: 2627 [==========>] Loss 0.14212667719885708  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.07830671130752015  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.08337664712915348  - accuracy: 0.84375\n",
      "At: 2630 [==========>] Loss 0.08847942242885927  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.1297865470667856  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.11712469127158667  - accuracy: 0.84375\n",
      "At: 2633 [==========>] Loss 0.07724634522695312  - accuracy: 0.875\n",
      "At: 2634 [==========>] Loss 0.06743311050556514  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.22218247159479626  - accuracy: 0.75\n",
      "At: 2636 [==========>] Loss 0.13251035298390634  - accuracy: 0.84375\n",
      "At: 2637 [==========>] Loss 0.12997833961792826  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.08274690153671926  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.08205457028807457  - accuracy: 0.90625\n",
      "At: 2640 [==========>] Loss 0.14083276381050136  - accuracy: 0.8125\n",
      "At: 2641 [==========>] Loss 0.16295548983186706  - accuracy: 0.78125\n",
      "At: 2642 [==========>] Loss 0.17569124960557442  - accuracy: 0.78125\n",
      "At: 2643 [==========>] Loss 0.0850858486833379  - accuracy: 0.90625\n",
      "At: 2644 [==========>] Loss 0.16676785622299153  - accuracy: 0.75\n",
      "At: 2645 [==========>] Loss 0.067067553575449  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.12935735170394094  - accuracy: 0.84375\n",
      "At: 2647 [==========>] Loss 0.11876551518144951  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.14191004064143992  - accuracy: 0.8125\n",
      "At: 2649 [==========>] Loss 0.1263090425344468  - accuracy: 0.875\n",
      "At: 2650 [==========>] Loss 0.11942778523467051  - accuracy: 0.8125\n",
      "At: 2651 [==========>] Loss 0.16129169359376513  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.0967568430250739  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.12552286866961757  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.10364769740970668  - accuracy: 0.90625\n",
      "At: 2655 [==========>] Loss 0.2369196809814268  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.01607877425538977  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.1179578072923739  - accuracy: 0.84375\n",
      "At: 2658 [==========>] Loss 0.08027969368874249  - accuracy: 0.9375\n",
      "At: 2659 [==========>] Loss 0.0578970690521199  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.06252823117027485  - accuracy: 0.9375\n",
      "At: 2661 [==========>] Loss 0.08890926351013513  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.08277625269132807  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.09521229596051194  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.0744210840700439  - accuracy: 0.9375\n",
      "At: 2665 [==========>] Loss 0.08319389840813443  - accuracy: 0.90625\n",
      "At: 2666 [==========>] Loss 0.15958771930247845  - accuracy: 0.78125\n",
      "At: 2667 [==========>] Loss 0.15423004959115927  - accuracy: 0.8125\n",
      "At: 2668 [==========>] Loss 0.13346554237680974  - accuracy: 0.78125\n",
      "At: 2669 [==========>] Loss 0.1576439636030421  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.12146915344118628  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.057256046180385683  - accuracy: 0.96875\n",
      "At: 2672 [==========>] Loss 0.08251612362369934  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.11989330369431084  - accuracy: 0.875\n",
      "At: 2674 [==========>] Loss 0.10875885380566028  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.13061051035560703  - accuracy: 0.8125\n",
      "At: 2676 [==========>] Loss 0.12475400709168935  - accuracy: 0.8125\n",
      "At: 2677 [==========>] Loss 0.1117166994105179  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.04824036949162043  - accuracy: 0.9375\n",
      "At: 2679 [==========>] Loss 0.08313180281583249  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.07298998197897522  - accuracy: 0.9375\n",
      "At: 2681 [==========>] Loss 0.09105745564743002  - accuracy: 0.875\n",
      "At: 2682 [==========>] Loss 0.1450066627978452  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.18542213151670023  - accuracy: 0.71875\n",
      "At: 2684 [==========>] Loss 0.09882803761882887  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.117951462338095  - accuracy: 0.8125\n",
      "At: 2686 [==========>] Loss 0.04984734910396163  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.12680576098570978  - accuracy: 0.90625\n",
      "At: 2688 [==========>] Loss 0.1725421867693763  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.123082084519169  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.15541822961630825  - accuracy: 0.75\n",
      "Epochs  6 / 10\n",
      "At: 1 [==========>] Loss 0.20022555127510677  - accuracy: 0.6875\n",
      "At: 2 [==========>] Loss 0.24331422294351013  - accuracy: 0.625\n",
      "At: 3 [==========>] Loss 0.2196785507285094  - accuracy: 0.71875\n",
      "At: 4 [==========>] Loss 0.18354344974054523  - accuracy: 0.8125\n",
      "At: 5 [==========>] Loss 0.11696472787974163  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.12650868439241392  - accuracy: 0.8125\n",
      "At: 7 [==========>] Loss 0.2033433085558145  - accuracy: 0.65625\n",
      "At: 8 [==========>] Loss 0.21162964761238073  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.2889358929882226  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.23061647564886706  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.24988063803536947  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.2019026092334406  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.20000086876726392  - accuracy: 0.78125\n",
      "At: 14 [==========>] Loss 0.10272942000888532  - accuracy: 0.90625\n",
      "At: 15 [==========>] Loss 0.14877971943645069  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.2313386466806404  - accuracy: 0.6875\n",
      "At: 17 [==========>] Loss 0.23307116238585857  - accuracy: 0.6875\n",
      "At: 18 [==========>] Loss 0.231037532441992  - accuracy: 0.65625\n",
      "At: 19 [==========>] Loss 0.15288729122791694  - accuracy: 0.78125\n",
      "At: 20 [==========>] Loss 0.15115489251496161  - accuracy: 0.71875\n",
      "At: 21 [==========>] Loss 0.26892383612402343  - accuracy: 0.625\n",
      "At: 22 [==========>] Loss 0.19619787780114778  - accuracy: 0.6875\n",
      "At: 23 [==========>] Loss 0.1068361530958847  - accuracy: 0.84375\n",
      "At: 24 [==========>] Loss 0.2835321119520642  - accuracy: 0.625\n",
      "At: 25 [==========>] Loss 0.21948058262003195  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.23920253160750116  - accuracy: 0.625\n",
      "At: 27 [==========>] Loss 0.23766592583753213  - accuracy: 0.6875\n",
      "At: 28 [==========>] Loss 0.22461809192128562  - accuracy: 0.65625\n",
      "At: 29 [==========>] Loss 0.19952536079891592  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.24423302691148208  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.27308884845990367  - accuracy: 0.625\n",
      "At: 32 [==========>] Loss 0.24404140430284035  - accuracy: 0.65625\n",
      "At: 33 [==========>] Loss 0.09359587678081557  - accuracy: 0.84375\n",
      "At: 34 [==========>] Loss 0.21255514251853536  - accuracy: 0.6875\n",
      "At: 35 [==========>] Loss 0.14964966831155874  - accuracy: 0.84375\n",
      "At: 36 [==========>] Loss 0.21308581096722917  - accuracy: 0.71875\n",
      "At: 37 [==========>] Loss 0.2600089982366158  - accuracy: 0.65625\n",
      "At: 38 [==========>] Loss 0.21486250230989212  - accuracy: 0.71875\n",
      "At: 39 [==========>] Loss 0.14738682481915272  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.2359988827561943  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.11516281366323362  - accuracy: 0.84375\n",
      "At: 42 [==========>] Loss 0.16796970020001573  - accuracy: 0.75\n",
      "At: 43 [==========>] Loss 0.17897582152766875  - accuracy: 0.75\n",
      "At: 44 [==========>] Loss 0.16476439000335832  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.08741437101588664  - accuracy: 0.875\n",
      "At: 46 [==========>] Loss 0.17615237910857956  - accuracy: 0.75\n",
      "At: 47 [==========>] Loss 0.18666694348971533  - accuracy: 0.71875\n",
      "At: 48 [==========>] Loss 0.18236613719020145  - accuracy: 0.71875\n",
      "At: 49 [==========>] Loss 0.08558415590991718  - accuracy: 0.875\n",
      "At: 50 [==========>] Loss 0.21487088883411504  - accuracy: 0.75\n",
      "At: 51 [==========>] Loss 0.25310589500026953  - accuracy: 0.65625\n",
      "At: 52 [==========>] Loss 0.2583819533418745  - accuracy: 0.71875\n",
      "At: 53 [==========>] Loss 0.19078203757990286  - accuracy: 0.75\n",
      "At: 54 [==========>] Loss 0.15661734997401439  - accuracy: 0.78125\n",
      "At: 55 [==========>] Loss 0.2493476565202926  - accuracy: 0.65625\n",
      "At: 56 [==========>] Loss 0.19699204762761607  - accuracy: 0.71875\n",
      "At: 57 [==========>] Loss 0.18179074967148567  - accuracy: 0.78125\n",
      "At: 58 [==========>] Loss 0.24086009306660977  - accuracy: 0.6875\n",
      "At: 59 [==========>] Loss 0.21322565422149276  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.23356407641368332  - accuracy: 0.6875\n",
      "At: 61 [==========>] Loss 0.2742251258434579  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.17817816738631753  - accuracy: 0.8125\n",
      "At: 63 [==========>] Loss 0.2193796010288061  - accuracy: 0.71875\n",
      "At: 64 [==========>] Loss 0.18713615780974396  - accuracy: 0.65625\n",
      "At: 65 [==========>] Loss 0.28500648067533185  - accuracy: 0.5625\n",
      "At: 66 [==========>] Loss 0.23654734264412325  - accuracy: 0.6875\n",
      "At: 67 [==========>] Loss 0.1768899878629257  - accuracy: 0.75\n",
      "At: 68 [==========>] Loss 0.15540259036135573  - accuracy: 0.8125\n",
      "At: 69 [==========>] Loss 0.148207294709905  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.1939308227793273  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.1841577541054011  - accuracy: 0.75\n",
      "At: 72 [==========>] Loss 0.13656141026169394  - accuracy: 0.78125\n",
      "At: 73 [==========>] Loss 0.1466916740959776  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.16754385787016826  - accuracy: 0.78125\n",
      "At: 75 [==========>] Loss 0.23525467875794684  - accuracy: 0.6875\n",
      "At: 76 [==========>] Loss 0.26092401509819774  - accuracy: 0.59375\n",
      "At: 77 [==========>] Loss 0.23535634748495537  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.15460907544404068  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.18802943899643235  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.212041504894577  - accuracy: 0.78125\n",
      "At: 81 [==========>] Loss 0.2021519331837784  - accuracy: 0.71875\n",
      "At: 82 [==========>] Loss 0.2437409573920543  - accuracy: 0.65625\n",
      "At: 83 [==========>] Loss 0.18197175034723306  - accuracy: 0.6875\n",
      "At: 84 [==========>] Loss 0.17875150297460532  - accuracy: 0.78125\n",
      "At: 85 [==========>] Loss 0.24121673727150894  - accuracy: 0.65625\n",
      "At: 86 [==========>] Loss 0.15950001727098517  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.17912181078102057  - accuracy: 0.78125\n",
      "At: 88 [==========>] Loss 0.35629268465248864  - accuracy: 0.4375\n",
      "At: 89 [==========>] Loss 0.24389621548231577  - accuracy: 0.71875\n",
      "At: 90 [==========>] Loss 0.24778889438187462  - accuracy: 0.65625\n",
      "At: 91 [==========>] Loss 0.21124552219506207  - accuracy: 0.71875\n",
      "At: 92 [==========>] Loss 0.11490417513488707  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.11233936866297362  - accuracy: 0.875\n",
      "At: 94 [==========>] Loss 0.17885124563230273  - accuracy: 0.78125\n",
      "At: 95 [==========>] Loss 0.19280821595731695  - accuracy: 0.75\n",
      "At: 96 [==========>] Loss 0.12277216645346223  - accuracy: 0.78125\n",
      "At: 97 [==========>] Loss 0.11462852989375547  - accuracy: 0.8125\n",
      "At: 98 [==========>] Loss 0.30245896434406216  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.13174443497958427  - accuracy: 0.875\n",
      "At: 100 [==========>] Loss 0.16889807664537354  - accuracy: 0.8125\n",
      "At: 101 [==========>] Loss 0.15551063209695026  - accuracy: 0.78125\n",
      "At: 102 [==========>] Loss 0.1484583467077562  - accuracy: 0.84375\n",
      "At: 103 [==========>] Loss 0.14064836018140342  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.15005237185930786  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.12389749466744743  - accuracy: 0.8125\n",
      "At: 106 [==========>] Loss 0.23214698460602332  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.2121837472477216  - accuracy: 0.71875\n",
      "At: 108 [==========>] Loss 0.2569565238942011  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.14466210600811266  - accuracy: 0.78125\n",
      "At: 110 [==========>] Loss 0.2702606745504273  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.11187493021377955  - accuracy: 0.84375\n",
      "At: 112 [==========>] Loss 0.17257083833558293  - accuracy: 0.75\n",
      "At: 113 [==========>] Loss 0.2304346435170514  - accuracy: 0.65625\n",
      "At: 114 [==========>] Loss 0.15908759735729974  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.19493018878622764  - accuracy: 0.71875\n",
      "At: 116 [==========>] Loss 0.21788553467413696  - accuracy: 0.71875\n",
      "At: 117 [==========>] Loss 0.1100609495105439  - accuracy: 0.84375\n",
      "At: 118 [==========>] Loss 0.26247318536283376  - accuracy: 0.59375\n",
      "At: 119 [==========>] Loss 0.12395309366578766  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.17665470575223358  - accuracy: 0.75\n",
      "At: 121 [==========>] Loss 0.1393875987759698  - accuracy: 0.8125\n",
      "At: 122 [==========>] Loss 0.20356542152085444  - accuracy: 0.71875\n",
      "At: 123 [==========>] Loss 0.18919630909370888  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.2808475265875488  - accuracy: 0.53125\n",
      "At: 125 [==========>] Loss 0.16590418702496001  - accuracy: 0.8125\n",
      "At: 126 [==========>] Loss 0.22901685642077418  - accuracy: 0.6875\n",
      "At: 127 [==========>] Loss 0.2214209385034838  - accuracy: 0.6875\n",
      "At: 128 [==========>] Loss 0.2526629443464529  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.16236325422697456  - accuracy: 0.78125\n",
      "At: 130 [==========>] Loss 0.2231186894079299  - accuracy: 0.78125\n",
      "At: 131 [==========>] Loss 0.16105356575347762  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.24796332710890287  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.24389568255411875  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.22426928470226393  - accuracy: 0.65625\n",
      "At: 135 [==========>] Loss 0.1915342652685332  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.19309018032083167  - accuracy: 0.71875\n",
      "At: 137 [==========>] Loss 0.06298139410290698  - accuracy: 0.9375\n",
      "At: 138 [==========>] Loss 0.14024527569168857  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.16748890837880764  - accuracy: 0.75\n",
      "At: 140 [==========>] Loss 0.15228790872099324  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.3340252587254251  - accuracy: 0.5\n",
      "At: 142 [==========>] Loss 0.17304463135313256  - accuracy: 0.6875\n",
      "At: 143 [==========>] Loss 0.1988214651700114  - accuracy: 0.71875\n",
      "At: 144 [==========>] Loss 0.15667452835429216  - accuracy: 0.75\n",
      "At: 145 [==========>] Loss 0.11734881019274945  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.13984068969452906  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.27544049678526095  - accuracy: 0.59375\n",
      "At: 148 [==========>] Loss 0.17722552154799953  - accuracy: 0.75\n",
      "At: 149 [==========>] Loss 0.1476224168264656  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.17287148542720793  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.18818463860188178  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.17494355956328395  - accuracy: 0.75\n",
      "At: 153 [==========>] Loss 0.19621201648830772  - accuracy: 0.6875\n",
      "At: 154 [==========>] Loss 0.1603426524931951  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.23839969165366456  - accuracy: 0.71875\n",
      "At: 156 [==========>] Loss 0.12744693763961992  - accuracy: 0.8125\n",
      "At: 157 [==========>] Loss 0.2314992653048416  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.14220254836740576  - accuracy: 0.71875\n",
      "At: 159 [==========>] Loss 0.1847372987700089  - accuracy: 0.75\n",
      "At: 160 [==========>] Loss 0.14731137395986219  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.10477621935799025  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.17638663170389046  - accuracy: 0.78125\n",
      "At: 163 [==========>] Loss 0.1566654024327731  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.16753970812246788  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.20697010568045396  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.18956594354650352  - accuracy: 0.8125\n",
      "At: 167 [==========>] Loss 0.14582351789064907  - accuracy: 0.78125\n",
      "At: 168 [==========>] Loss 0.2232230205848736  - accuracy: 0.6875\n",
      "At: 169 [==========>] Loss 0.15643799289053317  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.15980158739201067  - accuracy: 0.75\n",
      "At: 171 [==========>] Loss 0.21640049453113192  - accuracy: 0.65625\n",
      "At: 172 [==========>] Loss 0.20803127960107487  - accuracy: 0.71875\n",
      "At: 173 [==========>] Loss 0.2504501693597815  - accuracy: 0.6875\n",
      "At: 174 [==========>] Loss 0.20985418091136376  - accuracy: 0.71875\n",
      "At: 175 [==========>] Loss 0.1754198782793722  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.2032114498006906  - accuracy: 0.71875\n",
      "At: 177 [==========>] Loss 0.07720497748942023  - accuracy: 0.90625\n",
      "At: 178 [==========>] Loss 0.2126972881365354  - accuracy: 0.71875\n",
      "At: 179 [==========>] Loss 0.15724777125217188  - accuracy: 0.84375\n",
      "At: 180 [==========>] Loss 0.1493542692029658  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.05195185444293611  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.18492380936276148  - accuracy: 0.75\n",
      "At: 183 [==========>] Loss 0.15360768631395857  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.16214620675466362  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.12961352258547942  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.14886489139230463  - accuracy: 0.71875\n",
      "At: 187 [==========>] Loss 0.181522268629884  - accuracy: 0.75\n",
      "At: 188 [==========>] Loss 0.18303710262597356  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.20922215145250378  - accuracy: 0.75\n",
      "At: 190 [==========>] Loss 0.11449265514839133  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.3271216075590342  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.1780678332322792  - accuracy: 0.71875\n",
      "At: 193 [==========>] Loss 0.21403738531599875  - accuracy: 0.71875\n",
      "At: 194 [==========>] Loss 0.15413054245806823  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.1894567034274594  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.16130680362030408  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.16804922286534357  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.13834965819478923  - accuracy: 0.8125\n",
      "At: 199 [==========>] Loss 0.10012954064654009  - accuracy: 0.84375\n",
      "At: 200 [==========>] Loss 0.1701924298796582  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.1465861778257903  - accuracy: 0.8125\n",
      "At: 202 [==========>] Loss 0.12391451248419853  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.1859660710851907  - accuracy: 0.71875\n",
      "At: 204 [==========>] Loss 0.1849660395205459  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.16412470722080397  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.0670191651425225  - accuracy: 0.9375\n",
      "At: 207 [==========>] Loss 0.13226023092350364  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.2507266656106128  - accuracy: 0.625\n",
      "At: 209 [==========>] Loss 0.187539863900451  - accuracy: 0.78125\n",
      "At: 210 [==========>] Loss 0.14747101684560665  - accuracy: 0.78125\n",
      "At: 211 [==========>] Loss 0.21536159547124312  - accuracy: 0.65625\n",
      "At: 212 [==========>] Loss 0.21171284324797646  - accuracy: 0.71875\n",
      "At: 213 [==========>] Loss 0.15753050815430417  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.2284485302106255  - accuracy: 0.6875\n",
      "At: 215 [==========>] Loss 0.08469400706781417  - accuracy: 0.90625\n",
      "At: 216 [==========>] Loss 0.21632282474227155  - accuracy: 0.6875\n",
      "At: 217 [==========>] Loss 0.15914908913099696  - accuracy: 0.75\n",
      "At: 218 [==========>] Loss 0.14828227058413984  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.24003194057180016  - accuracy: 0.65625\n",
      "At: 220 [==========>] Loss 0.19249475365740137  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.1404120356984056  - accuracy: 0.8125\n",
      "At: 222 [==========>] Loss 0.0692468513206977  - accuracy: 0.9375\n",
      "At: 223 [==========>] Loss 0.27020777546470043  - accuracy: 0.59375\n",
      "At: 224 [==========>] Loss 0.2053836372578917  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.12121218210435539  - accuracy: 0.78125\n",
      "At: 226 [==========>] Loss 0.1524193852293133  - accuracy: 0.75\n",
      "At: 227 [==========>] Loss 0.17911946533923412  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.20102472490867845  - accuracy: 0.6875\n",
      "At: 229 [==========>] Loss 0.20054756309280072  - accuracy: 0.75\n",
      "At: 230 [==========>] Loss 0.2145278493806657  - accuracy: 0.6875\n",
      "At: 231 [==========>] Loss 0.2296872777109711  - accuracy: 0.65625\n",
      "At: 232 [==========>] Loss 0.23963435823625034  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.23325864823742973  - accuracy: 0.6875\n",
      "At: 234 [==========>] Loss 0.16099593028161394  - accuracy: 0.78125\n",
      "At: 235 [==========>] Loss 0.20355887529243535  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.17139494423746943  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.1332071277356831  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.1475446982846266  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.16278313634755398  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.23504531203216134  - accuracy: 0.65625\n",
      "At: 241 [==========>] Loss 0.135854797452984  - accuracy: 0.8125\n",
      "At: 242 [==========>] Loss 0.15800710941238105  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.12582401233818413  - accuracy: 0.84375\n",
      "At: 244 [==========>] Loss 0.16633628337757264  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.14041067749970476  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.1557000873858321  - accuracy: 0.78125\n",
      "At: 247 [==========>] Loss 0.17454430967732737  - accuracy: 0.75\n",
      "At: 248 [==========>] Loss 0.13200655942095801  - accuracy: 0.8125\n",
      "At: 249 [==========>] Loss 0.09512177250283976  - accuracy: 0.84375\n",
      "At: 250 [==========>] Loss 0.22722722653119357  - accuracy: 0.65625\n",
      "At: 251 [==========>] Loss 0.21926519935255012  - accuracy: 0.6875\n",
      "At: 252 [==========>] Loss 0.13598984940034192  - accuracy: 0.84375\n",
      "At: 253 [==========>] Loss 0.21347252602698702  - accuracy: 0.75\n",
      "At: 254 [==========>] Loss 0.07738047419156546  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.15412056267508004  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.18619093990478214  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.1294565223432152  - accuracy: 0.78125\n",
      "At: 258 [==========>] Loss 0.14672503261969563  - accuracy: 0.8125\n",
      "At: 259 [==========>] Loss 0.16491650923659895  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.1365946498189144  - accuracy: 0.8125\n",
      "At: 261 [==========>] Loss 0.09781592866036062  - accuracy: 0.84375\n",
      "At: 262 [==========>] Loss 0.15603104047176192  - accuracy: 0.75\n",
      "At: 263 [==========>] Loss 0.12610137873056995  - accuracy: 0.84375\n",
      "At: 264 [==========>] Loss 0.14781955736310382  - accuracy: 0.78125\n",
      "At: 265 [==========>] Loss 0.20695986398973437  - accuracy: 0.71875\n",
      "At: 266 [==========>] Loss 0.22634973021873356  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.16066244684482212  - accuracy: 0.75\n",
      "At: 268 [==========>] Loss 0.2507582492946583  - accuracy: 0.625\n",
      "At: 269 [==========>] Loss 0.16891068375537507  - accuracy: 0.78125\n",
      "At: 270 [==========>] Loss 0.27243802680001916  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.17484440032208093  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.10197590430049022  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.20848453668320713  - accuracy: 0.6875\n",
      "At: 274 [==========>] Loss 0.18061047494803267  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.07574281288552735  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.16473658198533633  - accuracy: 0.78125\n",
      "At: 277 [==========>] Loss 0.12159964587769054  - accuracy: 0.84375\n",
      "At: 278 [==========>] Loss 0.13010019077856472  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.1547723300547056  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.16500408716983497  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.14650537616781306  - accuracy: 0.84375\n",
      "At: 282 [==========>] Loss 0.20449366944775957  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.12929432934285648  - accuracy: 0.84375\n",
      "At: 284 [==========>] Loss 0.08674235823574179  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.12155704301787633  - accuracy: 0.75\n",
      "At: 286 [==========>] Loss 0.09577588654777032  - accuracy: 0.84375\n",
      "At: 287 [==========>] Loss 0.17158939729570816  - accuracy: 0.71875\n",
      "At: 288 [==========>] Loss 0.15263788830724068  - accuracy: 0.8125\n",
      "At: 289 [==========>] Loss 0.1083727903101327  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.11106412750684914  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.11044456575611337  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.14104849884979043  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.22248219360798546  - accuracy: 0.625\n",
      "At: 294 [==========>] Loss 0.2202997444331528  - accuracy: 0.71875\n",
      "At: 295 [==========>] Loss 0.14018763383653254  - accuracy: 0.84375\n",
      "At: 296 [==========>] Loss 0.10461849775076121  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.1667868189865896  - accuracy: 0.75\n",
      "At: 298 [==========>] Loss 0.16401592291057654  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.20825324809038928  - accuracy: 0.6875\n",
      "At: 300 [==========>] Loss 0.18103570278830655  - accuracy: 0.75\n",
      "At: 301 [==========>] Loss 0.17142592707573445  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.152069088621444  - accuracy: 0.78125\n",
      "At: 303 [==========>] Loss 0.10612313354659098  - accuracy: 0.84375\n",
      "At: 304 [==========>] Loss 0.2106388391665065  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.25043778943565814  - accuracy: 0.625\n",
      "At: 306 [==========>] Loss 0.12322837783159749  - accuracy: 0.875\n",
      "At: 307 [==========>] Loss 0.2451831116407021  - accuracy: 0.6875\n",
      "At: 308 [==========>] Loss 0.16096145099866985  - accuracy: 0.78125\n",
      "At: 309 [==========>] Loss 0.13166866290228477  - accuracy: 0.84375\n",
      "At: 310 [==========>] Loss 0.2067090940935458  - accuracy: 0.75\n",
      "At: 311 [==========>] Loss 0.07552704138746635  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.14222114016149887  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.13703713964950132  - accuracy: 0.8125\n",
      "At: 314 [==========>] Loss 0.22083487160627224  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.14068672826883144  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.18437386985209298  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.29522895419489736  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.15357069208316707  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.09249479012692874  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.2316824218140977  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.2237964885025132  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.12806878861861665  - accuracy: 0.8125\n",
      "At: 323 [==========>] Loss 0.1303170252736558  - accuracy: 0.84375\n",
      "At: 324 [==========>] Loss 0.15972730830759035  - accuracy: 0.75\n",
      "At: 325 [==========>] Loss 0.10836460083053381  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.19940982513351416  - accuracy: 0.6875\n",
      "At: 327 [==========>] Loss 0.15484017328378252  - accuracy: 0.78125\n",
      "At: 328 [==========>] Loss 0.1590995749175119  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.11336674328447018  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.1790909006036431  - accuracy: 0.75\n",
      "At: 331 [==========>] Loss 0.23226141125877056  - accuracy: 0.71875\n",
      "At: 332 [==========>] Loss 0.23762642476122092  - accuracy: 0.65625\n",
      "At: 333 [==========>] Loss 0.12198531953791616  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.10784652277990245  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.12789558148392036  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.15288055583735402  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.17314308386159688  - accuracy: 0.71875\n",
      "At: 338 [==========>] Loss 0.17062748479359452  - accuracy: 0.78125\n",
      "At: 339 [==========>] Loss 0.16893123731798784  - accuracy: 0.75\n",
      "At: 340 [==========>] Loss 0.152894482689579  - accuracy: 0.8125\n",
      "At: 341 [==========>] Loss 0.12023963029593508  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.16381668121298112  - accuracy: 0.75\n",
      "At: 343 [==========>] Loss 0.27640962185705936  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.2167837343443898  - accuracy: 0.75\n",
      "At: 345 [==========>] Loss 0.19064121589516897  - accuracy: 0.75\n",
      "At: 346 [==========>] Loss 0.14550638140719457  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.09138181421384975  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.12553358642263504  - accuracy: 0.78125\n",
      "At: 349 [==========>] Loss 0.19132859289342935  - accuracy: 0.78125\n",
      "At: 350 [==========>] Loss 0.13958710699892152  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.23021671532580057  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.11906170018672496  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.16509892617137792  - accuracy: 0.78125\n",
      "At: 354 [==========>] Loss 0.2034962523992933  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.11721359936194739  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.1708000614756612  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.10860437411507474  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.1409077318222552  - accuracy: 0.8125\n",
      "At: 359 [==========>] Loss 0.06731842460246118  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.16733142969317186  - accuracy: 0.75\n",
      "At: 361 [==========>] Loss 0.07016453918564339  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.14830849940571145  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.08400950856707101  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.19349283946319384  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.1051511410034582  - accuracy: 0.875\n",
      "At: 366 [==========>] Loss 0.2123108599166366  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.1921632431669183  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.21262165456976684  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.1300513339418848  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.2281502262578029  - accuracy: 0.71875\n",
      "At: 371 [==========>] Loss 0.09614874311991262  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.11331359810323169  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.21326094700738452  - accuracy: 0.625\n",
      "At: 374 [==========>] Loss 0.0846909477470382  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1650955687038913  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.10049668247760782  - accuracy: 0.875\n",
      "At: 377 [==========>] Loss 0.2131546324471364  - accuracy: 0.75\n",
      "At: 378 [==========>] Loss 0.203820772693731  - accuracy: 0.78125\n",
      "At: 379 [==========>] Loss 0.17592192207479654  - accuracy: 0.71875\n",
      "At: 380 [==========>] Loss 0.1624758328364675  - accuracy: 0.78125\n",
      "At: 381 [==========>] Loss 0.18399113629904895  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.13257658695028152  - accuracy: 0.75\n",
      "At: 383 [==========>] Loss 0.13396942979392734  - accuracy: 0.78125\n",
      "At: 384 [==========>] Loss 0.19966871901175573  - accuracy: 0.75\n",
      "At: 385 [==========>] Loss 0.1482843430112948  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.23494308420678944  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.11522057485548212  - accuracy: 0.875\n",
      "At: 388 [==========>] Loss 0.20870330075997956  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.197249439667131  - accuracy: 0.6875\n",
      "At: 390 [==========>] Loss 0.12221618863941658  - accuracy: 0.8125\n",
      "At: 391 [==========>] Loss 0.1617551719235356  - accuracy: 0.78125\n",
      "At: 392 [==========>] Loss 0.1478256934448513  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.19996471701678048  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.10630577083510966  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.16277992754086124  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.16758323487828486  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.1483212521327985  - accuracy: 0.78125\n",
      "At: 398 [==========>] Loss 0.2090068728266595  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.2424396530681786  - accuracy: 0.71875\n",
      "At: 400 [==========>] Loss 0.2188876188576394  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.14668451335103275  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.11960375658174036  - accuracy: 0.84375\n",
      "At: 403 [==========>] Loss 0.05172392552581137  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.10681877829204295  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.23842564768725896  - accuracy: 0.6875\n",
      "At: 406 [==========>] Loss 0.1872180899038431  - accuracy: 0.71875\n",
      "At: 407 [==========>] Loss 0.1857557726353996  - accuracy: 0.71875\n",
      "At: 408 [==========>] Loss 0.18070898379025574  - accuracy: 0.78125\n",
      "At: 409 [==========>] Loss 0.23083965388938454  - accuracy: 0.625\n",
      "At: 410 [==========>] Loss 0.1363685538944101  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10588211955611015  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.21343440051484022  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.128803201616001  - accuracy: 0.8125\n",
      "At: 414 [==========>] Loss 0.1993778058360322  - accuracy: 0.6875\n",
      "At: 415 [==========>] Loss 0.13349621556595692  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.20785540951479634  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.14839903419974448  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.13336175618912005  - accuracy: 0.8125\n",
      "At: 419 [==========>] Loss 0.11922285402959117  - accuracy: 0.84375\n",
      "At: 420 [==========>] Loss 0.159415370480409  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.1555007547352797  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.10040013379009652  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.1355758114361621  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.21079141588184214  - accuracy: 0.6875\n",
      "At: 425 [==========>] Loss 0.194699418992455  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.13171276534670168  - accuracy: 0.78125\n",
      "At: 427 [==========>] Loss 0.18251486720033164  - accuracy: 0.78125\n",
      "At: 428 [==========>] Loss 0.2404766447088622  - accuracy: 0.6875\n",
      "At: 429 [==========>] Loss 0.16284024992354765  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.1284884213798954  - accuracy: 0.8125\n",
      "At: 431 [==========>] Loss 0.11743763636555948  - accuracy: 0.8125\n",
      "At: 432 [==========>] Loss 0.17071159092002897  - accuracy: 0.71875\n",
      "At: 433 [==========>] Loss 0.0938374387601085  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.1322939756411349  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.17959751897019277  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.12400615100881027  - accuracy: 0.84375\n",
      "At: 437 [==========>] Loss 0.1450667299040736  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.18471244268951537  - accuracy: 0.75\n",
      "At: 439 [==========>] Loss 0.09899223724414008  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.07554611168516835  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.20917494998507657  - accuracy: 0.71875\n",
      "At: 442 [==========>] Loss 0.1832866110195547  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.11310028153011534  - accuracy: 0.875\n",
      "At: 444 [==========>] Loss 0.182773660290313  - accuracy: 0.65625\n",
      "At: 445 [==========>] Loss 0.16787855355096243  - accuracy: 0.75\n",
      "At: 446 [==========>] Loss 0.27320038934916974  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.13604169669866184  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.18949817923185833  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.13491515943603605  - accuracy: 0.78125\n",
      "At: 450 [==========>] Loss 0.17816891039067442  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.18001491988082535  - accuracy: 0.71875\n",
      "At: 452 [==========>] Loss 0.1436218429894427  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.16754805962739477  - accuracy: 0.78125\n",
      "At: 454 [==========>] Loss 0.23311067936632535  - accuracy: 0.625\n",
      "At: 455 [==========>] Loss 0.20977626734970248  - accuracy: 0.71875\n",
      "At: 456 [==========>] Loss 0.17304036895098707  - accuracy: 0.75\n",
      "At: 457 [==========>] Loss 0.13169228774642178  - accuracy: 0.75\n",
      "At: 458 [==========>] Loss 0.11273216187815033  - accuracy: 0.875\n",
      "At: 459 [==========>] Loss 0.2648620068337755  - accuracy: 0.625\n",
      "At: 460 [==========>] Loss 0.11696387540939406  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.2134650903799664  - accuracy: 0.6875\n",
      "At: 462 [==========>] Loss 0.16094434538082145  - accuracy: 0.8125\n",
      "At: 463 [==========>] Loss 0.15838367302530154  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.22771558647311158  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.22755367205584476  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.11035600083835882  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.2128056734309994  - accuracy: 0.65625\n",
      "At: 468 [==========>] Loss 0.0985714098084399  - accuracy: 0.84375\n",
      "At: 469 [==========>] Loss 0.16170768906655203  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.10296261394752826  - accuracy: 0.9375\n",
      "At: 471 [==========>] Loss 0.14730484153096896  - accuracy: 0.8125\n",
      "At: 472 [==========>] Loss 0.09538114183890956  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.17276171538284127  - accuracy: 0.75\n",
      "At: 474 [==========>] Loss 0.16899645454849116  - accuracy: 0.75\n",
      "At: 475 [==========>] Loss 0.14457412145776724  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.18193447772170018  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.11127929953151201  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.11164424432675943  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.17531577604721377  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.1987251308626415  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.13046804994337147  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.08845211998352057  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.12363767323083125  - accuracy: 0.84375\n",
      "At: 484 [==========>] Loss 0.1129550706714213  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.1256201254244997  - accuracy: 0.84375\n",
      "At: 486 [==========>] Loss 0.1904240644277097  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.1684733897103926  - accuracy: 0.75\n",
      "At: 488 [==========>] Loss 0.165402958316494  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.16520901951509384  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.18345326502010917  - accuracy: 0.75\n",
      "At: 491 [==========>] Loss 0.17611783415153862  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.19332079673240782  - accuracy: 0.65625\n",
      "At: 493 [==========>] Loss 0.16900767143116577  - accuracy: 0.75\n",
      "At: 494 [==========>] Loss 0.19320530940723468  - accuracy: 0.6875\n",
      "At: 495 [==========>] Loss 0.12296961633587972  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.1665592554650455  - accuracy: 0.8125\n",
      "At: 497 [==========>] Loss 0.19655145142159253  - accuracy: 0.65625\n",
      "At: 498 [==========>] Loss 0.07481776313463319  - accuracy: 0.96875\n",
      "At: 499 [==========>] Loss 0.19324899818270402  - accuracy: 0.75\n",
      "At: 500 [==========>] Loss 0.15460062523404727  - accuracy: 0.75\n",
      "At: 501 [==========>] Loss 0.15674578686892937  - accuracy: 0.75\n",
      "At: 502 [==========>] Loss 0.13969151374537764  - accuracy: 0.8125\n",
      "At: 503 [==========>] Loss 0.10346206334528513  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.12179664845808093  - accuracy: 0.78125\n",
      "At: 505 [==========>] Loss 0.20299424072501138  - accuracy: 0.6875\n",
      "At: 506 [==========>] Loss 0.31236106853462425  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.11212078316862109  - accuracy: 0.875\n",
      "At: 508 [==========>] Loss 0.10855514665621922  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.16086262945036983  - accuracy: 0.78125\n",
      "At: 510 [==========>] Loss 0.19183327168094896  - accuracy: 0.75\n",
      "At: 511 [==========>] Loss 0.1562614606496217  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.18625061796160566  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.24890917032106907  - accuracy: 0.6875\n",
      "At: 514 [==========>] Loss 0.19181374839130966  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.1253728587350778  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.1689782090902569  - accuracy: 0.71875\n",
      "At: 517 [==========>] Loss 0.1372013338427289  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.1638697927542755  - accuracy: 0.8125\n",
      "At: 519 [==========>] Loss 0.13383245807582397  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.12830186205074312  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.16177593942224675  - accuracy: 0.75\n",
      "At: 522 [==========>] Loss 0.15207496370426646  - accuracy: 0.78125\n",
      "At: 523 [==========>] Loss 0.17433122281824015  - accuracy: 0.75\n",
      "At: 524 [==========>] Loss 0.1135481318039174  - accuracy: 0.8125\n",
      "At: 525 [==========>] Loss 0.17170977722184907  - accuracy: 0.75\n",
      "At: 526 [==========>] Loss 0.17101924416323427  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.2782087690182644  - accuracy: 0.53125\n",
      "At: 528 [==========>] Loss 0.18477715212351192  - accuracy: 0.71875\n",
      "At: 529 [==========>] Loss 0.13175268262498777  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.23065221246112924  - accuracy: 0.6875\n",
      "At: 531 [==========>] Loss 0.12832048510282792  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.11351128932988024  - accuracy: 0.90625\n",
      "At: 533 [==========>] Loss 0.11254028371431815  - accuracy: 0.75\n",
      "At: 534 [==========>] Loss 0.16450087969923824  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.13913568035085178  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.20905751762156105  - accuracy: 0.65625\n",
      "At: 537 [==========>] Loss 0.11764851474328489  - accuracy: 0.8125\n",
      "At: 538 [==========>] Loss 0.12316929073757865  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.11761802957276575  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.21704262674605507  - accuracy: 0.78125\n",
      "At: 541 [==========>] Loss 0.19108428272038158  - accuracy: 0.6875\n",
      "At: 542 [==========>] Loss 0.12264523209857889  - accuracy: 0.84375\n",
      "At: 543 [==========>] Loss 0.14698954866861408  - accuracy: 0.75\n",
      "At: 544 [==========>] Loss 0.21696762162458672  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.09263779893129188  - accuracy: 0.90625\n",
      "At: 546 [==========>] Loss 0.1731824377722959  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.1349038267878238  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.11960538621116948  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.12325362771303432  - accuracy: 0.84375\n",
      "At: 550 [==========>] Loss 0.11111358733362686  - accuracy: 0.90625\n",
      "At: 551 [==========>] Loss 0.14200898650470478  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.16606881738744858  - accuracy: 0.71875\n",
      "At: 553 [==========>] Loss 0.1565407382195436  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.12506591428111552  - accuracy: 0.8125\n",
      "At: 555 [==========>] Loss 0.1629535269634378  - accuracy: 0.8125\n",
      "At: 556 [==========>] Loss 0.14154034004333352  - accuracy: 0.75\n",
      "At: 557 [==========>] Loss 0.14995429978196995  - accuracy: 0.8125\n",
      "At: 558 [==========>] Loss 0.1358570073197573  - accuracy: 0.8125\n",
      "At: 559 [==========>] Loss 0.16979085518563308  - accuracy: 0.78125\n",
      "At: 560 [==========>] Loss 0.14836497484906824  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.14137834224290113  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.10542154109358619  - accuracy: 0.875\n",
      "At: 563 [==========>] Loss 0.09825690849234417  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.20364968013228404  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.13473046452429166  - accuracy: 0.75\n",
      "At: 566 [==========>] Loss 0.18748867404846153  - accuracy: 0.65625\n",
      "At: 567 [==========>] Loss 0.19441633321411722  - accuracy: 0.65625\n",
      "At: 568 [==========>] Loss 0.2034664583278515  - accuracy: 0.6875\n",
      "At: 569 [==========>] Loss 0.1875402978077685  - accuracy: 0.75\n",
      "At: 570 [==========>] Loss 0.12035458490050327  - accuracy: 0.84375\n",
      "At: 571 [==========>] Loss 0.14286022026301784  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.11510468414947464  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.09414200893939073  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.10798925691582108  - accuracy: 0.84375\n",
      "At: 575 [==========>] Loss 0.143539554874968  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.09902485906091507  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.1944459407521667  - accuracy: 0.6875\n",
      "At: 578 [==========>] Loss 0.21510124289585492  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.11883374878059592  - accuracy: 0.84375\n",
      "At: 580 [==========>] Loss 0.16415313913760232  - accuracy: 0.71875\n",
      "At: 581 [==========>] Loss 0.1648461000679891  - accuracy: 0.75\n",
      "At: 582 [==========>] Loss 0.17791523486938265  - accuracy: 0.6875\n",
      "At: 583 [==========>] Loss 0.18178451215420566  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.12468550132962168  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.17711223585433195  - accuracy: 0.71875\n",
      "At: 586 [==========>] Loss 0.10026350667257711  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.15895249531232625  - accuracy: 0.78125\n",
      "At: 588 [==========>] Loss 0.14348398478446722  - accuracy: 0.8125\n",
      "At: 589 [==========>] Loss 0.161417744397195  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.07527019925445354  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.14024705503133883  - accuracy: 0.8125\n",
      "At: 592 [==========>] Loss 0.12883329153024095  - accuracy: 0.8125\n",
      "At: 593 [==========>] Loss 0.19303357283368622  - accuracy: 0.6875\n",
      "At: 594 [==========>] Loss 0.15763589682060955  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.1522908694665851  - accuracy: 0.84375\n",
      "At: 596 [==========>] Loss 0.12326845282668046  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.20171053903965336  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.138213352988784  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.1295703251944711  - accuracy: 0.8125\n",
      "At: 600 [==========>] Loss 0.0880484824801645  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.15877823457982845  - accuracy: 0.78125\n",
      "At: 602 [==========>] Loss 0.1240540807058365  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.17488624298986138  - accuracy: 0.6875\n",
      "At: 604 [==========>] Loss 0.2251596490835624  - accuracy: 0.625\n",
      "At: 605 [==========>] Loss 0.12215029712346553  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.19476892921325173  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.14931467442516616  - accuracy: 0.8125\n",
      "At: 608 [==========>] Loss 0.1401850118568188  - accuracy: 0.75\n",
      "At: 609 [==========>] Loss 0.14233250009912884  - accuracy: 0.75\n",
      "At: 610 [==========>] Loss 0.14364100025431686  - accuracy: 0.84375\n",
      "At: 611 [==========>] Loss 0.10426420291917146  - accuracy: 0.875\n",
      "At: 612 [==========>] Loss 0.1363454602690428  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.12642479179957433  - accuracy: 0.8125\n",
      "At: 614 [==========>] Loss 0.1421175679819665  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.18212837708716284  - accuracy: 0.78125\n",
      "At: 616 [==========>] Loss 0.15892797206670847  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.11469834773148897  - accuracy: 0.84375\n",
      "At: 618 [==========>] Loss 0.23052760255953217  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.12141529080382743  - accuracy: 0.84375\n",
      "At: 620 [==========>] Loss 0.1749748729180096  - accuracy: 0.71875\n",
      "At: 621 [==========>] Loss 0.07213236185181646  - accuracy: 0.875\n",
      "At: 622 [==========>] Loss 0.14994537752737422  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.11275141523118357  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.10947911882558561  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.17982124049742695  - accuracy: 0.71875\n",
      "At: 626 [==========>] Loss 0.15822022716609732  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.09236816514764731  - accuracy: 0.90625\n",
      "At: 628 [==========>] Loss 0.10600426563522569  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.16845911544577608  - accuracy: 0.78125\n",
      "At: 630 [==========>] Loss 0.2611479877304608  - accuracy: 0.53125\n",
      "At: 631 [==========>] Loss 0.19144967531907964  - accuracy: 0.6875\n",
      "At: 632 [==========>] Loss 0.1681508750720343  - accuracy: 0.6875\n",
      "At: 633 [==========>] Loss 0.16501075231244686  - accuracy: 0.71875\n",
      "At: 634 [==========>] Loss 0.15032986366909612  - accuracy: 0.8125\n",
      "At: 635 [==========>] Loss 0.13483448413281932  - accuracy: 0.875\n",
      "At: 636 [==========>] Loss 0.14853493690098646  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.13309233059784212  - accuracy: 0.84375\n",
      "At: 638 [==========>] Loss 0.1260722536580995  - accuracy: 0.8125\n",
      "At: 639 [==========>] Loss 0.11623177487619236  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.2234965716269834  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.16502088952156047  - accuracy: 0.71875\n",
      "At: 642 [==========>] Loss 0.14016149019268614  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.15628021137342246  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.08491177045059062  - accuracy: 0.875\n",
      "At: 645 [==========>] Loss 0.1249531641478344  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.09468067744321478  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.18678226674116172  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.15710591311790395  - accuracy: 0.6875\n",
      "At: 649 [==========>] Loss 0.17772654431409549  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.09657830583812588  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.16447314290435205  - accuracy: 0.78125\n",
      "At: 652 [==========>] Loss 0.07747207053085757  - accuracy: 0.875\n",
      "At: 653 [==========>] Loss 0.1280129018060307  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.11474288031578683  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.13528933237433227  - accuracy: 0.78125\n",
      "At: 656 [==========>] Loss 0.11283878077203066  - accuracy: 0.8125\n",
      "At: 657 [==========>] Loss 0.16404909173588392  - accuracy: 0.6875\n",
      "At: 658 [==========>] Loss 0.1363347102985627  - accuracy: 0.8125\n",
      "At: 659 [==========>] Loss 0.16615969907723016  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.1351784199643547  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.16855056551360814  - accuracy: 0.75\n",
      "At: 662 [==========>] Loss 0.12442804588104475  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.0892821879894294  - accuracy: 0.90625\n",
      "At: 664 [==========>] Loss 0.16057222989581924  - accuracy: 0.8125\n",
      "At: 665 [==========>] Loss 0.18080234604321976  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.18685377557116736  - accuracy: 0.75\n",
      "At: 667 [==========>] Loss 0.13545659523729095  - accuracy: 0.75\n",
      "At: 668 [==========>] Loss 0.17258973578063475  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.15139318167129867  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.2201456757433814  - accuracy: 0.65625\n",
      "At: 671 [==========>] Loss 0.10376652709013667  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.11160267508718072  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.05935542326422813  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.1274255505013452  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.10105256809388459  - accuracy: 0.875\n",
      "At: 676 [==========>] Loss 0.1500565884955804  - accuracy: 0.78125\n",
      "At: 677 [==========>] Loss 0.15572572391808714  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.13577386451419576  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.09411452620179431  - accuracy: 0.90625\n",
      "At: 680 [==========>] Loss 0.1249777086270341  - accuracy: 0.78125\n",
      "At: 681 [==========>] Loss 0.12056981742833175  - accuracy: 0.8125\n",
      "At: 682 [==========>] Loss 0.1157426350216636  - accuracy: 0.84375\n",
      "At: 683 [==========>] Loss 0.13109529708759388  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.1044622315092874  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.14249231002397506  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.11264140482104137  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.07354726272442749  - accuracy: 0.875\n",
      "At: 688 [==========>] Loss 0.08626171735896258  - accuracy: 0.90625\n",
      "At: 689 [==========>] Loss 0.1404585569596761  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.13006471660297836  - accuracy: 0.78125\n",
      "At: 691 [==========>] Loss 0.08805202240140743  - accuracy: 0.90625\n",
      "At: 692 [==========>] Loss 0.10790363234237395  - accuracy: 0.875\n",
      "At: 693 [==========>] Loss 0.14571264786513555  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.16493090390314094  - accuracy: 0.75\n",
      "At: 695 [==========>] Loss 0.16460143599614735  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.17671333296803224  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.1856563857933876  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.10081562933315104  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.15602158823520507  - accuracy: 0.75\n",
      "At: 700 [==========>] Loss 0.11445650036692814  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.13941723796718444  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.08745700649275956  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.1920239155228442  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.17173971650317688  - accuracy: 0.75\n",
      "At: 705 [==========>] Loss 0.21641791546447478  - accuracy: 0.71875\n",
      "At: 706 [==========>] Loss 0.13721164829957885  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.11580568876127703  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.17832612031512873  - accuracy: 0.71875\n",
      "At: 709 [==========>] Loss 0.17260301488776658  - accuracy: 0.78125\n",
      "At: 710 [==========>] Loss 0.15541142612176742  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.21482692592410235  - accuracy: 0.6875\n",
      "At: 712 [==========>] Loss 0.14842760203868247  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.18107909156538862  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.20816848043910055  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.11059448459920145  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.1289880443401027  - accuracy: 0.8125\n",
      "At: 717 [==========>] Loss 0.09051544897155922  - accuracy: 0.875\n",
      "At: 718 [==========>] Loss 0.2067209810214665  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.13021515148506266  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.13492772919732218  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.13342369047143227  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.1625153503763106  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.12550599484383965  - accuracy: 0.875\n",
      "At: 724 [==========>] Loss 0.08694407259140066  - accuracy: 0.9375\n",
      "At: 725 [==========>] Loss 0.16451443110515385  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.179460436482599  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.16135683716188873  - accuracy: 0.71875\n",
      "At: 728 [==========>] Loss 0.16380536636787113  - accuracy: 0.78125\n",
      "At: 729 [==========>] Loss 0.21244065272469176  - accuracy: 0.65625\n",
      "At: 730 [==========>] Loss 0.22026807389611197  - accuracy: 0.6875\n",
      "At: 731 [==========>] Loss 0.1343500828700403  - accuracy: 0.8125\n",
      "At: 732 [==========>] Loss 0.11765907720368865  - accuracy: 0.875\n",
      "At: 733 [==========>] Loss 0.10838765530929349  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.18476161183833653  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.14420925483290312  - accuracy: 0.84375\n",
      "At: 736 [==========>] Loss 0.12531490050064528  - accuracy: 0.8125\n",
      "At: 737 [==========>] Loss 0.1601922540618888  - accuracy: 0.75\n",
      "At: 738 [==========>] Loss 0.10713106644273261  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.08753461486538283  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.13075308822062714  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.13405611094496905  - accuracy: 0.75\n",
      "At: 742 [==========>] Loss 0.16673322516421743  - accuracy: 0.8125\n",
      "At: 743 [==========>] Loss 0.1446658662300538  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.17095805524038668  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.17967976799190305  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.16488567309928676  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.11101885792954189  - accuracy: 0.90625\n",
      "At: 748 [==========>] Loss 0.14216791388121167  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.17245893805233645  - accuracy: 0.6875\n",
      "At: 750 [==========>] Loss 0.1098152318959911  - accuracy: 0.875\n",
      "At: 751 [==========>] Loss 0.19376263335076688  - accuracy: 0.71875\n",
      "At: 752 [==========>] Loss 0.10004801296964153  - accuracy: 0.84375\n",
      "At: 753 [==========>] Loss 0.16371143655100506  - accuracy: 0.75\n",
      "At: 754 [==========>] Loss 0.13398490460063422  - accuracy: 0.875\n",
      "At: 755 [==========>] Loss 0.09749343029331603  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.20799388926152884  - accuracy: 0.6875\n",
      "At: 757 [==========>] Loss 0.08674755879826294  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.11618420215903591  - accuracy: 0.84375\n",
      "At: 759 [==========>] Loss 0.09626460757923777  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.15614039389763196  - accuracy: 0.78125\n",
      "At: 761 [==========>] Loss 0.11409378146877817  - accuracy: 0.84375\n",
      "At: 762 [==========>] Loss 0.13323729494239755  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.16591023884504297  - accuracy: 0.75\n",
      "At: 764 [==========>] Loss 0.14010009380476668  - accuracy: 0.78125\n",
      "At: 765 [==========>] Loss 0.19427485166150726  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.1431946573325319  - accuracy: 0.75\n",
      "At: 767 [==========>] Loss 0.11918080226018503  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.12884517417722258  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.1445156490103135  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.0995846691763888  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.20037651135464815  - accuracy: 0.71875\n",
      "At: 772 [==========>] Loss 0.12127973471751814  - accuracy: 0.8125\n",
      "At: 773 [==========>] Loss 0.09976005977643669  - accuracy: 0.84375\n",
      "At: 774 [==========>] Loss 0.11806097831637058  - accuracy: 0.8125\n",
      "At: 775 [==========>] Loss 0.19395909344336607  - accuracy: 0.65625\n",
      "At: 776 [==========>] Loss 0.17701821666322376  - accuracy: 0.75\n",
      "At: 777 [==========>] Loss 0.10789874186777187  - accuracy: 0.875\n",
      "At: 778 [==========>] Loss 0.17550048197862858  - accuracy: 0.78125\n",
      "At: 779 [==========>] Loss 0.10834519289080904  - accuracy: 0.875\n",
      "At: 780 [==========>] Loss 0.06441104138518719  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.13230843604597614  - accuracy: 0.8125\n",
      "At: 782 [==========>] Loss 0.14053352297760718  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.18654772199501668  - accuracy: 0.6875\n",
      "At: 784 [==========>] Loss 0.15384410746924854  - accuracy: 0.6875\n",
      "At: 785 [==========>] Loss 0.20792919876391824  - accuracy: 0.6875\n",
      "At: 786 [==========>] Loss 0.14897483406600226  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.16622832006702942  - accuracy: 0.78125\n",
      "At: 788 [==========>] Loss 0.08575740025499493  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.1404753485921492  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.13836102094940614  - accuracy: 0.78125\n",
      "At: 791 [==========>] Loss 0.17034098115868368  - accuracy: 0.78125\n",
      "At: 792 [==========>] Loss 0.1764862642435628  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.11738308834869816  - accuracy: 0.90625\n",
      "At: 794 [==========>] Loss 0.15322278846526374  - accuracy: 0.78125\n",
      "At: 795 [==========>] Loss 0.10994324097239591  - accuracy: 0.84375\n",
      "At: 796 [==========>] Loss 0.14307891136714593  - accuracy: 0.8125\n",
      "At: 797 [==========>] Loss 0.14011105449667807  - accuracy: 0.8125\n",
      "At: 798 [==========>] Loss 0.1720095167532094  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.077899739921126  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.13035046632864666  - accuracy: 0.78125\n",
      "At: 801 [==========>] Loss 0.1190785704106111  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.20696395308777607  - accuracy: 0.59375\n",
      "At: 803 [==========>] Loss 0.15605354046359185  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.1405780992883472  - accuracy: 0.84375\n",
      "At: 805 [==========>] Loss 0.19050342340142631  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.11475686287959352  - accuracy: 0.75\n",
      "At: 807 [==========>] Loss 0.10584801427073375  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.13792802767624052  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.1087049487572761  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.2068557223594905  - accuracy: 0.71875\n",
      "At: 811 [==========>] Loss 0.141817645521657  - accuracy: 0.8125\n",
      "At: 812 [==========>] Loss 0.13228866203429185  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.23415932546674206  - accuracy: 0.6875\n",
      "At: 814 [==========>] Loss 0.19295305227089665  - accuracy: 0.75\n",
      "At: 815 [==========>] Loss 0.12241391023249362  - accuracy: 0.84375\n",
      "At: 816 [==========>] Loss 0.14345249824482467  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.0880449526542508  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.1380390281855925  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.11715158776526641  - accuracy: 0.875\n",
      "At: 820 [==========>] Loss 0.1275969205922436  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.17208653478372976  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.15682556449952845  - accuracy: 0.75\n",
      "At: 823 [==========>] Loss 0.15046125661600673  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.16741956409367068  - accuracy: 0.78125\n",
      "At: 825 [==========>] Loss 0.20621543664188297  - accuracy: 0.6875\n",
      "At: 826 [==========>] Loss 0.11139808029158582  - accuracy: 0.90625\n",
      "At: 827 [==========>] Loss 0.08196603370333343  - accuracy: 0.9375\n",
      "At: 828 [==========>] Loss 0.07296024384328632  - accuracy: 0.875\n",
      "At: 829 [==========>] Loss 0.09293070145346183  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.0969277751835286  - accuracy: 0.8125\n",
      "At: 831 [==========>] Loss 0.08801033773068524  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.11424315550362987  - accuracy: 0.8125\n",
      "At: 833 [==========>] Loss 0.197215962711264  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.13289802078070553  - accuracy: 0.75\n",
      "At: 835 [==========>] Loss 0.08243329804576054  - accuracy: 0.84375\n",
      "At: 836 [==========>] Loss 0.14121958306251928  - accuracy: 0.8125\n",
      "At: 837 [==========>] Loss 0.11373667955901987  - accuracy: 0.84375\n",
      "At: 838 [==========>] Loss 0.10807092161051826  - accuracy: 0.8125\n",
      "At: 839 [==========>] Loss 0.11653869215649756  - accuracy: 0.78125\n",
      "At: 840 [==========>] Loss 0.13316041308189203  - accuracy: 0.8125\n",
      "At: 841 [==========>] Loss 0.07269541347140827  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.05590644535204248  - accuracy: 0.9375\n",
      "At: 843 [==========>] Loss 0.1834200680770841  - accuracy: 0.65625\n",
      "At: 844 [==========>] Loss 0.13356486596270745  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.15716822430358657  - accuracy: 0.78125\n",
      "At: 846 [==========>] Loss 0.14037145071438947  - accuracy: 0.78125\n",
      "At: 847 [==========>] Loss 0.08075651436360501  - accuracy: 0.90625\n",
      "At: 848 [==========>] Loss 0.12571937694483037  - accuracy: 0.8125\n",
      "At: 849 [==========>] Loss 0.1675948366789194  - accuracy: 0.71875\n",
      "At: 850 [==========>] Loss 0.08008968308471878  - accuracy: 0.9375\n",
      "At: 851 [==========>] Loss 0.12514642266578796  - accuracy: 0.78125\n",
      "At: 852 [==========>] Loss 0.1373533058783198  - accuracy: 0.71875\n",
      "At: 853 [==========>] Loss 0.18661156068374102  - accuracy: 0.75\n",
      "At: 854 [==========>] Loss 0.22576019755606164  - accuracy: 0.65625\n",
      "At: 855 [==========>] Loss 0.07542807720760103  - accuracy: 0.875\n",
      "At: 856 [==========>] Loss 0.11074393255612211  - accuracy: 0.78125\n",
      "At: 857 [==========>] Loss 0.10103761084688312  - accuracy: 0.875\n",
      "At: 858 [==========>] Loss 0.2397820949186083  - accuracy: 0.71875\n",
      "At: 859 [==========>] Loss 0.12536088470354043  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.1273986316309317  - accuracy: 0.78125\n",
      "At: 861 [==========>] Loss 0.10573794118122402  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.12872049268236396  - accuracy: 0.84375\n",
      "At: 863 [==========>] Loss 0.1598514076672112  - accuracy: 0.6875\n",
      "At: 864 [==========>] Loss 0.17573239650048697  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.22490205602638608  - accuracy: 0.625\n",
      "At: 866 [==========>] Loss 0.13201398954007507  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.0995713964186559  - accuracy: 0.90625\n",
      "At: 868 [==========>] Loss 0.19367492540390832  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.19541742411588503  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.12747394829713712  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.07398275159172182  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.10906616086925987  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.21279711594266754  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.15212042620250715  - accuracy: 0.6875\n",
      "At: 875 [==========>] Loss 0.12666008929763364  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.11236117816757805  - accuracy: 0.875\n",
      "At: 877 [==========>] Loss 0.14838526838515728  - accuracy: 0.8125\n",
      "At: 878 [==========>] Loss 0.05756534673473458  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.15889335235260338  - accuracy: 0.6875\n",
      "At: 880 [==========>] Loss 0.12347911596458458  - accuracy: 0.875\n",
      "At: 881 [==========>] Loss 0.14999896252274136  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.11327660182815377  - accuracy: 0.875\n",
      "At: 883 [==========>] Loss 0.15955907685133466  - accuracy: 0.75\n",
      "At: 884 [==========>] Loss 0.13973950135690857  - accuracy: 0.75\n",
      "At: 885 [==========>] Loss 0.13778355616217197  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.11979043356051285  - accuracy: 0.75\n",
      "At: 887 [==========>] Loss 0.1409367198305257  - accuracy: 0.78125\n",
      "At: 888 [==========>] Loss 0.15542764647766374  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.10693913846703727  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.13321197672039764  - accuracy: 0.75\n",
      "At: 891 [==========>] Loss 0.11851167422389447  - accuracy: 0.84375\n",
      "At: 892 [==========>] Loss 0.12927340678255994  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.15937244702474024  - accuracy: 0.75\n",
      "At: 894 [==========>] Loss 0.14749198379182418  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.10994685847940822  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.1281506946741272  - accuracy: 0.875\n",
      "At: 897 [==========>] Loss 0.13999507788806614  - accuracy: 0.8125\n",
      "At: 898 [==========>] Loss 0.1630806699173651  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.0870992483543174  - accuracy: 0.90625\n",
      "At: 900 [==========>] Loss 0.15507092246726373  - accuracy: 0.78125\n",
      "At: 901 [==========>] Loss 0.16910268275364426  - accuracy: 0.75\n",
      "At: 902 [==========>] Loss 0.12060923091833856  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.1289591026992461  - accuracy: 0.84375\n",
      "At: 904 [==========>] Loss 0.0995372429927152  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.10568768877259854  - accuracy: 0.8125\n",
      "At: 906 [==========>] Loss 0.10641710544384064  - accuracy: 0.8125\n",
      "At: 907 [==========>] Loss 0.12837600750390038  - accuracy: 0.75\n",
      "At: 908 [==========>] Loss 0.13243477372318166  - accuracy: 0.78125\n",
      "At: 909 [==========>] Loss 0.10239342331868868  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.1282718833914998  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.15034688260777979  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.1357369721841187  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.12345015776024296  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.12264934807699945  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.19139101413304024  - accuracy: 0.75\n",
      "At: 916 [==========>] Loss 0.17053151199557437  - accuracy: 0.78125\n",
      "At: 917 [==========>] Loss 0.17220255414784985  - accuracy: 0.71875\n",
      "At: 918 [==========>] Loss 0.19551474178894138  - accuracy: 0.71875\n",
      "At: 919 [==========>] Loss 0.09444347511048685  - accuracy: 0.84375\n",
      "At: 920 [==========>] Loss 0.12109320025373314  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.14318028678393716  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.14093677556888967  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.11362478686646466  - accuracy: 0.90625\n",
      "At: 924 [==========>] Loss 0.18197936137391374  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.16527167970687234  - accuracy: 0.75\n",
      "At: 926 [==========>] Loss 0.1295000334411565  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.11553960709221175  - accuracy: 0.78125\n",
      "At: 928 [==========>] Loss 0.10400716086383083  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.14382444433470298  - accuracy: 0.78125\n",
      "At: 930 [==========>] Loss 0.12255770637862831  - accuracy: 0.8125\n",
      "At: 931 [==========>] Loss 0.1553737592266353  - accuracy: 0.75\n",
      "At: 932 [==========>] Loss 0.09106034813428741  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.07629036661679027  - accuracy: 0.90625\n",
      "At: 934 [==========>] Loss 0.14758665266469867  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.056123251619652134  - accuracy: 0.96875\n",
      "At: 936 [==========>] Loss 0.15552700090582477  - accuracy: 0.8125\n",
      "At: 937 [==========>] Loss 0.19022033292719545  - accuracy: 0.75\n",
      "At: 938 [==========>] Loss 0.12719470094790403  - accuracy: 0.8125\n",
      "At: 939 [==========>] Loss 0.1152764239673145  - accuracy: 0.8125\n",
      "At: 940 [==========>] Loss 0.2095598304493947  - accuracy: 0.6875\n",
      "At: 941 [==========>] Loss 0.11261372798434631  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.16185201823791145  - accuracy: 0.78125\n",
      "At: 943 [==========>] Loss 0.100712565594374  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.0860064988608423  - accuracy: 0.875\n",
      "At: 945 [==========>] Loss 0.10545888617142739  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.11957904933694821  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.14865386079158216  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.17920598395297638  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.07622107611720116  - accuracy: 0.9375\n",
      "At: 950 [==========>] Loss 0.10792556136245876  - accuracy: 0.875\n",
      "At: 951 [==========>] Loss 0.09609960582737156  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.09205117083241383  - accuracy: 0.90625\n",
      "At: 953 [==========>] Loss 0.06962272560163044  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.11105975777794302  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.12991515981790466  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.0714816338362536  - accuracy: 0.9375\n",
      "At: 957 [==========>] Loss 0.12868780258245338  - accuracy: 0.8125\n",
      "At: 958 [==========>] Loss 0.0880841656742487  - accuracy: 0.875\n",
      "At: 959 [==========>] Loss 0.1236176728059356  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.12248765580566916  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.1274083910742445  - accuracy: 0.78125\n",
      "At: 962 [==========>] Loss 0.10622144486693322  - accuracy: 0.84375\n",
      "At: 963 [==========>] Loss 0.08885404427656593  - accuracy: 0.84375\n",
      "At: 964 [==========>] Loss 0.1768470452684065  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.14118125453417665  - accuracy: 0.8125\n",
      "At: 966 [==========>] Loss 0.17399063777881424  - accuracy: 0.71875\n",
      "At: 967 [==========>] Loss 0.13278563235966317  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.13577103648112276  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.1499011056780465  - accuracy: 0.75\n",
      "At: 970 [==========>] Loss 0.12805886771072483  - accuracy: 0.8125\n",
      "At: 971 [==========>] Loss 0.12807815181019117  - accuracy: 0.75\n",
      "At: 972 [==========>] Loss 0.0717513753102854  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.10299352597539668  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.10191286779254369  - accuracy: 0.84375\n",
      "At: 975 [==========>] Loss 0.1556405899401735  - accuracy: 0.71875\n",
      "At: 976 [==========>] Loss 0.13384924105145235  - accuracy: 0.75\n",
      "At: 977 [==========>] Loss 0.10908505142314084  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.15657064867143555  - accuracy: 0.71875\n",
      "At: 979 [==========>] Loss 0.1057534747147009  - accuracy: 0.8125\n",
      "At: 980 [==========>] Loss 0.17190833649288417  - accuracy: 0.71875\n",
      "At: 981 [==========>] Loss 0.18745190869672773  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.07681257997275773  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.1264263464537902  - accuracy: 0.84375\n",
      "At: 984 [==========>] Loss 0.11531266760422744  - accuracy: 0.8125\n",
      "At: 985 [==========>] Loss 0.15471024891033358  - accuracy: 0.78125\n",
      "At: 986 [==========>] Loss 0.14602119242907224  - accuracy: 0.75\n",
      "At: 987 [==========>] Loss 0.1341941507145859  - accuracy: 0.75\n",
      "At: 988 [==========>] Loss 0.10729471718023803  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.14746137620900082  - accuracy: 0.8125\n",
      "At: 990 [==========>] Loss 0.15109115558213596  - accuracy: 0.71875\n",
      "At: 991 [==========>] Loss 0.13499811320186234  - accuracy: 0.875\n",
      "At: 992 [==========>] Loss 0.2409040540003159  - accuracy: 0.625\n",
      "At: 993 [==========>] Loss 0.12571219221943672  - accuracy: 0.875\n",
      "At: 994 [==========>] Loss 0.12712355446512225  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.17134049212839042  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.07721374805490352  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.1355775120496578  - accuracy: 0.78125\n",
      "At: 998 [==========>] Loss 0.11543375001908258  - accuracy: 0.875\n",
      "At: 999 [==========>] Loss 0.15030630166136913  - accuracy: 0.75\n",
      "At: 1000 [==========>] Loss 0.2363134396896267  - accuracy: 0.625\n",
      "At: 1001 [==========>] Loss 0.1329605157924755  - accuracy: 0.8125\n",
      "At: 1002 [==========>] Loss 0.2158426070587902  - accuracy: 0.6875\n",
      "At: 1003 [==========>] Loss 0.1444170638013903  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.13034181350202462  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.06929902902008057  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.12606375266041314  - accuracy: 0.8125\n",
      "At: 1007 [==========>] Loss 0.11369960263430526  - accuracy: 0.875\n",
      "At: 1008 [==========>] Loss 0.1759642767453375  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.1544948158777385  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.1500499525163757  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.15025588451396577  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.10437996905049952  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.0809090822512776  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.12368078688008287  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.2127469795948532  - accuracy: 0.71875\n",
      "At: 1016 [==========>] Loss 0.1298972671885577  - accuracy: 0.75\n",
      "At: 1017 [==========>] Loss 0.14122327716448416  - accuracy: 0.8125\n",
      "At: 1018 [==========>] Loss 0.1479126541091837  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.21626352981262892  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.1528663930599225  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.13133045224373385  - accuracy: 0.78125\n",
      "At: 1022 [==========>] Loss 0.12273136311652283  - accuracy: 0.8125\n",
      "At: 1023 [==========>] Loss 0.1664997480963023  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.2100890593513643  - accuracy: 0.625\n",
      "At: 1025 [==========>] Loss 0.1819269493754858  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.13117850192546138  - accuracy: 0.75\n",
      "At: 1027 [==========>] Loss 0.12124115258377417  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.2285613132735092  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.1073168232783521  - accuracy: 0.84375\n",
      "At: 1030 [==========>] Loss 0.1125427315307381  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.18877530789448488  - accuracy: 0.71875\n",
      "At: 1032 [==========>] Loss 0.1393780748203407  - accuracy: 0.75\n",
      "At: 1033 [==========>] Loss 0.13829964127305372  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.08351006344693754  - accuracy: 0.96875\n",
      "At: 1035 [==========>] Loss 0.07668675424296811  - accuracy: 0.96875\n",
      "At: 1036 [==========>] Loss 0.17647281048774466  - accuracy: 0.71875\n",
      "At: 1037 [==========>] Loss 0.15404259365804546  - accuracy: 0.84375\n",
      "At: 1038 [==========>] Loss 0.10557380909374223  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.1083115168273851  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.11469431880211783  - accuracy: 0.78125\n",
      "At: 1041 [==========>] Loss 0.13929060198399668  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.11524905463569139  - accuracy: 0.78125\n",
      "At: 1043 [==========>] Loss 0.20387080596434198  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.15929920412784865  - accuracy: 0.78125\n",
      "At: 1045 [==========>] Loss 0.17375548064984192  - accuracy: 0.75\n",
      "At: 1046 [==========>] Loss 0.17927630861017962  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.1143059432764173  - accuracy: 0.875\n",
      "At: 1048 [==========>] Loss 0.17657426846087748  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.1607507007898316  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.15468822195987356  - accuracy: 0.75\n",
      "At: 1051 [==========>] Loss 0.08723540831697658  - accuracy: 0.90625\n",
      "At: 1052 [==========>] Loss 0.1374331359779266  - accuracy: 0.8125\n",
      "At: 1053 [==========>] Loss 0.10306656716728699  - accuracy: 0.84375\n",
      "At: 1054 [==========>] Loss 0.1221408762433937  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.18447533942643984  - accuracy: 0.75\n",
      "At: 1056 [==========>] Loss 0.1604749978388995  - accuracy: 0.8125\n",
      "At: 1057 [==========>] Loss 0.12054004322384262  - accuracy: 0.84375\n",
      "At: 1058 [==========>] Loss 0.062429095269556874  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.10219753880539872  - accuracy: 0.84375\n",
      "At: 1060 [==========>] Loss 0.11641529976249276  - accuracy: 0.8125\n",
      "At: 1061 [==========>] Loss 0.10097802829320854  - accuracy: 0.8125\n",
      "At: 1062 [==========>] Loss 0.15021192984435144  - accuracy: 0.75\n",
      "At: 1063 [==========>] Loss 0.14246392259428686  - accuracy: 0.78125\n",
      "At: 1064 [==========>] Loss 0.1346061273144186  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.10118041341512644  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.09273633672705447  - accuracy: 0.8125\n",
      "At: 1067 [==========>] Loss 0.10841207978710651  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.11898309395691155  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.12418060498275174  - accuracy: 0.78125\n",
      "At: 1070 [==========>] Loss 0.14891137697947446  - accuracy: 0.84375\n",
      "At: 1071 [==========>] Loss 0.09665846034443834  - accuracy: 0.875\n",
      "At: 1072 [==========>] Loss 0.13929629454658587  - accuracy: 0.8125\n",
      "At: 1073 [==========>] Loss 0.15456817061555797  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.18071488466007063  - accuracy: 0.75\n",
      "At: 1075 [==========>] Loss 0.09301355205734  - accuracy: 0.90625\n",
      "At: 1076 [==========>] Loss 0.1496999032322609  - accuracy: 0.84375\n",
      "At: 1077 [==========>] Loss 0.09503657594489776  - accuracy: 0.875\n",
      "At: 1078 [==========>] Loss 0.09991655092826623  - accuracy: 0.8125\n",
      "At: 1079 [==========>] Loss 0.15197840691566508  - accuracy: 0.8125\n",
      "At: 1080 [==========>] Loss 0.16375654665738842  - accuracy: 0.75\n",
      "At: 1081 [==========>] Loss 0.1423830303152117  - accuracy: 0.75\n",
      "At: 1082 [==========>] Loss 0.11730635327876965  - accuracy: 0.875\n",
      "At: 1083 [==========>] Loss 0.12012908708366214  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.08732929234589294  - accuracy: 0.96875\n",
      "At: 1085 [==========>] Loss 0.10380068252555634  - accuracy: 0.875\n",
      "At: 1086 [==========>] Loss 0.1337070302547041  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.1529480772669995  - accuracy: 0.78125\n",
      "At: 1088 [==========>] Loss 0.16297236383487662  - accuracy: 0.78125\n",
      "At: 1089 [==========>] Loss 0.09048492741490025  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.09217517034672784  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.19439287354780427  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.13075428843256487  - accuracy: 0.8125\n",
      "At: 1093 [==========>] Loss 0.1327308704934972  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.160466720816476  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.14126341687193955  - accuracy: 0.75\n",
      "At: 1096 [==========>] Loss 0.0906487288196876  - accuracy: 0.90625\n",
      "At: 1097 [==========>] Loss 0.07179416264048795  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.14695118522638734  - accuracy: 0.71875\n",
      "At: 1099 [==========>] Loss 0.13824308642697541  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.07406230977749874  - accuracy: 0.90625\n",
      "At: 1101 [==========>] Loss 0.1029200863609558  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.1218619036154005  - accuracy: 0.8125\n",
      "At: 1103 [==========>] Loss 0.09358988807708676  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.08526887691779388  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.10972408351231935  - accuracy: 0.84375\n",
      "At: 1106 [==========>] Loss 0.10778844950893793  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.17718653699605338  - accuracy: 0.71875\n",
      "At: 1108 [==========>] Loss 0.09689682548775724  - accuracy: 0.9375\n",
      "At: 1109 [==========>] Loss 0.06149195293211167  - accuracy: 0.96875\n",
      "At: 1110 [==========>] Loss 0.11262824725625645  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.18014607967539684  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.16206295291484146  - accuracy: 0.84375\n",
      "At: 1113 [==========>] Loss 0.17589680026482465  - accuracy: 0.6875\n",
      "At: 1114 [==========>] Loss 0.08890274388891674  - accuracy: 0.90625\n",
      "At: 1115 [==========>] Loss 0.13894669576573745  - accuracy: 0.8125\n",
      "At: 1116 [==========>] Loss 0.11399651298491592  - accuracy: 0.875\n",
      "At: 1117 [==========>] Loss 0.07977187835420059  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.1338302196823793  - accuracy: 0.78125\n",
      "At: 1119 [==========>] Loss 0.14703958244616233  - accuracy: 0.8125\n",
      "At: 1120 [==========>] Loss 0.0733152018746845  - accuracy: 0.90625\n",
      "At: 1121 [==========>] Loss 0.1170329713102801  - accuracy: 0.78125\n",
      "At: 1122 [==========>] Loss 0.07689452546638328  - accuracy: 0.875\n",
      "At: 1123 [==========>] Loss 0.13798045086176813  - accuracy: 0.84375\n",
      "At: 1124 [==========>] Loss 0.1274651341385067  - accuracy: 0.84375\n",
      "At: 1125 [==========>] Loss 0.16857852695231518  - accuracy: 0.8125\n",
      "At: 1126 [==========>] Loss 0.09039033358318768  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.153974214729695  - accuracy: 0.8125\n",
      "At: 1128 [==========>] Loss 0.07013458420905679  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.1571684076873899  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.1211645234730988  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.09743321728476126  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.1076872059448435  - accuracy: 0.875\n",
      "At: 1133 [==========>] Loss 0.13331220114097847  - accuracy: 0.71875\n",
      "At: 1134 [==========>] Loss 0.10677885681642997  - accuracy: 0.8125\n",
      "At: 1135 [==========>] Loss 0.11107880732595177  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.1594987809842363  - accuracy: 0.84375\n",
      "At: 1137 [==========>] Loss 0.09622167153659456  - accuracy: 0.78125\n",
      "At: 1138 [==========>] Loss 0.0801997073955717  - accuracy: 0.90625\n",
      "At: 1139 [==========>] Loss 0.07173681760253486  - accuracy: 0.96875\n",
      "At: 1140 [==========>] Loss 0.17759634473360042  - accuracy: 0.65625\n",
      "At: 1141 [==========>] Loss 0.15621793552731406  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.12903997954550994  - accuracy: 0.84375\n",
      "At: 1143 [==========>] Loss 0.0941844548848636  - accuracy: 0.90625\n",
      "At: 1144 [==========>] Loss 0.09780963551972802  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.144645751152033  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.12043260881380732  - accuracy: 0.875\n",
      "At: 1147 [==========>] Loss 0.20971743721731553  - accuracy: 0.75\n",
      "At: 1148 [==========>] Loss 0.0903661144343654  - accuracy: 0.9375\n",
      "At: 1149 [==========>] Loss 0.11978890659188382  - accuracy: 0.8125\n",
      "At: 1150 [==========>] Loss 0.12633389808465684  - accuracy: 0.75\n",
      "At: 1151 [==========>] Loss 0.20641182119872828  - accuracy: 0.625\n",
      "At: 1152 [==========>] Loss 0.12863632468739086  - accuracy: 0.75\n",
      "At: 1153 [==========>] Loss 0.18949147034295077  - accuracy: 0.6875\n",
      "At: 1154 [==========>] Loss 0.11238376939598714  - accuracy: 0.875\n",
      "At: 1155 [==========>] Loss 0.10897664349493898  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.14109099030399994  - accuracy: 0.8125\n",
      "At: 1157 [==========>] Loss 0.11077264428476674  - accuracy: 0.875\n",
      "At: 1158 [==========>] Loss 0.15639091807124347  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.11585955811272236  - accuracy: 0.875\n",
      "At: 1160 [==========>] Loss 0.11516449298820466  - accuracy: 0.84375\n",
      "At: 1161 [==========>] Loss 0.08784777582978986  - accuracy: 0.90625\n",
      "At: 1162 [==========>] Loss 0.14225486059074427  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.16167775614325774  - accuracy: 0.65625\n",
      "At: 1164 [==========>] Loss 0.09733146426609798  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.15412704105006875  - accuracy: 0.75\n",
      "At: 1166 [==========>] Loss 0.08141646229072642  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.1272793187065257  - accuracy: 0.84375\n",
      "At: 1168 [==========>] Loss 0.12016195269738777  - accuracy: 0.875\n",
      "At: 1169 [==========>] Loss 0.1081934858250263  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.17385455648270254  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.08002863618512349  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.10805011155265593  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.10960690273905885  - accuracy: 0.875\n",
      "At: 1174 [==========>] Loss 0.19601064360038462  - accuracy: 0.75\n",
      "At: 1175 [==========>] Loss 0.10623347385722154  - accuracy: 0.875\n",
      "At: 1176 [==========>] Loss 0.12317768280421657  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.10694012907449713  - accuracy: 0.84375\n",
      "At: 1178 [==========>] Loss 0.15861838716727483  - accuracy: 0.8125\n",
      "At: 1179 [==========>] Loss 0.10952322431953657  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.16943489592352057  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.10400808004890225  - accuracy: 0.875\n",
      "At: 1182 [==========>] Loss 0.09090068190604958  - accuracy: 0.9375\n",
      "At: 1183 [==========>] Loss 0.15576964511919794  - accuracy: 0.78125\n",
      "At: 1184 [==========>] Loss 0.1430345844575894  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.1074681141136664  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.14113726665129364  - accuracy: 0.78125\n",
      "At: 1187 [==========>] Loss 0.13990550696251638  - accuracy: 0.75\n",
      "At: 1188 [==========>] Loss 0.0900090671124552  - accuracy: 0.90625\n",
      "At: 1189 [==========>] Loss 0.161069841587628  - accuracy: 0.78125\n",
      "At: 1190 [==========>] Loss 0.11855573698390669  - accuracy: 0.78125\n",
      "At: 1191 [==========>] Loss 0.17560513781434095  - accuracy: 0.78125\n",
      "At: 1192 [==========>] Loss 0.07058085259739849  - accuracy: 0.9375\n",
      "At: 1193 [==========>] Loss 0.12459502073979703  - accuracy: 0.84375\n",
      "At: 1194 [==========>] Loss 0.1461171097188455  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.12651216659233552  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.09486323030032638  - accuracy: 0.84375\n",
      "At: 1197 [==========>] Loss 0.09532635734044836  - accuracy: 0.84375\n",
      "At: 1198 [==========>] Loss 0.08864902919256179  - accuracy: 0.90625\n",
      "At: 1199 [==========>] Loss 0.19634080125912978  - accuracy: 0.65625\n",
      "At: 1200 [==========>] Loss 0.09424257234716424  - accuracy: 0.9375\n",
      "At: 1201 [==========>] Loss 0.10091892817221916  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.15775444935017946  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.17755454304312865  - accuracy: 0.75\n",
      "At: 1204 [==========>] Loss 0.07466163101900977  - accuracy: 0.9375\n",
      "At: 1205 [==========>] Loss 0.06859116229822765  - accuracy: 0.90625\n",
      "At: 1206 [==========>] Loss 0.12222093980280047  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.132681266569278  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.10005171568612067  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.094974074463337  - accuracy: 0.875\n",
      "At: 1210 [==========>] Loss 0.13716063160497066  - accuracy: 0.78125\n",
      "At: 1211 [==========>] Loss 0.17536918265026402  - accuracy: 0.71875\n",
      "At: 1212 [==========>] Loss 0.11578850882033029  - accuracy: 0.75\n",
      "At: 1213 [==========>] Loss 0.18771993086843594  - accuracy: 0.71875\n",
      "At: 1214 [==========>] Loss 0.14547290747270075  - accuracy: 0.875\n",
      "At: 1215 [==========>] Loss 0.15619336441579823  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.11093365493906794  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.06946002281499718  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.1238081182305214  - accuracy: 0.78125\n",
      "At: 1219 [==========>] Loss 0.12572347306906445  - accuracy: 0.8125\n",
      "At: 1220 [==========>] Loss 0.10574479576068588  - accuracy: 0.875\n",
      "At: 1221 [==========>] Loss 0.09688174681790676  - accuracy: 0.84375\n",
      "At: 1222 [==========>] Loss 0.18931917371297163  - accuracy: 0.65625\n",
      "At: 1223 [==========>] Loss 0.1080276079416938  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.07400738959873965  - accuracy: 0.96875\n",
      "At: 1225 [==========>] Loss 0.09389358503350528  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.10389545851255763  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.16182699794018168  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.13728212522001754  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.11151152792050167  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.1650917699663621  - accuracy: 0.84375\n",
      "At: 1231 [==========>] Loss 0.13985715541496835  - accuracy: 0.78125\n",
      "At: 1232 [==========>] Loss 0.07132699913022202  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.10031726095510175  - accuracy: 0.78125\n",
      "At: 1234 [==========>] Loss 0.1266744899032392  - accuracy: 0.8125\n",
      "At: 1235 [==========>] Loss 0.08459955922125466  - accuracy: 0.9375\n",
      "At: 1236 [==========>] Loss 0.14423260941314547  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.09769344090132905  - accuracy: 0.90625\n",
      "At: 1238 [==========>] Loss 0.09731446339966064  - accuracy: 0.90625\n",
      "At: 1239 [==========>] Loss 0.18264558385645763  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.09427888724906419  - accuracy: 0.9375\n",
      "At: 1241 [==========>] Loss 0.11354829272145614  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.14623990793873703  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.1470636656321274  - accuracy: 0.75\n",
      "At: 1244 [==========>] Loss 0.1753482085248202  - accuracy: 0.71875\n",
      "At: 1245 [==========>] Loss 0.12598961912508108  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.0704185494945351  - accuracy: 0.9375\n",
      "At: 1247 [==========>] Loss 0.16807635043769856  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.11224622856999558  - accuracy: 0.875\n",
      "At: 1249 [==========>] Loss 0.128225919834773  - accuracy: 0.84375\n",
      "At: 1250 [==========>] Loss 0.10599114360299182  - accuracy: 0.84375\n",
      "At: 1251 [==========>] Loss 0.1232941263124931  - accuracy: 0.78125\n",
      "At: 1252 [==========>] Loss 0.0931756106654085  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.11096514792475305  - accuracy: 0.84375\n",
      "At: 1254 [==========>] Loss 0.1817917688833008  - accuracy: 0.6875\n",
      "At: 1255 [==========>] Loss 0.1231374895878102  - accuracy: 0.8125\n",
      "At: 1256 [==========>] Loss 0.12277647404506101  - accuracy: 0.8125\n",
      "At: 1257 [==========>] Loss 0.13328218183743762  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.09418497391563622  - accuracy: 0.84375\n",
      "At: 1259 [==========>] Loss 0.13318252253896684  - accuracy: 0.78125\n",
      "At: 1260 [==========>] Loss 0.11110057122443975  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.12820004346773578  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.14680341520620238  - accuracy: 0.78125\n",
      "At: 1263 [==========>] Loss 0.1142334048015477  - accuracy: 0.875\n",
      "At: 1264 [==========>] Loss 0.07756933627432498  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.1241790257169201  - accuracy: 0.875\n",
      "At: 1266 [==========>] Loss 0.1324842463175167  - accuracy: 0.8125\n",
      "At: 1267 [==========>] Loss 0.11949238468937948  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.14333704471913777  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.13992645168763646  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.12046612292296832  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.16119555069022237  - accuracy: 0.8125\n",
      "At: 1272 [==========>] Loss 0.05348970676420204  - accuracy: 0.9375\n",
      "At: 1273 [==========>] Loss 0.21013401809581783  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.1169660765841253  - accuracy: 0.84375\n",
      "At: 1275 [==========>] Loss 0.09532460262204127  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.09570831557555347  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.09094892461592469  - accuracy: 0.84375\n",
      "At: 1278 [==========>] Loss 0.16287962848504917  - accuracy: 0.78125\n",
      "At: 1279 [==========>] Loss 0.09555518716273681  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.09679661955498697  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.15705532287342377  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.121308833949286  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.13664028910081646  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.15951375264488932  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.09031015142585869  - accuracy: 0.84375\n",
      "At: 1286 [==========>] Loss 0.12663422813976338  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.12005867591823999  - accuracy: 0.84375\n",
      "At: 1288 [==========>] Loss 0.17114183509622735  - accuracy: 0.75\n",
      "At: 1289 [==========>] Loss 0.0982681864079909  - accuracy: 0.875\n",
      "At: 1290 [==========>] Loss 0.12407564949379839  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.1537849220597854  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.10337941548724369  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.15890749227686676  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.1300790307098098  - accuracy: 0.875\n",
      "At: 1295 [==========>] Loss 0.16207950251971967  - accuracy: 0.75\n",
      "At: 1296 [==========>] Loss 0.1609218209970102  - accuracy: 0.71875\n",
      "At: 1297 [==========>] Loss 0.13038002352211203  - accuracy: 0.84375\n",
      "At: 1298 [==========>] Loss 0.11407388693891182  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.1471729258919494  - accuracy: 0.8125\n",
      "At: 1300 [==========>] Loss 0.1292220594541856  - accuracy: 0.78125\n",
      "At: 1301 [==========>] Loss 0.12747220617578614  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.06801738239807908  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.11174815100953844  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.11777579184638243  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.10685146183535657  - accuracy: 0.84375\n",
      "At: 1306 [==========>] Loss 0.07991538819072629  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.14537873434981144  - accuracy: 0.78125\n",
      "At: 1308 [==========>] Loss 0.06786592697034166  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.16428065392561575  - accuracy: 0.75\n",
      "At: 1310 [==========>] Loss 0.1670705970639716  - accuracy: 0.71875\n",
      "At: 1311 [==========>] Loss 0.12641314174826798  - accuracy: 0.84375\n",
      "At: 1312 [==========>] Loss 0.0634859148250616  - accuracy: 0.9375\n",
      "At: 1313 [==========>] Loss 0.16712956594210565  - accuracy: 0.75\n",
      "At: 1314 [==========>] Loss 0.06314817525899644  - accuracy: 0.96875\n",
      "At: 1315 [==========>] Loss 0.14856880669781075  - accuracy: 0.75\n",
      "At: 1316 [==========>] Loss 0.1382933682287037  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.11289281271334815  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.13182551025441377  - accuracy: 0.78125\n",
      "At: 1319 [==========>] Loss 0.1273407575246041  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.12504966818970967  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.08107994132100574  - accuracy: 0.9375\n",
      "At: 1322 [==========>] Loss 0.14180364779747767  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.10549394717800785  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.14763842495830395  - accuracy: 0.75\n",
      "At: 1325 [==========>] Loss 0.08687648489282104  - accuracy: 0.90625\n",
      "At: 1326 [==========>] Loss 0.08439628171185068  - accuracy: 0.9375\n",
      "At: 1327 [==========>] Loss 0.1369984855831452  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.07664319865217213  - accuracy: 0.90625\n",
      "At: 1329 [==========>] Loss 0.06975970611734487  - accuracy: 0.875\n",
      "At: 1330 [==========>] Loss 0.10247483428490178  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.14086707624575  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.11170141719140755  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.14728875009593057  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.10528491111225224  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.13344397609662495  - accuracy: 0.8125\n",
      "At: 1336 [==========>] Loss 0.1334948495312778  - accuracy: 0.8125\n",
      "At: 1337 [==========>] Loss 0.1625424309240959  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.14158667090551896  - accuracy: 0.8125\n",
      "At: 1339 [==========>] Loss 0.12498236009706334  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.15945462839038121  - accuracy: 0.8125\n",
      "At: 1341 [==========>] Loss 0.10687232849350256  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.1409148051398663  - accuracy: 0.8125\n",
      "At: 1343 [==========>] Loss 0.19359173199949586  - accuracy: 0.71875\n",
      "At: 1344 [==========>] Loss 0.16612713279903463  - accuracy: 0.78125\n",
      "At: 1345 [==========>] Loss 0.09800641591729091  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.08373281598275396  - accuracy: 0.90625\n",
      "At: 1347 [==========>] Loss 0.10207481939719097  - accuracy: 0.84375\n",
      "At: 1348 [==========>] Loss 0.10377699429294387  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.15382177237215466  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.12358188316615973  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.10122625502920696  - accuracy: 0.8125\n",
      "At: 1352 [==========>] Loss 0.09255356214177374  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.15963053777894792  - accuracy: 0.6875\n",
      "At: 1354 [==========>] Loss 0.15008076609691715  - accuracy: 0.84375\n",
      "At: 1355 [==========>] Loss 0.06872402815131629  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.11835718880849354  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.1331097506089613  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.15442580159138172  - accuracy: 0.78125\n",
      "At: 1359 [==========>] Loss 0.08278365248416604  - accuracy: 0.84375\n",
      "At: 1360 [==========>] Loss 0.1713551460793721  - accuracy: 0.75\n",
      "At: 1361 [==========>] Loss 0.07837318427707748  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.13615910302736967  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.09907144633891576  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.1525026639262766  - accuracy: 0.84375\n",
      "At: 1365 [==========>] Loss 0.1185111055024916  - accuracy: 0.8125\n",
      "At: 1366 [==========>] Loss 0.13217156613308895  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.11315381286354902  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.1669807259922254  - accuracy: 0.6875\n",
      "At: 1369 [==========>] Loss 0.0896011835054088  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.12464916428691408  - accuracy: 0.84375\n",
      "At: 1371 [==========>] Loss 0.19124243164768082  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.11070997481988912  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.16999222252665835  - accuracy: 0.78125\n",
      "At: 1374 [==========>] Loss 0.12649825836560416  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.12412952353479643  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.10182681587238264  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.15095574276748644  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.10447264715083401  - accuracy: 0.90625\n",
      "At: 1379 [==========>] Loss 0.16024858764995104  - accuracy: 0.71875\n",
      "At: 1380 [==========>] Loss 0.13559081624858646  - accuracy: 0.78125\n",
      "At: 1381 [==========>] Loss 0.08697489200801911  - accuracy: 0.9375\n",
      "At: 1382 [==========>] Loss 0.14131003506962736  - accuracy: 0.8125\n",
      "At: 1383 [==========>] Loss 0.09233166697881341  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.09783037353138074  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.16299352848687065  - accuracy: 0.78125\n",
      "At: 1386 [==========>] Loss 0.18191636011230172  - accuracy: 0.71875\n",
      "At: 1387 [==========>] Loss 0.06453157620389086  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.1624874321425569  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.12167463277019432  - accuracy: 0.78125\n",
      "At: 1390 [==========>] Loss 0.1468300674679062  - accuracy: 0.78125\n",
      "At: 1391 [==========>] Loss 0.09169775346191873  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.06821295795747646  - accuracy: 0.9375\n",
      "At: 1393 [==========>] Loss 0.1348991374997891  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.09300826513135052  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.19970299991153606  - accuracy: 0.71875\n",
      "At: 1396 [==========>] Loss 0.05536531161650115  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.1404257787417075  - accuracy: 0.78125\n",
      "At: 1398 [==========>] Loss 0.10898036907688455  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.10228787741529985  - accuracy: 0.9375\n",
      "At: 1400 [==========>] Loss 0.1459772467325739  - accuracy: 0.78125\n",
      "At: 1401 [==========>] Loss 0.10102096322420448  - accuracy: 0.90625\n",
      "At: 1402 [==========>] Loss 0.16338149921040673  - accuracy: 0.75\n",
      "At: 1403 [==========>] Loss 0.12429032936530984  - accuracy: 0.78125\n",
      "At: 1404 [==========>] Loss 0.11821274420271598  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.08322668556094856  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.142716963734696  - accuracy: 0.8125\n",
      "At: 1407 [==========>] Loss 0.1167228935058677  - accuracy: 0.84375\n",
      "At: 1408 [==========>] Loss 0.12444625212732298  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.039394589452798466  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.12130810186935392  - accuracy: 0.875\n",
      "At: 1411 [==========>] Loss 0.1493846583882304  - accuracy: 0.75\n",
      "At: 1412 [==========>] Loss 0.1399374183036868  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.09989190850513062  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.17521820833525087  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.076191072049895  - accuracy: 0.9375\n",
      "At: 1416 [==========>] Loss 0.14116522709412993  - accuracy: 0.8125\n",
      "At: 1417 [==========>] Loss 0.10941616019115204  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.11628256724506222  - accuracy: 0.84375\n",
      "At: 1419 [==========>] Loss 0.11357589907499283  - accuracy: 0.8125\n",
      "At: 1420 [==========>] Loss 0.07494659421449612  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.10700397608350129  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.15325181569616694  - accuracy: 0.75\n",
      "At: 1423 [==========>] Loss 0.15146109775415068  - accuracy: 0.84375\n",
      "At: 1424 [==========>] Loss 0.1221735572109706  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.10330513281405405  - accuracy: 0.84375\n",
      "At: 1426 [==========>] Loss 0.10845823313799534  - accuracy: 0.9375\n",
      "At: 1427 [==========>] Loss 0.10606248189738657  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.10147128057367302  - accuracy: 0.90625\n",
      "At: 1429 [==========>] Loss 0.15798724113207793  - accuracy: 0.78125\n",
      "At: 1430 [==========>] Loss 0.07458725523257778  - accuracy: 0.90625\n",
      "At: 1431 [==========>] Loss 0.1019563575485366  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.09088529957145691  - accuracy: 0.8125\n",
      "At: 1433 [==========>] Loss 0.11116471003409847  - accuracy: 0.8125\n",
      "At: 1434 [==========>] Loss 0.16874075508620195  - accuracy: 0.78125\n",
      "At: 1435 [==========>] Loss 0.11813172852015877  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.06330560695294256  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.13581202031905343  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.17552949762410724  - accuracy: 0.71875\n",
      "At: 1439 [==========>] Loss 0.10785830574545556  - accuracy: 0.90625\n",
      "At: 1440 [==========>] Loss 0.12060527081964602  - accuracy: 0.8125\n",
      "At: 1441 [==========>] Loss 0.10305879514522324  - accuracy: 0.84375\n",
      "At: 1442 [==========>] Loss 0.09480492007196623  - accuracy: 0.9375\n",
      "At: 1443 [==========>] Loss 0.14400118597105546  - accuracy: 0.75\n",
      "At: 1444 [==========>] Loss 0.14201799498148782  - accuracy: 0.78125\n",
      "At: 1445 [==========>] Loss 0.19363970328617178  - accuracy: 0.71875\n",
      "At: 1446 [==========>] Loss 0.19066290347912834  - accuracy: 0.65625\n",
      "At: 1447 [==========>] Loss 0.14813006548885965  - accuracy: 0.84375\n",
      "At: 1448 [==========>] Loss 0.07240944347595277  - accuracy: 0.90625\n",
      "At: 1449 [==========>] Loss 0.15468307468874387  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.1464978673377036  - accuracy: 0.84375\n",
      "At: 1451 [==========>] Loss 0.12210967555057325  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.09190212335699462  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.06982707721792852  - accuracy: 0.90625\n",
      "At: 1454 [==========>] Loss 0.18176948182786407  - accuracy: 0.71875\n",
      "At: 1455 [==========>] Loss 0.105766304867598  - accuracy: 0.875\n",
      "At: 1456 [==========>] Loss 0.10614549572588343  - accuracy: 0.78125\n",
      "At: 1457 [==========>] Loss 0.08059300773398008  - accuracy: 0.90625\n",
      "At: 1458 [==========>] Loss 0.1433795294557615  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.11247691284038494  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.17205485594035946  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.1135700525096764  - accuracy: 0.875\n",
      "At: 1462 [==========>] Loss 0.16370131483849476  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.09094703854400946  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.17157294201607637  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.11673692217754908  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.11234285886786141  - accuracy: 0.84375\n",
      "At: 1467 [==========>] Loss 0.1796835581997034  - accuracy: 0.75\n",
      "At: 1468 [==========>] Loss 0.1655105032953101  - accuracy: 0.78125\n",
      "At: 1469 [==========>] Loss 0.1633456206093601  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.1297246229815295  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.14259841115978233  - accuracy: 0.78125\n",
      "At: 1472 [==========>] Loss 0.08999407116330307  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.12552281635221993  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.1734650209467212  - accuracy: 0.71875\n",
      "At: 1475 [==========>] Loss 0.14589403817903668  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.13487929782009478  - accuracy: 0.75\n",
      "At: 1477 [==========>] Loss 0.07720454908919033  - accuracy: 0.9375\n",
      "At: 1478 [==========>] Loss 0.09717923601197549  - accuracy: 0.875\n",
      "At: 1479 [==========>] Loss 0.1367284525655544  - accuracy: 0.875\n",
      "At: 1480 [==========>] Loss 0.11307677227359553  - accuracy: 0.84375\n",
      "At: 1481 [==========>] Loss 0.10401405915160827  - accuracy: 0.84375\n",
      "At: 1482 [==========>] Loss 0.12108142642856203  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.19083023535254448  - accuracy: 0.78125\n",
      "At: 1484 [==========>] Loss 0.14898464236721656  - accuracy: 0.75\n",
      "At: 1485 [==========>] Loss 0.1648010906053324  - accuracy: 0.71875\n",
      "At: 1486 [==========>] Loss 0.08272839438242374  - accuracy: 0.90625\n",
      "At: 1487 [==========>] Loss 0.06668328760945658  - accuracy: 0.9375\n",
      "At: 1488 [==========>] Loss 0.14114985231895544  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.20845786826879992  - accuracy: 0.6875\n",
      "At: 1490 [==========>] Loss 0.1217852833933413  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.15402392462799933  - accuracy: 0.78125\n",
      "At: 1492 [==========>] Loss 0.13674214732961085  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.16627876061532196  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.14840902325703803  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.13770918220574765  - accuracy: 0.78125\n",
      "At: 1496 [==========>] Loss 0.0780368730862458  - accuracy: 0.90625\n",
      "At: 1497 [==========>] Loss 0.15985929391705825  - accuracy: 0.78125\n",
      "At: 1498 [==========>] Loss 0.13795159015086006  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.12590846118446175  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.09773888178981165  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.0885438111766157  - accuracy: 0.875\n",
      "At: 1502 [==========>] Loss 0.16299594299507814  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.12640440641972236  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.10980941378721482  - accuracy: 0.90625\n",
      "At: 1505 [==========>] Loss 0.13927274552038305  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.1651889346443417  - accuracy: 0.75\n",
      "At: 1507 [==========>] Loss 0.11509022146009303  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.1882895845446784  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.10727406134070946  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.13388510606490497  - accuracy: 0.78125\n",
      "At: 1511 [==========>] Loss 0.10698355789350279  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.09102151648927481  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.14397600913594022  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.11592689618164126  - accuracy: 0.875\n",
      "At: 1515 [==========>] Loss 0.13074706419381335  - accuracy: 0.8125\n",
      "At: 1516 [==========>] Loss 0.11777390917182337  - accuracy: 0.8125\n",
      "At: 1517 [==========>] Loss 0.158334944965706  - accuracy: 0.84375\n",
      "At: 1518 [==========>] Loss 0.11228775718870954  - accuracy: 0.84375\n",
      "At: 1519 [==========>] Loss 0.14588309736126592  - accuracy: 0.84375\n",
      "At: 1520 [==========>] Loss 0.10271132041656321  - accuracy: 0.90625\n",
      "At: 1521 [==========>] Loss 0.08498085768683006  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.1727221589240267  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.11270365679756766  - accuracy: 0.90625\n",
      "At: 1524 [==========>] Loss 0.13706475167588184  - accuracy: 0.78125\n",
      "At: 1525 [==========>] Loss 0.12867799491389234  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.11003073282374506  - accuracy: 0.8125\n",
      "At: 1527 [==========>] Loss 0.1510277522435138  - accuracy: 0.78125\n",
      "At: 1528 [==========>] Loss 0.16539713824076285  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.08714989311644708  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.05858358279722681  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.12047206728542767  - accuracy: 0.8125\n",
      "At: 1532 [==========>] Loss 0.187495753176855  - accuracy: 0.71875\n",
      "At: 1533 [==========>] Loss 0.17086206305008766  - accuracy: 0.75\n",
      "At: 1534 [==========>] Loss 0.1003599205142737  - accuracy: 0.875\n",
      "At: 1535 [==========>] Loss 0.1398667049078468  - accuracy: 0.78125\n",
      "At: 1536 [==========>] Loss 0.14021345215759945  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.10901233591529177  - accuracy: 0.875\n",
      "At: 1538 [==========>] Loss 0.14243779143018653  - accuracy: 0.8125\n",
      "At: 1539 [==========>] Loss 0.10199927824042662  - accuracy: 0.84375\n",
      "At: 1540 [==========>] Loss 0.14899985382819092  - accuracy: 0.71875\n",
      "At: 1541 [==========>] Loss 0.12470988893306773  - accuracy: 0.90625\n",
      "At: 1542 [==========>] Loss 0.08505381827172989  - accuracy: 0.90625\n",
      "At: 1543 [==========>] Loss 0.1415532639587695  - accuracy: 0.71875\n",
      "At: 1544 [==========>] Loss 0.1276655314407881  - accuracy: 0.84375\n",
      "At: 1545 [==========>] Loss 0.22456665810405224  - accuracy: 0.65625\n",
      "At: 1546 [==========>] Loss 0.14005352940576815  - accuracy: 0.75\n",
      "At: 1547 [==========>] Loss 0.13440124215736948  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.15505033432875998  - accuracy: 0.84375\n",
      "At: 1549 [==========>] Loss 0.1335602293242188  - accuracy: 0.84375\n",
      "At: 1550 [==========>] Loss 0.08354792110577744  - accuracy: 0.90625\n",
      "At: 1551 [==========>] Loss 0.1804372604265443  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.10035220008764476  - accuracy: 0.90625\n",
      "At: 1553 [==========>] Loss 0.0803717053640418  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.1357488701175042  - accuracy: 0.875\n",
      "At: 1555 [==========>] Loss 0.12861546917776623  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1567413474047596  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.08588970472067027  - accuracy: 0.9375\n",
      "At: 1558 [==========>] Loss 0.1402532664493819  - accuracy: 0.78125\n",
      "At: 1559 [==========>] Loss 0.09005009096997706  - accuracy: 0.875\n",
      "At: 1560 [==========>] Loss 0.12004147396913106  - accuracy: 0.78125\n",
      "At: 1561 [==========>] Loss 0.15182099414096512  - accuracy: 0.84375\n",
      "At: 1562 [==========>] Loss 0.09658293522656258  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.10257820281508373  - accuracy: 0.8125\n",
      "At: 1564 [==========>] Loss 0.0914581477399567  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.13396428433174923  - accuracy: 0.8125\n",
      "At: 1566 [==========>] Loss 0.15127892327263098  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.15362948877801222  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.08697016164749422  - accuracy: 0.84375\n",
      "At: 1569 [==========>] Loss 0.11069711632604048  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.119568135392646  - accuracy: 0.84375\n",
      "At: 1571 [==========>] Loss 0.1534263169537951  - accuracy: 0.78125\n",
      "At: 1572 [==========>] Loss 0.14201888847558575  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.05529186962730551  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.13779390032324312  - accuracy: 0.8125\n",
      "At: 1575 [==========>] Loss 0.07517245956672877  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.12161100114871652  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.09451243681346261  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.08544805435208547  - accuracy: 0.9375\n",
      "At: 1579 [==========>] Loss 0.0965322106442274  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.1175870010880244  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.08008195772752894  - accuracy: 0.875\n",
      "At: 1582 [==========>] Loss 0.19125136765951956  - accuracy: 0.71875\n",
      "At: 1583 [==========>] Loss 0.09621353549753124  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.13808255529869706  - accuracy: 0.78125\n",
      "At: 1585 [==========>] Loss 0.09991236867516781  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.14168956015675122  - accuracy: 0.8125\n",
      "At: 1587 [==========>] Loss 0.10540565028943763  - accuracy: 0.90625\n",
      "At: 1588 [==========>] Loss 0.13120612939860396  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.1462309920714862  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.12801086305217152  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.11054303790469056  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.06620313869393057  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.18828817318974894  - accuracy: 0.71875\n",
      "At: 1594 [==========>] Loss 0.09021823083795638  - accuracy: 0.90625\n",
      "At: 1595 [==========>] Loss 0.11495594185580334  - accuracy: 0.875\n",
      "At: 1596 [==========>] Loss 0.1721251217460728  - accuracy: 0.6875\n",
      "At: 1597 [==========>] Loss 0.13444969181748273  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.17838626028141225  - accuracy: 0.8125\n",
      "At: 1599 [==========>] Loss 0.20320116601200333  - accuracy: 0.75\n",
      "At: 1600 [==========>] Loss 0.15550324398670948  - accuracy: 0.84375\n",
      "At: 1601 [==========>] Loss 0.08922931272913316  - accuracy: 0.90625\n",
      "At: 1602 [==========>] Loss 0.11070100173732947  - accuracy: 0.8125\n",
      "At: 1603 [==========>] Loss 0.19488894784623362  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.2618534513830708  - accuracy: 0.625\n",
      "At: 1605 [==========>] Loss 0.07573862107217025  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.13999823601222056  - accuracy: 0.8125\n",
      "At: 1607 [==========>] Loss 0.18694355096616855  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.14253981346654826  - accuracy: 0.71875\n",
      "At: 1609 [==========>] Loss 0.15419739823242803  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.17886239534744536  - accuracy: 0.8125\n",
      "At: 1611 [==========>] Loss 0.08307727392790842  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.08916764680413447  - accuracy: 0.875\n",
      "At: 1613 [==========>] Loss 0.14324394557566347  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.1609983295348925  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.08220106516955389  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.12277511736894259  - accuracy: 0.78125\n",
      "At: 1617 [==========>] Loss 0.09902192497446846  - accuracy: 0.84375\n",
      "At: 1618 [==========>] Loss 0.09461600324648964  - accuracy: 0.875\n",
      "At: 1619 [==========>] Loss 0.17487551951721503  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.09729370058266829  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.0993790964723087  - accuracy: 0.875\n",
      "At: 1622 [==========>] Loss 0.14295802231171806  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.09259708233581487  - accuracy: 0.8125\n",
      "At: 1624 [==========>] Loss 0.15904843923406542  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.13779352418111607  - accuracy: 0.84375\n",
      "At: 1626 [==========>] Loss 0.10429771365379059  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.12608229767846924  - accuracy: 0.84375\n",
      "At: 1628 [==========>] Loss 0.17354808196729077  - accuracy: 0.75\n",
      "At: 1629 [==========>] Loss 0.12852175023867363  - accuracy: 0.84375\n",
      "At: 1630 [==========>] Loss 0.10356311049025874  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.10085592108157118  - accuracy: 0.90625\n",
      "At: 1632 [==========>] Loss 0.10629464103611361  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08625109436701026  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.09380784003229624  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.1983011105075305  - accuracy: 0.75\n",
      "At: 1636 [==========>] Loss 0.09664145641061578  - accuracy: 0.90625\n",
      "At: 1637 [==========>] Loss 0.05651459068054733  - accuracy: 0.9375\n",
      "At: 1638 [==========>] Loss 0.141579661847575  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.17626579502778827  - accuracy: 0.6875\n",
      "At: 1640 [==========>] Loss 0.10662190642803275  - accuracy: 0.84375\n",
      "At: 1641 [==========>] Loss 0.1053170960914451  - accuracy: 0.90625\n",
      "At: 1642 [==========>] Loss 0.1130184494559825  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.10092890195639505  - accuracy: 0.84375\n",
      "At: 1644 [==========>] Loss 0.10622245932438486  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.06640216908990235  - accuracy: 0.90625\n",
      "At: 1646 [==========>] Loss 0.1510690387802983  - accuracy: 0.78125\n",
      "At: 1647 [==========>] Loss 0.15126498148064293  - accuracy: 0.8125\n",
      "At: 1648 [==========>] Loss 0.14605047225102596  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.08905488226154484  - accuracy: 0.90625\n",
      "At: 1650 [==========>] Loss 0.08303372978778177  - accuracy: 0.90625\n",
      "At: 1651 [==========>] Loss 0.13374670971665847  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.07633220911599238  - accuracy: 0.9375\n",
      "At: 1653 [==========>] Loss 0.10953992031551826  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.06061022720424677  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.16367233285920285  - accuracy: 0.8125\n",
      "At: 1656 [==========>] Loss 0.10007769115789465  - accuracy: 0.875\n",
      "At: 1657 [==========>] Loss 0.12328829360163604  - accuracy: 0.84375\n",
      "At: 1658 [==========>] Loss 0.17964333280421577  - accuracy: 0.71875\n",
      "At: 1659 [==========>] Loss 0.07467667427541055  - accuracy: 0.9375\n",
      "At: 1660 [==========>] Loss 0.051470450648709545  - accuracy: 0.96875\n",
      "At: 1661 [==========>] Loss 0.08482963212383927  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.12185744047910349  - accuracy: 0.84375\n",
      "At: 1663 [==========>] Loss 0.16589595779688893  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.12961058044288104  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.11214324504704748  - accuracy: 0.84375\n",
      "At: 1666 [==========>] Loss 0.10688387087890203  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.18780496272840186  - accuracy: 0.6875\n",
      "At: 1668 [==========>] Loss 0.14043090438818473  - accuracy: 0.875\n",
      "At: 1669 [==========>] Loss 0.14530155156790314  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.06454174321221195  - accuracy: 0.9375\n",
      "At: 1671 [==========>] Loss 0.13712821560058513  - accuracy: 0.8125\n",
      "At: 1672 [==========>] Loss 0.100381969364749  - accuracy: 0.875\n",
      "At: 1673 [==========>] Loss 0.0831047448726078  - accuracy: 0.875\n",
      "At: 1674 [==========>] Loss 0.15117849506742337  - accuracy: 0.71875\n",
      "At: 1675 [==========>] Loss 0.17735651588490803  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.1981564129417009  - accuracy: 0.65625\n",
      "At: 1677 [==========>] Loss 0.10384503786247581  - accuracy: 0.90625\n",
      "At: 1678 [==========>] Loss 0.16497061051601303  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.10432552233549418  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.1438087461947062  - accuracy: 0.78125\n",
      "At: 1681 [==========>] Loss 0.128443808656299  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.09889356148757153  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.1880616014249597  - accuracy: 0.78125\n",
      "At: 1684 [==========>] Loss 0.12287005999594197  - accuracy: 0.875\n",
      "At: 1685 [==========>] Loss 0.13138772241410607  - accuracy: 0.78125\n",
      "At: 1686 [==========>] Loss 0.13243544822447312  - accuracy: 0.875\n",
      "At: 1687 [==========>] Loss 0.15807117842406362  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.058507851529145664  - accuracy: 0.90625\n",
      "At: 1689 [==========>] Loss 0.12054301111227883  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.10855144907113687  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.10797467504961299  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.13548093514182097  - accuracy: 0.8125\n",
      "At: 1693 [==========>] Loss 0.12336156692856076  - accuracy: 0.875\n",
      "At: 1694 [==========>] Loss 0.0941238467158266  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.10997380436451283  - accuracy: 0.84375\n",
      "At: 1696 [==========>] Loss 0.15221023330540873  - accuracy: 0.71875\n",
      "At: 1697 [==========>] Loss 0.11374423495956228  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.0744622793738402  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.10321176623849813  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.14958596298926033  - accuracy: 0.78125\n",
      "At: 1701 [==========>] Loss 0.1102213561644246  - accuracy: 0.8125\n",
      "At: 1702 [==========>] Loss 0.09925521365587178  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.18362672969859367  - accuracy: 0.78125\n",
      "At: 1704 [==========>] Loss 0.09163594989652554  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.1274362046276452  - accuracy: 0.8125\n",
      "At: 1706 [==========>] Loss 0.17087468856234092  - accuracy: 0.8125\n",
      "At: 1707 [==========>] Loss 0.2271189712413394  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.09142590125786254  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.17211356944081485  - accuracy: 0.75\n",
      "At: 1710 [==========>] Loss 0.1465149252654847  - accuracy: 0.84375\n",
      "At: 1711 [==========>] Loss 0.0785568838750467  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.10212916207156314  - accuracy: 0.9375\n",
      "At: 1713 [==========>] Loss 0.0982668810439644  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.14307925313737174  - accuracy: 0.84375\n",
      "At: 1715 [==========>] Loss 0.11562004877438777  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.06206527119806656  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.07817368684282795  - accuracy: 0.90625\n",
      "At: 1718 [==========>] Loss 0.14669609569845835  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.12410287962421848  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.061174469275482216  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.16529167267131797  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.06472781790262128  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.21851988604707068  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.0706304563805879  - accuracy: 0.9375\n",
      "At: 1725 [==========>] Loss 0.12751606799652623  - accuracy: 0.78125\n",
      "At: 1726 [==========>] Loss 0.11554577549118893  - accuracy: 0.875\n",
      "At: 1727 [==========>] Loss 0.12812364676127053  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.10314080532812775  - accuracy: 0.84375\n",
      "At: 1729 [==========>] Loss 0.19068156109053588  - accuracy: 0.65625\n",
      "At: 1730 [==========>] Loss 0.12695300226677958  - accuracy: 0.8125\n",
      "At: 1731 [==========>] Loss 0.09072526230363939  - accuracy: 0.90625\n",
      "At: 1732 [==========>] Loss 0.07863076213206449  - accuracy: 0.96875\n",
      "At: 1733 [==========>] Loss 0.158093301575244  - accuracy: 0.78125\n",
      "At: 1734 [==========>] Loss 0.10745358251202558  - accuracy: 0.8125\n",
      "At: 1735 [==========>] Loss 0.15586204902519668  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.11528606093085017  - accuracy: 0.84375\n",
      "At: 1737 [==========>] Loss 0.15711647931015904  - accuracy: 0.71875\n",
      "At: 1738 [==========>] Loss 0.1369144617805524  - accuracy: 0.78125\n",
      "At: 1739 [==========>] Loss 0.13455897017203644  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.15058856959871839  - accuracy: 0.8125\n",
      "At: 1741 [==========>] Loss 0.15574098923258078  - accuracy: 0.78125\n",
      "At: 1742 [==========>] Loss 0.04587736543425061  - accuracy: 0.9375\n",
      "At: 1743 [==========>] Loss 0.1780927732325519  - accuracy: 0.75\n",
      "At: 1744 [==========>] Loss 0.09569750888335264  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.12068061292350939  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.1551704985873013  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.11652054160588507  - accuracy: 0.84375\n",
      "At: 1748 [==========>] Loss 0.10675667966854174  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.11529023828617156  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.0919663910094464  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.17072160627254424  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.08977796468997509  - accuracy: 0.84375\n",
      "At: 1753 [==========>] Loss 0.08431681594324636  - accuracy: 0.9375\n",
      "At: 1754 [==========>] Loss 0.1126441898182315  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.0984381399509564  - accuracy: 0.84375\n",
      "At: 1756 [==========>] Loss 0.158632756272862  - accuracy: 0.75\n",
      "At: 1757 [==========>] Loss 0.19666002572897345  - accuracy: 0.75\n",
      "At: 1758 [==========>] Loss 0.062223850495858525  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.10106293713957493  - accuracy: 0.84375\n",
      "At: 1760 [==========>] Loss 0.08305358794244964  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.1379662926237496  - accuracy: 0.75\n",
      "At: 1762 [==========>] Loss 0.15741524198593923  - accuracy: 0.84375\n",
      "At: 1763 [==========>] Loss 0.10951914219902713  - accuracy: 0.8125\n",
      "At: 1764 [==========>] Loss 0.12773106234987913  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.19195525831737859  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.07822422664676965  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.07295175388138668  - accuracy: 0.90625\n",
      "At: 1768 [==========>] Loss 0.09769095627925464  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.07070323417857456  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.06757044175840075  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.16010369928315218  - accuracy: 0.75\n",
      "At: 1772 [==========>] Loss 0.13181200420969352  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.10238574511036017  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.16556029430441135  - accuracy: 0.75\n",
      "At: 1775 [==========>] Loss 0.09433987749062342  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.11966832691886341  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.13132843906905778  - accuracy: 0.8125\n",
      "At: 1778 [==========>] Loss 0.1205318229684459  - accuracy: 0.78125\n",
      "At: 1779 [==========>] Loss 0.11125491639642493  - accuracy: 0.875\n",
      "At: 1780 [==========>] Loss 0.0958273472841428  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.17162912889349458  - accuracy: 0.8125\n",
      "At: 1782 [==========>] Loss 0.11180324799404254  - accuracy: 0.90625\n",
      "At: 1783 [==========>] Loss 0.134248402726924  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.08037250518504566  - accuracy: 0.875\n",
      "At: 1785 [==========>] Loss 0.09383540099806889  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.1317139226309073  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.12212456910438606  - accuracy: 0.875\n",
      "At: 1788 [==========>] Loss 0.12088929396542449  - accuracy: 0.8125\n",
      "At: 1789 [==========>] Loss 0.1053067120382187  - accuracy: 0.9375\n",
      "At: 1790 [==========>] Loss 0.17150808811211332  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.07868128940178284  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.11572619521079838  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.11282599447617069  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.1413401831335982  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.08101031458198692  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.14657597694868052  - accuracy: 0.75\n",
      "At: 1797 [==========>] Loss 0.1252402037516329  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.11297536314499759  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.08264849593448732  - accuracy: 0.875\n",
      "At: 1800 [==========>] Loss 0.11067824503275486  - accuracy: 0.8125\n",
      "At: 1801 [==========>] Loss 0.18756354408909448  - accuracy: 0.71875\n",
      "At: 1802 [==========>] Loss 0.1447828595395701  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.1563624441805445  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.12934152536196108  - accuracy: 0.875\n",
      "At: 1805 [==========>] Loss 0.043918654184526396  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.1446771068318199  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.16805220767406043  - accuracy: 0.75\n",
      "At: 1808 [==========>] Loss 0.18103933948456244  - accuracy: 0.78125\n",
      "At: 1809 [==========>] Loss 0.07994237771772664  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.14318243275494852  - accuracy: 0.78125\n",
      "At: 1811 [==========>] Loss 0.14835117176814935  - accuracy: 0.71875\n",
      "At: 1812 [==========>] Loss 0.10326835928299934  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.13973004534638303  - accuracy: 0.78125\n",
      "At: 1814 [==========>] Loss 0.11468485674947555  - accuracy: 0.84375\n",
      "At: 1815 [==========>] Loss 0.150840240681298  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.037318845006564856  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.13389405619942074  - accuracy: 0.8125\n",
      "At: 1818 [==========>] Loss 0.1424014603028527  - accuracy: 0.75\n",
      "At: 1819 [==========>] Loss 0.17396590886157443  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.12748790522817083  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.09451183083303652  - accuracy: 0.875\n",
      "At: 1822 [==========>] Loss 0.16958315883682645  - accuracy: 0.78125\n",
      "At: 1823 [==========>] Loss 0.1633874124298712  - accuracy: 0.78125\n",
      "At: 1824 [==========>] Loss 0.16761879065834687  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.1050224713550342  - accuracy: 0.9375\n",
      "At: 1826 [==========>] Loss 0.07256455432249362  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.11877087515183114  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.16933445851829043  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.15609897179315782  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.14073965392750976  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.11857821604963585  - accuracy: 0.8125\n",
      "At: 1832 [==========>] Loss 0.10758243540521084  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.10161652648095756  - accuracy: 0.84375\n",
      "At: 1834 [==========>] Loss 0.08554967405357203  - accuracy: 0.84375\n",
      "At: 1835 [==========>] Loss 0.15944807287608573  - accuracy: 0.84375\n",
      "At: 1836 [==========>] Loss 0.11459587710581653  - accuracy: 0.84375\n",
      "At: 1837 [==========>] Loss 0.033324671498704224  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.10172645505173798  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.09360138574397328  - accuracy: 0.90625\n",
      "At: 1840 [==========>] Loss 0.1069787932943279  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.10559593954089797  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.1456812476610573  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.12187147250666949  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.0903565322787844  - accuracy: 0.84375\n",
      "At: 1845 [==========>] Loss 0.1934247302321771  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.14061297527275646  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.05297785421321366  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.06996118618918247  - accuracy: 0.875\n",
      "At: 1849 [==========>] Loss 0.1763544560332655  - accuracy: 0.78125\n",
      "At: 1850 [==========>] Loss 0.04465572902872429  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.16859989807345255  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.07542834165724241  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.10958623155551374  - accuracy: 0.90625\n",
      "At: 1854 [==========>] Loss 0.13829790897846309  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.15420017863425173  - accuracy: 0.84375\n",
      "At: 1856 [==========>] Loss 0.09722161629200592  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.1597503264184543  - accuracy: 0.75\n",
      "At: 1858 [==========>] Loss 0.10722269561899357  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.13277055878072022  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.12869443435402037  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.10589788187790701  - accuracy: 0.8125\n",
      "At: 1862 [==========>] Loss 0.17888830878370898  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.13315509457796676  - accuracy: 0.8125\n",
      "At: 1864 [==========>] Loss 0.13784151435874104  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.10214982400930013  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.2049923190680945  - accuracy: 0.6875\n",
      "At: 1867 [==========>] Loss 0.12884085687279956  - accuracy: 0.78125\n",
      "At: 1868 [==========>] Loss 0.15191064925122794  - accuracy: 0.75\n",
      "At: 1869 [==========>] Loss 0.1788814894446492  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.11437858094796055  - accuracy: 0.84375\n",
      "At: 1871 [==========>] Loss 0.13328802369678625  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.13064857645436517  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.07749306359511793  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.17016910341793162  - accuracy: 0.78125\n",
      "At: 1875 [==========>] Loss 0.09393738947891705  - accuracy: 0.9375\n",
      "At: 1876 [==========>] Loss 0.19680403359760737  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.10909928566803176  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.10051271463593857  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.14644745704525855  - accuracy: 0.75\n",
      "At: 1880 [==========>] Loss 0.08506116401758732  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.08584120168428537  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.12053328001886363  - accuracy: 0.875\n",
      "At: 1883 [==========>] Loss 0.1432656030454939  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.11485204433291522  - accuracy: 0.84375\n",
      "At: 1885 [==========>] Loss 0.1046151727288471  - accuracy: 0.84375\n",
      "At: 1886 [==========>] Loss 0.13987195924252385  - accuracy: 0.71875\n",
      "At: 1887 [==========>] Loss 0.07469855386009988  - accuracy: 0.875\n",
      "At: 1888 [==========>] Loss 0.16605534281844303  - accuracy: 0.75\n",
      "At: 1889 [==========>] Loss 0.10110098137324038  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.16855589260224954  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.07607769957938776  - accuracy: 0.875\n",
      "At: 1892 [==========>] Loss 0.07413079564903816  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.06147786531748035  - accuracy: 0.96875\n",
      "At: 1894 [==========>] Loss 0.09622974781665176  - accuracy: 0.9375\n",
      "At: 1895 [==========>] Loss 0.091060187736997  - accuracy: 0.84375\n",
      "At: 1896 [==========>] Loss 0.1439661809987969  - accuracy: 0.71875\n",
      "At: 1897 [==========>] Loss 0.08261359410625588  - accuracy: 0.875\n",
      "At: 1898 [==========>] Loss 0.12039726919297948  - accuracy: 0.84375\n",
      "At: 1899 [==========>] Loss 0.09474600079122548  - accuracy: 0.84375\n",
      "At: 1900 [==========>] Loss 0.12932634153633255  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.1068154491776043  - accuracy: 0.90625\n",
      "At: 1902 [==========>] Loss 0.15821535732316316  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.13312742450218293  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.06944243392768866  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.12284157944891766  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.09858600449816332  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.09139682124168866  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.10647027688468645  - accuracy: 0.78125\n",
      "At: 1909 [==========>] Loss 0.10871145208715016  - accuracy: 0.84375\n",
      "At: 1910 [==========>] Loss 0.06582099326446483  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.1296909922521391  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.11968454869320422  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.1665328283748686  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.06604300518858777  - accuracy: 0.9375\n",
      "At: 1915 [==========>] Loss 0.12982020643478007  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.1574337370850995  - accuracy: 0.78125\n",
      "At: 1917 [==========>] Loss 0.1693426075436043  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.15105687194483156  - accuracy: 0.78125\n",
      "At: 1919 [==========>] Loss 0.08944049146108729  - accuracy: 0.90625\n",
      "At: 1920 [==========>] Loss 0.09865932014670273  - accuracy: 0.875\n",
      "At: 1921 [==========>] Loss 0.13387812439706515  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.1493694600274112  - accuracy: 0.71875\n",
      "At: 1923 [==========>] Loss 0.15819768175075988  - accuracy: 0.75\n",
      "At: 1924 [==========>] Loss 0.1371044021833821  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.1650176931187129  - accuracy: 0.8125\n",
      "At: 1926 [==========>] Loss 0.09592582669625635  - accuracy: 0.78125\n",
      "At: 1927 [==========>] Loss 0.11471031758878303  - accuracy: 0.8125\n",
      "At: 1928 [==========>] Loss 0.13644036941177073  - accuracy: 0.71875\n",
      "At: 1929 [==========>] Loss 0.17206452891844004  - accuracy: 0.71875\n",
      "At: 1930 [==========>] Loss 0.16525729366242975  - accuracy: 0.71875\n",
      "At: 1931 [==========>] Loss 0.11176072122004928  - accuracy: 0.8125\n",
      "At: 1932 [==========>] Loss 0.17079942147715207  - accuracy: 0.75\n",
      "At: 1933 [==========>] Loss 0.09080261214271938  - accuracy: 0.875\n",
      "At: 1934 [==========>] Loss 0.1360294949918821  - accuracy: 0.875\n",
      "At: 1935 [==========>] Loss 0.13164231069665192  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.1022875865739282  - accuracy: 0.875\n",
      "At: 1937 [==========>] Loss 0.13782436293256797  - accuracy: 0.75\n",
      "At: 1938 [==========>] Loss 0.18611186787690173  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.12181501562966512  - accuracy: 0.8125\n",
      "At: 1940 [==========>] Loss 0.11810048910118155  - accuracy: 0.8125\n",
      "At: 1941 [==========>] Loss 0.14254492610472624  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.14517665133726754  - accuracy: 0.84375\n",
      "At: 1943 [==========>] Loss 0.1626284955453805  - accuracy: 0.78125\n",
      "At: 1944 [==========>] Loss 0.1036278907221512  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.16576556390561187  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.1265186558465425  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.10444426792896225  - accuracy: 0.875\n",
      "At: 1948 [==========>] Loss 0.1376915040969835  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.05770667354594357  - accuracy: 0.9375\n",
      "At: 1950 [==========>] Loss 0.12952538479385506  - accuracy: 0.84375\n",
      "At: 1951 [==========>] Loss 0.14119955932967204  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.07907474714321187  - accuracy: 0.9375\n",
      "At: 1953 [==========>] Loss 0.07338595163859625  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.18057529743754466  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.07120773119853976  - accuracy: 0.9375\n",
      "At: 1956 [==========>] Loss 0.10458583290472154  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.08921890357981402  - accuracy: 0.90625\n",
      "At: 1958 [==========>] Loss 0.08405659563436803  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.11029354702429747  - accuracy: 0.84375\n",
      "At: 1960 [==========>] Loss 0.054905256703844815  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.17224594589340858  - accuracy: 0.71875\n",
      "At: 1962 [==========>] Loss 0.19236595675151463  - accuracy: 0.6875\n",
      "At: 1963 [==========>] Loss 0.08050405541497899  - accuracy: 0.875\n",
      "At: 1964 [==========>] Loss 0.1898705094573404  - accuracy: 0.75\n",
      "At: 1965 [==========>] Loss 0.13944887116340413  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.12824441886631474  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.12281030819365846  - accuracy: 0.75\n",
      "At: 1968 [==========>] Loss 0.20889120704570455  - accuracy: 0.625\n",
      "At: 1969 [==========>] Loss 0.17956425130912101  - accuracy: 0.75\n",
      "At: 1970 [==========>] Loss 0.08768207381394605  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.1963880795988382  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.09283551696195669  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.14047595740114277  - accuracy: 0.75\n",
      "At: 1974 [==========>] Loss 0.1245327257985372  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.14817158321489676  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.07515728158739726  - accuracy: 0.90625\n",
      "At: 1977 [==========>] Loss 0.08516134195170633  - accuracy: 0.90625\n",
      "At: 1978 [==========>] Loss 0.1469373462403336  - accuracy: 0.78125\n",
      "At: 1979 [==========>] Loss 0.11858256884299476  - accuracy: 0.875\n",
      "At: 1980 [==========>] Loss 0.12831904392310217  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.13703503598257774  - accuracy: 0.78125\n",
      "At: 1982 [==========>] Loss 0.05163280241190815  - accuracy: 0.9375\n",
      "At: 1983 [==========>] Loss 0.13889682418403132  - accuracy: 0.84375\n",
      "At: 1984 [==========>] Loss 0.08356494323688407  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.16883517237554607  - accuracy: 0.75\n",
      "At: 1986 [==========>] Loss 0.1941591857883604  - accuracy: 0.6875\n",
      "At: 1987 [==========>] Loss 0.12008370873715493  - accuracy: 0.78125\n",
      "At: 1988 [==========>] Loss 0.10038941230181339  - accuracy: 0.9375\n",
      "At: 1989 [==========>] Loss 0.0882801561614367  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.110140828589787  - accuracy: 0.875\n",
      "At: 1991 [==========>] Loss 0.14289920932928074  - accuracy: 0.84375\n",
      "At: 1992 [==========>] Loss 0.12268958022938889  - accuracy: 0.84375\n",
      "At: 1993 [==========>] Loss 0.1584257969304898  - accuracy: 0.75\n",
      "At: 1994 [==========>] Loss 0.1258192095932773  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.17661682631686532  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.08700534641520452  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.17512809750515002  - accuracy: 0.75\n",
      "At: 1998 [==========>] Loss 0.1362389695107552  - accuracy: 0.78125\n",
      "At: 1999 [==========>] Loss 0.08338805553542278  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.13904941228688406  - accuracy: 0.84375\n",
      "At: 2001 [==========>] Loss 0.07758913778381099  - accuracy: 0.875\n",
      "At: 2002 [==========>] Loss 0.078165069846921  - accuracy: 0.90625\n",
      "At: 2003 [==========>] Loss 0.12659717529400924  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.15755488920944785  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.12620121017900177  - accuracy: 0.875\n",
      "At: 2006 [==========>] Loss 0.13001023068554898  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.09884402427825167  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.1362126829756627  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.14589040494417485  - accuracy: 0.84375\n",
      "At: 2010 [==========>] Loss 0.12131093420393335  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.11642396392634552  - accuracy: 0.90625\n",
      "At: 2012 [==========>] Loss 0.11670498868689122  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.08351259473556386  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.218294218872927  - accuracy: 0.5625\n",
      "At: 2015 [==========>] Loss 0.06366495423521723  - accuracy: 0.9375\n",
      "At: 2016 [==========>] Loss 0.11607954135412038  - accuracy: 0.8125\n",
      "At: 2017 [==========>] Loss 0.08377693104831056  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.09341823102632237  - accuracy: 0.84375\n",
      "At: 2019 [==========>] Loss 0.13867372307791057  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.0579497807724445  - accuracy: 0.9375\n",
      "At: 2021 [==========>] Loss 0.11684635773155014  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.13078162829438844  - accuracy: 0.78125\n",
      "At: 2023 [==========>] Loss 0.10707479572678796  - accuracy: 0.84375\n",
      "At: 2024 [==========>] Loss 0.09944687641881553  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.18894630643892574  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.09164584266267986  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.15290066469400931  - accuracy: 0.8125\n",
      "At: 2028 [==========>] Loss 0.0996400900212228  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.12287330356189433  - accuracy: 0.84375\n",
      "At: 2030 [==========>] Loss 0.15246322765401057  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.13808398191398874  - accuracy: 0.78125\n",
      "At: 2032 [==========>] Loss 0.15662487249468265  - accuracy: 0.75\n",
      "At: 2033 [==========>] Loss 0.1486875110926196  - accuracy: 0.8125\n",
      "At: 2034 [==========>] Loss 0.19869157861565045  - accuracy: 0.71875\n",
      "At: 2035 [==========>] Loss 0.12261425773456379  - accuracy: 0.84375\n",
      "At: 2036 [==========>] Loss 0.1043570743785385  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.11497745509799194  - accuracy: 0.875\n",
      "At: 2038 [==========>] Loss 0.11164778620542384  - accuracy: 0.84375\n",
      "At: 2039 [==========>] Loss 0.09883141622109073  - accuracy: 0.875\n",
      "At: 2040 [==========>] Loss 0.11309768306702075  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.056585335489696575  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.11866756345786132  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.10158242528193343  - accuracy: 0.84375\n",
      "At: 2044 [==========>] Loss 0.07799595455269681  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.21085478546404485  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.06245397946555653  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.0691986874005383  - accuracy: 0.84375\n",
      "At: 2048 [==========>] Loss 0.1074227435370903  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.15178230706071624  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.18521733926444953  - accuracy: 0.71875\n",
      "At: 2051 [==========>] Loss 0.17902622037088298  - accuracy: 0.75\n",
      "At: 2052 [==========>] Loss 0.06557894786562518  - accuracy: 0.90625\n",
      "At: 2053 [==========>] Loss 0.11092555020995042  - accuracy: 0.875\n",
      "At: 2054 [==========>] Loss 0.10463557386025302  - accuracy: 0.75\n",
      "At: 2055 [==========>] Loss 0.0860572304048393  - accuracy: 0.875\n",
      "At: 2056 [==========>] Loss 0.11221223624204787  - accuracy: 0.875\n",
      "At: 2057 [==========>] Loss 0.15096837408524416  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.11322303033605519  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.1815567125965031  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.11955247121970537  - accuracy: 0.8125\n",
      "At: 2061 [==========>] Loss 0.14993112168468323  - accuracy: 0.78125\n",
      "At: 2062 [==========>] Loss 0.11874296636772551  - accuracy: 0.78125\n",
      "At: 2063 [==========>] Loss 0.09965431033266581  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.18281261611336885  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.05796567325031201  - accuracy: 0.875\n",
      "At: 2066 [==========>] Loss 0.1506622453623297  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.09918274952264666  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.11133225606509893  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.11763334247377538  - accuracy: 0.8125\n",
      "At: 2070 [==========>] Loss 0.14211550115256785  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.14128725919824422  - accuracy: 0.78125\n",
      "At: 2072 [==========>] Loss 0.0705908627021253  - accuracy: 0.9375\n",
      "At: 2073 [==========>] Loss 0.13192638935125375  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.09867517505640931  - accuracy: 0.875\n",
      "At: 2075 [==========>] Loss 0.10246423037518579  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.13443956778436617  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.15931154394129904  - accuracy: 0.78125\n",
      "At: 2078 [==========>] Loss 0.10130857080223805  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.08095989314229383  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.12626969925760229  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.14192452152657967  - accuracy: 0.84375\n",
      "At: 2082 [==========>] Loss 0.12563748925445806  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.19007026194407128  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.09637570599458785  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.10122685454156663  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.11502513278201897  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.16801127512537123  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.10911224117387094  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.10926297356025841  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.08705821963449474  - accuracy: 0.9375\n",
      "At: 2091 [==========>] Loss 0.15741652302186226  - accuracy: 0.75\n",
      "At: 2092 [==========>] Loss 0.10998087669169404  - accuracy: 0.875\n",
      "At: 2093 [==========>] Loss 0.15374285467579984  - accuracy: 0.78125\n",
      "At: 2094 [==========>] Loss 0.14985034850685025  - accuracy: 0.75\n",
      "At: 2095 [==========>] Loss 0.11732328205535775  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.17002181968026442  - accuracy: 0.75\n",
      "At: 2097 [==========>] Loss 0.09989521950939202  - accuracy: 0.84375\n",
      "At: 2098 [==========>] Loss 0.13031828158554815  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.09831515932027907  - accuracy: 0.8125\n",
      "At: 2100 [==========>] Loss 0.06104210886279605  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.1751291355265026  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.08189629332607323  - accuracy: 0.90625\n",
      "At: 2103 [==========>] Loss 0.13768671946414  - accuracy: 0.84375\n",
      "At: 2104 [==========>] Loss 0.11536467620424812  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.1449470728009953  - accuracy: 0.78125\n",
      "At: 2106 [==========>] Loss 0.15417047732337436  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.09347347827504478  - accuracy: 0.875\n",
      "At: 2108 [==========>] Loss 0.1343107442793702  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.11621660495089114  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.07177243005069944  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.11468390998560887  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.1306927114483361  - accuracy: 0.8125\n",
      "At: 2113 [==========>] Loss 0.1101197291940636  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.1281845822791255  - accuracy: 0.84375\n",
      "At: 2115 [==========>] Loss 0.11257338454975781  - accuracy: 0.84375\n",
      "At: 2116 [==========>] Loss 0.10611700352249098  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.13088105514721526  - accuracy: 0.75\n",
      "At: 2118 [==========>] Loss 0.12312131575773555  - accuracy: 0.8125\n",
      "At: 2119 [==========>] Loss 0.07414443854856567  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.14548902792181154  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.1392397212587744  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.14013309313356603  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.16863493434804994  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.13079025238621972  - accuracy: 0.84375\n",
      "At: 2125 [==========>] Loss 0.1006203787125716  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.04102179935220184  - accuracy: 1.0\n",
      "At: 2127 [==========>] Loss 0.09170272295864568  - accuracy: 0.9375\n",
      "At: 2128 [==========>] Loss 0.11293821255676666  - accuracy: 0.84375\n",
      "At: 2129 [==========>] Loss 0.1756819471570384  - accuracy: 0.71875\n",
      "At: 2130 [==========>] Loss 0.0678255379221938  - accuracy: 0.84375\n",
      "At: 2131 [==========>] Loss 0.10380495274916784  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.2242358447607195  - accuracy: 0.65625\n",
      "At: 2133 [==========>] Loss 0.16494043207984654  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.1148222953179774  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.09276125152907927  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.14095074980478906  - accuracy: 0.78125\n",
      "At: 2137 [==========>] Loss 0.10230371649085444  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.10703454835041895  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.18268515178952155  - accuracy: 0.6875\n",
      "At: 2140 [==========>] Loss 0.10914157950684428  - accuracy: 0.8125\n",
      "At: 2141 [==========>] Loss 0.12201676467506015  - accuracy: 0.875\n",
      "At: 2142 [==========>] Loss 0.11915074203800483  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.09078845806895948  - accuracy: 0.84375\n",
      "At: 2144 [==========>] Loss 0.07411770619851901  - accuracy: 0.875\n",
      "At: 2145 [==========>] Loss 0.11264231919528545  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.17331886391669757  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.11911157445091868  - accuracy: 0.78125\n",
      "At: 2148 [==========>] Loss 0.19686221928168401  - accuracy: 0.71875\n",
      "At: 2149 [==========>] Loss 0.10271459623459654  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.12142476546576793  - accuracy: 0.875\n",
      "At: 2151 [==========>] Loss 0.1206791709141328  - accuracy: 0.875\n",
      "At: 2152 [==========>] Loss 0.2402114004883727  - accuracy: 0.625\n",
      "At: 2153 [==========>] Loss 0.1620000166607221  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.18024816741328223  - accuracy: 0.71875\n",
      "At: 2155 [==========>] Loss 0.12725568682262836  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.11410049062129134  - accuracy: 0.875\n",
      "At: 2157 [==========>] Loss 0.12523018321256177  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.1273261677016762  - accuracy: 0.78125\n",
      "At: 2159 [==========>] Loss 0.07904830102329474  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.12459374576340324  - accuracy: 0.84375\n",
      "At: 2161 [==========>] Loss 0.09560512197611726  - accuracy: 0.90625\n",
      "At: 2162 [==========>] Loss 0.09093461211808954  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.12680477589012623  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.16367934918090318  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.09797570309503692  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.13550074985005342  - accuracy: 0.8125\n",
      "At: 2167 [==========>] Loss 0.0825821968253189  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.0963450745571626  - accuracy: 0.84375\n",
      "At: 2169 [==========>] Loss 0.11250128860736874  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.09002892455814065  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.16184430770559038  - accuracy: 0.71875\n",
      "At: 2172 [==========>] Loss 0.11085192203828033  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.12211127926073814  - accuracy: 0.75\n",
      "At: 2174 [==========>] Loss 0.1190134459644524  - accuracy: 0.875\n",
      "At: 2175 [==========>] Loss 0.12795998798337702  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.11257636857456522  - accuracy: 0.84375\n",
      "At: 2177 [==========>] Loss 0.15704268424463094  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.10844603769764838  - accuracy: 0.875\n",
      "At: 2179 [==========>] Loss 0.12288862828022408  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.12839469861272748  - accuracy: 0.84375\n",
      "At: 2181 [==========>] Loss 0.17830817500743498  - accuracy: 0.6875\n",
      "At: 2182 [==========>] Loss 0.12516887371489752  - accuracy: 0.78125\n",
      "At: 2183 [==========>] Loss 0.21477467050117932  - accuracy: 0.6875\n",
      "At: 2184 [==========>] Loss 0.08756743527301744  - accuracy: 0.875\n",
      "At: 2185 [==========>] Loss 0.11974887249443544  - accuracy: 0.75\n",
      "At: 2186 [==========>] Loss 0.14614066330081718  - accuracy: 0.875\n",
      "At: 2187 [==========>] Loss 0.1731291965833067  - accuracy: 0.75\n",
      "At: 2188 [==========>] Loss 0.06981815128256624  - accuracy: 0.9375\n",
      "At: 2189 [==========>] Loss 0.08852840466673242  - accuracy: 0.875\n",
      "At: 2190 [==========>] Loss 0.11907566485696652  - accuracy: 0.84375\n",
      "At: 2191 [==========>] Loss 0.13835289708035334  - accuracy: 0.78125\n",
      "At: 2192 [==========>] Loss 0.10819193853249914  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.15165679665368698  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.09238117155813522  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.17624012298777858  - accuracy: 0.75\n",
      "At: 2196 [==========>] Loss 0.1456179249054955  - accuracy: 0.71875\n",
      "At: 2197 [==========>] Loss 0.09403865654357911  - accuracy: 0.875\n",
      "At: 2198 [==========>] Loss 0.09273036046140593  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.04953117456858473  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.039247014993220766  - accuracy: 0.96875\n",
      "At: 2201 [==========>] Loss 0.12336757400205273  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.09141317269084692  - accuracy: 0.84375\n",
      "At: 2203 [==========>] Loss 0.08757595183629655  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.13510097677735214  - accuracy: 0.78125\n",
      "At: 2205 [==========>] Loss 0.13801511844708947  - accuracy: 0.8125\n",
      "At: 2206 [==========>] Loss 0.0938297397120765  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.08892626169619725  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.1462942070405698  - accuracy: 0.75\n",
      "At: 2209 [==========>] Loss 0.1922721514907786  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.14174220972888849  - accuracy: 0.78125\n",
      "At: 2211 [==========>] Loss 0.13160708815376498  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.11736199703452375  - accuracy: 0.84375\n",
      "At: 2213 [==========>] Loss 0.13198632216347367  - accuracy: 0.8125\n",
      "At: 2214 [==========>] Loss 0.15538748892108797  - accuracy: 0.75\n",
      "At: 2215 [==========>] Loss 0.14133272405648298  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.12970721042652056  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.1290175748624446  - accuracy: 0.78125\n",
      "At: 2218 [==========>] Loss 0.16666463008225635  - accuracy: 0.78125\n",
      "At: 2219 [==========>] Loss 0.08838808319386664  - accuracy: 0.90625\n",
      "At: 2220 [==========>] Loss 0.11064467408463563  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.19151345549829882  - accuracy: 0.75\n",
      "At: 2222 [==========>] Loss 0.10016050815762044  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.1420384492787416  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.15635839780218858  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.1323524345485884  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.11360126988687166  - accuracy: 0.84375\n",
      "At: 2227 [==========>] Loss 0.1806170956332443  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.08354996634717227  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.1715436578364094  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.10798743953921794  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.1360813388315318  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.15338215717705303  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.17361819115870006  - accuracy: 0.78125\n",
      "At: 2234 [==========>] Loss 0.16141742430840855  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.12481897266103341  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.0786687653727566  - accuracy: 0.875\n",
      "At: 2237 [==========>] Loss 0.1350684228961942  - accuracy: 0.78125\n",
      "At: 2238 [==========>] Loss 0.14634226929048866  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.18826121974304122  - accuracy: 0.71875\n",
      "At: 2240 [==========>] Loss 0.13957076359027104  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.13440246974241327  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.16979613663104  - accuracy: 0.71875\n",
      "At: 2243 [==========>] Loss 0.08582461263920732  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.10354958181949105  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.08490232738420275  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.14892381355619624  - accuracy: 0.78125\n",
      "At: 2247 [==========>] Loss 0.11867184109814452  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.18964151501309756  - accuracy: 0.71875\n",
      "At: 2249 [==========>] Loss 0.10445679006998525  - accuracy: 0.84375\n",
      "At: 2250 [==========>] Loss 0.0954437487071172  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.0857403965687133  - accuracy: 0.8125\n",
      "At: 2252 [==========>] Loss 0.12810906167891242  - accuracy: 0.75\n",
      "At: 2253 [==========>] Loss 0.1421373264741363  - accuracy: 0.71875\n",
      "At: 2254 [==========>] Loss 0.15099773505878164  - accuracy: 0.71875\n",
      "At: 2255 [==========>] Loss 0.13412609094529515  - accuracy: 0.78125\n",
      "At: 2256 [==========>] Loss 0.15554696608644872  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.1004740318451409  - accuracy: 0.875\n",
      "At: 2258 [==========>] Loss 0.128052179667651  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.13529689865618924  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.18118652450268324  - accuracy: 0.75\n",
      "At: 2261 [==========>] Loss 0.12190608723065463  - accuracy: 0.875\n",
      "At: 2262 [==========>] Loss 0.15828940039326458  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.16418562549784244  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.11236756567885965  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.09288874548568993  - accuracy: 0.84375\n",
      "At: 2266 [==========>] Loss 0.14128530047490961  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.056600757684100333  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.10550520987765455  - accuracy: 0.84375\n",
      "At: 2269 [==========>] Loss 0.043212081006242006  - accuracy: 0.96875\n",
      "At: 2270 [==========>] Loss 0.09973818465680478  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.12675458365665596  - accuracy: 0.84375\n",
      "At: 2272 [==========>] Loss 0.08438834649429267  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.09941035293199585  - accuracy: 0.875\n",
      "At: 2274 [==========>] Loss 0.09054841656499542  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.10543897254250627  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.10091798082238934  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.16394825085909592  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.09968166195753583  - accuracy: 0.875\n",
      "At: 2279 [==========>] Loss 0.13146998777106778  - accuracy: 0.875\n",
      "At: 2280 [==========>] Loss 0.16329660468275076  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.12450349849000053  - accuracy: 0.8125\n",
      "At: 2282 [==========>] Loss 0.07223960085982048  - accuracy: 0.96875\n",
      "At: 2283 [==========>] Loss 0.1659817707609174  - accuracy: 0.71875\n",
      "At: 2284 [==========>] Loss 0.15048535798705726  - accuracy: 0.75\n",
      "At: 2285 [==========>] Loss 0.14061579156122872  - accuracy: 0.78125\n",
      "At: 2286 [==========>] Loss 0.11448289573482251  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.1645323245185351  - accuracy: 0.75\n",
      "At: 2288 [==========>] Loss 0.089021361098955  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.13490194370459302  - accuracy: 0.8125\n",
      "At: 2290 [==========>] Loss 0.06394112328502283  - accuracy: 0.90625\n",
      "At: 2291 [==========>] Loss 0.13156990893768183  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.09062620578435121  - accuracy: 0.9375\n",
      "At: 2293 [==========>] Loss 0.07366188235343392  - accuracy: 0.84375\n",
      "At: 2294 [==========>] Loss 0.05863809797984977  - accuracy: 0.96875\n",
      "At: 2295 [==========>] Loss 0.14392929246151265  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.15490582414740317  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.08070786341681332  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.12069744441020137  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.09792876166720507  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.1422860701041159  - accuracy: 0.71875\n",
      "At: 2301 [==========>] Loss 0.1618848297796897  - accuracy: 0.75\n",
      "At: 2302 [==========>] Loss 0.17531680746007391  - accuracy: 0.75\n",
      "At: 2303 [==========>] Loss 0.07069421964135057  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.08463991411178882  - accuracy: 0.84375\n",
      "At: 2305 [==========>] Loss 0.11601948273954289  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.12571080782880253  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.15983944394188912  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.19017789218904302  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.1136942811594441  - accuracy: 0.84375\n",
      "At: 2310 [==========>] Loss 0.1429383298112175  - accuracy: 0.84375\n",
      "At: 2311 [==========>] Loss 0.13382070664474446  - accuracy: 0.8125\n",
      "At: 2312 [==========>] Loss 0.11130305050766268  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.07413059815933137  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.10897296034239545  - accuracy: 0.84375\n",
      "At: 2315 [==========>] Loss 0.11381407240762645  - accuracy: 0.84375\n",
      "At: 2316 [==========>] Loss 0.15377485203317032  - accuracy: 0.6875\n",
      "At: 2317 [==========>] Loss 0.17100222758333067  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.15221110416249534  - accuracy: 0.75\n",
      "At: 2319 [==========>] Loss 0.12260951602442041  - accuracy: 0.8125\n",
      "At: 2320 [==========>] Loss 0.1104016457178571  - accuracy: 0.84375\n",
      "At: 2321 [==========>] Loss 0.11766540966912116  - accuracy: 0.875\n",
      "At: 2322 [==========>] Loss 0.18052831860072416  - accuracy: 0.78125\n",
      "At: 2323 [==========>] Loss 0.1560965996276072  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.13967085720826228  - accuracy: 0.8125\n",
      "At: 2325 [==========>] Loss 0.11702249458194178  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.06974158186363774  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.07207749417690332  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.12869232483010087  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.12123836071519413  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.1602692674979302  - accuracy: 0.75\n",
      "At: 2331 [==========>] Loss 0.09406071435787783  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.1129039835204661  - accuracy: 0.8125\n",
      "At: 2333 [==========>] Loss 0.12369598265355108  - accuracy: 0.8125\n",
      "At: 2334 [==========>] Loss 0.1722408937204074  - accuracy: 0.75\n",
      "At: 2335 [==========>] Loss 0.11357373403159765  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.101934299679933  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.15055333846249797  - accuracy: 0.71875\n",
      "At: 2338 [==========>] Loss 0.10866568793729255  - accuracy: 0.84375\n",
      "At: 2339 [==========>] Loss 0.10325701634034277  - accuracy: 0.84375\n",
      "At: 2340 [==========>] Loss 0.16333669198484144  - accuracy: 0.8125\n",
      "At: 2341 [==========>] Loss 0.14108633212299948  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.18578451824894557  - accuracy: 0.71875\n",
      "At: 2343 [==========>] Loss 0.09162914758725907  - accuracy: 0.84375\n",
      "At: 2344 [==========>] Loss 0.19810591512354606  - accuracy: 0.6875\n",
      "At: 2345 [==========>] Loss 0.13723480207059663  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.08640736483334732  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.14322925635309292  - accuracy: 0.75\n",
      "At: 2348 [==========>] Loss 0.05784162409612397  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.06891118922673439  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.11030022911627915  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.07541568431443565  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.11786686884497269  - accuracy: 0.84375\n",
      "At: 2353 [==========>] Loss 0.0915584007546121  - accuracy: 0.875\n",
      "At: 2354 [==========>] Loss 0.17385693799129354  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.05995444805935142  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.10527861125756025  - accuracy: 0.84375\n",
      "At: 2357 [==========>] Loss 0.11969497622257569  - accuracy: 0.875\n",
      "At: 2358 [==========>] Loss 0.157697692497502  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.1863242004802792  - accuracy: 0.625\n",
      "At: 2360 [==========>] Loss 0.1319682814681125  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.13954711271735842  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.06422647378110242  - accuracy: 0.9375\n",
      "At: 2363 [==========>] Loss 0.18542072478317578  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.07339422708124617  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.09794549285107926  - accuracy: 0.875\n",
      "At: 2366 [==========>] Loss 0.18702277047598082  - accuracy: 0.6875\n",
      "At: 2367 [==========>] Loss 0.0842926855520793  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.1116052212894888  - accuracy: 0.84375\n",
      "At: 2369 [==========>] Loss 0.11354798150302478  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.09503967703552355  - accuracy: 0.875\n",
      "At: 2371 [==========>] Loss 0.08556474207105758  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.13937191710909372  - accuracy: 0.8125\n",
      "At: 2373 [==========>] Loss 0.11883773933295694  - accuracy: 0.75\n",
      "At: 2374 [==========>] Loss 0.10433663483935299  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.053089068459979114  - accuracy: 0.9375\n",
      "At: 2376 [==========>] Loss 0.08272914045776542  - accuracy: 0.8125\n",
      "At: 2377 [==========>] Loss 0.10600685939057244  - accuracy: 0.78125\n",
      "At: 2378 [==========>] Loss 0.12044579078874437  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.1616928484822471  - accuracy: 0.78125\n",
      "At: 2380 [==========>] Loss 0.06532369715943374  - accuracy: 0.875\n",
      "At: 2381 [==========>] Loss 0.10803611588610307  - accuracy: 0.875\n",
      "At: 2382 [==========>] Loss 0.1086302286852755  - accuracy: 0.8125\n",
      "At: 2383 [==========>] Loss 0.13435281204487828  - accuracy: 0.78125\n",
      "At: 2384 [==========>] Loss 0.1289406502153963  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.11711790941528775  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.09555289760721877  - accuracy: 0.875\n",
      "At: 2387 [==========>] Loss 0.07486068538930268  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.11825191809268149  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.03803204685776026  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.07087423252302259  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.16791858628269749  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.11070661073214355  - accuracy: 0.90625\n",
      "At: 2393 [==========>] Loss 0.11574403119919455  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.06710508797414962  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.10265245203954243  - accuracy: 0.84375\n",
      "At: 2396 [==========>] Loss 0.0688574453101689  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.08369879344201567  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.11032030883108693  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.12688472130142472  - accuracy: 0.875\n",
      "At: 2400 [==========>] Loss 0.10064653052298278  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.09423105783307659  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.09928331295042757  - accuracy: 0.90625\n",
      "At: 2403 [==========>] Loss 0.17562187580779862  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.13965475598210286  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.07001417301313458  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.12946757157924987  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.10929781777887665  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.10576296608227806  - accuracy: 0.84375\n",
      "At: 2409 [==========>] Loss 0.15814473224095357  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.1414824164587683  - accuracy: 0.78125\n",
      "At: 2411 [==========>] Loss 0.11453599338931075  - accuracy: 0.84375\n",
      "At: 2412 [==========>] Loss 0.09558009467446121  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.08929239036348785  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.06312254276763145  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.0884622752516672  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.10317132991722314  - accuracy: 0.84375\n",
      "At: 2417 [==========>] Loss 0.1910608214810295  - accuracy: 0.75\n",
      "At: 2418 [==========>] Loss 0.1269589881811404  - accuracy: 0.84375\n",
      "At: 2419 [==========>] Loss 0.1271761341432649  - accuracy: 0.875\n",
      "At: 2420 [==========>] Loss 0.13450297536672018  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.09130862406380758  - accuracy: 0.9375\n",
      "At: 2422 [==========>] Loss 0.16200211567489317  - accuracy: 0.75\n",
      "At: 2423 [==========>] Loss 0.13396227737893374  - accuracy: 0.8125\n",
      "At: 2424 [==========>] Loss 0.09427674999508537  - accuracy: 0.90625\n",
      "At: 2425 [==========>] Loss 0.07786037400667534  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.19787755641827842  - accuracy: 0.65625\n",
      "At: 2427 [==========>] Loss 0.13364295586455766  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.0697848479176077  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.12148199684130524  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.15025898960808226  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.13726501537342273  - accuracy: 0.71875\n",
      "At: 2432 [==========>] Loss 0.07928501972029804  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.058189649442505084  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.0522539116568632  - accuracy: 0.9375\n",
      "At: 2435 [==========>] Loss 0.15502273538736588  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.11673503989340872  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.17866087785679152  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.10182017150158833  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.15387208032608823  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.10474831558495555  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.12320183785634443  - accuracy: 0.78125\n",
      "At: 2442 [==========>] Loss 0.1448976771851028  - accuracy: 0.84375\n",
      "At: 2443 [==========>] Loss 0.12376459996316821  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.08614937993164093  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.05890948032611888  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.1894087033658133  - accuracy: 0.75\n",
      "At: 2447 [==========>] Loss 0.1853692688718718  - accuracy: 0.71875\n",
      "At: 2448 [==========>] Loss 0.10144794508789046  - accuracy: 0.8125\n",
      "At: 2449 [==========>] Loss 0.08771740375875167  - accuracy: 0.875\n",
      "At: 2450 [==========>] Loss 0.062023108631953754  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.04947062769873832  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.10071263970523932  - accuracy: 0.875\n",
      "At: 2453 [==========>] Loss 0.12555818866310445  - accuracy: 0.8125\n",
      "At: 2454 [==========>] Loss 0.163005950096297  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.14189321266244548  - accuracy: 0.78125\n",
      "At: 2456 [==========>] Loss 0.11799014534569624  - accuracy: 0.78125\n",
      "At: 2457 [==========>] Loss 0.1590187521812023  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.07395823250724548  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.12428797047226711  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.0668688222538208  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.07325126296818288  - accuracy: 0.875\n",
      "At: 2462 [==========>] Loss 0.16280528547169798  - accuracy: 0.71875\n",
      "At: 2463 [==========>] Loss 0.09101701996218337  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.1780610852289543  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.14313486413296223  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.09663738061231114  - accuracy: 0.875\n",
      "At: 2467 [==========>] Loss 0.10247899371854838  - accuracy: 0.84375\n",
      "At: 2468 [==========>] Loss 0.07583250401628085  - accuracy: 0.90625\n",
      "At: 2469 [==========>] Loss 0.13176849475129795  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.11592494684519018  - accuracy: 0.90625\n",
      "At: 2471 [==========>] Loss 0.09302621255971766  - accuracy: 0.9375\n",
      "At: 2472 [==========>] Loss 0.10864188105096402  - accuracy: 0.84375\n",
      "At: 2473 [==========>] Loss 0.10964676685853345  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.07596300636539498  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.12463646015844335  - accuracy: 0.8125\n",
      "At: 2476 [==========>] Loss 0.0773626279541678  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.15119408667903816  - accuracy: 0.78125\n",
      "At: 2478 [==========>] Loss 0.11309778007223674  - accuracy: 0.8125\n",
      "At: 2479 [==========>] Loss 0.08458148566087109  - accuracy: 0.90625\n",
      "At: 2480 [==========>] Loss 0.1260070523592985  - accuracy: 0.78125\n",
      "At: 2481 [==========>] Loss 0.08155874045945627  - accuracy: 0.90625\n",
      "At: 2482 [==========>] Loss 0.16017418950375129  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.1194857382134778  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.10297233003643902  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.09763540533403635  - accuracy: 0.90625\n",
      "At: 2486 [==========>] Loss 0.1269084114315291  - accuracy: 0.8125\n",
      "At: 2487 [==========>] Loss 0.13773229971436976  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.1803769311935826  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.21666616696413166  - accuracy: 0.65625\n",
      "At: 2490 [==========>] Loss 0.10326337465105928  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.14223424505406734  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.13546284997844615  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.09436309444787527  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.11150043216965715  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.10347842372573007  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.07228699915986538  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.2418844029460093  - accuracy: 0.625\n",
      "At: 2498 [==========>] Loss 0.11163572791538834  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.0540430831392303  - accuracy: 0.96875\n",
      "At: 2500 [==========>] Loss 0.18180923325121123  - accuracy: 0.71875\n",
      "At: 2501 [==========>] Loss 0.14717255720587535  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.10561381389459784  - accuracy: 0.90625\n",
      "At: 2503 [==========>] Loss 0.11881325852541313  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.14867097919847155  - accuracy: 0.84375\n",
      "At: 2505 [==========>] Loss 0.11252346088879266  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.12610630924998442  - accuracy: 0.8125\n",
      "At: 2507 [==========>] Loss 0.13435176033210444  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.11320878657954658  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.149901824025239  - accuracy: 0.75\n",
      "At: 2510 [==========>] Loss 0.12774989496672273  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14470533375421266  - accuracy: 0.8125\n",
      "At: 2512 [==========>] Loss 0.10291672000369899  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.14941457392648608  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.12201019907207314  - accuracy: 0.8125\n",
      "At: 2515 [==========>] Loss 0.1837036747293693  - accuracy: 0.8125\n",
      "At: 2516 [==========>] Loss 0.17150889962088778  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.10577392056767462  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.12320988121882959  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.12837607850670407  - accuracy: 0.78125\n",
      "At: 2520 [==========>] Loss 0.1305036100363436  - accuracy: 0.8125\n",
      "At: 2521 [==========>] Loss 0.1003779040518875  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.21958916094539327  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.11812228920027606  - accuracy: 0.78125\n",
      "At: 2524 [==========>] Loss 0.16268962335423648  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.06724166103306703  - accuracy: 0.9375\n",
      "At: 2526 [==========>] Loss 0.1175590953151541  - accuracy: 0.8125\n",
      "At: 2527 [==========>] Loss 0.109055209782724  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.08663444367756884  - accuracy: 0.875\n",
      "At: 2529 [==========>] Loss 0.14134627005447306  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.14498835846709657  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.04818709026334995  - accuracy: 0.9375\n",
      "At: 2532 [==========>] Loss 0.11856634519839873  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.10514304456369292  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.07918660869513874  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.08381454576303937  - accuracy: 0.875\n",
      "At: 2536 [==========>] Loss 0.16881559926257558  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.11309347453096148  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.14603581080129924  - accuracy: 0.75\n",
      "At: 2539 [==========>] Loss 0.09069389264757283  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.13350381818238094  - accuracy: 0.8125\n",
      "At: 2541 [==========>] Loss 0.0791507698641473  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.052731620754484225  - accuracy: 0.96875\n",
      "At: 2543 [==========>] Loss 0.16008657915620125  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.14267169429057203  - accuracy: 0.75\n",
      "At: 2545 [==========>] Loss 0.06792332000379708  - accuracy: 0.96875\n",
      "At: 2546 [==========>] Loss 0.15288663996790164  - accuracy: 0.78125\n",
      "At: 2547 [==========>] Loss 0.09145974531540707  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.08136409893364627  - accuracy: 0.875\n",
      "At: 2549 [==========>] Loss 0.08958995884007213  - accuracy: 0.90625\n",
      "At: 2550 [==========>] Loss 0.14672719302857024  - accuracy: 0.75\n",
      "At: 2551 [==========>] Loss 0.1337670028583544  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.1101187602263407  - accuracy: 0.84375\n",
      "At: 2553 [==========>] Loss 0.091507056895684  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.1361074572686358  - accuracy: 0.84375\n",
      "At: 2555 [==========>] Loss 0.1789266800102809  - accuracy: 0.71875\n",
      "At: 2556 [==========>] Loss 0.07606995225477525  - accuracy: 0.9375\n",
      "At: 2557 [==========>] Loss 0.12020949431120828  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.11123278436044916  - accuracy: 0.8125\n",
      "At: 2559 [==========>] Loss 0.09159891133769704  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.1732622179969431  - accuracy: 0.75\n",
      "At: 2561 [==========>] Loss 0.08202556870357935  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.11401828265586592  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.13489849522993  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.1068055260276855  - accuracy: 0.84375\n",
      "At: 2565 [==========>] Loss 0.1301021235181347  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.17635911376476782  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.12011217913314418  - accuracy: 0.8125\n",
      "At: 2568 [==========>] Loss 0.10505227759199856  - accuracy: 0.84375\n",
      "At: 2569 [==========>] Loss 0.07443233519676426  - accuracy: 0.90625\n",
      "At: 2570 [==========>] Loss 0.186895976761415  - accuracy: 0.71875\n",
      "At: 2571 [==========>] Loss 0.08036338753572969  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.17394229491484908  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.19668124746646826  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.1307010614448022  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.06865230897563551  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.09679205812849247  - accuracy: 0.90625\n",
      "At: 2577 [==========>] Loss 0.21281471791333512  - accuracy: 0.75\n",
      "At: 2578 [==========>] Loss 0.18022080387517733  - accuracy: 0.75\n",
      "At: 2579 [==========>] Loss 0.18157413728167304  - accuracy: 0.625\n",
      "At: 2580 [==========>] Loss 0.14920451584208966  - accuracy: 0.71875\n",
      "At: 2581 [==========>] Loss 0.07634377754513891  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.19783856276891365  - accuracy: 0.8125\n",
      "At: 2583 [==========>] Loss 0.08476761045570207  - accuracy: 0.84375\n",
      "At: 2584 [==========>] Loss 0.1874797833060813  - accuracy: 0.65625\n",
      "At: 2585 [==========>] Loss 0.09999651318428814  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.09921518324487502  - accuracy: 0.8125\n",
      "At: 2587 [==========>] Loss 0.1566156788987012  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.1382161250680822  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.11215720291763207  - accuracy: 0.78125\n",
      "At: 2590 [==========>] Loss 0.19130227412908435  - accuracy: 0.75\n",
      "At: 2591 [==========>] Loss 0.12526399976540137  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.11956656410173025  - accuracy: 0.8125\n",
      "At: 2593 [==========>] Loss 0.08485329808887783  - accuracy: 0.90625\n",
      "At: 2594 [==========>] Loss 0.08253722211417076  - accuracy: 0.9375\n",
      "At: 2595 [==========>] Loss 0.08780304159381865  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.1192768472670494  - accuracy: 0.8125\n",
      "At: 2597 [==========>] Loss 0.06729463351121859  - accuracy: 0.9375\n",
      "At: 2598 [==========>] Loss 0.11912694876845027  - accuracy: 0.8125\n",
      "At: 2599 [==========>] Loss 0.14427489542311625  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.14862105519030183  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.15726787429515177  - accuracy: 0.78125\n",
      "At: 2602 [==========>] Loss 0.0833870729575049  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.15420062989769456  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.10606173679181485  - accuracy: 0.84375\n",
      "At: 2605 [==========>] Loss 0.15106216815637855  - accuracy: 0.78125\n",
      "At: 2606 [==========>] Loss 0.14212656406695173  - accuracy: 0.84375\n",
      "At: 2607 [==========>] Loss 0.12865535107637932  - accuracy: 0.875\n",
      "At: 2608 [==========>] Loss 0.07288165152349285  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.10017170395889854  - accuracy: 0.84375\n",
      "At: 2610 [==========>] Loss 0.14711192813099755  - accuracy: 0.71875\n",
      "At: 2611 [==========>] Loss 0.12697828415048662  - accuracy: 0.84375\n",
      "At: 2612 [==========>] Loss 0.1202790738223757  - accuracy: 0.78125\n",
      "At: 2613 [==========>] Loss 0.09894160378380706  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.11476088818660295  - accuracy: 0.84375\n",
      "At: 2615 [==========>] Loss 0.0893503413417445  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.0700776238624506  - accuracy: 0.875\n",
      "At: 2617 [==========>] Loss 0.09264100630504019  - accuracy: 0.875\n",
      "At: 2618 [==========>] Loss 0.07644031433882678  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.10343353761163873  - accuracy: 0.90625\n",
      "At: 2620 [==========>] Loss 0.10823129771490522  - accuracy: 0.875\n",
      "At: 2621 [==========>] Loss 0.1065157827258836  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.13168196433232135  - accuracy: 0.78125\n",
      "At: 2623 [==========>] Loss 0.08024153223124365  - accuracy: 0.9375\n",
      "At: 2624 [==========>] Loss 0.1470961354464016  - accuracy: 0.78125\n",
      "At: 2625 [==========>] Loss 0.06664865441124831  - accuracy: 0.96875\n",
      "At: 2626 [==========>] Loss 0.07076425158301222  - accuracy: 0.84375\n",
      "At: 2627 [==========>] Loss 0.12835469191716786  - accuracy: 0.84375\n",
      "At: 2628 [==========>] Loss 0.08153989528061717  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.10272658064937  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.09935336697054795  - accuracy: 0.90625\n",
      "At: 2631 [==========>] Loss 0.13833049547559417  - accuracy: 0.75\n",
      "At: 2632 [==========>] Loss 0.14861921292927036  - accuracy: 0.75\n",
      "At: 2633 [==========>] Loss 0.09248806801664876  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.0848247675515205  - accuracy: 0.84375\n",
      "At: 2635 [==========>] Loss 0.2159990021574998  - accuracy: 0.65625\n",
      "At: 2636 [==========>] Loss 0.13519919430744104  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.14515244992765905  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.08585067405446997  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.0692895450037795  - accuracy: 0.9375\n",
      "At: 2640 [==========>] Loss 0.10393535387345529  - accuracy: 0.84375\n",
      "At: 2641 [==========>] Loss 0.16558672563356347  - accuracy: 0.78125\n",
      "At: 2642 [==========>] Loss 0.16201443024000942  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.0817362411121122  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.1456401640239598  - accuracy: 0.8125\n",
      "At: 2645 [==========>] Loss 0.06768163529775874  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.15247982389188683  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.11310492850919254  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.15950175550574597  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.09844629206997091  - accuracy: 0.875\n",
      "At: 2650 [==========>] Loss 0.1280472385116447  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.1830662288917732  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.10797129867823928  - accuracy: 0.875\n",
      "At: 2653 [==========>] Loss 0.10027272750735253  - accuracy: 0.875\n",
      "At: 2654 [==========>] Loss 0.11585972385924961  - accuracy: 0.84375\n",
      "At: 2655 [==========>] Loss 0.2504227410034999  - accuracy: 0.65625\n",
      "At: 2656 [==========>] Loss 0.023758102925208257  - accuracy: 0.96875\n",
      "At: 2657 [==========>] Loss 0.08934090383584098  - accuracy: 0.90625\n",
      "At: 2658 [==========>] Loss 0.07754207816832093  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.07437695458318357  - accuracy: 0.875\n",
      "At: 2660 [==========>] Loss 0.10947215021265616  - accuracy: 0.90625\n",
      "At: 2661 [==========>] Loss 0.07233564999693612  - accuracy: 0.9375\n",
      "At: 2662 [==========>] Loss 0.07311888340939536  - accuracy: 0.96875\n",
      "At: 2663 [==========>] Loss 0.09081972865867521  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.0875563779233407  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.08973763133746968  - accuracy: 0.90625\n",
      "At: 2666 [==========>] Loss 0.12969490777115575  - accuracy: 0.84375\n",
      "At: 2667 [==========>] Loss 0.12859610255701992  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.170876929954538  - accuracy: 0.6875\n",
      "At: 2669 [==========>] Loss 0.14906436389866273  - accuracy: 0.75\n",
      "At: 2670 [==========>] Loss 0.12260111669913361  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.09417486202322299  - accuracy: 0.84375\n",
      "At: 2672 [==========>] Loss 0.10177230839006525  - accuracy: 0.84375\n",
      "At: 2673 [==========>] Loss 0.13475076012492113  - accuracy: 0.8125\n",
      "At: 2674 [==========>] Loss 0.10966464418415814  - accuracy: 0.875\n",
      "At: 2675 [==========>] Loss 0.13103783067507724  - accuracy: 0.84375\n",
      "At: 2676 [==========>] Loss 0.10766271653350833  - accuracy: 0.875\n",
      "At: 2677 [==========>] Loss 0.1053249038076886  - accuracy: 0.8125\n",
      "At: 2678 [==========>] Loss 0.04327743969662261  - accuracy: 0.96875\n",
      "At: 2679 [==========>] Loss 0.05269452709203435  - accuracy: 0.96875\n",
      "At: 2680 [==========>] Loss 0.09357113684969418  - accuracy: 0.84375\n",
      "At: 2681 [==========>] Loss 0.08137716025375652  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.13148673860769866  - accuracy: 0.8125\n",
      "At: 2683 [==========>] Loss 0.19734291855539132  - accuracy: 0.6875\n",
      "At: 2684 [==========>] Loss 0.09751235082563933  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.11097285231847455  - accuracy: 0.84375\n",
      "At: 2686 [==========>] Loss 0.061924437994544  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.1288513192554362  - accuracy: 0.875\n",
      "At: 2688 [==========>] Loss 0.1829228888292958  - accuracy: 0.78125\n",
      "At: 2689 [==========>] Loss 0.11180978605928273  - accuracy: 0.90625\n",
      "At: 2690 [==========>] Loss 0.1490012722253331  - accuracy: 0.71875\n",
      "Epochs  7 / 10\n",
      "At: 1 [==========>] Loss 0.1563155277573206  - accuracy: 0.8125\n",
      "At: 2 [==========>] Loss 0.23138090554819568  - accuracy: 0.6875\n",
      "At: 3 [==========>] Loss 0.14888270814991234  - accuracy: 0.8125\n",
      "At: 4 [==========>] Loss 0.19923408791312414  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.1116934983494714  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.12758629521079706  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.20131692970376092  - accuracy: 0.71875\n",
      "At: 8 [==========>] Loss 0.23652030624763698  - accuracy: 0.6875\n",
      "At: 9 [==========>] Loss 0.3401628007026951  - accuracy: 0.59375\n",
      "At: 10 [==========>] Loss 0.23477176203652417  - accuracy: 0.78125\n",
      "At: 11 [==========>] Loss 0.22749638701288688  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.2040727251105113  - accuracy: 0.78125\n",
      "At: 13 [==========>] Loss 0.19994021336897677  - accuracy: 0.75\n",
      "At: 14 [==========>] Loss 0.13528495594151368  - accuracy: 0.84375\n",
      "At: 15 [==========>] Loss 0.1708325471549678  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.2553642207819156  - accuracy: 0.6875\n",
      "At: 17 [==========>] Loss 0.17511156350587861  - accuracy: 0.75\n",
      "At: 18 [==========>] Loss 0.24785971914682342  - accuracy: 0.71875\n",
      "At: 19 [==========>] Loss 0.2141959076354008  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.14539121108846081  - accuracy: 0.75\n",
      "At: 21 [==========>] Loss 0.2489570596566138  - accuracy: 0.71875\n",
      "At: 22 [==========>] Loss 0.1940427035191889  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.149786322424895  - accuracy: 0.78125\n",
      "At: 24 [==========>] Loss 0.20421905379632677  - accuracy: 0.8125\n",
      "At: 25 [==========>] Loss 0.23649111704877288  - accuracy: 0.71875\n",
      "At: 26 [==========>] Loss 0.2523546092438278  - accuracy: 0.65625\n",
      "At: 27 [==========>] Loss 0.2150515859461194  - accuracy: 0.75\n",
      "At: 28 [==========>] Loss 0.19815724987899735  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.16277706464634012  - accuracy: 0.84375\n",
      "At: 30 [==========>] Loss 0.2286980951952487  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.301042002936959  - accuracy: 0.625\n",
      "At: 32 [==========>] Loss 0.19686962399278896  - accuracy: 0.8125\n",
      "At: 33 [==========>] Loss 0.0876726972970554  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.18306557197493895  - accuracy: 0.78125\n",
      "At: 35 [==========>] Loss 0.16513641483032965  - accuracy: 0.78125\n",
      "At: 36 [==========>] Loss 0.23347792189758557  - accuracy: 0.71875\n",
      "At: 37 [==========>] Loss 0.2698649036017843  - accuracy: 0.6875\n",
      "At: 38 [==========>] Loss 0.29704361233904963  - accuracy: 0.625\n",
      "At: 39 [==========>] Loss 0.15348639922822976  - accuracy: 0.8125\n",
      "At: 40 [==========>] Loss 0.26251266601961565  - accuracy: 0.71875\n",
      "At: 41 [==========>] Loss 0.11277985588843613  - accuracy: 0.875\n",
      "At: 42 [==========>] Loss 0.15863645139241817  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.20069476794886082  - accuracy: 0.78125\n",
      "At: 44 [==========>] Loss 0.17097510449799036  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.08264442471180947  - accuracy: 0.9375\n",
      "At: 46 [==========>] Loss 0.20347847267238106  - accuracy: 0.8125\n",
      "At: 47 [==========>] Loss 0.258679704070415  - accuracy: 0.6875\n",
      "At: 48 [==========>] Loss 0.1682947342341561  - accuracy: 0.78125\n",
      "At: 49 [==========>] Loss 0.13692584047794398  - accuracy: 0.8125\n",
      "At: 50 [==========>] Loss 0.19871917565592406  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.21745636025501602  - accuracy: 0.71875\n",
      "At: 52 [==========>] Loss 0.2530227704596989  - accuracy: 0.6875\n",
      "At: 53 [==========>] Loss 0.17209058090743262  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.13661957140570039  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.2366016301296941  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.18046890106023958  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.17641225236899802  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.20554819336331126  - accuracy: 0.65625\n",
      "At: 59 [==========>] Loss 0.2240996905563074  - accuracy: 0.75\n",
      "At: 60 [==========>] Loss 0.2086796022488736  - accuracy: 0.71875\n",
      "At: 61 [==========>] Loss 0.24483412186779174  - accuracy: 0.71875\n",
      "At: 62 [==========>] Loss 0.18775404285434394  - accuracy: 0.78125\n",
      "At: 63 [==========>] Loss 0.2169673685140025  - accuracy: 0.71875\n",
      "At: 64 [==========>] Loss 0.19356874629464177  - accuracy: 0.78125\n",
      "At: 65 [==========>] Loss 0.32589061354601767  - accuracy: 0.625\n",
      "At: 66 [==========>] Loss 0.2548606670021667  - accuracy: 0.71875\n",
      "At: 67 [==========>] Loss 0.2195643017053105  - accuracy: 0.65625\n",
      "At: 68 [==========>] Loss 0.11526427799224864  - accuracy: 0.90625\n",
      "At: 69 [==========>] Loss 0.17497937227425653  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.2516051756548715  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.18538056211403464  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.16197797572670397  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.15783763555028874  - accuracy: 0.78125\n",
      "At: 74 [==========>] Loss 0.20694058075188843  - accuracy: 0.78125\n",
      "At: 75 [==========>] Loss 0.2055898446020938  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.27276718566338914  - accuracy: 0.6875\n",
      "At: 77 [==========>] Loss 0.269200636866566  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.16282406011283804  - accuracy: 0.78125\n",
      "At: 79 [==========>] Loss 0.17287529300579857  - accuracy: 0.8125\n",
      "At: 80 [==========>] Loss 0.2366433864874639  - accuracy: 0.71875\n",
      "At: 81 [==========>] Loss 0.17052607444813178  - accuracy: 0.8125\n",
      "At: 82 [==========>] Loss 0.2606584096708099  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.17076297681950328  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.23816821652153986  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.19623155826057104  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.19843505447654128  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.15771143345268504  - accuracy: 0.84375\n",
      "At: 88 [==========>] Loss 0.35573660642768407  - accuracy: 0.53125\n",
      "At: 89 [==========>] Loss 0.20266057634411339  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.24355397325317646  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.2044445762380029  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.10564991896633488  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.1307090746737949  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.13552673469907375  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.17940512207259035  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.1585104106017141  - accuracy: 0.8125\n",
      "At: 97 [==========>] Loss 0.08695226656048968  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.2873258070408453  - accuracy: 0.625\n",
      "At: 99 [==========>] Loss 0.15520638206837467  - accuracy: 0.78125\n",
      "At: 100 [==========>] Loss 0.15812555598871003  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.17291777830228022  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.15746619174992538  - accuracy: 0.78125\n",
      "At: 103 [==========>] Loss 0.1691887759794517  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.1490712430706388  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.18141085146845082  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.22947625864412205  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.20674514253322734  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.23193090074291162  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.09041047866920399  - accuracy: 0.875\n",
      "At: 110 [==========>] Loss 0.2587747171788304  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.10416131122079482  - accuracy: 0.90625\n",
      "At: 112 [==========>] Loss 0.1835385735101019  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.20968329949411796  - accuracy: 0.71875\n",
      "At: 114 [==========>] Loss 0.19143829000876716  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.19791422712606152  - accuracy: 0.75\n",
      "At: 116 [==========>] Loss 0.20216394426340334  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.185992968077559  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.25091655325552  - accuracy: 0.6875\n",
      "At: 119 [==========>] Loss 0.11262171095981871  - accuracy: 0.875\n",
      "At: 120 [==========>] Loss 0.17425459471633342  - accuracy: 0.75\n",
      "At: 121 [==========>] Loss 0.16587909845192927  - accuracy: 0.75\n",
      "At: 122 [==========>] Loss 0.19466922050195248  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.22288747280726007  - accuracy: 0.75\n",
      "At: 124 [==========>] Loss 0.2602579193376422  - accuracy: 0.59375\n",
      "At: 125 [==========>] Loss 0.19814096897379802  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.20118047382476906  - accuracy: 0.75\n",
      "At: 127 [==========>] Loss 0.19618036242957798  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.2846169956698993  - accuracy: 0.625\n",
      "At: 129 [==========>] Loss 0.13292276700571604  - accuracy: 0.84375\n",
      "At: 130 [==========>] Loss 0.23665537543208476  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.1980329807400886  - accuracy: 0.71875\n",
      "At: 132 [==========>] Loss 0.24548096180431528  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.20936351729164007  - accuracy: 0.75\n",
      "At: 134 [==========>] Loss 0.15743667590115107  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.18602471534275744  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.17790531778614388  - accuracy: 0.8125\n",
      "At: 137 [==========>] Loss 0.08404483505944166  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.17735494652796815  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.13357954027746027  - accuracy: 0.84375\n",
      "At: 140 [==========>] Loss 0.15215196415155785  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.2825481374555746  - accuracy: 0.625\n",
      "At: 142 [==========>] Loss 0.20914414019318162  - accuracy: 0.71875\n",
      "At: 143 [==========>] Loss 0.2176297513445722  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.13007475716500141  - accuracy: 0.875\n",
      "At: 145 [==========>] Loss 0.11981275244308484  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.127300553633723  - accuracy: 0.8125\n",
      "At: 147 [==========>] Loss 0.19566100882640136  - accuracy: 0.78125\n",
      "At: 148 [==========>] Loss 0.1329730806638154  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.21345673528569684  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.17619140922215104  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.18092030654963448  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.1749648629418378  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.18921358688053758  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.22853598481020518  - accuracy: 0.71875\n",
      "At: 155 [==========>] Loss 0.18400435151781597  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.14756360269536378  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.21920053368932838  - accuracy: 0.75\n",
      "At: 158 [==========>] Loss 0.1492399630659337  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.1599778210193839  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.14718945796831062  - accuracy: 0.8125\n",
      "At: 161 [==========>] Loss 0.13336628750229418  - accuracy: 0.8125\n",
      "At: 162 [==========>] Loss 0.1924671891011454  - accuracy: 0.75\n",
      "At: 163 [==========>] Loss 0.1811067910436298  - accuracy: 0.8125\n",
      "At: 164 [==========>] Loss 0.18707176705959994  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.24015674293080128  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.2207925643573892  - accuracy: 0.65625\n",
      "At: 167 [==========>] Loss 0.14533220725592988  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.22972364325185937  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.17147620902437896  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.1623058115711559  - accuracy: 0.75\n",
      "At: 171 [==========>] Loss 0.23033043084991928  - accuracy: 0.71875\n",
      "At: 172 [==========>] Loss 0.19616136776728565  - accuracy: 0.71875\n",
      "At: 173 [==========>] Loss 0.2304511668930458  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.2058858567661057  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.17469697156166003  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.22106894546959419  - accuracy: 0.6875\n",
      "At: 177 [==========>] Loss 0.11754806831915982  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.19715115223616486  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.11074334100600346  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.1608582862475113  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.07760847314249325  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.1931087433046904  - accuracy: 0.78125\n",
      "At: 183 [==========>] Loss 0.18088033198829326  - accuracy: 0.78125\n",
      "At: 184 [==========>] Loss 0.19246130263565506  - accuracy: 0.8125\n",
      "At: 185 [==========>] Loss 0.12133043579284075  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.15427222900649845  - accuracy: 0.75\n",
      "At: 187 [==========>] Loss 0.1120539992678798  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.23141640227623878  - accuracy: 0.71875\n",
      "At: 189 [==========>] Loss 0.19872987921786878  - accuracy: 0.71875\n",
      "At: 190 [==========>] Loss 0.13185273813959508  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.3120376273137279  - accuracy: 0.5625\n",
      "At: 192 [==========>] Loss 0.15550231024198458  - accuracy: 0.8125\n",
      "At: 193 [==========>] Loss 0.2456579728401836  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.21643341433745494  - accuracy: 0.75\n",
      "At: 195 [==========>] Loss 0.1847323860878425  - accuracy: 0.75\n",
      "At: 196 [==========>] Loss 0.21009878329956705  - accuracy: 0.71875\n",
      "At: 197 [==========>] Loss 0.16640934531722412  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.14171459780592127  - accuracy: 0.8125\n",
      "At: 199 [==========>] Loss 0.11122569772383795  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.20133197453796856  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.12393338427990848  - accuracy: 0.875\n",
      "At: 202 [==========>] Loss 0.11873415551388047  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.1912617586655984  - accuracy: 0.75\n",
      "At: 204 [==========>] Loss 0.21352395855387765  - accuracy: 0.71875\n",
      "At: 205 [==========>] Loss 0.1955763221028274  - accuracy: 0.75\n",
      "At: 206 [==========>] Loss 0.1180478823290949  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.11419181151855716  - accuracy: 0.875\n",
      "At: 208 [==========>] Loss 0.24180785283663175  - accuracy: 0.65625\n",
      "At: 209 [==========>] Loss 0.18607363220367945  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.11882707162025792  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.1723242780689786  - accuracy: 0.78125\n",
      "At: 212 [==========>] Loss 0.19814086321284619  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.18822794505530943  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.2224567164833659  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.12622145588336783  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.20601580437076383  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.24026690868541467  - accuracy: 0.65625\n",
      "At: 218 [==========>] Loss 0.18577349870913673  - accuracy: 0.8125\n",
      "At: 219 [==========>] Loss 0.20726851571425825  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.229937622186715  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.1815399845638435  - accuracy: 0.75\n",
      "At: 222 [==========>] Loss 0.10098749593429637  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.2500470193588292  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.20829837592768677  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.09603733479463039  - accuracy: 0.875\n",
      "At: 226 [==========>] Loss 0.22176030745782332  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.1779306631457671  - accuracy: 0.78125\n",
      "At: 228 [==========>] Loss 0.2218277127834823  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.18064431158256222  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.1712623221682269  - accuracy: 0.8125\n",
      "At: 231 [==========>] Loss 0.2431202354630085  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.21309082012253688  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.22651076705212103  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.14825369032357696  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.18861724080657521  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.1946033907409925  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.13570412319943254  - accuracy: 0.8125\n",
      "At: 238 [==========>] Loss 0.16693232599145197  - accuracy: 0.78125\n",
      "At: 239 [==========>] Loss 0.16682948262638442  - accuracy: 0.78125\n",
      "At: 240 [==========>] Loss 0.2141241377619591  - accuracy: 0.6875\n",
      "At: 241 [==========>] Loss 0.12464740548596981  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.20069524484559048  - accuracy: 0.75\n",
      "At: 243 [==========>] Loss 0.1353528263429287  - accuracy: 0.8125\n",
      "At: 244 [==========>] Loss 0.1562848929945319  - accuracy: 0.78125\n",
      "At: 245 [==========>] Loss 0.15372054423954407  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.14470635701430665  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.20839473734496633  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.08862520848381523  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.10954216446105969  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.2517390583018376  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.2246785262871593  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.1085320583248991  - accuracy: 0.84375\n",
      "At: 253 [==========>] Loss 0.16442977094464456  - accuracy: 0.8125\n",
      "At: 254 [==========>] Loss 0.09832567909174564  - accuracy: 0.875\n",
      "At: 255 [==========>] Loss 0.19470280955267896  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.21086035223956473  - accuracy: 0.75\n",
      "At: 257 [==========>] Loss 0.1245750232984914  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.15262707691076394  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.20178569455386328  - accuracy: 0.78125\n",
      "At: 260 [==========>] Loss 0.13691550582851447  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.07357687054263945  - accuracy: 0.9375\n",
      "At: 262 [==========>] Loss 0.18176968466322313  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.11231543782583822  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.12437838844000995  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.20848226140744142  - accuracy: 0.71875\n",
      "At: 266 [==========>] Loss 0.23065020652002433  - accuracy: 0.6875\n",
      "At: 267 [==========>] Loss 0.15408461869581158  - accuracy: 0.84375\n",
      "At: 268 [==========>] Loss 0.2099847090823176  - accuracy: 0.71875\n",
      "At: 269 [==========>] Loss 0.12725680744031126  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.22363586349252473  - accuracy: 0.65625\n",
      "At: 271 [==========>] Loss 0.23082558474573092  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.10695764029969815  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.23527467820427622  - accuracy: 0.6875\n",
      "At: 274 [==========>] Loss 0.1960742101610114  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.08305745153066821  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.21475891074663955  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.10471504147303656  - accuracy: 0.90625\n",
      "At: 278 [==========>] Loss 0.14139551949271273  - accuracy: 0.8125\n",
      "At: 279 [==========>] Loss 0.1723164460423492  - accuracy: 0.75\n",
      "At: 280 [==========>] Loss 0.11508972878639911  - accuracy: 0.84375\n",
      "At: 281 [==========>] Loss 0.14462319163858778  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.22518615419040738  - accuracy: 0.65625\n",
      "At: 283 [==========>] Loss 0.13713531473061202  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.09067944706877866  - accuracy: 0.84375\n",
      "At: 285 [==========>] Loss 0.1497263209607465  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.08527180944546756  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.19928065070934714  - accuracy: 0.71875\n",
      "At: 288 [==========>] Loss 0.11345201308698272  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.0797953298528889  - accuracy: 0.9375\n",
      "At: 290 [==========>] Loss 0.13448735216525198  - accuracy: 0.84375\n",
      "At: 291 [==========>] Loss 0.12353883427142008  - accuracy: 0.84375\n",
      "At: 292 [==========>] Loss 0.14912540300659732  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.24692272945807564  - accuracy: 0.65625\n",
      "At: 294 [==========>] Loss 0.19372378961296965  - accuracy: 0.78125\n",
      "At: 295 [==========>] Loss 0.1880872995115876  - accuracy: 0.75\n",
      "At: 296 [==========>] Loss 0.12850882198403096  - accuracy: 0.8125\n",
      "At: 297 [==========>] Loss 0.13518893896035353  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.1786068412780492  - accuracy: 0.78125\n",
      "At: 299 [==========>] Loss 0.21411439197269708  - accuracy: 0.6875\n",
      "At: 300 [==========>] Loss 0.19607880154141072  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.1815892014110059  - accuracy: 0.75\n",
      "At: 302 [==========>] Loss 0.1273832520617957  - accuracy: 0.84375\n",
      "At: 303 [==========>] Loss 0.11091493973274513  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.21597249596833354  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.24051754547197132  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.1354092461532092  - accuracy: 0.78125\n",
      "At: 307 [==========>] Loss 0.25485674378992895  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.21949930781476906  - accuracy: 0.71875\n",
      "At: 309 [==========>] Loss 0.09002839204238355  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.21579711464918439  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.10781248960573692  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.12054620315241024  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.13043023287975541  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.2262167649782633  - accuracy: 0.6875\n",
      "At: 315 [==========>] Loss 0.15623017336253797  - accuracy: 0.8125\n",
      "At: 316 [==========>] Loss 0.19700881948203353  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.2945228978429948  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.1535769506739173  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.10466926566751511  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.22123483002532512  - accuracy: 0.71875\n",
      "At: 321 [==========>] Loss 0.23304991578989037  - accuracy: 0.75\n",
      "At: 322 [==========>] Loss 0.10830804063813163  - accuracy: 0.90625\n",
      "At: 323 [==========>] Loss 0.12072935611969297  - accuracy: 0.84375\n",
      "At: 324 [==========>] Loss 0.19637449364202827  - accuracy: 0.6875\n",
      "At: 325 [==========>] Loss 0.10752361526640611  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.18193100689819233  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.1338609858213576  - accuracy: 0.8125\n",
      "At: 328 [==========>] Loss 0.14354654513984083  - accuracy: 0.8125\n",
      "At: 329 [==========>] Loss 0.15091871912114127  - accuracy: 0.78125\n",
      "At: 330 [==========>] Loss 0.1629688972312831  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.19785873883596078  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.24302242473693197  - accuracy: 0.59375\n",
      "At: 333 [==========>] Loss 0.18398125278854446  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.13019576258538265  - accuracy: 0.8125\n",
      "At: 335 [==========>] Loss 0.12975963419192982  - accuracy: 0.84375\n",
      "At: 336 [==========>] Loss 0.1516114423761935  - accuracy: 0.8125\n",
      "At: 337 [==========>] Loss 0.18479404714624165  - accuracy: 0.78125\n",
      "At: 338 [==========>] Loss 0.15211783869630158  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.1895286244262473  - accuracy: 0.78125\n",
      "At: 340 [==========>] Loss 0.1658286708794598  - accuracy: 0.8125\n",
      "At: 341 [==========>] Loss 0.0981996745134116  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.13785954062206565  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.25263518284843645  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.17116986298626008  - accuracy: 0.78125\n",
      "At: 345 [==========>] Loss 0.19381233231412867  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.1631603186649064  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.10738806464548202  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.10923191832120255  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.19515567082775126  - accuracy: 0.6875\n",
      "At: 350 [==========>] Loss 0.10806501917612013  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.24851201925382613  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.15172110020712995  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.15911957215117983  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.2097284799166414  - accuracy: 0.6875\n",
      "At: 355 [==========>] Loss 0.11024718713217323  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.16863968132290058  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.14817632463713226  - accuracy: 0.8125\n",
      "At: 358 [==========>] Loss 0.14904113118169557  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.08802938618958459  - accuracy: 0.875\n",
      "At: 360 [==========>] Loss 0.1679947022304787  - accuracy: 0.75\n",
      "At: 361 [==========>] Loss 0.10723060182671709  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.18219467116452676  - accuracy: 0.71875\n",
      "At: 363 [==========>] Loss 0.10821125481007195  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.19571177232663844  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.14191035393593893  - accuracy: 0.8125\n",
      "At: 366 [==========>] Loss 0.17279440221105524  - accuracy: 0.8125\n",
      "At: 367 [==========>] Loss 0.176040692470623  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.21109906873308187  - accuracy: 0.75\n",
      "At: 369 [==========>] Loss 0.1801531955201715  - accuracy: 0.75\n",
      "At: 370 [==========>] Loss 0.1557906737557942  - accuracy: 0.71875\n",
      "At: 371 [==========>] Loss 0.07440453843256714  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.11538364013772387  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.2457207978808127  - accuracy: 0.65625\n",
      "At: 374 [==========>] Loss 0.0908428178289308  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1625479526010874  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.10563095690829241  - accuracy: 0.84375\n",
      "At: 377 [==========>] Loss 0.182373911795106  - accuracy: 0.71875\n",
      "At: 378 [==========>] Loss 0.1988425088452195  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.12383002689694655  - accuracy: 0.84375\n",
      "At: 380 [==========>] Loss 0.13939271908039058  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.18169222426818227  - accuracy: 0.71875\n",
      "At: 382 [==========>] Loss 0.10259007856659619  - accuracy: 0.8125\n",
      "At: 383 [==========>] Loss 0.23557364978580667  - accuracy: 0.71875\n",
      "At: 384 [==========>] Loss 0.19274296094401677  - accuracy: 0.75\n",
      "At: 385 [==========>] Loss 0.1202761411055733  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.20812918446823278  - accuracy: 0.65625\n",
      "At: 387 [==========>] Loss 0.07443202690619907  - accuracy: 0.875\n",
      "At: 388 [==========>] Loss 0.20499673081206032  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.19936598830916546  - accuracy: 0.71875\n",
      "At: 390 [==========>] Loss 0.114966358690524  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.14645182233199383  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.20036498169530031  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.22010431815305936  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.10231095973557848  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.18970319055434068  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.15720141446193264  - accuracy: 0.8125\n",
      "At: 397 [==========>] Loss 0.13748854185847204  - accuracy: 0.78125\n",
      "At: 398 [==========>] Loss 0.204904257422112  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.17994468289154397  - accuracy: 0.78125\n",
      "At: 400 [==========>] Loss 0.20120902921810152  - accuracy: 0.6875\n",
      "At: 401 [==========>] Loss 0.13110356524944755  - accuracy: 0.8125\n",
      "At: 402 [==========>] Loss 0.11798637411956914  - accuracy: 0.84375\n",
      "At: 403 [==========>] Loss 0.04653388286414657  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.1071724630591188  - accuracy: 0.90625\n",
      "At: 405 [==========>] Loss 0.20468173119221783  - accuracy: 0.71875\n",
      "At: 406 [==========>] Loss 0.19636551340249267  - accuracy: 0.71875\n",
      "At: 407 [==========>] Loss 0.14338194262202025  - accuracy: 0.78125\n",
      "At: 408 [==========>] Loss 0.21713926760276145  - accuracy: 0.75\n",
      "At: 409 [==========>] Loss 0.22232126021576792  - accuracy: 0.6875\n",
      "At: 410 [==========>] Loss 0.16156107742326734  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.10372386667327763  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.20139586305877868  - accuracy: 0.6875\n",
      "At: 413 [==========>] Loss 0.11697709090801721  - accuracy: 0.84375\n",
      "At: 414 [==========>] Loss 0.19224402614681663  - accuracy: 0.71875\n",
      "At: 415 [==========>] Loss 0.17624968520957124  - accuracy: 0.78125\n",
      "At: 416 [==========>] Loss 0.20289561898237232  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.16565719729395592  - accuracy: 0.8125\n",
      "At: 418 [==========>] Loss 0.15355846160873055  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.13774492509908168  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.21103333241532518  - accuracy: 0.6875\n",
      "At: 421 [==========>] Loss 0.12367840511482707  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.13628316490450904  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.11432445031642463  - accuracy: 0.84375\n",
      "At: 424 [==========>] Loss 0.21124831187133286  - accuracy: 0.71875\n",
      "At: 425 [==========>] Loss 0.24001175800959745  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.15995808835365932  - accuracy: 0.78125\n",
      "At: 427 [==========>] Loss 0.1921383996595514  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.2238308454023029  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.20327611063420134  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.10887859265492887  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.13819572527704438  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.15894040364431486  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.12516879211602638  - accuracy: 0.8125\n",
      "At: 434 [==========>] Loss 0.13103577087269747  - accuracy: 0.875\n",
      "At: 435 [==========>] Loss 0.18402249906918283  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.11763022779215915  - accuracy: 0.8125\n",
      "At: 437 [==========>] Loss 0.1607914732320771  - accuracy: 0.78125\n",
      "At: 438 [==========>] Loss 0.16878333921613742  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.11866143895709813  - accuracy: 0.84375\n",
      "At: 440 [==========>] Loss 0.09697695602519316  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.19942792481803356  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.1799661246863365  - accuracy: 0.75\n",
      "At: 443 [==========>] Loss 0.13586382848598513  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.17114589399268942  - accuracy: 0.8125\n",
      "At: 445 [==========>] Loss 0.1341635773721463  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.24598049561425436  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.15997566219106765  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.17109474491351054  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.10341583574581067  - accuracy: 0.84375\n",
      "At: 450 [==========>] Loss 0.16673595477429176  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.1498093386083223  - accuracy: 0.78125\n",
      "At: 452 [==========>] Loss 0.1368261934866265  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.14044531358174944  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.22391893091874493  - accuracy: 0.71875\n",
      "At: 455 [==========>] Loss 0.18029290637708167  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.15080611416945017  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.16131699190660842  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.13608633599230796  - accuracy: 0.8125\n",
      "At: 459 [==========>] Loss 0.21547332135131622  - accuracy: 0.65625\n",
      "At: 460 [==========>] Loss 0.09357586565617101  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.23217162079665993  - accuracy: 0.625\n",
      "At: 462 [==========>] Loss 0.14884142583010157  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.13503023483424098  - accuracy: 0.84375\n",
      "At: 464 [==========>] Loss 0.163316499079197  - accuracy: 0.71875\n",
      "At: 465 [==========>] Loss 0.21702674017383694  - accuracy: 0.71875\n",
      "At: 466 [==========>] Loss 0.1067386521677984  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.17998877953068013  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.10682119530101075  - accuracy: 0.875\n",
      "At: 469 [==========>] Loss 0.1478100053051923  - accuracy: 0.75\n",
      "At: 470 [==========>] Loss 0.12055578301851541  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.18483500174615877  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.11609888777356805  - accuracy: 0.78125\n",
      "At: 473 [==========>] Loss 0.1554467400237024  - accuracy: 0.78125\n",
      "At: 474 [==========>] Loss 0.1531033145994748  - accuracy: 0.84375\n",
      "At: 475 [==========>] Loss 0.14468902168874154  - accuracy: 0.8125\n",
      "At: 476 [==========>] Loss 0.16871206298927152  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.14482515766438137  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.12655412741796623  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.14656720347711177  - accuracy: 0.78125\n",
      "At: 480 [==========>] Loss 0.18922222917456477  - accuracy: 0.71875\n",
      "At: 481 [==========>] Loss 0.1327505579592598  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.10808806770517163  - accuracy: 0.84375\n",
      "At: 483 [==========>] Loss 0.13292434054655017  - accuracy: 0.8125\n",
      "At: 484 [==========>] Loss 0.10049183680837534  - accuracy: 0.84375\n",
      "At: 485 [==========>] Loss 0.1402613153733318  - accuracy: 0.78125\n",
      "At: 486 [==========>] Loss 0.20127603435898966  - accuracy: 0.65625\n",
      "At: 487 [==========>] Loss 0.1621480636476449  - accuracy: 0.75\n",
      "At: 488 [==========>] Loss 0.14748199069739926  - accuracy: 0.78125\n",
      "At: 489 [==========>] Loss 0.14074044272764724  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.1675079471738834  - accuracy: 0.75\n",
      "At: 491 [==========>] Loss 0.16260158647369688  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.19179462632463384  - accuracy: 0.71875\n",
      "At: 493 [==========>] Loss 0.13179251519625265  - accuracy: 0.84375\n",
      "At: 494 [==========>] Loss 0.1862362845634068  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.15847836196033363  - accuracy: 0.78125\n",
      "At: 496 [==========>] Loss 0.18240698821635706  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.17824374641098503  - accuracy: 0.75\n",
      "At: 498 [==========>] Loss 0.06951245900049567  - accuracy: 0.9375\n",
      "At: 499 [==========>] Loss 0.18159741429336937  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.1401935059826336  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.16439288307092298  - accuracy: 0.75\n",
      "At: 502 [==========>] Loss 0.1384936517073902  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.11240072151526215  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.10782429352512563  - accuracy: 0.90625\n",
      "At: 505 [==========>] Loss 0.18587367462424342  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.24898243004897724  - accuracy: 0.59375\n",
      "At: 507 [==========>] Loss 0.12195481620821975  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.10211240064209978  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.1851482432236033  - accuracy: 0.71875\n",
      "At: 510 [==========>] Loss 0.21021370403862435  - accuracy: 0.65625\n",
      "At: 511 [==========>] Loss 0.14367248909877137  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.15456202406371602  - accuracy: 0.75\n",
      "At: 513 [==========>] Loss 0.20065723457121645  - accuracy: 0.75\n",
      "At: 514 [==========>] Loss 0.1874526129540478  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.12592101524664046  - accuracy: 0.8125\n",
      "At: 516 [==========>] Loss 0.15229219888503837  - accuracy: 0.78125\n",
      "At: 517 [==========>] Loss 0.1460171642801469  - accuracy: 0.75\n",
      "At: 518 [==========>] Loss 0.15598308453101134  - accuracy: 0.84375\n",
      "At: 519 [==========>] Loss 0.1235848414091731  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.1390210381435089  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.14010823707636175  - accuracy: 0.8125\n",
      "At: 522 [==========>] Loss 0.14002407833327798  - accuracy: 0.78125\n",
      "At: 523 [==========>] Loss 0.193047146119812  - accuracy: 0.65625\n",
      "At: 524 [==========>] Loss 0.10741361767205426  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.1434252133783777  - accuracy: 0.8125\n",
      "At: 526 [==========>] Loss 0.15951944835293383  - accuracy: 0.75\n",
      "At: 527 [==========>] Loss 0.24023281011033565  - accuracy: 0.5625\n",
      "At: 528 [==========>] Loss 0.1889986663731068  - accuracy: 0.78125\n",
      "At: 529 [==========>] Loss 0.10936992387274672  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.19762471249145142  - accuracy: 0.71875\n",
      "At: 531 [==========>] Loss 0.16340679613791587  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.11641283879930198  - accuracy: 0.84375\n",
      "At: 533 [==========>] Loss 0.09377888217142194  - accuracy: 0.875\n",
      "At: 534 [==========>] Loss 0.17578986317146728  - accuracy: 0.8125\n",
      "At: 535 [==========>] Loss 0.10930930846551368  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.18454535023694812  - accuracy: 0.71875\n",
      "At: 537 [==========>] Loss 0.1071109360022525  - accuracy: 0.8125\n",
      "At: 538 [==========>] Loss 0.17629872025968002  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.12001534003100953  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.22602784221191546  - accuracy: 0.65625\n",
      "At: 541 [==========>] Loss 0.18913018728757153  - accuracy: 0.78125\n",
      "At: 542 [==========>] Loss 0.15563918247353098  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.17556061852180585  - accuracy: 0.75\n",
      "At: 544 [==========>] Loss 0.23095504936707975  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.08397189665932717  - accuracy: 0.90625\n",
      "At: 546 [==========>] Loss 0.1561999545439674  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.12950607264439895  - accuracy: 0.8125\n",
      "At: 548 [==========>] Loss 0.10547029262055142  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.13435088410943147  - accuracy: 0.78125\n",
      "At: 550 [==========>] Loss 0.12371019266089113  - accuracy: 0.84375\n",
      "At: 551 [==========>] Loss 0.12639255795790622  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.14577494616635517  - accuracy: 0.75\n",
      "At: 553 [==========>] Loss 0.11858574426182189  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.09259992061804431  - accuracy: 0.84375\n",
      "At: 555 [==========>] Loss 0.17953995057177147  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.14691566196058514  - accuracy: 0.8125\n",
      "At: 557 [==========>] Loss 0.15639356759506984  - accuracy: 0.75\n",
      "At: 558 [==========>] Loss 0.1502991094902984  - accuracy: 0.78125\n",
      "At: 559 [==========>] Loss 0.19604245931866626  - accuracy: 0.78125\n",
      "At: 560 [==========>] Loss 0.15058307364250054  - accuracy: 0.78125\n",
      "At: 561 [==========>] Loss 0.1341097173249478  - accuracy: 0.8125\n",
      "At: 562 [==========>] Loss 0.07616032755744272  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.11763049656882524  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.17940366669254834  - accuracy: 0.71875\n",
      "At: 565 [==========>] Loss 0.12523410038005695  - accuracy: 0.84375\n",
      "At: 566 [==========>] Loss 0.16990143171493743  - accuracy: 0.6875\n",
      "At: 567 [==========>] Loss 0.2012986880526017  - accuracy: 0.6875\n",
      "At: 568 [==========>] Loss 0.234445350143846  - accuracy: 0.59375\n",
      "At: 569 [==========>] Loss 0.14332111305223139  - accuracy: 0.84375\n",
      "At: 570 [==========>] Loss 0.09634872180945056  - accuracy: 0.84375\n",
      "At: 571 [==========>] Loss 0.13776400487402723  - accuracy: 0.84375\n",
      "At: 572 [==========>] Loss 0.13685968581430602  - accuracy: 0.78125\n",
      "At: 573 [==========>] Loss 0.0895844249418781  - accuracy: 0.9375\n",
      "At: 574 [==========>] Loss 0.1813762748567357  - accuracy: 0.71875\n",
      "At: 575 [==========>] Loss 0.15584110836646628  - accuracy: 0.78125\n",
      "At: 576 [==========>] Loss 0.08639876952932993  - accuracy: 0.9375\n",
      "At: 577 [==========>] Loss 0.1928863990075336  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.21809692703610672  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.11607081741394723  - accuracy: 0.875\n",
      "At: 580 [==========>] Loss 0.14749791517409533  - accuracy: 0.8125\n",
      "At: 581 [==========>] Loss 0.1526601221876473  - accuracy: 0.78125\n",
      "At: 582 [==========>] Loss 0.16813602764287153  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.19373277074243356  - accuracy: 0.71875\n",
      "At: 584 [==========>] Loss 0.10050019594401635  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.18033648487564502  - accuracy: 0.75\n",
      "At: 586 [==========>] Loss 0.07972452631492344  - accuracy: 0.9375\n",
      "At: 587 [==========>] Loss 0.15379067663665014  - accuracy: 0.78125\n",
      "At: 588 [==========>] Loss 0.13485667163332232  - accuracy: 0.75\n",
      "At: 589 [==========>] Loss 0.16583593467012742  - accuracy: 0.8125\n",
      "At: 590 [==========>] Loss 0.06539961409604247  - accuracy: 0.96875\n",
      "At: 591 [==========>] Loss 0.13776813678666633  - accuracy: 0.875\n",
      "At: 592 [==========>] Loss 0.08134415047021239  - accuracy: 0.90625\n",
      "At: 593 [==========>] Loss 0.20086514076491505  - accuracy: 0.75\n",
      "At: 594 [==========>] Loss 0.17772461747276105  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.1135617513901384  - accuracy: 0.8125\n",
      "At: 596 [==========>] Loss 0.11854615022486062  - accuracy: 0.8125\n",
      "At: 597 [==========>] Loss 0.20203731465919456  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.11815243440094494  - accuracy: 0.875\n",
      "At: 599 [==========>] Loss 0.0993463382065854  - accuracy: 0.90625\n",
      "At: 600 [==========>] Loss 0.09767066188763474  - accuracy: 0.84375\n",
      "At: 601 [==========>] Loss 0.14603938287655238  - accuracy: 0.78125\n",
      "At: 602 [==========>] Loss 0.17851045518394337  - accuracy: 0.78125\n",
      "At: 603 [==========>] Loss 0.1796577981218416  - accuracy: 0.71875\n",
      "At: 604 [==========>] Loss 0.22763436690871774  - accuracy: 0.6875\n",
      "At: 605 [==========>] Loss 0.12137993089964393  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.16671037218865703  - accuracy: 0.71875\n",
      "At: 607 [==========>] Loss 0.15391338682935135  - accuracy: 0.8125\n",
      "At: 608 [==========>] Loss 0.12751582212949325  - accuracy: 0.84375\n",
      "At: 609 [==========>] Loss 0.09498842733664711  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.16187985221961232  - accuracy: 0.71875\n",
      "At: 611 [==========>] Loss 0.10857833887408405  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.156303525038873  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.12590202782118082  - accuracy: 0.84375\n",
      "At: 614 [==========>] Loss 0.11706719903099713  - accuracy: 0.875\n",
      "At: 615 [==========>] Loss 0.19370506502475737  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.18591684305000075  - accuracy: 0.8125\n",
      "At: 617 [==========>] Loss 0.12602389656624663  - accuracy: 0.84375\n",
      "At: 618 [==========>] Loss 0.21248755324933233  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.13872943676322316  - accuracy: 0.84375\n",
      "At: 620 [==========>] Loss 0.13705898013220227  - accuracy: 0.84375\n",
      "At: 621 [==========>] Loss 0.0689746031058905  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.15992297602242722  - accuracy: 0.78125\n",
      "At: 623 [==========>] Loss 0.13107768879667558  - accuracy: 0.78125\n",
      "At: 624 [==========>] Loss 0.12704518807429516  - accuracy: 0.84375\n",
      "At: 625 [==========>] Loss 0.14964359264502167  - accuracy: 0.78125\n",
      "At: 626 [==========>] Loss 0.14416189979900798  - accuracy: 0.8125\n",
      "At: 627 [==========>] Loss 0.1351065694443592  - accuracy: 0.8125\n",
      "At: 628 [==========>] Loss 0.11491672535848635  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.18083028982427862  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.2330486724396657  - accuracy: 0.625\n",
      "At: 631 [==========>] Loss 0.1681929127308562  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.165804457512008  - accuracy: 0.71875\n",
      "At: 633 [==========>] Loss 0.12824649325865212  - accuracy: 0.8125\n",
      "At: 634 [==========>] Loss 0.1429679185699859  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.09555190854452142  - accuracy: 0.875\n",
      "At: 636 [==========>] Loss 0.1202952490425899  - accuracy: 0.84375\n",
      "At: 637 [==========>] Loss 0.16337079280070577  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.1099700094705965  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.13212258727707288  - accuracy: 0.875\n",
      "At: 640 [==========>] Loss 0.21012384142011906  - accuracy: 0.65625\n",
      "At: 641 [==========>] Loss 0.16274130486556565  - accuracy: 0.75\n",
      "At: 642 [==========>] Loss 0.18933437929895017  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.1398738355559178  - accuracy: 0.78125\n",
      "At: 644 [==========>] Loss 0.06631229129942737  - accuracy: 0.96875\n",
      "At: 645 [==========>] Loss 0.13053914296256536  - accuracy: 0.875\n",
      "At: 646 [==========>] Loss 0.11466648342077396  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.1611842018317103  - accuracy: 0.8125\n",
      "At: 648 [==========>] Loss 0.16698602674274443  - accuracy: 0.78125\n",
      "At: 649 [==========>] Loss 0.1643117101268272  - accuracy: 0.65625\n",
      "At: 650 [==========>] Loss 0.11038735543157481  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.19032676925570297  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.1230552037622753  - accuracy: 0.84375\n",
      "At: 653 [==========>] Loss 0.1240989670758659  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.09739182907286828  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.1447321591456831  - accuracy: 0.78125\n",
      "At: 656 [==========>] Loss 0.12895489178999506  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.14125717440134902  - accuracy: 0.8125\n",
      "At: 658 [==========>] Loss 0.12255770133327301  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.14051165059301973  - accuracy: 0.84375\n",
      "At: 660 [==========>] Loss 0.11253166547149207  - accuracy: 0.84375\n",
      "At: 661 [==========>] Loss 0.13111861607165137  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.13224913912467917  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.09726671072337247  - accuracy: 0.90625\n",
      "At: 664 [==========>] Loss 0.13356045312657683  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.20065969112645127  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.17968854345192486  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.12132102365575376  - accuracy: 0.84375\n",
      "At: 668 [==========>] Loss 0.1512505050855092  - accuracy: 0.84375\n",
      "At: 669 [==========>] Loss 0.1352577992102331  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.20270077608261272  - accuracy: 0.71875\n",
      "At: 671 [==========>] Loss 0.1227646961574024  - accuracy: 0.8125\n",
      "At: 672 [==========>] Loss 0.15420679335227225  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.08178266074601234  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.11671432681906828  - accuracy: 0.84375\n",
      "At: 675 [==========>] Loss 0.13745377206207693  - accuracy: 0.78125\n",
      "At: 676 [==========>] Loss 0.15994792760039314  - accuracy: 0.75\n",
      "At: 677 [==========>] Loss 0.15827193462502165  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.15335287413855864  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.12079756822535102  - accuracy: 0.8125\n",
      "At: 680 [==========>] Loss 0.12222396068952093  - accuracy: 0.84375\n",
      "At: 681 [==========>] Loss 0.12293344062272032  - accuracy: 0.90625\n",
      "At: 682 [==========>] Loss 0.14791204401584057  - accuracy: 0.78125\n",
      "At: 683 [==========>] Loss 0.18303382224972076  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.13206865007647764  - accuracy: 0.8125\n",
      "At: 685 [==========>] Loss 0.1391345542681393  - accuracy: 0.84375\n",
      "At: 686 [==========>] Loss 0.10089470468808838  - accuracy: 0.90625\n",
      "At: 687 [==========>] Loss 0.07703339867349222  - accuracy: 0.9375\n",
      "At: 688 [==========>] Loss 0.09257960421431485  - accuracy: 0.875\n",
      "At: 689 [==========>] Loss 0.16994556098056013  - accuracy: 0.75\n",
      "At: 690 [==========>] Loss 0.12353018366073859  - accuracy: 0.84375\n",
      "At: 691 [==========>] Loss 0.1089349823862673  - accuracy: 0.8125\n",
      "At: 692 [==========>] Loss 0.11379994616384698  - accuracy: 0.8125\n",
      "At: 693 [==========>] Loss 0.12811305339577073  - accuracy: 0.875\n",
      "At: 694 [==========>] Loss 0.16633807472145057  - accuracy: 0.78125\n",
      "At: 695 [==========>] Loss 0.1745239706181912  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.18781236097940673  - accuracy: 0.71875\n",
      "At: 697 [==========>] Loss 0.17508038124358508  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.10712633292686974  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.13365167232210834  - accuracy: 0.8125\n",
      "At: 700 [==========>] Loss 0.11268092025849247  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.14094398328080227  - accuracy: 0.84375\n",
      "At: 702 [==========>] Loss 0.11598594409239224  - accuracy: 0.8125\n",
      "At: 703 [==========>] Loss 0.15589165455202797  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.2025894976914947  - accuracy: 0.65625\n",
      "At: 705 [==========>] Loss 0.2061667156288282  - accuracy: 0.71875\n",
      "At: 706 [==========>] Loss 0.14597362691213098  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.11471654585475415  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.18541371719551059  - accuracy: 0.75\n",
      "At: 709 [==========>] Loss 0.1251794555969348  - accuracy: 0.8125\n",
      "At: 710 [==========>] Loss 0.1633531118325559  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.20644344529973585  - accuracy: 0.6875\n",
      "At: 712 [==========>] Loss 0.1548086628638907  - accuracy: 0.78125\n",
      "At: 713 [==========>] Loss 0.17616102885421192  - accuracy: 0.71875\n",
      "At: 714 [==========>] Loss 0.22284099611999153  - accuracy: 0.6875\n",
      "At: 715 [==========>] Loss 0.12684545003723127  - accuracy: 0.875\n",
      "At: 716 [==========>] Loss 0.09151281993734703  - accuracy: 0.875\n",
      "At: 717 [==========>] Loss 0.08251479418348437  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.20501832187885963  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.1064265126275035  - accuracy: 0.875\n",
      "At: 720 [==========>] Loss 0.134901689747364  - accuracy: 0.75\n",
      "At: 721 [==========>] Loss 0.11119202853655567  - accuracy: 0.875\n",
      "At: 722 [==========>] Loss 0.14596236324854195  - accuracy: 0.78125\n",
      "At: 723 [==========>] Loss 0.14792341079609675  - accuracy: 0.84375\n",
      "At: 724 [==========>] Loss 0.10125142067273898  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.14611113833690786  - accuracy: 0.84375\n",
      "At: 726 [==========>] Loss 0.1937145588762814  - accuracy: 0.75\n",
      "At: 727 [==========>] Loss 0.15358207958427475  - accuracy: 0.8125\n",
      "At: 728 [==========>] Loss 0.17272645339599157  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.19454175269756238  - accuracy: 0.625\n",
      "At: 730 [==========>] Loss 0.18787952395580448  - accuracy: 0.78125\n",
      "At: 731 [==========>] Loss 0.14005362126965815  - accuracy: 0.84375\n",
      "At: 732 [==========>] Loss 0.18659633638660783  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.10061138750835463  - accuracy: 0.84375\n",
      "At: 734 [==========>] Loss 0.15507979613124256  - accuracy: 0.8125\n",
      "At: 735 [==========>] Loss 0.13206555305520748  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.11125483013973032  - accuracy: 0.84375\n",
      "At: 737 [==========>] Loss 0.18180807569239288  - accuracy: 0.78125\n",
      "At: 738 [==========>] Loss 0.08791768010349804  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.08977720131536306  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.1429726653192037  - accuracy: 0.75\n",
      "At: 741 [==========>] Loss 0.10584089117792589  - accuracy: 0.84375\n",
      "At: 742 [==========>] Loss 0.14478332625249118  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.1553130433127492  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.1423541148202933  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.19451755439643845  - accuracy: 0.625\n",
      "At: 746 [==========>] Loss 0.12854722137920094  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.14401906642950899  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.15515116165171905  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.16350335198207333  - accuracy: 0.71875\n",
      "At: 750 [==========>] Loss 0.10404025994890365  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.17522843308103092  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.09943289420324522  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.1810891609592058  - accuracy: 0.78125\n",
      "At: 754 [==========>] Loss 0.12874530857368827  - accuracy: 0.875\n",
      "At: 755 [==========>] Loss 0.08879414836798721  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.2091808222423062  - accuracy: 0.6875\n",
      "At: 757 [==========>] Loss 0.11140826940231614  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.13915817287566234  - accuracy: 0.78125\n",
      "At: 759 [==========>] Loss 0.0959837561933817  - accuracy: 0.875\n",
      "At: 760 [==========>] Loss 0.19695604545623924  - accuracy: 0.71875\n",
      "At: 761 [==========>] Loss 0.14183472198863525  - accuracy: 0.78125\n",
      "At: 762 [==========>] Loss 0.13072858123319528  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.11123027072473161  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.11078153089434797  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.14317410815721884  - accuracy: 0.8125\n",
      "At: 766 [==========>] Loss 0.14728360978474966  - accuracy: 0.8125\n",
      "At: 767 [==========>] Loss 0.11016464560263463  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.16350244699504618  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.1483251132737501  - accuracy: 0.75\n",
      "At: 770 [==========>] Loss 0.12183004375985336  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.1975515258908313  - accuracy: 0.65625\n",
      "At: 772 [==========>] Loss 0.14673312942819028  - accuracy: 0.8125\n",
      "At: 773 [==========>] Loss 0.07393832220342986  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.16635836880066104  - accuracy: 0.71875\n",
      "At: 775 [==========>] Loss 0.1733107425229072  - accuracy: 0.71875\n",
      "At: 776 [==========>] Loss 0.14286928162677165  - accuracy: 0.8125\n",
      "At: 777 [==========>] Loss 0.07455144360660308  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.1683878343761344  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.10464998799658734  - accuracy: 0.84375\n",
      "At: 780 [==========>] Loss 0.08363155020610638  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.15986506200767836  - accuracy: 0.8125\n",
      "At: 782 [==========>] Loss 0.1742848400232646  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.1499103899872144  - accuracy: 0.78125\n",
      "At: 784 [==========>] Loss 0.17177273981765848  - accuracy: 0.71875\n",
      "At: 785 [==========>] Loss 0.2020607599022241  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.13352383843287885  - accuracy: 0.84375\n",
      "At: 787 [==========>] Loss 0.15031582098108098  - accuracy: 0.6875\n",
      "At: 788 [==========>] Loss 0.08809155295439036  - accuracy: 0.96875\n",
      "At: 789 [==========>] Loss 0.18451674486744857  - accuracy: 0.71875\n",
      "At: 790 [==========>] Loss 0.1374139601621268  - accuracy: 0.875\n",
      "At: 791 [==========>] Loss 0.18166670006429536  - accuracy: 0.75\n",
      "At: 792 [==========>] Loss 0.1866056190789347  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.11907563954540562  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.11683741097333668  - accuracy: 0.8125\n",
      "At: 795 [==========>] Loss 0.12215991845884523  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.15652377346547272  - accuracy: 0.78125\n",
      "At: 797 [==========>] Loss 0.14365146394382947  - accuracy: 0.8125\n",
      "At: 798 [==========>] Loss 0.157680602661768  - accuracy: 0.78125\n",
      "At: 799 [==========>] Loss 0.07688634326414417  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.14077329979805137  - accuracy: 0.75\n",
      "At: 801 [==========>] Loss 0.11884424641611747  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.16855547167191293  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.1455782880642129  - accuracy: 0.78125\n",
      "At: 804 [==========>] Loss 0.1442973594101542  - accuracy: 0.78125\n",
      "At: 805 [==========>] Loss 0.17187295743053593  - accuracy: 0.71875\n",
      "At: 806 [==========>] Loss 0.11315994751461936  - accuracy: 0.8125\n",
      "At: 807 [==========>] Loss 0.11601495336705298  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.15191391274845664  - accuracy: 0.78125\n",
      "At: 809 [==========>] Loss 0.11488889978927827  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.18454524517362664  - accuracy: 0.71875\n",
      "At: 811 [==========>] Loss 0.1547545029177949  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.13269058341211465  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.16697190410260992  - accuracy: 0.8125\n",
      "At: 814 [==========>] Loss 0.17012610020664232  - accuracy: 0.71875\n",
      "At: 815 [==========>] Loss 0.11735944832839534  - accuracy: 0.90625\n",
      "At: 816 [==========>] Loss 0.14293533067970254  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.06640899721726887  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.12554549468477544  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.11149211555681618  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.13596498056102677  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.16255444870268612  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.16179341981497078  - accuracy: 0.6875\n",
      "At: 823 [==========>] Loss 0.15353193695231293  - accuracy: 0.78125\n",
      "At: 824 [==========>] Loss 0.13935942624974137  - accuracy: 0.84375\n",
      "At: 825 [==========>] Loss 0.19129303392400923  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.15052916004283157  - accuracy: 0.78125\n",
      "At: 827 [==========>] Loss 0.07653068817135483  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.04987683436009912  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.08994573759497884  - accuracy: 0.90625\n",
      "At: 830 [==========>] Loss 0.09778001812394693  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.0685937743175903  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.09412046099957239  - accuracy: 0.84375\n",
      "At: 833 [==========>] Loss 0.20698457854951502  - accuracy: 0.6875\n",
      "At: 834 [==========>] Loss 0.12374185306162042  - accuracy: 0.84375\n",
      "At: 835 [==========>] Loss 0.06899040219396485  - accuracy: 0.9375\n",
      "At: 836 [==========>] Loss 0.10390157037010533  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.11070203065873209  - accuracy: 0.875\n",
      "At: 838 [==========>] Loss 0.11443664658602376  - accuracy: 0.875\n",
      "At: 839 [==========>] Loss 0.13929832260428612  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.12217798242225456  - accuracy: 0.78125\n",
      "At: 841 [==========>] Loss 0.07368030306661108  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.060335280914898246  - accuracy: 0.96875\n",
      "At: 843 [==========>] Loss 0.17302908859973262  - accuracy: 0.75\n",
      "At: 844 [==========>] Loss 0.11371447683026366  - accuracy: 0.84375\n",
      "At: 845 [==========>] Loss 0.1507527213926516  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.13468435362047337  - accuracy: 0.84375\n",
      "At: 847 [==========>] Loss 0.09936868635693943  - accuracy: 0.96875\n",
      "At: 848 [==========>] Loss 0.1748339253935431  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.14840351765593104  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.09977917119296217  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.10515767793713265  - accuracy: 0.875\n",
      "At: 852 [==========>] Loss 0.15276082969760468  - accuracy: 0.8125\n",
      "At: 853 [==========>] Loss 0.169606141427678  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.17369094799909318  - accuracy: 0.8125\n",
      "At: 855 [==========>] Loss 0.08894447153622156  - accuracy: 0.84375\n",
      "At: 856 [==========>] Loss 0.09925810375932431  - accuracy: 0.875\n",
      "At: 857 [==========>] Loss 0.09872609458996337  - accuracy: 0.875\n",
      "At: 858 [==========>] Loss 0.26317604189889793  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.11734841725542142  - accuracy: 0.84375\n",
      "At: 860 [==========>] Loss 0.1485236100907581  - accuracy: 0.78125\n",
      "At: 861 [==========>] Loss 0.11108939938030105  - accuracy: 0.84375\n",
      "At: 862 [==========>] Loss 0.09608067224987174  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.12436567218266828  - accuracy: 0.78125\n",
      "At: 864 [==========>] Loss 0.18765822894878287  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.20315160338097626  - accuracy: 0.625\n",
      "At: 866 [==========>] Loss 0.1446376986765518  - accuracy: 0.78125\n",
      "At: 867 [==========>] Loss 0.07067785667334324  - accuracy: 0.90625\n",
      "At: 868 [==========>] Loss 0.18444065644343152  - accuracy: 0.75\n",
      "At: 869 [==========>] Loss 0.1666661731866135  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.13287154519073588  - accuracy: 0.78125\n",
      "At: 871 [==========>] Loss 0.0653586233316449  - accuracy: 0.9375\n",
      "At: 872 [==========>] Loss 0.10543903192593071  - accuracy: 0.84375\n",
      "At: 873 [==========>] Loss 0.18139729432121762  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.14765931125761284  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.11450348726072347  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.11884023306285765  - accuracy: 0.8125\n",
      "At: 877 [==========>] Loss 0.168772461002682  - accuracy: 0.75\n",
      "At: 878 [==========>] Loss 0.04971812743225693  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.15376784408053526  - accuracy: 0.8125\n",
      "At: 880 [==========>] Loss 0.13584289523557846  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.16394890966417466  - accuracy: 0.71875\n",
      "At: 882 [==========>] Loss 0.11335133883621777  - accuracy: 0.90625\n",
      "At: 883 [==========>] Loss 0.1441744254213449  - accuracy: 0.75\n",
      "At: 884 [==========>] Loss 0.1409311723037372  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.11866166950043949  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.07921646008690092  - accuracy: 0.90625\n",
      "At: 887 [==========>] Loss 0.14366319420869456  - accuracy: 0.78125\n",
      "At: 888 [==========>] Loss 0.16217089600241053  - accuracy: 0.75\n",
      "At: 889 [==========>] Loss 0.1176962247100926  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.12539765749060144  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.09951154364440566  - accuracy: 0.90625\n",
      "At: 892 [==========>] Loss 0.12616571182330275  - accuracy: 0.8125\n",
      "At: 893 [==========>] Loss 0.14619798447915355  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.13591449709673065  - accuracy: 0.8125\n",
      "At: 895 [==========>] Loss 0.14822781164896354  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.12366576599782009  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.1312198928353546  - accuracy: 0.84375\n",
      "At: 898 [==========>] Loss 0.16207599448168694  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.08658869492088472  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.13757227955807663  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.19020782245346804  - accuracy: 0.6875\n",
      "At: 902 [==========>] Loss 0.11192324099094389  - accuracy: 0.84375\n",
      "At: 903 [==========>] Loss 0.14506466935925008  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.09520395119177555  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.11937524581811194  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.10147870665858874  - accuracy: 0.78125\n",
      "At: 907 [==========>] Loss 0.1523934307834785  - accuracy: 0.8125\n",
      "At: 908 [==========>] Loss 0.12507099891558518  - accuracy: 0.875\n",
      "At: 909 [==========>] Loss 0.10017908290580957  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.15393610176213743  - accuracy: 0.75\n",
      "At: 911 [==========>] Loss 0.1317770534395957  - accuracy: 0.875\n",
      "At: 912 [==========>] Loss 0.12248882750308997  - accuracy: 0.8125\n",
      "At: 913 [==========>] Loss 0.09008992958873019  - accuracy: 0.90625\n",
      "At: 914 [==========>] Loss 0.1195365216109003  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.14140736158086642  - accuracy: 0.78125\n",
      "At: 916 [==========>] Loss 0.1780235939071035  - accuracy: 0.71875\n",
      "At: 917 [==========>] Loss 0.14600898710065113  - accuracy: 0.8125\n",
      "At: 918 [==========>] Loss 0.21182970701682657  - accuracy: 0.625\n",
      "At: 919 [==========>] Loss 0.09605504228197477  - accuracy: 0.84375\n",
      "At: 920 [==========>] Loss 0.10481900033671687  - accuracy: 0.90625\n",
      "At: 921 [==========>] Loss 0.12950147671403883  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.13946487396967813  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.11348754846896618  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.17113561257057774  - accuracy: 0.78125\n",
      "At: 925 [==========>] Loss 0.13775678599704752  - accuracy: 0.8125\n",
      "At: 926 [==========>] Loss 0.15179281529330732  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.13877958013695124  - accuracy: 0.84375\n",
      "At: 928 [==========>] Loss 0.11492323243018845  - accuracy: 0.875\n",
      "At: 929 [==========>] Loss 0.13156835088741137  - accuracy: 0.8125\n",
      "At: 930 [==========>] Loss 0.11529847919669121  - accuracy: 0.84375\n",
      "At: 931 [==========>] Loss 0.17635420287043524  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.10767732566275974  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.10656101793033329  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.1435826028145611  - accuracy: 0.84375\n",
      "At: 935 [==========>] Loss 0.06780075857381741  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.1757090383942245  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.177818058722915  - accuracy: 0.71875\n",
      "At: 938 [==========>] Loss 0.1436881940493388  - accuracy: 0.75\n",
      "At: 939 [==========>] Loss 0.10926888897383054  - accuracy: 0.78125\n",
      "At: 940 [==========>] Loss 0.2447985384441318  - accuracy: 0.625\n",
      "At: 941 [==========>] Loss 0.11409002018184364  - accuracy: 0.875\n",
      "At: 942 [==========>] Loss 0.15151410118715136  - accuracy: 0.78125\n",
      "At: 943 [==========>] Loss 0.12602888586018185  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.11585666328729696  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.11905572017110393  - accuracy: 0.84375\n",
      "At: 946 [==========>] Loss 0.1319688437239422  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.14374885340778124  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.2425163815456919  - accuracy: 0.59375\n",
      "At: 949 [==========>] Loss 0.07683735894225596  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.10029880083387385  - accuracy: 0.875\n",
      "At: 951 [==========>] Loss 0.0973150133143848  - accuracy: 0.90625\n",
      "At: 952 [==========>] Loss 0.10723570234295937  - accuracy: 0.84375\n",
      "At: 953 [==========>] Loss 0.08133537421445258  - accuracy: 0.90625\n",
      "At: 954 [==========>] Loss 0.09660906784827622  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.1368706411601116  - accuracy: 0.78125\n",
      "At: 956 [==========>] Loss 0.07584166007549029  - accuracy: 0.9375\n",
      "At: 957 [==========>] Loss 0.1494559244923474  - accuracy: 0.78125\n",
      "At: 958 [==========>] Loss 0.08039972554719022  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.1406869641340829  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.13183389633432313  - accuracy: 0.8125\n",
      "At: 961 [==========>] Loss 0.12055487188254646  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.09289955349358406  - accuracy: 0.90625\n",
      "At: 963 [==========>] Loss 0.07835550018379374  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.16452013525440856  - accuracy: 0.78125\n",
      "At: 965 [==========>] Loss 0.1305325508946703  - accuracy: 0.78125\n",
      "At: 966 [==========>] Loss 0.17493401207092915  - accuracy: 0.6875\n",
      "At: 967 [==========>] Loss 0.11103908475685378  - accuracy: 0.875\n",
      "At: 968 [==========>] Loss 0.13179955057826187  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.13043437829917412  - accuracy: 0.8125\n",
      "At: 970 [==========>] Loss 0.12827353551448512  - accuracy: 0.84375\n",
      "At: 971 [==========>] Loss 0.10789119564345787  - accuracy: 0.84375\n",
      "At: 972 [==========>] Loss 0.06418517960292543  - accuracy: 0.9375\n",
      "At: 973 [==========>] Loss 0.12727951959439246  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.07914669013051856  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.15115232556912167  - accuracy: 0.71875\n",
      "At: 976 [==========>] Loss 0.10275330196067321  - accuracy: 0.78125\n",
      "At: 977 [==========>] Loss 0.10373452919401527  - accuracy: 0.875\n",
      "At: 978 [==========>] Loss 0.1657444316700793  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.10405314256852959  - accuracy: 0.84375\n",
      "At: 980 [==========>] Loss 0.15415002498594682  - accuracy: 0.8125\n",
      "At: 981 [==========>] Loss 0.16523233508314072  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.09582169664267504  - accuracy: 0.875\n",
      "At: 983 [==========>] Loss 0.10755316321258265  - accuracy: 0.875\n",
      "At: 984 [==========>] Loss 0.0953749189620411  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.16505182685227726  - accuracy: 0.8125\n",
      "At: 986 [==========>] Loss 0.12818113009901144  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.13264817023725403  - accuracy: 0.78125\n",
      "At: 988 [==========>] Loss 0.11223235692953373  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.13966842151304698  - accuracy: 0.84375\n",
      "At: 990 [==========>] Loss 0.1343092802021793  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.13515363885442458  - accuracy: 0.8125\n",
      "At: 992 [==========>] Loss 0.22545888933484634  - accuracy: 0.6875\n",
      "At: 993 [==========>] Loss 0.13432721481119125  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.14454440002418684  - accuracy: 0.8125\n",
      "At: 995 [==========>] Loss 0.1678419309517499  - accuracy: 0.8125\n",
      "At: 996 [==========>] Loss 0.09476080845222838  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.12971660707767096  - accuracy: 0.75\n",
      "At: 998 [==========>] Loss 0.1231268789919508  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.14596889827108406  - accuracy: 0.75\n",
      "At: 1000 [==========>] Loss 0.1908029523019873  - accuracy: 0.65625\n",
      "At: 1001 [==========>] Loss 0.14513608698751157  - accuracy: 0.75\n",
      "At: 1002 [==========>] Loss 0.18389835972763607  - accuracy: 0.6875\n",
      "At: 1003 [==========>] Loss 0.14838939677037816  - accuracy: 0.78125\n",
      "At: 1004 [==========>] Loss 0.1374175050315103  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.07859900670766451  - accuracy: 0.90625\n",
      "At: 1006 [==========>] Loss 0.11416886753342251  - accuracy: 0.8125\n",
      "At: 1007 [==========>] Loss 0.11770708456427369  - accuracy: 0.84375\n",
      "At: 1008 [==========>] Loss 0.1810427436637672  - accuracy: 0.75\n",
      "At: 1009 [==========>] Loss 0.12626049551183643  - accuracy: 0.84375\n",
      "At: 1010 [==========>] Loss 0.16576577095346107  - accuracy: 0.78125\n",
      "At: 1011 [==========>] Loss 0.15601520583545772  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.11417616717421258  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.07932863083877344  - accuracy: 0.84375\n",
      "At: 1014 [==========>] Loss 0.09974840642485891  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.2234229059671103  - accuracy: 0.65625\n",
      "At: 1016 [==========>] Loss 0.16387267268121936  - accuracy: 0.71875\n",
      "At: 1017 [==========>] Loss 0.1682029881597221  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.16421000452153495  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.20013300065675238  - accuracy: 0.75\n",
      "At: 1020 [==========>] Loss 0.1588206674442287  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.11445550480529132  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.14593270558628402  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.20211507550201188  - accuracy: 0.6875\n",
      "At: 1024 [==========>] Loss 0.14803638193933455  - accuracy: 0.78125\n",
      "At: 1025 [==========>] Loss 0.19542083165888624  - accuracy: 0.6875\n",
      "At: 1026 [==========>] Loss 0.1374342373039112  - accuracy: 0.75\n",
      "At: 1027 [==========>] Loss 0.12657129590151825  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.2322758993110408  - accuracy: 0.625\n",
      "At: 1029 [==========>] Loss 0.08539829038836913  - accuracy: 0.90625\n",
      "At: 1030 [==========>] Loss 0.11062487189743825  - accuracy: 0.84375\n",
      "At: 1031 [==========>] Loss 0.16754302624618256  - accuracy: 0.8125\n",
      "At: 1032 [==========>] Loss 0.1442240822762733  - accuracy: 0.78125\n",
      "At: 1033 [==========>] Loss 0.12870426104481114  - accuracy: 0.8125\n",
      "At: 1034 [==========>] Loss 0.06863953248405857  - accuracy: 0.9375\n",
      "At: 1035 [==========>] Loss 0.07737601853826565  - accuracy: 0.90625\n",
      "At: 1036 [==========>] Loss 0.1590805421748454  - accuracy: 0.75\n",
      "At: 1037 [==========>] Loss 0.15570474093458256  - accuracy: 0.75\n",
      "At: 1038 [==========>] Loss 0.1102646297030949  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.10084149393795452  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.13159165225085756  - accuracy: 0.8125\n",
      "At: 1041 [==========>] Loss 0.12520274609709425  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.08896858928951852  - accuracy: 0.84375\n",
      "At: 1043 [==========>] Loss 0.17528533166619895  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.14656165055563192  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.1594380405468377  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.1450840928042027  - accuracy: 0.8125\n",
      "At: 1047 [==========>] Loss 0.12647160271472144  - accuracy: 0.78125\n",
      "At: 1048 [==========>] Loss 0.17257689920015395  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.14970501209892845  - accuracy: 0.8125\n",
      "At: 1050 [==========>] Loss 0.17603362312536633  - accuracy: 0.71875\n",
      "At: 1051 [==========>] Loss 0.08420434488462078  - accuracy: 0.875\n",
      "At: 1052 [==========>] Loss 0.1126512040017366  - accuracy: 0.90625\n",
      "At: 1053 [==========>] Loss 0.0954772447228015  - accuracy: 0.875\n",
      "At: 1054 [==========>] Loss 0.12051581486175714  - accuracy: 0.875\n",
      "At: 1055 [==========>] Loss 0.169745943913288  - accuracy: 0.78125\n",
      "At: 1056 [==========>] Loss 0.15808139800165927  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.1351254885043725  - accuracy: 0.84375\n",
      "At: 1058 [==========>] Loss 0.08244470620392816  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.0838432846204746  - accuracy: 0.90625\n",
      "At: 1060 [==========>] Loss 0.10858428612943224  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.09066000303994967  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.16295165214727528  - accuracy: 0.71875\n",
      "At: 1063 [==========>] Loss 0.12820790507395463  - accuracy: 0.8125\n",
      "At: 1064 [==========>] Loss 0.14127373466583865  - accuracy: 0.78125\n",
      "At: 1065 [==========>] Loss 0.12232695177628515  - accuracy: 0.78125\n",
      "At: 1066 [==========>] Loss 0.08705932521692704  - accuracy: 0.9375\n",
      "At: 1067 [==========>] Loss 0.12467442014172841  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.11147740135765707  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.14547176618392796  - accuracy: 0.84375\n",
      "At: 1070 [==========>] Loss 0.1276137621894926  - accuracy: 0.875\n",
      "At: 1071 [==========>] Loss 0.09094811088728172  - accuracy: 0.90625\n",
      "At: 1072 [==========>] Loss 0.10740349187464679  - accuracy: 0.8125\n",
      "At: 1073 [==========>] Loss 0.16183865402260597  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.1838970433529959  - accuracy: 0.75\n",
      "At: 1075 [==========>] Loss 0.09495751935612538  - accuracy: 0.875\n",
      "At: 1076 [==========>] Loss 0.1362474679204703  - accuracy: 0.84375\n",
      "At: 1077 [==========>] Loss 0.07305506754311308  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.09230432975932032  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.13176403525117336  - accuracy: 0.84375\n",
      "At: 1080 [==========>] Loss 0.13345102228507766  - accuracy: 0.78125\n",
      "At: 1081 [==========>] Loss 0.12191565913084537  - accuracy: 0.78125\n",
      "At: 1082 [==========>] Loss 0.1109719334598286  - accuracy: 0.875\n",
      "At: 1083 [==========>] Loss 0.10568519920431774  - accuracy: 0.875\n",
      "At: 1084 [==========>] Loss 0.10760225119016262  - accuracy: 0.875\n",
      "At: 1085 [==========>] Loss 0.1305117710244715  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.1137823000856934  - accuracy: 0.84375\n",
      "At: 1087 [==========>] Loss 0.13182604502539202  - accuracy: 0.84375\n",
      "At: 1088 [==========>] Loss 0.17702249592011918  - accuracy: 0.71875\n",
      "At: 1089 [==========>] Loss 0.06892350248381462  - accuracy: 1.0\n",
      "At: 1090 [==========>] Loss 0.08340290847174282  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.2124150026769986  - accuracy: 0.71875\n",
      "At: 1092 [==========>] Loss 0.10869084300072654  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.12332142669898638  - accuracy: 0.875\n",
      "At: 1094 [==========>] Loss 0.13757824672172625  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.17688156709998984  - accuracy: 0.75\n",
      "At: 1096 [==========>] Loss 0.08472559312579248  - accuracy: 0.90625\n",
      "At: 1097 [==========>] Loss 0.08477591764767696  - accuracy: 0.90625\n",
      "At: 1098 [==========>] Loss 0.1300084930952396  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.15983545212058264  - accuracy: 0.75\n",
      "At: 1100 [==========>] Loss 0.09703441092659704  - accuracy: 0.84375\n",
      "At: 1101 [==========>] Loss 0.10121581370949591  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.107387859509111  - accuracy: 0.90625\n",
      "At: 1103 [==========>] Loss 0.09381193149302307  - accuracy: 0.84375\n",
      "At: 1104 [==========>] Loss 0.06482721725626817  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.09904745573131729  - accuracy: 0.90625\n",
      "At: 1106 [==========>] Loss 0.07912202244198364  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.20101055526100708  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.12431319013906279  - accuracy: 0.8125\n",
      "At: 1109 [==========>] Loss 0.06530755981114929  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.0904519850653955  - accuracy: 0.875\n",
      "At: 1111 [==========>] Loss 0.17065809273786936  - accuracy: 0.78125\n",
      "At: 1112 [==========>] Loss 0.15708564607131034  - accuracy: 0.84375\n",
      "At: 1113 [==========>] Loss 0.13700596098344206  - accuracy: 0.8125\n",
      "At: 1114 [==========>] Loss 0.13022360085904983  - accuracy: 0.84375\n",
      "At: 1115 [==========>] Loss 0.1110402835888202  - accuracy: 0.875\n",
      "At: 1116 [==========>] Loss 0.12855724713911654  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.06987138879300286  - accuracy: 0.96875\n",
      "At: 1118 [==========>] Loss 0.11447397234550162  - accuracy: 0.8125\n",
      "At: 1119 [==========>] Loss 0.13202277726869022  - accuracy: 0.875\n",
      "At: 1120 [==========>] Loss 0.08552794684701895  - accuracy: 0.84375\n",
      "At: 1121 [==========>] Loss 0.11121016002535097  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.10061247767404771  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.12966366805447735  - accuracy: 0.8125\n",
      "At: 1124 [==========>] Loss 0.11487280609426526  - accuracy: 0.84375\n",
      "At: 1125 [==========>] Loss 0.16668257392692626  - accuracy: 0.75\n",
      "At: 1126 [==========>] Loss 0.08953751366713184  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.13906749275462868  - accuracy: 0.75\n",
      "At: 1128 [==========>] Loss 0.0804961016722081  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.14595422568261324  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.14300445257911854  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.08777166423640938  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.10610079743889968  - accuracy: 0.875\n",
      "At: 1133 [==========>] Loss 0.13689392158467995  - accuracy: 0.8125\n",
      "At: 1134 [==========>] Loss 0.09166310268947625  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.1295286204881693  - accuracy: 0.8125\n",
      "At: 1136 [==========>] Loss 0.15002570300243406  - accuracy: 0.84375\n",
      "At: 1137 [==========>] Loss 0.12863929251794182  - accuracy: 0.8125\n",
      "At: 1138 [==========>] Loss 0.09207157490784448  - accuracy: 0.90625\n",
      "At: 1139 [==========>] Loss 0.08676758069664606  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.17558488556331767  - accuracy: 0.59375\n",
      "At: 1141 [==========>] Loss 0.11856420599691089  - accuracy: 0.78125\n",
      "At: 1142 [==========>] Loss 0.1335125944075584  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.082026414243525  - accuracy: 0.90625\n",
      "At: 1144 [==========>] Loss 0.11949988524280475  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.16377285562746102  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.12299154774917981  - accuracy: 0.8125\n",
      "At: 1147 [==========>] Loss 0.19142395859287642  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.08946992987151076  - accuracy: 0.875\n",
      "At: 1149 [==========>] Loss 0.13766191739445133  - accuracy: 0.78125\n",
      "At: 1150 [==========>] Loss 0.12741521300541175  - accuracy: 0.78125\n",
      "At: 1151 [==========>] Loss 0.15049726112765938  - accuracy: 0.78125\n",
      "At: 1152 [==========>] Loss 0.09279986967733624  - accuracy: 0.9375\n",
      "At: 1153 [==========>] Loss 0.1573305954905969  - accuracy: 0.84375\n",
      "At: 1154 [==========>] Loss 0.11915818431504516  - accuracy: 0.84375\n",
      "At: 1155 [==========>] Loss 0.10886204404803829  - accuracy: 0.8125\n",
      "At: 1156 [==========>] Loss 0.1640700725335923  - accuracy: 0.71875\n",
      "At: 1157 [==========>] Loss 0.13298188191747534  - accuracy: 0.8125\n",
      "At: 1158 [==========>] Loss 0.17200392741319986  - accuracy: 0.78125\n",
      "At: 1159 [==========>] Loss 0.11178992867534598  - accuracy: 0.90625\n",
      "At: 1160 [==========>] Loss 0.09648890903131976  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.08577484873836262  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.1253187803169013  - accuracy: 0.78125\n",
      "At: 1163 [==========>] Loss 0.15430198437196574  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.08795289469607913  - accuracy: 0.90625\n",
      "At: 1165 [==========>] Loss 0.14202735879323067  - accuracy: 0.75\n",
      "At: 1166 [==========>] Loss 0.08161420010100116  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.15145839734359562  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.11800609419925709  - accuracy: 0.9375\n",
      "At: 1169 [==========>] Loss 0.09942192133058858  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.1648033139863516  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.0679664056808267  - accuracy: 0.9375\n",
      "At: 1172 [==========>] Loss 0.11654040326967308  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.13577857212991723  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.17882021464993333  - accuracy: 0.78125\n",
      "At: 1175 [==========>] Loss 0.10429004915790852  - accuracy: 0.84375\n",
      "At: 1176 [==========>] Loss 0.11744497396228895  - accuracy: 0.84375\n",
      "At: 1177 [==========>] Loss 0.08240576590288712  - accuracy: 0.90625\n",
      "At: 1178 [==========>] Loss 0.1641037241290535  - accuracy: 0.8125\n",
      "At: 1179 [==========>] Loss 0.1237650163286616  - accuracy: 0.8125\n",
      "At: 1180 [==========>] Loss 0.21490089525419176  - accuracy: 0.71875\n",
      "At: 1181 [==========>] Loss 0.1292742227775081  - accuracy: 0.8125\n",
      "At: 1182 [==========>] Loss 0.10208647365181264  - accuracy: 0.875\n",
      "At: 1183 [==========>] Loss 0.1396675761886545  - accuracy: 0.875\n",
      "At: 1184 [==========>] Loss 0.15485642601265462  - accuracy: 0.78125\n",
      "At: 1185 [==========>] Loss 0.10209680691734468  - accuracy: 0.84375\n",
      "At: 1186 [==========>] Loss 0.12246858780844125  - accuracy: 0.8125\n",
      "At: 1187 [==========>] Loss 0.1264613414578163  - accuracy: 0.84375\n",
      "At: 1188 [==========>] Loss 0.06816245770377374  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.1526411853391828  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.10417408476283704  - accuracy: 0.875\n",
      "At: 1191 [==========>] Loss 0.19021557553969498  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.09099141212702085  - accuracy: 0.84375\n",
      "At: 1193 [==========>] Loss 0.15098972385142673  - accuracy: 0.75\n",
      "At: 1194 [==========>] Loss 0.12678852952748806  - accuracy: 0.84375\n",
      "At: 1195 [==========>] Loss 0.13933879354881196  - accuracy: 0.78125\n",
      "At: 1196 [==========>] Loss 0.13896541820154074  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.10374681210449652  - accuracy: 0.90625\n",
      "At: 1198 [==========>] Loss 0.09269458221940031  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.2118576495375497  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.070194148099106  - accuracy: 0.9375\n",
      "At: 1201 [==========>] Loss 0.11188746083512019  - accuracy: 0.875\n",
      "At: 1202 [==========>] Loss 0.13601999213078603  - accuracy: 0.8125\n",
      "At: 1203 [==========>] Loss 0.1363901177053541  - accuracy: 0.78125\n",
      "At: 1204 [==========>] Loss 0.09332377950576048  - accuracy: 0.875\n",
      "At: 1205 [==========>] Loss 0.08015216587456571  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.10313329116438943  - accuracy: 0.875\n",
      "At: 1207 [==========>] Loss 0.14513587637904596  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.08334150837597704  - accuracy: 0.90625\n",
      "At: 1209 [==========>] Loss 0.10522108272482845  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.14518983076889053  - accuracy: 0.78125\n",
      "At: 1211 [==========>] Loss 0.14937025787484504  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.10256021409082336  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.1860757549054334  - accuracy: 0.71875\n",
      "At: 1214 [==========>] Loss 0.15964727610625495  - accuracy: 0.75\n",
      "At: 1215 [==========>] Loss 0.1329926802001036  - accuracy: 0.84375\n",
      "At: 1216 [==========>] Loss 0.09622290805302017  - accuracy: 0.875\n",
      "At: 1217 [==========>] Loss 0.07473893390191294  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.11646535232011526  - accuracy: 0.84375\n",
      "At: 1219 [==========>] Loss 0.13441401874768288  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.11108483904824552  - accuracy: 0.8125\n",
      "At: 1221 [==========>] Loss 0.07254216632444326  - accuracy: 0.96875\n",
      "At: 1222 [==========>] Loss 0.1953771882902186  - accuracy: 0.6875\n",
      "At: 1223 [==========>] Loss 0.1078912283763466  - accuracy: 0.8125\n",
      "At: 1224 [==========>] Loss 0.07673003972849357  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.07726255391483046  - accuracy: 0.9375\n",
      "At: 1226 [==========>] Loss 0.09409704276929733  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.17740932662433084  - accuracy: 0.75\n",
      "At: 1228 [==========>] Loss 0.14976663864835588  - accuracy: 0.78125\n",
      "At: 1229 [==========>] Loss 0.10885841221072912  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.15946637016002163  - accuracy: 0.78125\n",
      "At: 1231 [==========>] Loss 0.16564877627952063  - accuracy: 0.78125\n",
      "At: 1232 [==========>] Loss 0.0725583733000224  - accuracy: 0.90625\n",
      "At: 1233 [==========>] Loss 0.11780367073723771  - accuracy: 0.78125\n",
      "At: 1234 [==========>] Loss 0.13422215725510278  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.0981226574381769  - accuracy: 0.84375\n",
      "At: 1236 [==========>] Loss 0.16947242579929042  - accuracy: 0.71875\n",
      "At: 1237 [==========>] Loss 0.09816672005935159  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.10823382002593017  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.1767333770959197  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.0733215149910798  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.12496674586643003  - accuracy: 0.8125\n",
      "At: 1242 [==========>] Loss 0.11774420528302527  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.16506605336011182  - accuracy: 0.75\n",
      "At: 1244 [==========>] Loss 0.1490929250816836  - accuracy: 0.75\n",
      "At: 1245 [==========>] Loss 0.11672793257214631  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.07057235374221957  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.13684829243248503  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.08000620241764454  - accuracy: 0.90625\n",
      "At: 1249 [==========>] Loss 0.1182220989235403  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.09966902330200714  - accuracy: 0.90625\n",
      "At: 1251 [==========>] Loss 0.11970589210815893  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.08135046576079247  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.11777347332889089  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.1846719059599626  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.10396259186600096  - accuracy: 0.84375\n",
      "At: 1256 [==========>] Loss 0.12585471977692814  - accuracy: 0.75\n",
      "At: 1257 [==========>] Loss 0.1339628520604288  - accuracy: 0.875\n",
      "At: 1258 [==========>] Loss 0.07765740715411079  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.1150806174430836  - accuracy: 0.84375\n",
      "At: 1260 [==========>] Loss 0.13824219054446168  - accuracy: 0.8125\n",
      "At: 1261 [==========>] Loss 0.13173831278508055  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.1363689052524113  - accuracy: 0.78125\n",
      "At: 1263 [==========>] Loss 0.14149193258074022  - accuracy: 0.71875\n",
      "At: 1264 [==========>] Loss 0.0728912060490685  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.10887034465769237  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.114398819629205  - accuracy: 0.8125\n",
      "At: 1267 [==========>] Loss 0.1446546237382807  - accuracy: 0.75\n",
      "At: 1268 [==========>] Loss 0.14079876184928336  - accuracy: 0.75\n",
      "At: 1269 [==========>] Loss 0.139676946067756  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.12773046505695065  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.14552636360944982  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.07301023100716542  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.1816769435218412  - accuracy: 0.78125\n",
      "At: 1274 [==========>] Loss 0.12379246469732368  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.09466901350697093  - accuracy: 0.875\n",
      "At: 1276 [==========>] Loss 0.0846422144870019  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.06946359714279551  - accuracy: 0.9375\n",
      "At: 1278 [==========>] Loss 0.16301198029787362  - accuracy: 0.75\n",
      "At: 1279 [==========>] Loss 0.11249989214952183  - accuracy: 0.875\n",
      "At: 1280 [==========>] Loss 0.1226439588670348  - accuracy: 0.8125\n",
      "At: 1281 [==========>] Loss 0.16577650234168664  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.14373517887995235  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.13434859592301657  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.16658992151624374  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.10035456329438971  - accuracy: 0.84375\n",
      "At: 1286 [==========>] Loss 0.1158185726020628  - accuracy: 0.875\n",
      "At: 1287 [==========>] Loss 0.10950873648588691  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.14808274306514493  - accuracy: 0.8125\n",
      "At: 1289 [==========>] Loss 0.12163619714419616  - accuracy: 0.8125\n",
      "At: 1290 [==========>] Loss 0.141065492654317  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.17110945939621758  - accuracy: 0.78125\n",
      "At: 1292 [==========>] Loss 0.11192961963532409  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.1722750893988909  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.13412874302787883  - accuracy: 0.78125\n",
      "At: 1295 [==========>] Loss 0.17683498579577298  - accuracy: 0.75\n",
      "At: 1296 [==========>] Loss 0.1533377460213728  - accuracy: 0.78125\n",
      "At: 1297 [==========>] Loss 0.13764820932818056  - accuracy: 0.78125\n",
      "At: 1298 [==========>] Loss 0.09812868469374719  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.16096387643577884  - accuracy: 0.71875\n",
      "At: 1300 [==========>] Loss 0.13065277112395718  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.12021233708230228  - accuracy: 0.875\n",
      "At: 1302 [==========>] Loss 0.09804075266574325  - accuracy: 0.875\n",
      "At: 1303 [==========>] Loss 0.1106859362225425  - accuracy: 0.8125\n",
      "At: 1304 [==========>] Loss 0.13624638228540648  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.13634518663123135  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.07767380489874201  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.15239665769655378  - accuracy: 0.78125\n",
      "At: 1308 [==========>] Loss 0.07267584876922442  - accuracy: 0.9375\n",
      "At: 1309 [==========>] Loss 0.16451086664840653  - accuracy: 0.8125\n",
      "At: 1310 [==========>] Loss 0.15671967723458066  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.1239832713755541  - accuracy: 0.84375\n",
      "At: 1312 [==========>] Loss 0.08219483635432591  - accuracy: 0.875\n",
      "At: 1313 [==========>] Loss 0.17027405910500912  - accuracy: 0.71875\n",
      "At: 1314 [==========>] Loss 0.08879512979016939  - accuracy: 0.8125\n",
      "At: 1315 [==========>] Loss 0.14462277147772015  - accuracy: 0.78125\n",
      "At: 1316 [==========>] Loss 0.15221141756226364  - accuracy: 0.75\n",
      "At: 1317 [==========>] Loss 0.10065282874204513  - accuracy: 0.875\n",
      "At: 1318 [==========>] Loss 0.1507255996596324  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.14707739991431185  - accuracy: 0.8125\n",
      "At: 1320 [==========>] Loss 0.12342268140501823  - accuracy: 0.875\n",
      "At: 1321 [==========>] Loss 0.10138929817675027  - accuracy: 0.84375\n",
      "At: 1322 [==========>] Loss 0.13503685453103784  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.12557941642632048  - accuracy: 0.875\n",
      "At: 1324 [==========>] Loss 0.1734366031333961  - accuracy: 0.78125\n",
      "At: 1325 [==========>] Loss 0.07447825496855323  - accuracy: 0.90625\n",
      "At: 1326 [==========>] Loss 0.10134592790656866  - accuracy: 0.9375\n",
      "At: 1327 [==========>] Loss 0.15166144193000086  - accuracy: 0.8125\n",
      "At: 1328 [==========>] Loss 0.11226107794949684  - accuracy: 0.78125\n",
      "At: 1329 [==========>] Loss 0.06151905743430504  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.11366450781378065  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.16454744775293417  - accuracy: 0.78125\n",
      "At: 1332 [==========>] Loss 0.13558859349113828  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.146205569184663  - accuracy: 0.75\n",
      "At: 1334 [==========>] Loss 0.09072565381641423  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.1656432099093176  - accuracy: 0.75\n",
      "At: 1336 [==========>] Loss 0.10162111425474271  - accuracy: 0.90625\n",
      "At: 1337 [==========>] Loss 0.1648843600210561  - accuracy: 0.8125\n",
      "At: 1338 [==========>] Loss 0.12416245383857172  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.10594993806702649  - accuracy: 0.875\n",
      "At: 1340 [==========>] Loss 0.12157336936753463  - accuracy: 0.75\n",
      "At: 1341 [==========>] Loss 0.09576218895384528  - accuracy: 0.8125\n",
      "At: 1342 [==========>] Loss 0.1110660695132187  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.18728191465949584  - accuracy: 0.75\n",
      "At: 1344 [==========>] Loss 0.1917250984445027  - accuracy: 0.71875\n",
      "At: 1345 [==========>] Loss 0.0757422856407341  - accuracy: 0.90625\n",
      "At: 1346 [==========>] Loss 0.10605569603407852  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.11461214725976304  - accuracy: 0.84375\n",
      "At: 1348 [==========>] Loss 0.09968583771732344  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.14850386464694035  - accuracy: 0.71875\n",
      "At: 1350 [==========>] Loss 0.1284729812544725  - accuracy: 0.8125\n",
      "At: 1351 [==========>] Loss 0.09661023826437942  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.09385960524232878  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.16909481469531729  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.17177826282880798  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.07245687006623615  - accuracy: 0.875\n",
      "At: 1356 [==========>] Loss 0.12195820130697319  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.14571709202644423  - accuracy: 0.8125\n",
      "At: 1358 [==========>] Loss 0.13610656109242963  - accuracy: 0.75\n",
      "At: 1359 [==========>] Loss 0.07554469130751379  - accuracy: 0.875\n",
      "At: 1360 [==========>] Loss 0.1934711186320866  - accuracy: 0.75\n",
      "At: 1361 [==========>] Loss 0.0966531815574273  - accuracy: 0.84375\n",
      "At: 1362 [==========>] Loss 0.12958752600238965  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.10846189169171316  - accuracy: 0.84375\n",
      "At: 1364 [==========>] Loss 0.13443210536022432  - accuracy: 0.78125\n",
      "At: 1365 [==========>] Loss 0.10830461982485871  - accuracy: 0.84375\n",
      "At: 1366 [==========>] Loss 0.10013305477738248  - accuracy: 0.9375\n",
      "At: 1367 [==========>] Loss 0.12118379867513096  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.17550401061596535  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.07816861402654082  - accuracy: 0.90625\n",
      "At: 1370 [==========>] Loss 0.11320186490242161  - accuracy: 0.875\n",
      "At: 1371 [==========>] Loss 0.21116157663772253  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.10279613764572398  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.13573726966729466  - accuracy: 0.78125\n",
      "At: 1374 [==========>] Loss 0.1283769319860406  - accuracy: 0.84375\n",
      "At: 1375 [==========>] Loss 0.10433502449850149  - accuracy: 0.84375\n",
      "At: 1376 [==========>] Loss 0.09800393641659828  - accuracy: 0.90625\n",
      "At: 1377 [==========>] Loss 0.16627149830381854  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.12234971062895769  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.16627145699277623  - accuracy: 0.75\n",
      "At: 1380 [==========>] Loss 0.13344761573282737  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.07332122356302062  - accuracy: 0.96875\n",
      "At: 1382 [==========>] Loss 0.1373126601967618  - accuracy: 0.75\n",
      "At: 1383 [==========>] Loss 0.08943713426488795  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.0974144819807299  - accuracy: 0.90625\n",
      "At: 1385 [==========>] Loss 0.1958946668122149  - accuracy: 0.6875\n",
      "At: 1386 [==========>] Loss 0.15523620221456375  - accuracy: 0.8125\n",
      "At: 1387 [==========>] Loss 0.07332886989059519  - accuracy: 0.875\n",
      "At: 1388 [==========>] Loss 0.16103358771909237  - accuracy: 0.78125\n",
      "At: 1389 [==========>] Loss 0.09857825252740217  - accuracy: 0.90625\n",
      "At: 1390 [==========>] Loss 0.1484847877523104  - accuracy: 0.78125\n",
      "At: 1391 [==========>] Loss 0.1065384927725295  - accuracy: 0.90625\n",
      "At: 1392 [==========>] Loss 0.11461431020041726  - accuracy: 0.8125\n",
      "At: 1393 [==========>] Loss 0.14550683127734082  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.08545033039030243  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.23288141980697835  - accuracy: 0.59375\n",
      "At: 1396 [==========>] Loss 0.058709004018635616  - accuracy: 0.9375\n",
      "At: 1397 [==========>] Loss 0.11207973740558204  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.12620086306140396  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.11065323977686951  - accuracy: 0.875\n",
      "At: 1400 [==========>] Loss 0.16391762930843687  - accuracy: 0.75\n",
      "At: 1401 [==========>] Loss 0.056508655125909885  - accuracy: 0.9375\n",
      "At: 1402 [==========>] Loss 0.19496296971705812  - accuracy: 0.71875\n",
      "At: 1403 [==========>] Loss 0.1205708460577042  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.10920199467279479  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.0842240509650629  - accuracy: 0.8125\n",
      "At: 1406 [==========>] Loss 0.1594067537326331  - accuracy: 0.84375\n",
      "At: 1407 [==========>] Loss 0.10956873830508299  - accuracy: 0.8125\n",
      "At: 1408 [==========>] Loss 0.1499226247340498  - accuracy: 0.8125\n",
      "At: 1409 [==========>] Loss 0.02067788180960406  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.12768493689036753  - accuracy: 0.8125\n",
      "At: 1411 [==========>] Loss 0.170431624248139  - accuracy: 0.71875\n",
      "At: 1412 [==========>] Loss 0.13523611198570384  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.0949728600267184  - accuracy: 0.875\n",
      "At: 1414 [==========>] Loss 0.19451104068141922  - accuracy: 0.71875\n",
      "At: 1415 [==========>] Loss 0.07211028743391451  - accuracy: 0.96875\n",
      "At: 1416 [==========>] Loss 0.16847693721437731  - accuracy: 0.78125\n",
      "At: 1417 [==========>] Loss 0.11501225353321458  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.14673125958223346  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.09963132902414742  - accuracy: 0.875\n",
      "At: 1420 [==========>] Loss 0.10289812401157758  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.09794319983823124  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.12000041403194593  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.14873254812748624  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.12900373061771964  - accuracy: 0.875\n",
      "At: 1425 [==========>] Loss 0.08478648555180118  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.12617768895368747  - accuracy: 0.75\n",
      "At: 1427 [==========>] Loss 0.11608982060790131  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.10133839118309534  - accuracy: 0.90625\n",
      "At: 1429 [==========>] Loss 0.16542249925976352  - accuracy: 0.78125\n",
      "At: 1430 [==========>] Loss 0.07367739657517325  - accuracy: 0.9375\n",
      "At: 1431 [==========>] Loss 0.11627787444592164  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.07920710307478449  - accuracy: 0.9375\n",
      "At: 1433 [==========>] Loss 0.11543147846539425  - accuracy: 0.8125\n",
      "At: 1434 [==========>] Loss 0.12198394744376641  - accuracy: 0.84375\n",
      "At: 1435 [==========>] Loss 0.12306940553172922  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.07078528936172783  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.14407883540193014  - accuracy: 0.78125\n",
      "At: 1438 [==========>] Loss 0.14547827458313575  - accuracy: 0.84375\n",
      "At: 1439 [==========>] Loss 0.12352529192298779  - accuracy: 0.78125\n",
      "At: 1440 [==========>] Loss 0.08027542833789336  - accuracy: 0.90625\n",
      "At: 1441 [==========>] Loss 0.07741406424786884  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.1017008708600492  - accuracy: 0.78125\n",
      "At: 1443 [==========>] Loss 0.13948898858501202  - accuracy: 0.75\n",
      "At: 1444 [==========>] Loss 0.13491103264752896  - accuracy: 0.78125\n",
      "At: 1445 [==========>] Loss 0.16698401537775032  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.180481921106249  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.1543994804550739  - accuracy: 0.78125\n",
      "At: 1448 [==========>] Loss 0.07278590233640218  - accuracy: 0.875\n",
      "At: 1449 [==========>] Loss 0.14338947265933527  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.1094971579731118  - accuracy: 0.875\n",
      "At: 1451 [==========>] Loss 0.12630620630385378  - accuracy: 0.75\n",
      "At: 1452 [==========>] Loss 0.09691815142752912  - accuracy: 0.84375\n",
      "At: 1453 [==========>] Loss 0.05086166414146202  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.17869596115818537  - accuracy: 0.71875\n",
      "At: 1455 [==========>] Loss 0.11510754656743406  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.09106350779470569  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.10676070938222594  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.14812091902505486  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.1395052518748986  - accuracy: 0.84375\n",
      "At: 1460 [==========>] Loss 0.18564631613598295  - accuracy: 0.6875\n",
      "At: 1461 [==========>] Loss 0.09482858721666479  - accuracy: 0.90625\n",
      "At: 1462 [==========>] Loss 0.16196939446977393  - accuracy: 0.78125\n",
      "At: 1463 [==========>] Loss 0.10806973060440625  - accuracy: 0.84375\n",
      "At: 1464 [==========>] Loss 0.17271855186620264  - accuracy: 0.78125\n",
      "At: 1465 [==========>] Loss 0.10737997917899095  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.0816184133166524  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.17560339285357185  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.1457399171509186  - accuracy: 0.8125\n",
      "At: 1469 [==========>] Loss 0.1645083776326465  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.13069158711141793  - accuracy: 0.8125\n",
      "At: 1471 [==========>] Loss 0.12892339581811557  - accuracy: 0.84375\n",
      "At: 1472 [==========>] Loss 0.0999053919896927  - accuracy: 0.84375\n",
      "At: 1473 [==========>] Loss 0.12456086454223508  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.1939540531564232  - accuracy: 0.6875\n",
      "At: 1475 [==========>] Loss 0.15475675121705115  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.11059334176790421  - accuracy: 0.8125\n",
      "At: 1477 [==========>] Loss 0.08073954987195905  - accuracy: 0.90625\n",
      "At: 1478 [==========>] Loss 0.0765817083440872  - accuracy: 0.96875\n",
      "At: 1479 [==========>] Loss 0.13607573316269814  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.08408059286215434  - accuracy: 0.875\n",
      "At: 1481 [==========>] Loss 0.17448493853365932  - accuracy: 0.75\n",
      "At: 1482 [==========>] Loss 0.13982773548800315  - accuracy: 0.78125\n",
      "At: 1483 [==========>] Loss 0.19788444390468363  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.1456600702589007  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.1647434377748633  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.08838823279261428  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.07365248660064419  - accuracy: 0.84375\n",
      "At: 1488 [==========>] Loss 0.12090767800748761  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.19241689784523958  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.12108022141034575  - accuracy: 0.8125\n",
      "At: 1491 [==========>] Loss 0.14560589375485444  - accuracy: 0.71875\n",
      "At: 1492 [==========>] Loss 0.15780828416656417  - accuracy: 0.71875\n",
      "At: 1493 [==========>] Loss 0.15619907236661043  - accuracy: 0.78125\n",
      "At: 1494 [==========>] Loss 0.14451292039816518  - accuracy: 0.84375\n",
      "At: 1495 [==========>] Loss 0.11952563413529088  - accuracy: 0.8125\n",
      "At: 1496 [==========>] Loss 0.09846879087185018  - accuracy: 0.84375\n",
      "At: 1497 [==========>] Loss 0.17994146586398302  - accuracy: 0.8125\n",
      "At: 1498 [==========>] Loss 0.17420635699780682  - accuracy: 0.71875\n",
      "At: 1499 [==========>] Loss 0.09392430604863541  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.0906704913682837  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.09276755501350264  - accuracy: 0.84375\n",
      "At: 1502 [==========>] Loss 0.13591592214744813  - accuracy: 0.84375\n",
      "At: 1503 [==========>] Loss 0.14327526227351428  - accuracy: 0.78125\n",
      "At: 1504 [==========>] Loss 0.14166661473868464  - accuracy: 0.8125\n",
      "At: 1505 [==========>] Loss 0.11670693054428195  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.14559467799858136  - accuracy: 0.78125\n",
      "At: 1507 [==========>] Loss 0.13602339150469053  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.2318891140684584  - accuracy: 0.65625\n",
      "At: 1509 [==========>] Loss 0.09088440212635773  - accuracy: 0.875\n",
      "At: 1510 [==========>] Loss 0.12234385240024481  - accuracy: 0.78125\n",
      "At: 1511 [==========>] Loss 0.10834296516887351  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.06720808698773403  - accuracy: 0.9375\n",
      "At: 1513 [==========>] Loss 0.13752778012192277  - accuracy: 0.875\n",
      "At: 1514 [==========>] Loss 0.14945390387350227  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.1798060923712909  - accuracy: 0.6875\n",
      "At: 1516 [==========>] Loss 0.09732931821870792  - accuracy: 0.90625\n",
      "At: 1517 [==========>] Loss 0.14770851478545358  - accuracy: 0.78125\n",
      "At: 1518 [==========>] Loss 0.12247713378276481  - accuracy: 0.8125\n",
      "At: 1519 [==========>] Loss 0.1437712795139355  - accuracy: 0.8125\n",
      "At: 1520 [==========>] Loss 0.10944401009808384  - accuracy: 0.875\n",
      "At: 1521 [==========>] Loss 0.0900339499462412  - accuracy: 0.8125\n",
      "At: 1522 [==========>] Loss 0.16331904333788383  - accuracy: 0.78125\n",
      "At: 1523 [==========>] Loss 0.10727553393672909  - accuracy: 0.875\n",
      "At: 1524 [==========>] Loss 0.1287685312944582  - accuracy: 0.875\n",
      "At: 1525 [==========>] Loss 0.1368863642431815  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.07470401313806137  - accuracy: 0.9375\n",
      "At: 1527 [==========>] Loss 0.12863401078582642  - accuracy: 0.84375\n",
      "At: 1528 [==========>] Loss 0.17076399371185993  - accuracy: 0.75\n",
      "At: 1529 [==========>] Loss 0.07320318114584157  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.06066954217135817  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.11743697292076183  - accuracy: 0.90625\n",
      "At: 1532 [==========>] Loss 0.17466572653187012  - accuracy: 0.78125\n",
      "At: 1533 [==========>] Loss 0.1422375048528357  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.11473209014751864  - accuracy: 0.8125\n",
      "At: 1535 [==========>] Loss 0.1632773725547973  - accuracy: 0.78125\n",
      "At: 1536 [==========>] Loss 0.1336006281704081  - accuracy: 0.84375\n",
      "At: 1537 [==========>] Loss 0.10656433498813894  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.12099501556403003  - accuracy: 0.78125\n",
      "At: 1539 [==========>] Loss 0.07954974095604983  - accuracy: 0.96875\n",
      "At: 1540 [==========>] Loss 0.13649149138531153  - accuracy: 0.75\n",
      "At: 1541 [==========>] Loss 0.10932307801621649  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.09742135441700195  - accuracy: 0.84375\n",
      "At: 1543 [==========>] Loss 0.13608039038233996  - accuracy: 0.84375\n",
      "At: 1544 [==========>] Loss 0.16297014565224588  - accuracy: 0.75\n",
      "At: 1545 [==========>] Loss 0.21588890500504032  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.13991825668534968  - accuracy: 0.75\n",
      "At: 1547 [==========>] Loss 0.15318032399195253  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.13014487729573537  - accuracy: 0.875\n",
      "At: 1549 [==========>] Loss 0.16105753075724494  - accuracy: 0.78125\n",
      "At: 1550 [==========>] Loss 0.09737334323263093  - accuracy: 0.875\n",
      "At: 1551 [==========>] Loss 0.1623463677348367  - accuracy: 0.71875\n",
      "At: 1552 [==========>] Loss 0.10140343538136967  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.062339224320717646  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.14652845949306628  - accuracy: 0.78125\n",
      "At: 1555 [==========>] Loss 0.14122114864183882  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.14851771199392358  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.10197101176339676  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.15303592985068415  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.0753786119219664  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.12022453852564827  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.15606585165071127  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.09972847135254351  - accuracy: 0.90625\n",
      "At: 1563 [==========>] Loss 0.09690575288936876  - accuracy: 0.84375\n",
      "At: 1564 [==========>] Loss 0.10644922206746121  - accuracy: 0.84375\n",
      "At: 1565 [==========>] Loss 0.12516299031084252  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.13720516002550165  - accuracy: 0.78125\n",
      "At: 1567 [==========>] Loss 0.16508136569076454  - accuracy: 0.71875\n",
      "At: 1568 [==========>] Loss 0.0755575566885791  - accuracy: 0.875\n",
      "At: 1569 [==========>] Loss 0.11178195163801394  - accuracy: 0.84375\n",
      "At: 1570 [==========>] Loss 0.10270867120057411  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.16348607480522492  - accuracy: 0.75\n",
      "At: 1572 [==========>] Loss 0.13404818412923164  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.05066285838535255  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.13109780193157677  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.1047973461143778  - accuracy: 0.84375\n",
      "At: 1576 [==========>] Loss 0.1294576251404805  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.08503236082604546  - accuracy: 0.875\n",
      "At: 1578 [==========>] Loss 0.06515064515316824  - accuracy: 0.9375\n",
      "At: 1579 [==========>] Loss 0.10241527417974741  - accuracy: 0.875\n",
      "At: 1580 [==========>] Loss 0.12648867898271823  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.10537323015393885  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.16206302406391263  - accuracy: 0.8125\n",
      "At: 1583 [==========>] Loss 0.0677121168082625  - accuracy: 0.9375\n",
      "At: 1584 [==========>] Loss 0.14361219430928976  - accuracy: 0.8125\n",
      "At: 1585 [==========>] Loss 0.101509397048587  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.1238484319382854  - accuracy: 0.84375\n",
      "At: 1587 [==========>] Loss 0.09005958623498156  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.1432306871071098  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.16082158957172743  - accuracy: 0.78125\n",
      "At: 1590 [==========>] Loss 0.12613677257479683  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.11773562919280706  - accuracy: 0.875\n",
      "At: 1592 [==========>] Loss 0.0731261151902006  - accuracy: 0.9375\n",
      "At: 1593 [==========>] Loss 0.18328200392449565  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.08767590865959933  - accuracy: 0.9375\n",
      "At: 1595 [==========>] Loss 0.1426304491923968  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.1637671844989974  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.13365587815352603  - accuracy: 0.84375\n",
      "At: 1598 [==========>] Loss 0.1759856997462414  - accuracy: 0.78125\n",
      "At: 1599 [==========>] Loss 0.2464294142506756  - accuracy: 0.6875\n",
      "At: 1600 [==========>] Loss 0.17040635117421415  - accuracy: 0.78125\n",
      "At: 1601 [==========>] Loss 0.08904747712428654  - accuracy: 0.9375\n",
      "At: 1602 [==========>] Loss 0.12545886586253507  - accuracy: 0.84375\n",
      "At: 1603 [==========>] Loss 0.17069795407494423  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.21161998424766845  - accuracy: 0.6875\n",
      "At: 1605 [==========>] Loss 0.08733320987292546  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.11548848850211124  - accuracy: 0.84375\n",
      "At: 1607 [==========>] Loss 0.16034334344800766  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.14784966541298603  - accuracy: 0.71875\n",
      "At: 1609 [==========>] Loss 0.1380910966259492  - accuracy: 0.84375\n",
      "At: 1610 [==========>] Loss 0.17239379097014437  - accuracy: 0.75\n",
      "At: 1611 [==========>] Loss 0.07832618004839047  - accuracy: 0.90625\n",
      "At: 1612 [==========>] Loss 0.06732584397274614  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.14699661520572632  - accuracy: 0.78125\n",
      "At: 1614 [==========>] Loss 0.15695697442585677  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.10010856071715896  - accuracy: 0.8125\n",
      "At: 1616 [==========>] Loss 0.1426888694833559  - accuracy: 0.75\n",
      "At: 1617 [==========>] Loss 0.10496180841392747  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11820908840733582  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.16667775395577214  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.07161455246206051  - accuracy: 0.90625\n",
      "At: 1621 [==========>] Loss 0.09201431566728258  - accuracy: 0.90625\n",
      "At: 1622 [==========>] Loss 0.17841227871387633  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.0822120687446653  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.15256142702715714  - accuracy: 0.71875\n",
      "At: 1625 [==========>] Loss 0.15660545566507433  - accuracy: 0.75\n",
      "At: 1626 [==========>] Loss 0.09932106903542401  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.1200103948728708  - accuracy: 0.84375\n",
      "At: 1628 [==========>] Loss 0.17808325238618897  - accuracy: 0.6875\n",
      "At: 1629 [==========>] Loss 0.12374967776729484  - accuracy: 0.84375\n",
      "At: 1630 [==========>] Loss 0.09763923717103151  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.10190617867186509  - accuracy: 0.84375\n",
      "At: 1632 [==========>] Loss 0.07581661809059619  - accuracy: 0.9375\n",
      "At: 1633 [==========>] Loss 0.0691808020563808  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.11594012070931026  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.23024796746906817  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.13129449889251166  - accuracy: 0.78125\n",
      "At: 1637 [==========>] Loss 0.08165821157573956  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.1344214087752752  - accuracy: 0.8125\n",
      "At: 1639 [==========>] Loss 0.16868180644581438  - accuracy: 0.78125\n",
      "At: 1640 [==========>] Loss 0.14659724197411733  - accuracy: 0.78125\n",
      "At: 1641 [==========>] Loss 0.08023828257567212  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.09931254498096917  - accuracy: 0.8125\n",
      "At: 1643 [==========>] Loss 0.08259021949017084  - accuracy: 0.90625\n",
      "At: 1644 [==========>] Loss 0.10127064802078627  - accuracy: 0.875\n",
      "At: 1645 [==========>] Loss 0.0637875135306277  - accuracy: 0.9375\n",
      "At: 1646 [==========>] Loss 0.14100145782630666  - accuracy: 0.8125\n",
      "At: 1647 [==========>] Loss 0.1774858702741302  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.14497915925161106  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.11062551025162154  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.08619704616471535  - accuracy: 0.90625\n",
      "At: 1651 [==========>] Loss 0.10918576461743731  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.08595699449861459  - accuracy: 0.84375\n",
      "At: 1653 [==========>] Loss 0.09234518108859066  - accuracy: 0.84375\n",
      "At: 1654 [==========>] Loss 0.05512223622125948  - accuracy: 0.9375\n",
      "At: 1655 [==========>] Loss 0.17886313605503595  - accuracy: 0.78125\n",
      "At: 1656 [==========>] Loss 0.12466113925229179  - accuracy: 0.78125\n",
      "At: 1657 [==========>] Loss 0.15117621041759916  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.2150382360092134  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.08117439007734668  - accuracy: 0.90625\n",
      "At: 1660 [==========>] Loss 0.05134657644310102  - accuracy: 0.9375\n",
      "At: 1661 [==========>] Loss 0.09580637183574477  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.1301651541064448  - accuracy: 0.8125\n",
      "At: 1663 [==========>] Loss 0.16747565529244354  - accuracy: 0.6875\n",
      "At: 1664 [==========>] Loss 0.12217962733187686  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.08827734020163752  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.08130621129911564  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.16882174505838063  - accuracy: 0.71875\n",
      "At: 1668 [==========>] Loss 0.15557306301565166  - accuracy: 0.78125\n",
      "At: 1669 [==========>] Loss 0.1349078683402516  - accuracy: 0.8125\n",
      "At: 1670 [==========>] Loss 0.09028924724275758  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.11178423979917862  - accuracy: 0.90625\n",
      "At: 1672 [==========>] Loss 0.14708586489531872  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.06164678222204191  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.1575386863612674  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.1837878081425987  - accuracy: 0.78125\n",
      "At: 1676 [==========>] Loss 0.20076053995401952  - accuracy: 0.71875\n",
      "At: 1677 [==========>] Loss 0.102812668665427  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.1675631914288689  - accuracy: 0.75\n",
      "At: 1679 [==========>] Loss 0.10793849103226363  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.15641787529218812  - accuracy: 0.78125\n",
      "At: 1681 [==========>] Loss 0.11463036318308066  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.08220756949320779  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.18295784404701973  - accuracy: 0.71875\n",
      "At: 1684 [==========>] Loss 0.1271834970339134  - accuracy: 0.875\n",
      "At: 1685 [==========>] Loss 0.11050360543978606  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.13922302071919404  - accuracy: 0.875\n",
      "At: 1687 [==========>] Loss 0.15974189032168562  - accuracy: 0.71875\n",
      "At: 1688 [==========>] Loss 0.04350348494240907  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.11832095197953976  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.11412463438607981  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.09178550662947105  - accuracy: 0.875\n",
      "At: 1692 [==========>] Loss 0.15372747215699764  - accuracy: 0.6875\n",
      "At: 1693 [==========>] Loss 0.08374113137937779  - accuracy: 0.9375\n",
      "At: 1694 [==========>] Loss 0.09199466539869845  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.13751429342527383  - accuracy: 0.8125\n",
      "At: 1696 [==========>] Loss 0.15451801862672715  - accuracy: 0.78125\n",
      "At: 1697 [==========>] Loss 0.09389867030212568  - accuracy: 0.90625\n",
      "At: 1698 [==========>] Loss 0.07055753311777682  - accuracy: 0.9375\n",
      "At: 1699 [==========>] Loss 0.13602055500135007  - accuracy: 0.78125\n",
      "At: 1700 [==========>] Loss 0.1352480326892044  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.10521122023097315  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.09458736433514515  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.21968272781558498  - accuracy: 0.6875\n",
      "At: 1704 [==========>] Loss 0.08300099069665254  - accuracy: 0.84375\n",
      "At: 1705 [==========>] Loss 0.13183060278127223  - accuracy: 0.84375\n",
      "At: 1706 [==========>] Loss 0.1567455632210399  - accuracy: 0.8125\n",
      "At: 1707 [==========>] Loss 0.18422333576719355  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.09704616772399537  - accuracy: 0.875\n",
      "At: 1709 [==========>] Loss 0.1784332649383444  - accuracy: 0.65625\n",
      "At: 1710 [==========>] Loss 0.1507592685178599  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.07275156804558486  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.10750313901873955  - accuracy: 0.84375\n",
      "At: 1713 [==========>] Loss 0.09720782593581193  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.15872092916596253  - accuracy: 0.78125\n",
      "At: 1715 [==========>] Loss 0.11858162798689476  - accuracy: 0.8125\n",
      "At: 1716 [==========>] Loss 0.05860601639720288  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.10916149089428046  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.13772913526878047  - accuracy: 0.8125\n",
      "At: 1719 [==========>] Loss 0.10158182628448428  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.06934420038455864  - accuracy: 0.875\n",
      "At: 1721 [==========>] Loss 0.16621696168494476  - accuracy: 0.78125\n",
      "At: 1722 [==========>] Loss 0.04681100873530088  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.18354519087845986  - accuracy: 0.71875\n",
      "At: 1724 [==========>] Loss 0.0756373633116218  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.11957661463871802  - accuracy: 0.84375\n",
      "At: 1726 [==========>] Loss 0.11126310692928684  - accuracy: 0.875\n",
      "At: 1727 [==========>] Loss 0.13298887388609948  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.11252040258666134  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.19115940415258564  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.13420879326089935  - accuracy: 0.84375\n",
      "At: 1731 [==========>] Loss 0.0965335464399224  - accuracy: 0.875\n",
      "At: 1732 [==========>] Loss 0.0794257089260785  - accuracy: 0.90625\n",
      "At: 1733 [==========>] Loss 0.1657391932850756  - accuracy: 0.78125\n",
      "At: 1734 [==========>] Loss 0.08891728187378864  - accuracy: 0.90625\n",
      "At: 1735 [==========>] Loss 0.1294381607808075  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.11729210079283534  - accuracy: 0.90625\n",
      "At: 1737 [==========>] Loss 0.17403521692695095  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.13940894268090512  - accuracy: 0.78125\n",
      "At: 1739 [==========>] Loss 0.13040950235403026  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.12383106810122109  - accuracy: 0.875\n",
      "At: 1741 [==========>] Loss 0.15194375590558767  - accuracy: 0.84375\n",
      "At: 1742 [==========>] Loss 0.044309558832764176  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.1374558371971401  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.092999074132716  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.12256661111482843  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.13933780185031747  - accuracy: 0.875\n",
      "At: 1747 [==========>] Loss 0.12545886828061026  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.13267932686738626  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.11806440683877811  - accuracy: 0.8125\n",
      "At: 1750 [==========>] Loss 0.12171746810960579  - accuracy: 0.8125\n",
      "At: 1751 [==========>] Loss 0.16885632462329042  - accuracy: 0.71875\n",
      "At: 1752 [==========>] Loss 0.10990129850197886  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.09524489849061324  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.11136629164356612  - accuracy: 0.875\n",
      "At: 1755 [==========>] Loss 0.07755181430692336  - accuracy: 0.84375\n",
      "At: 1756 [==========>] Loss 0.14190574181390603  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.1626699757701705  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.08306696408029554  - accuracy: 0.84375\n",
      "At: 1759 [==========>] Loss 0.0997012271537475  - accuracy: 0.84375\n",
      "At: 1760 [==========>] Loss 0.08064107930984381  - accuracy: 0.90625\n",
      "At: 1761 [==========>] Loss 0.1265032816346344  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.167406028153848  - accuracy: 0.71875\n",
      "At: 1763 [==========>] Loss 0.10518248281708292  - accuracy: 0.875\n",
      "At: 1764 [==========>] Loss 0.13341856176741707  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.13873062514982265  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.06844397341821742  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.0916036826368262  - accuracy: 0.875\n",
      "At: 1768 [==========>] Loss 0.10119261876071114  - accuracy: 0.8125\n",
      "At: 1769 [==========>] Loss 0.07422281882362078  - accuracy: 0.84375\n",
      "At: 1770 [==========>] Loss 0.0532541243323406  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.1668507308382001  - accuracy: 0.71875\n",
      "At: 1772 [==========>] Loss 0.139395245702988  - accuracy: 0.78125\n",
      "At: 1773 [==========>] Loss 0.08803671802686784  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.13600127612164856  - accuracy: 0.84375\n",
      "At: 1775 [==========>] Loss 0.09046731187261595  - accuracy: 0.90625\n",
      "At: 1776 [==========>] Loss 0.1160300594537459  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.10156981995393646  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.10073332036750161  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.08415107827425583  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.1220956805443629  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.1875993306490628  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.09493062041788126  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.12092756915583366  - accuracy: 0.78125\n",
      "At: 1784 [==========>] Loss 0.08494858741104647  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.09337017174842734  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.11920937394913757  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.13103265971522624  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.09940628055914841  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.10687165875818619  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.20209492958478803  - accuracy: 0.6875\n",
      "At: 1791 [==========>] Loss 0.07231920295960055  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.13053338409153678  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.08908009867223982  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.16943961312933015  - accuracy: 0.65625\n",
      "At: 1795 [==========>] Loss 0.08913528087169167  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.135146208619571  - accuracy: 0.78125\n",
      "At: 1797 [==========>] Loss 0.12386568751723631  - accuracy: 0.875\n",
      "At: 1798 [==========>] Loss 0.11891292205926657  - accuracy: 0.84375\n",
      "At: 1799 [==========>] Loss 0.07073020616135656  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.10724935840528246  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.17705886367820473  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.12300302587888409  - accuracy: 0.84375\n",
      "At: 1803 [==========>] Loss 0.17106340937148076  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.12597098405305376  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.03455045231799025  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.1456710812601711  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.17988409408583894  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.17213685163712056  - accuracy: 0.8125\n",
      "At: 1809 [==========>] Loss 0.08201692303125982  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.13506186483241944  - accuracy: 0.75\n",
      "At: 1811 [==========>] Loss 0.12256844347916271  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.10376415759089967  - accuracy: 0.875\n",
      "At: 1813 [==========>] Loss 0.12044664234669601  - accuracy: 0.8125\n",
      "At: 1814 [==========>] Loss 0.09870594841218572  - accuracy: 0.8125\n",
      "At: 1815 [==========>] Loss 0.17557325721364647  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.04269250358837305  - accuracy: 0.9375\n",
      "At: 1817 [==========>] Loss 0.15231130912733204  - accuracy: 0.71875\n",
      "At: 1818 [==========>] Loss 0.13953699955430565  - accuracy: 0.78125\n",
      "At: 1819 [==========>] Loss 0.18073802942743522  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.11607761447325854  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.09910087732573775  - accuracy: 0.8125\n",
      "At: 1822 [==========>] Loss 0.14448275735572863  - accuracy: 0.75\n",
      "At: 1823 [==========>] Loss 0.15539088671548495  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.1484858976799519  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.12981227577918927  - accuracy: 0.84375\n",
      "At: 1826 [==========>] Loss 0.060028735202461894  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11791349717351422  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.16730988364070437  - accuracy: 0.8125\n",
      "At: 1829 [==========>] Loss 0.11262703141908995  - accuracy: 0.875\n",
      "At: 1830 [==========>] Loss 0.13967987698536186  - accuracy: 0.71875\n",
      "At: 1831 [==========>] Loss 0.13881835388345476  - accuracy: 0.875\n",
      "At: 1832 [==========>] Loss 0.13534173148533066  - accuracy: 0.75\n",
      "At: 1833 [==========>] Loss 0.11408429584503672  - accuracy: 0.875\n",
      "At: 1834 [==========>] Loss 0.08221053534756011  - accuracy: 0.84375\n",
      "At: 1835 [==========>] Loss 0.1401447494342684  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.11437223395402735  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.0474737564632159  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.08565304117450626  - accuracy: 0.90625\n",
      "At: 1839 [==========>] Loss 0.11454774122404325  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.13659148613182193  - accuracy: 0.78125\n",
      "At: 1841 [==========>] Loss 0.09297532395814057  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.13803001691970046  - accuracy: 0.8125\n",
      "At: 1843 [==========>] Loss 0.12539246752762662  - accuracy: 0.78125\n",
      "At: 1844 [==========>] Loss 0.08318480257487226  - accuracy: 0.875\n",
      "At: 1845 [==========>] Loss 0.1598631503438852  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.14458278115734116  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.06601914410154738  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.07443159973219791  - accuracy: 0.875\n",
      "At: 1849 [==========>] Loss 0.19639988152546278  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.04083528238952376  - accuracy: 1.0\n",
      "At: 1851 [==========>] Loss 0.17632748488506989  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.07439805132879795  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.11731919502250816  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.13755178074961066  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.1988775578465354  - accuracy: 0.78125\n",
      "At: 1856 [==========>] Loss 0.12470561667044061  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.13437391150779582  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.09854587023105824  - accuracy: 0.90625\n",
      "At: 1859 [==========>] Loss 0.14012310301919734  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.13419602921321638  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.09003081119103787  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.20834698328532708  - accuracy: 0.71875\n",
      "At: 1863 [==========>] Loss 0.14648881433472585  - accuracy: 0.8125\n",
      "At: 1864 [==========>] Loss 0.1308186722825484  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.1003811134013885  - accuracy: 0.84375\n",
      "At: 1866 [==========>] Loss 0.1967269304487349  - accuracy: 0.6875\n",
      "At: 1867 [==========>] Loss 0.07902745556028655  - accuracy: 0.96875\n",
      "At: 1868 [==========>] Loss 0.14956818745123462  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.1697185089273478  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.09379365897844505  - accuracy: 0.8125\n",
      "At: 1871 [==========>] Loss 0.13259215768541854  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.16040609524988397  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.09369834948733528  - accuracy: 0.84375\n",
      "At: 1874 [==========>] Loss 0.15935466772612153  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.10649564201178309  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.2007363459938457  - accuracy: 0.65625\n",
      "At: 1877 [==========>] Loss 0.11503155693586717  - accuracy: 0.875\n",
      "At: 1878 [==========>] Loss 0.11699657132141034  - accuracy: 0.8125\n",
      "At: 1879 [==========>] Loss 0.12219518330904142  - accuracy: 0.875\n",
      "At: 1880 [==========>] Loss 0.09096785719812608  - accuracy: 0.875\n",
      "At: 1881 [==========>] Loss 0.09226850459941728  - accuracy: 0.84375\n",
      "At: 1882 [==========>] Loss 0.11295861350799216  - accuracy: 0.90625\n",
      "At: 1883 [==========>] Loss 0.15193318367074754  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.0879305559931892  - accuracy: 0.90625\n",
      "At: 1885 [==========>] Loss 0.11789556033191625  - accuracy: 0.75\n",
      "At: 1886 [==========>] Loss 0.13280280674951234  - accuracy: 0.8125\n",
      "At: 1887 [==========>] Loss 0.07190280311072601  - accuracy: 0.9375\n",
      "At: 1888 [==========>] Loss 0.1564848429591639  - accuracy: 0.8125\n",
      "At: 1889 [==========>] Loss 0.09841558383999757  - accuracy: 0.875\n",
      "At: 1890 [==========>] Loss 0.1553562768417283  - accuracy: 0.84375\n",
      "At: 1891 [==========>] Loss 0.05174734138921873  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.0599524332485315  - accuracy: 0.96875\n",
      "At: 1893 [==========>] Loss 0.0834987887257976  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.09184311227264441  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.05982984257143383  - accuracy: 0.9375\n",
      "At: 1896 [==========>] Loss 0.13935502664496652  - accuracy: 0.78125\n",
      "At: 1897 [==========>] Loss 0.0664642787088135  - accuracy: 0.9375\n",
      "At: 1898 [==========>] Loss 0.10106565858297967  - accuracy: 0.875\n",
      "At: 1899 [==========>] Loss 0.09127953494232421  - accuracy: 0.90625\n",
      "At: 1900 [==========>] Loss 0.0963419998998793  - accuracy: 0.875\n",
      "At: 1901 [==========>] Loss 0.12210902340225917  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.14362249070815936  - accuracy: 0.75\n",
      "At: 1903 [==========>] Loss 0.11944602657901858  - accuracy: 0.84375\n",
      "At: 1904 [==========>] Loss 0.0572939917948862  - accuracy: 0.9375\n",
      "At: 1905 [==========>] Loss 0.1353394149298964  - accuracy: 0.78125\n",
      "At: 1906 [==========>] Loss 0.10789908419268632  - accuracy: 0.84375\n",
      "At: 1907 [==========>] Loss 0.06698269410748506  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.09262127941043001  - accuracy: 0.84375\n",
      "At: 1909 [==========>] Loss 0.09843272379284182  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.04823977575672883  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.12892237115957533  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.10895916604665802  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.1660208114882782  - accuracy: 0.78125\n",
      "At: 1914 [==========>] Loss 0.07706232085207496  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.11374481343115972  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.13502921813277716  - accuracy: 0.84375\n",
      "At: 1917 [==========>] Loss 0.16253040014280434  - accuracy: 0.78125\n",
      "At: 1918 [==========>] Loss 0.1633412022188072  - accuracy: 0.71875\n",
      "At: 1919 [==========>] Loss 0.10277738477124407  - accuracy: 0.875\n",
      "At: 1920 [==========>] Loss 0.10387112837188031  - accuracy: 0.875\n",
      "At: 1921 [==========>] Loss 0.15904988296052303  - accuracy: 0.78125\n",
      "At: 1922 [==========>] Loss 0.12377648569972671  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.18180964793391907  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.13177353531667715  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.14533062370284228  - accuracy: 0.8125\n",
      "At: 1926 [==========>] Loss 0.09772760716257584  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.0992180707818824  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.13953128215955618  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.15269044077960642  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.18084011735367628  - accuracy: 0.65625\n",
      "At: 1931 [==========>] Loss 0.11280111581102985  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.15479018659230093  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.09264394523416694  - accuracy: 0.84375\n",
      "At: 1934 [==========>] Loss 0.16333149476400016  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.14257321992495045  - accuracy: 0.78125\n",
      "At: 1936 [==========>] Loss 0.10065798185348294  - accuracy: 0.84375\n",
      "At: 1937 [==========>] Loss 0.16535984766202177  - accuracy: 0.71875\n",
      "At: 1938 [==========>] Loss 0.19164312249461307  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.1029687915796089  - accuracy: 0.84375\n",
      "At: 1940 [==========>] Loss 0.12156441814631888  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.11976949055778062  - accuracy: 0.875\n",
      "At: 1942 [==========>] Loss 0.16292448862707057  - accuracy: 0.71875\n",
      "At: 1943 [==========>] Loss 0.14122792131181452  - accuracy: 0.8125\n",
      "At: 1944 [==========>] Loss 0.11709292999098253  - accuracy: 0.8125\n",
      "At: 1945 [==========>] Loss 0.1585218413827382  - accuracy: 0.84375\n",
      "At: 1946 [==========>] Loss 0.09975889292261234  - accuracy: 0.875\n",
      "At: 1947 [==========>] Loss 0.10895177931628526  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.1237012441641913  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.0781091141792105  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.15705088147200716  - accuracy: 0.78125\n",
      "At: 1951 [==========>] Loss 0.1255004572566762  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.07817054989875019  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.06831975283977096  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.20876171253397924  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.05636605917816283  - accuracy: 0.9375\n",
      "At: 1956 [==========>] Loss 0.11711201388536749  - accuracy: 0.90625\n",
      "At: 1957 [==========>] Loss 0.07882119048900764  - accuracy: 0.875\n",
      "At: 1958 [==========>] Loss 0.08337398777854993  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.14483156657947358  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.05167441554986407  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.1912780355086414  - accuracy: 0.71875\n",
      "At: 1962 [==========>] Loss 0.19037864114621036  - accuracy: 0.6875\n",
      "At: 1963 [==========>] Loss 0.07017096579006364  - accuracy: 0.90625\n",
      "At: 1964 [==========>] Loss 0.18391600992555296  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.13582267671765613  - accuracy: 0.8125\n",
      "At: 1966 [==========>] Loss 0.1191966358871189  - accuracy: 0.84375\n",
      "At: 1967 [==========>] Loss 0.13049330313618313  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.18220127402607866  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.18768141459860646  - accuracy: 0.71875\n",
      "At: 1970 [==========>] Loss 0.10436561393388077  - accuracy: 0.84375\n",
      "At: 1971 [==========>] Loss 0.17190565721442397  - accuracy: 0.78125\n",
      "At: 1972 [==========>] Loss 0.08670015523380017  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.12230571916429607  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.09779285924822917  - accuracy: 0.84375\n",
      "At: 1975 [==========>] Loss 0.16785320122061972  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.07841554501452033  - accuracy: 0.875\n",
      "At: 1977 [==========>] Loss 0.08069833794500478  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.12182376240697376  - accuracy: 0.875\n",
      "At: 1979 [==========>] Loss 0.12262063271075718  - accuracy: 0.78125\n",
      "At: 1980 [==========>] Loss 0.1090634781755261  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.15501692013629514  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06025721453277444  - accuracy: 0.90625\n",
      "At: 1983 [==========>] Loss 0.1252977067940859  - accuracy: 0.78125\n",
      "At: 1984 [==========>] Loss 0.10775745545573136  - accuracy: 0.84375\n",
      "At: 1985 [==========>] Loss 0.13568303913685756  - accuracy: 0.84375\n",
      "At: 1986 [==========>] Loss 0.1716850268095913  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.13283775383576663  - accuracy: 0.8125\n",
      "At: 1988 [==========>] Loss 0.0942523539488358  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.10621284897772368  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.10231800926920662  - accuracy: 0.875\n",
      "At: 1991 [==========>] Loss 0.13700099063501003  - accuracy: 0.84375\n",
      "At: 1992 [==========>] Loss 0.09983990746961761  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.13388473427021547  - accuracy: 0.8125\n",
      "At: 1994 [==========>] Loss 0.1129011532381807  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.17645281587846331  - accuracy: 0.78125\n",
      "At: 1996 [==========>] Loss 0.10707283320230522  - accuracy: 0.78125\n",
      "At: 1997 [==========>] Loss 0.20555990633586332  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.14024928360458952  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.09640242586716803  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.11642353221551549  - accuracy: 0.84375\n",
      "At: 2001 [==========>] Loss 0.0874713604019072  - accuracy: 0.875\n",
      "At: 2002 [==========>] Loss 0.07162116355358225  - accuracy: 0.875\n",
      "At: 2003 [==========>] Loss 0.12622694399220177  - accuracy: 0.78125\n",
      "At: 2004 [==========>] Loss 0.14233412473435741  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.14754493875978564  - accuracy: 0.78125\n",
      "At: 2006 [==========>] Loss 0.11013500592087871  - accuracy: 0.875\n",
      "At: 2007 [==========>] Loss 0.12650750975480107  - accuracy: 0.8125\n",
      "At: 2008 [==========>] Loss 0.10689302457883407  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.1394007938341584  - accuracy: 0.8125\n",
      "At: 2010 [==========>] Loss 0.09516174469748435  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.10957031038942203  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.09989439301966394  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.09731449438048087  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.23031882897654976  - accuracy: 0.625\n",
      "At: 2015 [==========>] Loss 0.05971866731320121  - accuracy: 0.9375\n",
      "At: 2016 [==========>] Loss 0.10130523068763868  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.09482143712023558  - accuracy: 0.875\n",
      "At: 2018 [==========>] Loss 0.08978255767723034  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.13382392839894725  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.07663816365425254  - accuracy: 0.90625\n",
      "At: 2021 [==========>] Loss 0.10474934376442188  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.1440597381871445  - accuracy: 0.78125\n",
      "At: 2023 [==========>] Loss 0.0815812580620363  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.09184917280902799  - accuracy: 0.84375\n",
      "At: 2025 [==========>] Loss 0.14185650353335708  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.10870547210446171  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.1796619990309375  - accuracy: 0.71875\n",
      "At: 2028 [==========>] Loss 0.10242900442270249  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.1019710162496995  - accuracy: 0.90625\n",
      "At: 2030 [==========>] Loss 0.15931744584595117  - accuracy: 0.8125\n",
      "At: 2031 [==========>] Loss 0.1490816696531988  - accuracy: 0.6875\n",
      "At: 2032 [==========>] Loss 0.15544451749635552  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.13516412007762094  - accuracy: 0.8125\n",
      "At: 2034 [==========>] Loss 0.1994409158915478  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.11191772661859639  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09972183417400268  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.10275245481743144  - accuracy: 0.84375\n",
      "At: 2038 [==========>] Loss 0.11404127971979125  - accuracy: 0.875\n",
      "At: 2039 [==========>] Loss 0.08214845737757914  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.12317153111623944  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.061723226861517294  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.12501919045241625  - accuracy: 0.78125\n",
      "At: 2043 [==========>] Loss 0.10504272892091862  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.08945309953800117  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.2250194972432601  - accuracy: 0.65625\n",
      "At: 2046 [==========>] Loss 0.06807821356057123  - accuracy: 0.90625\n",
      "At: 2047 [==========>] Loss 0.09413669477695086  - accuracy: 0.84375\n",
      "At: 2048 [==========>] Loss 0.09771537745766751  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.15767612420923394  - accuracy: 0.8125\n",
      "At: 2050 [==========>] Loss 0.13249274832819427  - accuracy: 0.8125\n",
      "At: 2051 [==========>] Loss 0.17661752863664307  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.06681198967760907  - accuracy: 0.9375\n",
      "At: 2053 [==========>] Loss 0.10803425853098572  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.11578293406629156  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.08834534425192954  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.11756639049831513  - accuracy: 0.875\n",
      "At: 2057 [==========>] Loss 0.13791839889243615  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.1506175198970235  - accuracy: 0.78125\n",
      "At: 2059 [==========>] Loss 0.21378032179848672  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.11682099420508832  - accuracy: 0.8125\n",
      "At: 2061 [==========>] Loss 0.15014670643031242  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.12406997990898957  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.08255900471121141  - accuracy: 0.90625\n",
      "At: 2064 [==========>] Loss 0.1900171966631185  - accuracy: 0.71875\n",
      "At: 2065 [==========>] Loss 0.040371870405893476  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.17904777732101912  - accuracy: 0.6875\n",
      "At: 2067 [==========>] Loss 0.10690170884350834  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.11060106155259969  - accuracy: 0.8125\n",
      "At: 2069 [==========>] Loss 0.07801538222137336  - accuracy: 0.90625\n",
      "At: 2070 [==========>] Loss 0.1435301387592824  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.1278269922229261  - accuracy: 0.75\n",
      "At: 2072 [==========>] Loss 0.08291011098127327  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.10175300316130442  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.07769013508465861  - accuracy: 0.9375\n",
      "At: 2075 [==========>] Loss 0.10808108896903162  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.1272464704098026  - accuracy: 0.84375\n",
      "At: 2077 [==========>] Loss 0.12852023542847238  - accuracy: 0.875\n",
      "At: 2078 [==========>] Loss 0.09746056268144236  - accuracy: 0.875\n",
      "At: 2079 [==========>] Loss 0.0598247931625505  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.10286669080512924  - accuracy: 0.84375\n",
      "At: 2081 [==========>] Loss 0.13369353014627833  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.11323153068045896  - accuracy: 0.875\n",
      "At: 2083 [==========>] Loss 0.18458801744149134  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.08323027052970858  - accuracy: 0.84375\n",
      "At: 2085 [==========>] Loss 0.10981564118529182  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.10636218241001805  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.15858404431627804  - accuracy: 0.75\n",
      "At: 2088 [==========>] Loss 0.10852942638519558  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.1220768379211241  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.07979331079142299  - accuracy: 0.90625\n",
      "At: 2091 [==========>] Loss 0.14731913218189122  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.08712606561734389  - accuracy: 0.875\n",
      "At: 2093 [==========>] Loss 0.13531316036551871  - accuracy: 0.875\n",
      "At: 2094 [==========>] Loss 0.10977531037658171  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.10017750760200786  - accuracy: 0.90625\n",
      "At: 2096 [==========>] Loss 0.13012972365575876  - accuracy: 0.8125\n",
      "At: 2097 [==========>] Loss 0.13437692548410618  - accuracy: 0.78125\n",
      "At: 2098 [==========>] Loss 0.11695389757760767  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.09746728542089485  - accuracy: 0.875\n",
      "At: 2100 [==========>] Loss 0.06446644616226413  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.15072107897794224  - accuracy: 0.78125\n",
      "At: 2102 [==========>] Loss 0.10015403574713197  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.15275022127527232  - accuracy: 0.84375\n",
      "At: 2104 [==========>] Loss 0.10876476082593106  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.15881325673314012  - accuracy: 0.75\n",
      "At: 2106 [==========>] Loss 0.14348303058286835  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.09349044946195859  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.10852034546839978  - accuracy: 0.84375\n",
      "At: 2109 [==========>] Loss 0.105210827614936  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.05586768347984564  - accuracy: 0.9375\n",
      "At: 2111 [==========>] Loss 0.11303825475808214  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.12796950249291844  - accuracy: 0.78125\n",
      "At: 2113 [==========>] Loss 0.1010213150334498  - accuracy: 0.875\n",
      "At: 2114 [==========>] Loss 0.12169905499213163  - accuracy: 0.875\n",
      "At: 2115 [==========>] Loss 0.09944695754986056  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.10978124946720533  - accuracy: 0.8125\n",
      "At: 2117 [==========>] Loss 0.13044720365169418  - accuracy: 0.8125\n",
      "At: 2118 [==========>] Loss 0.1202839998709135  - accuracy: 0.84375\n",
      "At: 2119 [==========>] Loss 0.06204254498009906  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.13611330545340747  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.15190007028570562  - accuracy: 0.78125\n",
      "At: 2122 [==========>] Loss 0.1302272145695359  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.1809571695814118  - accuracy: 0.6875\n",
      "At: 2124 [==========>] Loss 0.1392826571997618  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10745857416238669  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.04095299294902413  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.11366901353949148  - accuracy: 0.84375\n",
      "At: 2128 [==========>] Loss 0.13273256242672143  - accuracy: 0.75\n",
      "At: 2129 [==========>] Loss 0.1532847928736414  - accuracy: 0.84375\n",
      "At: 2130 [==========>] Loss 0.07154993282477881  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.09279754686371502  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.2160858041042356  - accuracy: 0.65625\n",
      "At: 2133 [==========>] Loss 0.16931637636549146  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.12228978553096065  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.0922279109441945  - accuracy: 0.9375\n",
      "At: 2136 [==========>] Loss 0.1225269727369925  - accuracy: 0.84375\n",
      "At: 2137 [==========>] Loss 0.11377049717216677  - accuracy: 0.875\n",
      "At: 2138 [==========>] Loss 0.09739836454094222  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.17665175513535727  - accuracy: 0.6875\n",
      "At: 2140 [==========>] Loss 0.11380274028096357  - accuracy: 0.875\n",
      "At: 2141 [==========>] Loss 0.15060562382299753  - accuracy: 0.78125\n",
      "At: 2142 [==========>] Loss 0.10689040424889643  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.08336005264818802  - accuracy: 0.9375\n",
      "At: 2144 [==========>] Loss 0.05398406061031498  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.1193757197866415  - accuracy: 0.8125\n",
      "At: 2146 [==========>] Loss 0.14350439331081005  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.14980732753608555  - accuracy: 0.78125\n",
      "At: 2148 [==========>] Loss 0.17648461816916533  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.11764968694854103  - accuracy: 0.84375\n",
      "At: 2150 [==========>] Loss 0.07327484612455137  - accuracy: 0.90625\n",
      "At: 2151 [==========>] Loss 0.09214395638688769  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.2195436452566495  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.1951051670804878  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.13481615017564336  - accuracy: 0.8125\n",
      "At: 2155 [==========>] Loss 0.11009711772226574  - accuracy: 0.8125\n",
      "At: 2156 [==========>] Loss 0.12531157744263904  - accuracy: 0.8125\n",
      "At: 2157 [==========>] Loss 0.13686055857433138  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.13036823218249954  - accuracy: 0.78125\n",
      "At: 2159 [==========>] Loss 0.08073142130994401  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.16888522113323196  - accuracy: 0.75\n",
      "At: 2161 [==========>] Loss 0.0808172443778663  - accuracy: 0.875\n",
      "At: 2162 [==========>] Loss 0.10099719039666807  - accuracy: 0.8125\n",
      "At: 2163 [==========>] Loss 0.14652052547878286  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.15496386361593661  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.10109281922508251  - accuracy: 0.875\n",
      "At: 2166 [==========>] Loss 0.10447979564248691  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.10737085726968626  - accuracy: 0.875\n",
      "At: 2168 [==========>] Loss 0.07996962545868466  - accuracy: 0.875\n",
      "At: 2169 [==========>] Loss 0.12458370347669753  - accuracy: 0.8125\n",
      "At: 2170 [==========>] Loss 0.11698137487413061  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.16664143592124453  - accuracy: 0.71875\n",
      "At: 2172 [==========>] Loss 0.09938330375145402  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.14228717829812976  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.10959833116188178  - accuracy: 0.8125\n",
      "At: 2175 [==========>] Loss 0.14406751514339952  - accuracy: 0.8125\n",
      "At: 2176 [==========>] Loss 0.10545634691462764  - accuracy: 0.875\n",
      "At: 2177 [==========>] Loss 0.14389735274683252  - accuracy: 0.84375\n",
      "At: 2178 [==========>] Loss 0.11178823087491489  - accuracy: 0.8125\n",
      "At: 2179 [==========>] Loss 0.13082907628184184  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.09720184739704407  - accuracy: 0.875\n",
      "At: 2181 [==========>] Loss 0.1397437807299789  - accuracy: 0.78125\n",
      "At: 2182 [==========>] Loss 0.09493580257513642  - accuracy: 0.90625\n",
      "At: 2183 [==========>] Loss 0.22569840218328288  - accuracy: 0.71875\n",
      "At: 2184 [==========>] Loss 0.08571836630543359  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.12484087591436793  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.1532616702425238  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.17525038068651067  - accuracy: 0.71875\n",
      "At: 2188 [==========>] Loss 0.06856043023956812  - accuracy: 0.875\n",
      "At: 2189 [==========>] Loss 0.05728887679484748  - accuracy: 0.9375\n",
      "At: 2190 [==========>] Loss 0.10825341800073623  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.1326164541244688  - accuracy: 0.875\n",
      "At: 2192 [==========>] Loss 0.09971277720341676  - accuracy: 0.84375\n",
      "At: 2193 [==========>] Loss 0.1740858909375428  - accuracy: 0.6875\n",
      "At: 2194 [==========>] Loss 0.11364517304014671  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.18510949820041628  - accuracy: 0.71875\n",
      "At: 2196 [==========>] Loss 0.12745323801835096  - accuracy: 0.8125\n",
      "At: 2197 [==========>] Loss 0.07728722944412866  - accuracy: 0.9375\n",
      "At: 2198 [==========>] Loss 0.09133212483820496  - accuracy: 0.875\n",
      "At: 2199 [==========>] Loss 0.04555723497627497  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.06257415698048291  - accuracy: 0.90625\n",
      "At: 2201 [==========>] Loss 0.11547130747211481  - accuracy: 0.875\n",
      "At: 2202 [==========>] Loss 0.0765308226574983  - accuracy: 0.9375\n",
      "At: 2203 [==========>] Loss 0.09411703818549741  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.12966677694997558  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.09501062655141011  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.08447612654911435  - accuracy: 0.875\n",
      "At: 2207 [==========>] Loss 0.08565172644377736  - accuracy: 0.875\n",
      "At: 2208 [==========>] Loss 0.13554234849108415  - accuracy: 0.84375\n",
      "At: 2209 [==========>] Loss 0.184583047045184  - accuracy: 0.75\n",
      "At: 2210 [==========>] Loss 0.12184046700549223  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.12094685595023394  - accuracy: 0.875\n",
      "At: 2212 [==========>] Loss 0.09461786096133042  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.10268796658686849  - accuracy: 0.875\n",
      "At: 2214 [==========>] Loss 0.12026161854145637  - accuracy: 0.84375\n",
      "At: 2215 [==========>] Loss 0.13505249352074628  - accuracy: 0.78125\n",
      "At: 2216 [==========>] Loss 0.1264863433392073  - accuracy: 0.84375\n",
      "At: 2217 [==========>] Loss 0.10437371464392498  - accuracy: 0.84375\n",
      "At: 2218 [==========>] Loss 0.16433925272532388  - accuracy: 0.71875\n",
      "At: 2219 [==========>] Loss 0.07906498723812325  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.10532783836258991  - accuracy: 0.875\n",
      "At: 2221 [==========>] Loss 0.17888633689109557  - accuracy: 0.71875\n",
      "At: 2222 [==========>] Loss 0.11766243772176457  - accuracy: 0.8125\n",
      "At: 2223 [==========>] Loss 0.15544878897215658  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.12442011813748699  - accuracy: 0.875\n",
      "At: 2225 [==========>] Loss 0.13734810047850532  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.15247806000173272  - accuracy: 0.8125\n",
      "At: 2227 [==========>] Loss 0.18369044623930214  - accuracy: 0.71875\n",
      "At: 2228 [==========>] Loss 0.07221718644662735  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.19366549039843348  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.12457430528791136  - accuracy: 0.8125\n",
      "At: 2231 [==========>] Loss 0.1569915522039108  - accuracy: 0.8125\n",
      "At: 2232 [==========>] Loss 0.1529297178076896  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.1367260520931881  - accuracy: 0.84375\n",
      "At: 2234 [==========>] Loss 0.14389074394069934  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.1293207908002943  - accuracy: 0.78125\n",
      "At: 2236 [==========>] Loss 0.08242421306386385  - accuracy: 0.875\n",
      "At: 2237 [==========>] Loss 0.12329292438808172  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.15343000130427714  - accuracy: 0.78125\n",
      "At: 2239 [==========>] Loss 0.1741279273319997  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.13023129916392262  - accuracy: 0.78125\n",
      "At: 2241 [==========>] Loss 0.11946733530296655  - accuracy: 0.875\n",
      "At: 2242 [==========>] Loss 0.16190531615982828  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.0782404631904273  - accuracy: 0.84375\n",
      "At: 2244 [==========>] Loss 0.09822764503281857  - accuracy: 0.875\n",
      "At: 2245 [==========>] Loss 0.08287890013605066  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.14681761598450482  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.10510142367793379  - accuracy: 0.84375\n",
      "At: 2248 [==========>] Loss 0.16685832024848896  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.09643664144128455  - accuracy: 0.90625\n",
      "At: 2250 [==========>] Loss 0.09838057622893571  - accuracy: 0.84375\n",
      "At: 2251 [==========>] Loss 0.07286837948359566  - accuracy: 0.90625\n",
      "At: 2252 [==========>] Loss 0.09639179236581204  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.11995102204050659  - accuracy: 0.84375\n",
      "At: 2254 [==========>] Loss 0.118942512772284  - accuracy: 0.84375\n",
      "At: 2255 [==========>] Loss 0.1298217554307281  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.15356889222349623  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.09183627257587684  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.1302695705930524  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.13690696363781865  - accuracy: 0.84375\n",
      "At: 2260 [==========>] Loss 0.16394020823085914  - accuracy: 0.71875\n",
      "At: 2261 [==========>] Loss 0.10181292588191707  - accuracy: 0.9375\n",
      "At: 2262 [==========>] Loss 0.1570385043547341  - accuracy: 0.8125\n",
      "At: 2263 [==========>] Loss 0.12000302978633803  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.08562340905401913  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.11415876957231255  - accuracy: 0.8125\n",
      "At: 2266 [==========>] Loss 0.13732189284780982  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.06649767436768118  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.08356953984491117  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.042025427004330776  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.09708025782094161  - accuracy: 0.90625\n",
      "At: 2271 [==========>] Loss 0.10508712635873893  - accuracy: 0.875\n",
      "At: 2272 [==========>] Loss 0.07957219499523528  - accuracy: 0.90625\n",
      "At: 2273 [==========>] Loss 0.12414989494693313  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.08024147864741181  - accuracy: 0.875\n",
      "At: 2275 [==========>] Loss 0.0708752504555838  - accuracy: 0.90625\n",
      "At: 2276 [==========>] Loss 0.08885362772466826  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.1275898153601362  - accuracy: 0.78125\n",
      "At: 2278 [==========>] Loss 0.09689889549261874  - accuracy: 0.84375\n",
      "At: 2279 [==========>] Loss 0.1288730112072579  - accuracy: 0.8125\n",
      "At: 2280 [==========>] Loss 0.12983304342255844  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.11325189854068463  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.06764947112770381  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.1375675526339797  - accuracy: 0.8125\n",
      "At: 2284 [==========>] Loss 0.12031168746859876  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.13177970442081102  - accuracy: 0.78125\n",
      "At: 2286 [==========>] Loss 0.1273118868303776  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.12997403246138745  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.08985545100876835  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.13054627967673949  - accuracy: 0.8125\n",
      "At: 2290 [==========>] Loss 0.06822352414210103  - accuracy: 0.96875\n",
      "At: 2291 [==========>] Loss 0.12784541253126766  - accuracy: 0.8125\n",
      "At: 2292 [==========>] Loss 0.08617807105338895  - accuracy: 0.875\n",
      "At: 2293 [==========>] Loss 0.05331252359240175  - accuracy: 1.0\n",
      "At: 2294 [==========>] Loss 0.05112565114168348  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.14384180564962545  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.17377903835735198  - accuracy: 0.75\n",
      "At: 2297 [==========>] Loss 0.07659930466202373  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.1043959700357048  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.11841539631632003  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.14601968367977985  - accuracy: 0.78125\n",
      "At: 2301 [==========>] Loss 0.19344217772201394  - accuracy: 0.75\n",
      "At: 2302 [==========>] Loss 0.17875012433375215  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.04796350856732373  - accuracy: 1.0\n",
      "At: 2304 [==========>] Loss 0.08499591441155852  - accuracy: 0.9375\n",
      "At: 2305 [==========>] Loss 0.11241779261634924  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.1203907458636024  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.16205539595593302  - accuracy: 0.75\n",
      "At: 2308 [==========>] Loss 0.18650056829927453  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.11777782693966553  - accuracy: 0.84375\n",
      "At: 2310 [==========>] Loss 0.14526787313858908  - accuracy: 0.75\n",
      "At: 2311 [==========>] Loss 0.13865242820482213  - accuracy: 0.84375\n",
      "At: 2312 [==========>] Loss 0.09644515205878451  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.06335924159978817  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.11105712721203989  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.12111806816048248  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.13848534052013572  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.17587699527123482  - accuracy: 0.78125\n",
      "At: 2318 [==========>] Loss 0.20760258180605634  - accuracy: 0.6875\n",
      "At: 2319 [==========>] Loss 0.13226021749435402  - accuracy: 0.71875\n",
      "At: 2320 [==========>] Loss 0.10221034271344376  - accuracy: 0.875\n",
      "At: 2321 [==========>] Loss 0.14102260196111888  - accuracy: 0.8125\n",
      "At: 2322 [==========>] Loss 0.1859342879479931  - accuracy: 0.78125\n",
      "At: 2323 [==========>] Loss 0.15108030662068694  - accuracy: 0.8125\n",
      "At: 2324 [==========>] Loss 0.15244868285016125  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.1374038733683788  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.07432201957570134  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.061059666079554195  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.119169934809108  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.0882358278519601  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.1446967727098933  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.11739251653157803  - accuracy: 0.8125\n",
      "At: 2332 [==========>] Loss 0.11426171223104026  - accuracy: 0.8125\n",
      "At: 2333 [==========>] Loss 0.11494902714961364  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.19853985674120106  - accuracy: 0.6875\n",
      "At: 2335 [==========>] Loss 0.10615185609959063  - accuracy: 0.875\n",
      "At: 2336 [==========>] Loss 0.09700592186085644  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.1453641501327038  - accuracy: 0.78125\n",
      "At: 2338 [==========>] Loss 0.10855187896326005  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.09423685785896987  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.13774378556976913  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.14616311636185156  - accuracy: 0.8125\n",
      "At: 2342 [==========>] Loss 0.15390569537478574  - accuracy: 0.8125\n",
      "At: 2343 [==========>] Loss 0.08612612881829296  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.19002406632244234  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.12890103474306164  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.06971135222920075  - accuracy: 0.9375\n",
      "At: 2347 [==========>] Loss 0.13619098830739845  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.05318509594358327  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.07923396600595722  - accuracy: 0.84375\n",
      "At: 2350 [==========>] Loss 0.1021682554865551  - accuracy: 0.875\n",
      "At: 2351 [==========>] Loss 0.08545829271606506  - accuracy: 0.8125\n",
      "At: 2352 [==========>] Loss 0.09264107841083437  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.08745989306614003  - accuracy: 0.875\n",
      "At: 2354 [==========>] Loss 0.15717895702755288  - accuracy: 0.78125\n",
      "At: 2355 [==========>] Loss 0.06242669281018659  - accuracy: 0.9375\n",
      "At: 2356 [==========>] Loss 0.11621287168497764  - accuracy: 0.84375\n",
      "At: 2357 [==========>] Loss 0.14429275477466286  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.15399476395612005  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.19186074374099915  - accuracy: 0.71875\n",
      "At: 2360 [==========>] Loss 0.12588123672290694  - accuracy: 0.8125\n",
      "At: 2361 [==========>] Loss 0.14922324911812868  - accuracy: 0.8125\n",
      "At: 2362 [==========>] Loss 0.07816057883824884  - accuracy: 0.90625\n",
      "At: 2363 [==========>] Loss 0.15960556134017995  - accuracy: 0.75\n",
      "At: 2364 [==========>] Loss 0.0872731042183593  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.08765825874914171  - accuracy: 0.84375\n",
      "At: 2366 [==========>] Loss 0.15085899395676355  - accuracy: 0.75\n",
      "At: 2367 [==========>] Loss 0.08777144300645728  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.10104693565168446  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.09434380433856732  - accuracy: 0.875\n",
      "At: 2370 [==========>] Loss 0.09167428558007595  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.08950939992388654  - accuracy: 0.84375\n",
      "At: 2372 [==========>] Loss 0.11680349906436661  - accuracy: 0.84375\n",
      "At: 2373 [==========>] Loss 0.0913520887905909  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.1173007897382867  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.06451665476995957  - accuracy: 0.90625\n",
      "At: 2376 [==========>] Loss 0.10018146622343746  - accuracy: 0.8125\n",
      "At: 2377 [==========>] Loss 0.1206991575079843  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.11490432421069684  - accuracy: 0.875\n",
      "At: 2379 [==========>] Loss 0.160136953066159  - accuracy: 0.8125\n",
      "At: 2380 [==========>] Loss 0.0676800234300859  - accuracy: 0.9375\n",
      "At: 2381 [==========>] Loss 0.10129358782259931  - accuracy: 0.875\n",
      "At: 2382 [==========>] Loss 0.08453090521313113  - accuracy: 0.875\n",
      "At: 2383 [==========>] Loss 0.13356995711982636  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.107589873039409  - accuracy: 0.875\n",
      "At: 2385 [==========>] Loss 0.12880301400664693  - accuracy: 0.875\n",
      "At: 2386 [==========>] Loss 0.10148799544611263  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.06440744984195067  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.12078223044929103  - accuracy: 0.875\n",
      "At: 2389 [==========>] Loss 0.04447519068297798  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.07168887682059012  - accuracy: 0.9375\n",
      "At: 2391 [==========>] Loss 0.15590430934929872  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.12065676225119523  - accuracy: 0.84375\n",
      "At: 2393 [==========>] Loss 0.11770489976794891  - accuracy: 0.8125\n",
      "At: 2394 [==========>] Loss 0.07639089165277083  - accuracy: 0.875\n",
      "At: 2395 [==========>] Loss 0.1120598556971934  - accuracy: 0.875\n",
      "At: 2396 [==========>] Loss 0.06653045978834364  - accuracy: 0.90625\n",
      "At: 2397 [==========>] Loss 0.08537668408249281  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.11432426933004328  - accuracy: 0.84375\n",
      "At: 2399 [==========>] Loss 0.14869152442348565  - accuracy: 0.84375\n",
      "At: 2400 [==========>] Loss 0.08033318233055839  - accuracy: 0.9375\n",
      "At: 2401 [==========>] Loss 0.0970267588754912  - accuracy: 0.875\n",
      "At: 2402 [==========>] Loss 0.09746787873376896  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.17583749580207592  - accuracy: 0.71875\n",
      "At: 2404 [==========>] Loss 0.1401403333408358  - accuracy: 0.8125\n",
      "At: 2405 [==========>] Loss 0.053657526226524  - accuracy: 0.90625\n",
      "At: 2406 [==========>] Loss 0.15458817622175822  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.09512719302305578  - accuracy: 0.90625\n",
      "At: 2408 [==========>] Loss 0.08130993689095661  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.16385418490418643  - accuracy: 0.78125\n",
      "At: 2410 [==========>] Loss 0.15176249278761927  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.09372587035904582  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.0863209066065779  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.06956608853816236  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.06705904662372976  - accuracy: 0.90625\n",
      "At: 2415 [==========>] Loss 0.09626998607527992  - accuracy: 0.90625\n",
      "At: 2416 [==========>] Loss 0.0998381577171299  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.17692873228418182  - accuracy: 0.6875\n",
      "At: 2418 [==========>] Loss 0.13463717816418397  - accuracy: 0.84375\n",
      "At: 2419 [==========>] Loss 0.1354540751499689  - accuracy: 0.78125\n",
      "At: 2420 [==========>] Loss 0.14531728718461911  - accuracy: 0.75\n",
      "At: 2421 [==========>] Loss 0.100453942694644  - accuracy: 0.84375\n",
      "At: 2422 [==========>] Loss 0.14242532268029673  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.11281506267096719  - accuracy: 0.90625\n",
      "At: 2424 [==========>] Loss 0.09912711109545291  - accuracy: 0.90625\n",
      "At: 2425 [==========>] Loss 0.07532026138811843  - accuracy: 0.9375\n",
      "At: 2426 [==========>] Loss 0.17037869129913685  - accuracy: 0.78125\n",
      "At: 2427 [==========>] Loss 0.10306870237471685  - accuracy: 0.84375\n",
      "At: 2428 [==========>] Loss 0.07738765308429939  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.11030040822158432  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.1412859894844839  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.15553112967362345  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.06774468687879473  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.04986524310823622  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.05154622743792858  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.16921387656224957  - accuracy: 0.78125\n",
      "At: 2436 [==========>] Loss 0.08390981480519688  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.17628730488457117  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.10554632059489319  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.12602012631612836  - accuracy: 0.78125\n",
      "At: 2440 [==========>] Loss 0.12294206284406642  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.1073848588813199  - accuracy: 0.875\n",
      "At: 2442 [==========>] Loss 0.11654155772553376  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.12724020580149784  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.10639771410756177  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.07088227534149989  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.16497189409277624  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.16711029170358482  - accuracy: 0.6875\n",
      "At: 2448 [==========>] Loss 0.13642061691629648  - accuracy: 0.75\n",
      "At: 2449 [==========>] Loss 0.07788697706328304  - accuracy: 0.875\n",
      "At: 2450 [==========>] Loss 0.06591475468479424  - accuracy: 0.84375\n",
      "At: 2451 [==========>] Loss 0.049967669981734845  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.13370052827487827  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.12488608657394543  - accuracy: 0.78125\n",
      "At: 2454 [==========>] Loss 0.15258155133177126  - accuracy: 0.84375\n",
      "At: 2455 [==========>] Loss 0.11893441775345523  - accuracy: 0.875\n",
      "At: 2456 [==========>] Loss 0.1123843188207033  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.17846700536985483  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.07038291679585074  - accuracy: 0.90625\n",
      "At: 2459 [==========>] Loss 0.12277468449573561  - accuracy: 0.84375\n",
      "At: 2460 [==========>] Loss 0.06355203609746496  - accuracy: 0.96875\n",
      "At: 2461 [==========>] Loss 0.0705901365058432  - accuracy: 0.90625\n",
      "At: 2462 [==========>] Loss 0.16186801397935927  - accuracy: 0.78125\n",
      "At: 2463 [==========>] Loss 0.09949901380725736  - accuracy: 0.84375\n",
      "At: 2464 [==========>] Loss 0.14285707238607692  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.14579683152925677  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.08968043158838623  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.10142290117862061  - accuracy: 0.875\n",
      "At: 2468 [==========>] Loss 0.09297181009405575  - accuracy: 0.8125\n",
      "At: 2469 [==========>] Loss 0.12058433421927543  - accuracy: 0.84375\n",
      "At: 2470 [==========>] Loss 0.0945621784418764  - accuracy: 0.9375\n",
      "At: 2471 [==========>] Loss 0.12472586956554164  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.09280268958191554  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.11304807448699339  - accuracy: 0.8125\n",
      "At: 2474 [==========>] Loss 0.06629185815431624  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.13120773350260467  - accuracy: 0.78125\n",
      "At: 2476 [==========>] Loss 0.08612736733242095  - accuracy: 0.875\n",
      "At: 2477 [==========>] Loss 0.14629473640155355  - accuracy: 0.75\n",
      "At: 2478 [==========>] Loss 0.12077696059573736  - accuracy: 0.875\n",
      "At: 2479 [==========>] Loss 0.06006578684633793  - accuracy: 0.96875\n",
      "At: 2480 [==========>] Loss 0.12748871259455974  - accuracy: 0.75\n",
      "At: 2481 [==========>] Loss 0.07011404180529265  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.1540299610817175  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.10539436280155176  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.09638352788572394  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.11651426370192765  - accuracy: 0.78125\n",
      "At: 2486 [==========>] Loss 0.12981315383442676  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.137301619208222  - accuracy: 0.78125\n",
      "At: 2488 [==========>] Loss 0.15519241623969338  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.19808035165560336  - accuracy: 0.6875\n",
      "At: 2490 [==========>] Loss 0.10138921571292264  - accuracy: 0.90625\n",
      "At: 2491 [==========>] Loss 0.11068580418045017  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.11097983283848833  - accuracy: 0.8125\n",
      "At: 2493 [==========>] Loss 0.08494690464529266  - accuracy: 0.90625\n",
      "At: 2494 [==========>] Loss 0.11148970976224468  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.07294539330541663  - accuracy: 0.90625\n",
      "At: 2496 [==========>] Loss 0.06308657635336615  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.20619182382359308  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.11399266091241086  - accuracy: 0.84375\n",
      "At: 2499 [==========>] Loss 0.06750029914160002  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.1711890140495832  - accuracy: 0.71875\n",
      "At: 2501 [==========>] Loss 0.15919286716091116  - accuracy: 0.71875\n",
      "At: 2502 [==========>] Loss 0.13971569192969777  - accuracy: 0.78125\n",
      "At: 2503 [==========>] Loss 0.13313276677168645  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.15028063196990835  - accuracy: 0.78125\n",
      "At: 2505 [==========>] Loss 0.10326179730768126  - accuracy: 0.84375\n",
      "At: 2506 [==========>] Loss 0.1195924661706388  - accuracy: 0.84375\n",
      "At: 2507 [==========>] Loss 0.15240007321579663  - accuracy: 0.8125\n",
      "At: 2508 [==========>] Loss 0.09925568411836207  - accuracy: 0.84375\n",
      "At: 2509 [==========>] Loss 0.1377342800305073  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.13233010533936462  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.1563318460466845  - accuracy: 0.78125\n",
      "At: 2512 [==========>] Loss 0.10244919175447194  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.15803064983423765  - accuracy: 0.8125\n",
      "At: 2514 [==========>] Loss 0.12630662279115148  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.18989894803202875  - accuracy: 0.6875\n",
      "At: 2516 [==========>] Loss 0.2131941881706369  - accuracy: 0.6875\n",
      "At: 2517 [==========>] Loss 0.1190677107003679  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.11972981259050879  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.09004321762614809  - accuracy: 0.875\n",
      "At: 2520 [==========>] Loss 0.14203819942346665  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.0898481076340384  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.22522914018456186  - accuracy: 0.625\n",
      "At: 2523 [==========>] Loss 0.1312965100365857  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.18879222199585854  - accuracy: 0.71875\n",
      "At: 2525 [==========>] Loss 0.07714042152609438  - accuracy: 0.875\n",
      "At: 2526 [==========>] Loss 0.11591737612473434  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.1427390717942497  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.11155704222959302  - accuracy: 0.8125\n",
      "At: 2529 [==========>] Loss 0.14508465153899608  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.1171331237076822  - accuracy: 0.8125\n",
      "At: 2531 [==========>] Loss 0.045115815337259446  - accuracy: 0.9375\n",
      "At: 2532 [==========>] Loss 0.15096159334344436  - accuracy: 0.75\n",
      "At: 2533 [==========>] Loss 0.10956765181959112  - accuracy: 0.875\n",
      "At: 2534 [==========>] Loss 0.05921650380616286  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.06792633649688262  - accuracy: 0.96875\n",
      "At: 2536 [==========>] Loss 0.13915846601821635  - accuracy: 0.8125\n",
      "At: 2537 [==========>] Loss 0.10057870547653044  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.1544060163784403  - accuracy: 0.8125\n",
      "At: 2539 [==========>] Loss 0.09277081888858243  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.13067730497388305  - accuracy: 0.75\n",
      "At: 2541 [==========>] Loss 0.07913578737550431  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.0620674241361564  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.1884403470904985  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.12809815178944087  - accuracy: 0.78125\n",
      "At: 2545 [==========>] Loss 0.0648861150329333  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.14584852858842545  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.10683911378383534  - accuracy: 0.8125\n",
      "At: 2548 [==========>] Loss 0.08390348971120057  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.11160571875375334  - accuracy: 0.84375\n",
      "At: 2550 [==========>] Loss 0.13580403761274723  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.13033370382932516  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.11320634812158631  - accuracy: 0.9375\n",
      "At: 2553 [==========>] Loss 0.06406552658782035  - accuracy: 0.9375\n",
      "At: 2554 [==========>] Loss 0.13865045033632445  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.18253254967466395  - accuracy: 0.75\n",
      "At: 2556 [==========>] Loss 0.08347912371666569  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.11275975069425115  - accuracy: 0.8125\n",
      "At: 2558 [==========>] Loss 0.08523688574005675  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.09548173851638211  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.16630238054718427  - accuracy: 0.75\n",
      "At: 2561 [==========>] Loss 0.09726797227032738  - accuracy: 0.875\n",
      "At: 2562 [==========>] Loss 0.10108181354625535  - accuracy: 0.84375\n",
      "At: 2563 [==========>] Loss 0.12704536049906184  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.09521716180755352  - accuracy: 0.84375\n",
      "At: 2565 [==========>] Loss 0.13097624592941803  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.1729125769955635  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.11016491031288439  - accuracy: 0.875\n",
      "At: 2568 [==========>] Loss 0.09939123156802074  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.07995881741150038  - accuracy: 0.90625\n",
      "At: 2570 [==========>] Loss 0.18168704767979033  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.1143985151584621  - accuracy: 0.8125\n",
      "At: 2572 [==========>] Loss 0.19214111375348944  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.2011364259296357  - accuracy: 0.65625\n",
      "At: 2574 [==========>] Loss 0.12611887289997212  - accuracy: 0.84375\n",
      "At: 2575 [==========>] Loss 0.0477334229342961  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.09582705761790969  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.22707995946114234  - accuracy: 0.6875\n",
      "At: 2578 [==========>] Loss 0.1913215696742231  - accuracy: 0.71875\n",
      "At: 2579 [==========>] Loss 0.2065442504664699  - accuracy: 0.71875\n",
      "At: 2580 [==========>] Loss 0.15580732045356582  - accuracy: 0.875\n",
      "At: 2581 [==========>] Loss 0.07380122599174019  - accuracy: 0.90625\n",
      "At: 2582 [==========>] Loss 0.1926236301754183  - accuracy: 0.78125\n",
      "At: 2583 [==========>] Loss 0.07669432198527942  - accuracy: 0.90625\n",
      "At: 2584 [==========>] Loss 0.19648737287540402  - accuracy: 0.75\n",
      "At: 2585 [==========>] Loss 0.09123245113699105  - accuracy: 0.9375\n",
      "At: 2586 [==========>] Loss 0.08685352434051069  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.16741224752778328  - accuracy: 0.65625\n",
      "At: 2588 [==========>] Loss 0.1277028746707534  - accuracy: 0.875\n",
      "At: 2589 [==========>] Loss 0.10329820158401472  - accuracy: 0.90625\n",
      "At: 2590 [==========>] Loss 0.20827762968188374  - accuracy: 0.75\n",
      "At: 2591 [==========>] Loss 0.13381958485955192  - accuracy: 0.875\n",
      "At: 2592 [==========>] Loss 0.12193345507473183  - accuracy: 0.75\n",
      "At: 2593 [==========>] Loss 0.10034790378775905  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.09675364760592725  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.08748837774159653  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.10920826523136119  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.10414518833978656  - accuracy: 0.84375\n",
      "At: 2598 [==========>] Loss 0.10985663703727594  - accuracy: 0.8125\n",
      "At: 2599 [==========>] Loss 0.1473205039427205  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.13854725475545565  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.13050262551909303  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.09701370674547928  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.14858174543146477  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.10237751338623663  - accuracy: 0.84375\n",
      "At: 2605 [==========>] Loss 0.12563444672909768  - accuracy: 0.84375\n",
      "At: 2606 [==========>] Loss 0.12752787194788398  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.12705422616321235  - accuracy: 0.875\n",
      "At: 2608 [==========>] Loss 0.06349586019491917  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.10060437483077841  - accuracy: 0.9375\n",
      "At: 2610 [==========>] Loss 0.1405430164495336  - accuracy: 0.71875\n",
      "At: 2611 [==========>] Loss 0.13439687236523826  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.09034464519928373  - accuracy: 0.84375\n",
      "At: 2613 [==========>] Loss 0.0885753308567917  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.11882011943664048  - accuracy: 0.8125\n",
      "At: 2615 [==========>] Loss 0.09173265773539921  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06363340451636076  - accuracy: 0.96875\n",
      "At: 2617 [==========>] Loss 0.09448708444637469  - accuracy: 0.875\n",
      "At: 2618 [==========>] Loss 0.07688026840315401  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.09587008801394684  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.09317531859642471  - accuracy: 0.84375\n",
      "At: 2621 [==========>] Loss 0.11598149161127111  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.09685281375661844  - accuracy: 0.84375\n",
      "At: 2623 [==========>] Loss 0.09723992842745505  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.140262888230484  - accuracy: 0.75\n",
      "At: 2625 [==========>] Loss 0.06373321294856427  - accuracy: 0.90625\n",
      "At: 2626 [==========>] Loss 0.028710299236874664  - accuracy: 1.0\n",
      "At: 2627 [==========>] Loss 0.13883322154516656  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.06925556003642407  - accuracy: 0.875\n",
      "At: 2629 [==========>] Loss 0.09393745830743194  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.11711754560760251  - accuracy: 0.84375\n",
      "At: 2631 [==========>] Loss 0.1288740503314087  - accuracy: 0.78125\n",
      "At: 2632 [==========>] Loss 0.14848961653887846  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.10431753315261985  - accuracy: 0.875\n",
      "At: 2634 [==========>] Loss 0.05864285765285611  - accuracy: 0.9375\n",
      "At: 2635 [==========>] Loss 0.21724532961539317  - accuracy: 0.71875\n",
      "At: 2636 [==========>] Loss 0.11669467757203182  - accuracy: 0.8125\n",
      "At: 2637 [==========>] Loss 0.14453722285099224  - accuracy: 0.84375\n",
      "At: 2638 [==========>] Loss 0.07310114600478246  - accuracy: 0.96875\n",
      "At: 2639 [==========>] Loss 0.07034476730496948  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.12848657004199776  - accuracy: 0.8125\n",
      "At: 2641 [==========>] Loss 0.15476779217819328  - accuracy: 0.8125\n",
      "At: 2642 [==========>] Loss 0.1746339409721021  - accuracy: 0.65625\n",
      "At: 2643 [==========>] Loss 0.07981151201511717  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.15740004047295922  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.08757666346422362  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.14454706513713628  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.1132680101814096  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.1574738337319338  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.10414641048026246  - accuracy: 0.84375\n",
      "At: 2650 [==========>] Loss 0.11172215891224316  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.18174980965721724  - accuracy: 0.6875\n",
      "At: 2652 [==========>] Loss 0.09918772953829753  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.12359940419724952  - accuracy: 0.8125\n",
      "At: 2654 [==========>] Loss 0.10190343422859914  - accuracy: 0.875\n",
      "At: 2655 [==========>] Loss 0.23452205811801388  - accuracy: 0.6875\n",
      "At: 2656 [==========>] Loss 0.019966307805859104  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.08676496508390222  - accuracy: 0.9375\n",
      "At: 2658 [==========>] Loss 0.07193898621691328  - accuracy: 0.9375\n",
      "At: 2659 [==========>] Loss 0.06500046122446773  - accuracy: 0.90625\n",
      "At: 2660 [==========>] Loss 0.09600874329571478  - accuracy: 0.84375\n",
      "At: 2661 [==========>] Loss 0.07719600829031174  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.06984451645185875  - accuracy: 0.9375\n",
      "At: 2663 [==========>] Loss 0.09100841086077108  - accuracy: 0.90625\n",
      "At: 2664 [==========>] Loss 0.0768273329977931  - accuracy: 0.9375\n",
      "At: 2665 [==========>] Loss 0.08510489618776199  - accuracy: 0.90625\n",
      "At: 2666 [==========>] Loss 0.12898178508072944  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.12834896780328653  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.1266961057590708  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.13370042320332923  - accuracy: 0.84375\n",
      "At: 2670 [==========>] Loss 0.09915942327514002  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.08813221975801327  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.10818480440180325  - accuracy: 0.875\n",
      "At: 2673 [==========>] Loss 0.11002396006711215  - accuracy: 0.875\n",
      "At: 2674 [==========>] Loss 0.08373446693097418  - accuracy: 0.90625\n",
      "At: 2675 [==========>] Loss 0.12567894823085274  - accuracy: 0.875\n",
      "At: 2676 [==========>] Loss 0.11745371054715567  - accuracy: 0.8125\n",
      "At: 2677 [==========>] Loss 0.10709493284860117  - accuracy: 0.8125\n",
      "At: 2678 [==========>] Loss 0.06380036678088469  - accuracy: 0.875\n",
      "At: 2679 [==========>] Loss 0.05832724125987156  - accuracy: 0.96875\n",
      "At: 2680 [==========>] Loss 0.10812568255619064  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.08432106413310789  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.15222078535385963  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.17826104331486345  - accuracy: 0.78125\n",
      "At: 2684 [==========>] Loss 0.08016864749366165  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.12586587686485542  - accuracy: 0.84375\n",
      "At: 2686 [==========>] Loss 0.0589521912130459  - accuracy: 0.9375\n",
      "At: 2687 [==========>] Loss 0.15497872796633852  - accuracy: 0.78125\n",
      "At: 2688 [==========>] Loss 0.15941449184899947  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.11640966369024874  - accuracy: 0.875\n",
      "At: 2690 [==========>] Loss 0.12700214492233866  - accuracy: 0.78125\n",
      "Epochs  8 / 10\n",
      "At: 1 [==========>] Loss 0.1711326287115399  - accuracy: 0.84375\n",
      "At: 2 [==========>] Loss 0.2708573659003376  - accuracy: 0.6875\n",
      "At: 3 [==========>] Loss 0.1551492488265668  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.2427149588605468  - accuracy: 0.6875\n",
      "At: 5 [==========>] Loss 0.1288482663515118  - accuracy: 0.84375\n",
      "At: 6 [==========>] Loss 0.11293904077336568  - accuracy: 0.90625\n",
      "At: 7 [==========>] Loss 0.2100321617603088  - accuracy: 0.6875\n",
      "At: 8 [==========>] Loss 0.31189358663251077  - accuracy: 0.59375\n",
      "At: 9 [==========>] Loss 0.31651833251932415  - accuracy: 0.5625\n",
      "At: 10 [==========>] Loss 0.25823888176345233  - accuracy: 0.6875\n",
      "At: 11 [==========>] Loss 0.27217959204155684  - accuracy: 0.75\n",
      "At: 12 [==========>] Loss 0.22373596881592206  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.21217428120036247  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.1289497200819634  - accuracy: 0.875\n",
      "At: 15 [==========>] Loss 0.1834249106929329  - accuracy: 0.75\n",
      "At: 16 [==========>] Loss 0.21973962317486886  - accuracy: 0.75\n",
      "At: 17 [==========>] Loss 0.25147886041775147  - accuracy: 0.6875\n",
      "At: 18 [==========>] Loss 0.304001393113903  - accuracy: 0.65625\n",
      "At: 19 [==========>] Loss 0.23917580956582574  - accuracy: 0.75\n",
      "At: 20 [==========>] Loss 0.159011508794818  - accuracy: 0.78125\n",
      "At: 21 [==========>] Loss 0.2525375743236303  - accuracy: 0.6875\n",
      "At: 22 [==========>] Loss 0.20201056530132502  - accuracy: 0.78125\n",
      "At: 23 [==========>] Loss 0.12624925280839058  - accuracy: 0.84375\n",
      "At: 24 [==========>] Loss 0.24562419044575814  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.273201877883018  - accuracy: 0.6875\n",
      "At: 26 [==========>] Loss 0.2982327216579189  - accuracy: 0.625\n",
      "At: 27 [==========>] Loss 0.2511313303780402  - accuracy: 0.6875\n",
      "At: 28 [==========>] Loss 0.18629577250042412  - accuracy: 0.8125\n",
      "At: 29 [==========>] Loss 0.19170475609905363  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.1992382214695537  - accuracy: 0.75\n",
      "At: 31 [==========>] Loss 0.31751789336814223  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.22887224425636143  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.12085481282192469  - accuracy: 0.90625\n",
      "At: 34 [==========>] Loss 0.19170338743521526  - accuracy: 0.75\n",
      "At: 35 [==========>] Loss 0.2074477748397432  - accuracy: 0.75\n",
      "At: 36 [==========>] Loss 0.24053212159996268  - accuracy: 0.6875\n",
      "At: 37 [==========>] Loss 0.2861518783500364  - accuracy: 0.59375\n",
      "At: 38 [==========>] Loss 0.3088344577640452  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.15306203800603374  - accuracy: 0.8125\n",
      "At: 40 [==========>] Loss 0.3073537491767786  - accuracy: 0.65625\n",
      "At: 41 [==========>] Loss 0.08573130364780653  - accuracy: 0.9375\n",
      "At: 42 [==========>] Loss 0.1499556447029566  - accuracy: 0.8125\n",
      "At: 43 [==========>] Loss 0.19783288851604078  - accuracy: 0.8125\n",
      "At: 44 [==========>] Loss 0.19607158011661188  - accuracy: 0.8125\n",
      "At: 45 [==========>] Loss 0.10219201152641764  - accuracy: 0.90625\n",
      "At: 46 [==========>] Loss 0.21928146246665026  - accuracy: 0.71875\n",
      "At: 47 [==========>] Loss 0.239477720850797  - accuracy: 0.71875\n",
      "At: 48 [==========>] Loss 0.18134168139460596  - accuracy: 0.84375\n",
      "At: 49 [==========>] Loss 0.172635138995336  - accuracy: 0.8125\n",
      "At: 50 [==========>] Loss 0.2606031670897575  - accuracy: 0.6875\n",
      "At: 51 [==========>] Loss 0.23997723901608567  - accuracy: 0.65625\n",
      "At: 52 [==========>] Loss 0.315883081380982  - accuracy: 0.625\n",
      "At: 53 [==========>] Loss 0.2123615244264631  - accuracy: 0.78125\n",
      "At: 54 [==========>] Loss 0.16250710217150144  - accuracy: 0.8125\n",
      "At: 55 [==========>] Loss 0.2669168892440107  - accuracy: 0.75\n",
      "At: 56 [==========>] Loss 0.22442251141514663  - accuracy: 0.75\n",
      "At: 57 [==========>] Loss 0.15799652170829198  - accuracy: 0.84375\n",
      "At: 58 [==========>] Loss 0.22274019420573493  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.24516511071552133  - accuracy: 0.71875\n",
      "At: 60 [==========>] Loss 0.20955183983166964  - accuracy: 0.75\n",
      "At: 61 [==========>] Loss 0.3041183344711606  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.19203321114263228  - accuracy: 0.8125\n",
      "At: 63 [==========>] Loss 0.20446087688645104  - accuracy: 0.75\n",
      "At: 64 [==========>] Loss 0.2142502568943662  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.3268631851175915  - accuracy: 0.59375\n",
      "At: 66 [==========>] Loss 0.276512270021015  - accuracy: 0.6875\n",
      "At: 67 [==========>] Loss 0.2805744069326072  - accuracy: 0.65625\n",
      "At: 68 [==========>] Loss 0.1609336374082759  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.16224608830623777  - accuracy: 0.84375\n",
      "At: 70 [==========>] Loss 0.24765933211104652  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.1915779332273955  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.15796952416781873  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.1627366725524187  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.21940930145569024  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.23386290593205566  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.3188944684094727  - accuracy: 0.65625\n",
      "At: 77 [==========>] Loss 0.2835217322986061  - accuracy: 0.6875\n",
      "At: 78 [==========>] Loss 0.1965727555393756  - accuracy: 0.78125\n",
      "At: 79 [==========>] Loss 0.18692084166000206  - accuracy: 0.75\n",
      "At: 80 [==========>] Loss 0.250565853173209  - accuracy: 0.6875\n",
      "At: 81 [==========>] Loss 0.21060317584577737  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.25843452216877405  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.1810599550085233  - accuracy: 0.8125\n",
      "At: 84 [==========>] Loss 0.2439663578457536  - accuracy: 0.71875\n",
      "At: 85 [==========>] Loss 0.23571043594455984  - accuracy: 0.71875\n",
      "At: 86 [==========>] Loss 0.18089857383037505  - accuracy: 0.8125\n",
      "At: 87 [==========>] Loss 0.1819328303757935  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.4000190110824627  - accuracy: 0.46875\n",
      "At: 89 [==========>] Loss 0.21228957154044326  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.27474030379970443  - accuracy: 0.6875\n",
      "At: 91 [==========>] Loss 0.23049547560907663  - accuracy: 0.71875\n",
      "At: 92 [==========>] Loss 0.1430307020327549  - accuracy: 0.8125\n",
      "At: 93 [==========>] Loss 0.1520330774242686  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.14722172231632735  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.21773477774036987  - accuracy: 0.75\n",
      "At: 96 [==========>] Loss 0.14836578581897214  - accuracy: 0.8125\n",
      "At: 97 [==========>] Loss 0.11756801036178571  - accuracy: 0.875\n",
      "At: 98 [==========>] Loss 0.2743596781826798  - accuracy: 0.6875\n",
      "At: 99 [==========>] Loss 0.14696527730416448  - accuracy: 0.84375\n",
      "At: 100 [==========>] Loss 0.1531465592447509  - accuracy: 0.875\n",
      "At: 101 [==========>] Loss 0.21444322293612966  - accuracy: 0.75\n",
      "At: 102 [==========>] Loss 0.18163321321305553  - accuracy: 0.75\n",
      "At: 103 [==========>] Loss 0.14435491140849185  - accuracy: 0.84375\n",
      "At: 104 [==========>] Loss 0.15785838868781588  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.22455675439190215  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.26096564055640836  - accuracy: 0.71875\n",
      "At: 107 [==========>] Loss 0.21194283094992733  - accuracy: 0.75\n",
      "At: 108 [==========>] Loss 0.26681537187371773  - accuracy: 0.65625\n",
      "At: 109 [==========>] Loss 0.10749077568249375  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.2719188987277764  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.12220703215378154  - accuracy: 0.84375\n",
      "At: 112 [==========>] Loss 0.2055881087268701  - accuracy: 0.8125\n",
      "At: 113 [==========>] Loss 0.24977197753329894  - accuracy: 0.71875\n",
      "At: 114 [==========>] Loss 0.18080908694816572  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.20823302377677524  - accuracy: 0.78125\n",
      "At: 116 [==========>] Loss 0.1947889408858211  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.16994786767715292  - accuracy: 0.8125\n",
      "At: 118 [==========>] Loss 0.2771180699947546  - accuracy: 0.65625\n",
      "At: 119 [==========>] Loss 0.14352894305234554  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.21048992579740544  - accuracy: 0.78125\n",
      "At: 121 [==========>] Loss 0.13424016991977566  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.21156635853853883  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.2532570431499635  - accuracy: 0.71875\n",
      "At: 124 [==========>] Loss 0.28435358941882927  - accuracy: 0.6875\n",
      "At: 125 [==========>] Loss 0.18476214838396465  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.26171854687670326  - accuracy: 0.65625\n",
      "At: 127 [==========>] Loss 0.2434363283140738  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.33362238954408563  - accuracy: 0.625\n",
      "At: 129 [==========>] Loss 0.15691696798345015  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.2222224885940463  - accuracy: 0.75\n",
      "At: 131 [==========>] Loss 0.21648979989327857  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.2589607185313407  - accuracy: 0.6875\n",
      "At: 133 [==========>] Loss 0.2595892345296784  - accuracy: 0.6875\n",
      "At: 134 [==========>] Loss 0.18019415498804106  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.21680027115260794  - accuracy: 0.78125\n",
      "At: 136 [==========>] Loss 0.22577853062912615  - accuracy: 0.71875\n",
      "At: 137 [==========>] Loss 0.10458929033431352  - accuracy: 0.90625\n",
      "At: 138 [==========>] Loss 0.21116080486654745  - accuracy: 0.78125\n",
      "At: 139 [==========>] Loss 0.1318429413528493  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.15742962896914525  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.37949616046273194  - accuracy: 0.5625\n",
      "At: 142 [==========>] Loss 0.20509208417157626  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.21070292122349632  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.17175948341379776  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.13161436982617544  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.168797737616327  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.21816016713618056  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.15705639502585308  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.21263067325004137  - accuracy: 0.78125\n",
      "At: 150 [==========>] Loss 0.19039625250338404  - accuracy: 0.78125\n",
      "At: 151 [==========>] Loss 0.23366084238694262  - accuracy: 0.75\n",
      "At: 152 [==========>] Loss 0.21900210650165597  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.2634250820615786  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.22691822381891175  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.21145116377233314  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1588562450780806  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.2969745184549526  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.16186772237138755  - accuracy: 0.8125\n",
      "At: 159 [==========>] Loss 0.16595367768236036  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.15072329896989406  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.1447273304916697  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.2084461960635551  - accuracy: 0.78125\n",
      "At: 163 [==========>] Loss 0.22925240100054595  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.2119593209589452  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.28005881680035866  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.21392619024138032  - accuracy: 0.78125\n",
      "At: 167 [==========>] Loss 0.1679256041454511  - accuracy: 0.84375\n",
      "At: 168 [==========>] Loss 0.2738542418281083  - accuracy: 0.71875\n",
      "At: 169 [==========>] Loss 0.18243113034949668  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.1855443318707709  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.281033997141824  - accuracy: 0.6875\n",
      "At: 172 [==========>] Loss 0.21941737218687002  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.31379861921960284  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.25225349358112026  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.19653934773401616  - accuracy: 0.8125\n",
      "At: 176 [==========>] Loss 0.20035117669804106  - accuracy: 0.75\n",
      "At: 177 [==========>] Loss 0.1291947322428515  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.24591293983339965  - accuracy: 0.75\n",
      "At: 179 [==========>] Loss 0.14684729487586384  - accuracy: 0.8125\n",
      "At: 180 [==========>] Loss 0.17534518858926307  - accuracy: 0.78125\n",
      "At: 181 [==========>] Loss 0.06334062297737333  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.27548334151636966  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.22614553266100743  - accuracy: 0.75\n",
      "At: 184 [==========>] Loss 0.218937690514104  - accuracy: 0.78125\n",
      "At: 185 [==========>] Loss 0.1294596427227026  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.20099895832945563  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.15813083633271874  - accuracy: 0.8125\n",
      "At: 188 [==========>] Loss 0.22752766027401355  - accuracy: 0.78125\n",
      "At: 189 [==========>] Loss 0.24325873696066863  - accuracy: 0.6875\n",
      "At: 190 [==========>] Loss 0.16972681223487324  - accuracy: 0.8125\n",
      "At: 191 [==========>] Loss 0.3558205936838194  - accuracy: 0.59375\n",
      "At: 192 [==========>] Loss 0.18643054366772627  - accuracy: 0.8125\n",
      "At: 193 [==========>] Loss 0.2828757990612708  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.19969376502956462  - accuracy: 0.78125\n",
      "At: 195 [==========>] Loss 0.21271866856114952  - accuracy: 0.78125\n",
      "At: 196 [==========>] Loss 0.23009308692453878  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.18368178981835953  - accuracy: 0.8125\n",
      "At: 198 [==========>] Loss 0.17273995712476156  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.15256524305865515  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.2694061058244092  - accuracy: 0.6875\n",
      "At: 201 [==========>] Loss 0.1265174131440835  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.11597120001389645  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.18541612463192378  - accuracy: 0.8125\n",
      "At: 204 [==========>] Loss 0.23773462039007354  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.19144908617811401  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.1138854717768131  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.12725613338901276  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.2979977910899221  - accuracy: 0.6875\n",
      "At: 209 [==========>] Loss 0.230228951571477  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.11483339653776455  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.1896745552656542  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.24104448968320344  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.21962862357977828  - accuracy: 0.71875\n",
      "At: 214 [==========>] Loss 0.2627632463738647  - accuracy: 0.71875\n",
      "At: 215 [==========>] Loss 0.1134249561720705  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.2538501133181138  - accuracy: 0.71875\n",
      "At: 217 [==========>] Loss 0.24705997935897647  - accuracy: 0.65625\n",
      "At: 218 [==========>] Loss 0.2090642114744991  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.21713108762050015  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.2674023654114913  - accuracy: 0.71875\n",
      "At: 221 [==========>] Loss 0.18994113868712187  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.10379098600348732  - accuracy: 0.875\n",
      "At: 223 [==========>] Loss 0.30357796966170103  - accuracy: 0.65625\n",
      "At: 224 [==========>] Loss 0.21734459874110204  - accuracy: 0.78125\n",
      "At: 225 [==========>] Loss 0.15372283608093645  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.2228339649141241  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.2333463386339144  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.2624660986736234  - accuracy: 0.75\n",
      "At: 229 [==========>] Loss 0.18198308982452877  - accuracy: 0.75\n",
      "At: 230 [==========>] Loss 0.1860513129528531  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.27565368661384726  - accuracy: 0.65625\n",
      "At: 232 [==========>] Loss 0.27577029533532127  - accuracy: 0.65625\n",
      "At: 233 [==========>] Loss 0.22396216278656267  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.13302828178776016  - accuracy: 0.84375\n",
      "At: 235 [==========>] Loss 0.22679742705248662  - accuracy: 0.71875\n",
      "At: 236 [==========>] Loss 0.2372389466031708  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.1410577228127992  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.167300590890629  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.18689749474191783  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.3118944351754257  - accuracy: 0.65625\n",
      "At: 241 [==========>] Loss 0.13705408566636948  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.21748070642859368  - accuracy: 0.78125\n",
      "At: 243 [==========>] Loss 0.12221843248402162  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.17332928127928693  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.18395171426186002  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.1748250270089343  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.21844936999369094  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.11461624814294792  - accuracy: 0.875\n",
      "At: 249 [==========>] Loss 0.09976722672114677  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.27185959296999795  - accuracy: 0.71875\n",
      "At: 251 [==========>] Loss 0.2688859234129221  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.1462731118805265  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.21029757004024685  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.09379277533953026  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.20043651801570095  - accuracy: 0.8125\n",
      "At: 256 [==========>] Loss 0.2673055117798025  - accuracy: 0.6875\n",
      "At: 257 [==========>] Loss 0.1774342990222615  - accuracy: 0.84375\n",
      "At: 258 [==========>] Loss 0.18538869601548635  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.17015535360622927  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.11582217386274694  - accuracy: 0.84375\n",
      "At: 261 [==========>] Loss 0.08940843668766085  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.20379337497942776  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.12975875497150285  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.17962921375292976  - accuracy: 0.8125\n",
      "At: 265 [==========>] Loss 0.2918088823174051  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.3071631535112005  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.1980895223271923  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.2868127838860133  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.17622430758227045  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.3435482721777959  - accuracy: 0.59375\n",
      "At: 271 [==========>] Loss 0.270681695070821  - accuracy: 0.71875\n",
      "At: 272 [==========>] Loss 0.09352270775227256  - accuracy: 0.90625\n",
      "At: 273 [==========>] Loss 0.2508705023086468  - accuracy: 0.75\n",
      "At: 274 [==========>] Loss 0.19640916229193095  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.07391681344012802  - accuracy: 0.90625\n",
      "At: 276 [==========>] Loss 0.23567744577267757  - accuracy: 0.71875\n",
      "At: 277 [==========>] Loss 0.12143429345926791  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.16155807129463934  - accuracy: 0.84375\n",
      "At: 279 [==========>] Loss 0.2131755034492692  - accuracy: 0.78125\n",
      "At: 280 [==========>] Loss 0.1534276081122719  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.17264104371842495  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.274502688571488  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.17064236362117918  - accuracy: 0.8125\n",
      "At: 284 [==========>] Loss 0.0983413788571848  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.17656048770484745  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.10260267618679074  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.2417810054379321  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.1463309007649358  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.1126298327226601  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.11797273829373954  - accuracy: 0.90625\n",
      "At: 291 [==========>] Loss 0.13546549226577492  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.1564380052551096  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.2842607966977866  - accuracy: 0.65625\n",
      "At: 294 [==========>] Loss 0.2370404700117933  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.18924069681220282  - accuracy: 0.78125\n",
      "At: 296 [==========>] Loss 0.15982254077230054  - accuracy: 0.84375\n",
      "At: 297 [==========>] Loss 0.18099731667629038  - accuracy: 0.8125\n",
      "At: 298 [==========>] Loss 0.1494260337493596  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.27543050985292455  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.20741341364343469  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.2138560983182589  - accuracy: 0.75\n",
      "At: 302 [==========>] Loss 0.1196190008234465  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.11422502041501464  - accuracy: 0.84375\n",
      "At: 304 [==========>] Loss 0.24634152259713185  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.29750158891990797  - accuracy: 0.625\n",
      "At: 306 [==========>] Loss 0.14412440912963612  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.30511989714923904  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.2516046476923599  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.09318718569490304  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.25408941749316905  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.1086325773443338  - accuracy: 0.875\n",
      "At: 312 [==========>] Loss 0.15562134458792637  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.15685173231592653  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.26124962071189606  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.16230761737002225  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.22523755114356886  - accuracy: 0.71875\n",
      "At: 317 [==========>] Loss 0.3362796351193862  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.19034675896354242  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.13484832063555507  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.27344719854611355  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.29869442160621607  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.13238418291880016  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.0965905161573331  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.19262840485685284  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.11632998361668251  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.19924096196773577  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.1492754123765755  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.15984724726312066  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.1419258846877094  - accuracy: 0.8125\n",
      "At: 330 [==========>] Loss 0.1926184539001734  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.23397266909368755  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.3529929547527736  - accuracy: 0.59375\n",
      "At: 333 [==========>] Loss 0.18455602980167649  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.13987232020037338  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.17423717237292996  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.13215836329935188  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.20517200858288137  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.19926142742142178  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.22328807448967514  - accuracy: 0.75\n",
      "At: 340 [==========>] Loss 0.13313708684621267  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.09871279922926388  - accuracy: 0.875\n",
      "At: 342 [==========>] Loss 0.1856195281952056  - accuracy: 0.78125\n",
      "At: 343 [==========>] Loss 0.27148237281116994  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.23290427534530192  - accuracy: 0.71875\n",
      "At: 345 [==========>] Loss 0.2558595126270151  - accuracy: 0.71875\n",
      "At: 346 [==========>] Loss 0.1560556167078074  - accuracy: 0.84375\n",
      "At: 347 [==========>] Loss 0.1462178277070887  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.1696680063894883  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.24641051743006792  - accuracy: 0.75\n",
      "At: 350 [==========>] Loss 0.1222402852792123  - accuracy: 0.875\n",
      "At: 351 [==========>] Loss 0.3308147658814874  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.1441599971563134  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.17621230116106174  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.2759477767157088  - accuracy: 0.6875\n",
      "At: 355 [==========>] Loss 0.12321746846539328  - accuracy: 0.875\n",
      "At: 356 [==========>] Loss 0.2412929361881343  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.15785842730218527  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.17816806663913196  - accuracy: 0.8125\n",
      "At: 359 [==========>] Loss 0.06406016447480237  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.21030957439012904  - accuracy: 0.78125\n",
      "At: 361 [==========>] Loss 0.11097732877025557  - accuracy: 0.90625\n",
      "At: 362 [==========>] Loss 0.2608503264259849  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.11564617794990303  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.23909553134658107  - accuracy: 0.71875\n",
      "At: 365 [==========>] Loss 0.14087730000387247  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.24305764026551618  - accuracy: 0.71875\n",
      "At: 367 [==========>] Loss 0.23109472898207958  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.19362476499624945  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.18352109366369418  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.1757730054463911  - accuracy: 0.75\n",
      "At: 371 [==========>] Loss 0.0847412284121089  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.15606309842715263  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.27977697407107754  - accuracy: 0.6875\n",
      "At: 374 [==========>] Loss 0.07388292224456072  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.17422678959003288  - accuracy: 0.8125\n",
      "At: 376 [==========>] Loss 0.09464347347082656  - accuracy: 0.9375\n",
      "At: 377 [==========>] Loss 0.21411889650651889  - accuracy: 0.75\n",
      "At: 378 [==========>] Loss 0.2025308026536184  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.24133374814231034  - accuracy: 0.75\n",
      "At: 380 [==========>] Loss 0.14063661281420903  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.2238491867347851  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.09466098839877267  - accuracy: 0.90625\n",
      "At: 383 [==========>] Loss 0.2717796142111121  - accuracy: 0.6875\n",
      "At: 384 [==========>] Loss 0.18082869513606714  - accuracy: 0.78125\n",
      "At: 385 [==========>] Loss 0.14739039584776917  - accuracy: 0.8125\n",
      "At: 386 [==========>] Loss 0.28378641996436815  - accuracy: 0.6875\n",
      "At: 387 [==========>] Loss 0.10803407396527341  - accuracy: 0.90625\n",
      "At: 388 [==========>] Loss 0.2384829682550727  - accuracy: 0.71875\n",
      "At: 389 [==========>] Loss 0.27065346069580315  - accuracy: 0.71875\n",
      "At: 390 [==========>] Loss 0.13837599822437238  - accuracy: 0.84375\n",
      "At: 391 [==========>] Loss 0.1871543073925377  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.18791509317758104  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.23987934555444274  - accuracy: 0.65625\n",
      "At: 394 [==========>] Loss 0.10447268518518646  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.21694238870737792  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.19525869358416803  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.15320561941111044  - accuracy: 0.78125\n",
      "At: 398 [==========>] Loss 0.2477353099331041  - accuracy: 0.6875\n",
      "At: 399 [==========>] Loss 0.2646749263111461  - accuracy: 0.6875\n",
      "At: 400 [==========>] Loss 0.24564623644696018  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.15170933972446887  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.11710425525187208  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.01645753525047767  - accuracy: 1.0\n",
      "At: 404 [==========>] Loss 0.12790299079098505  - accuracy: 0.84375\n",
      "At: 405 [==========>] Loss 0.22553732449952452  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.2177804767629002  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.19065437405027047  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.26554972852801906  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.3093118424824918  - accuracy: 0.59375\n",
      "At: 410 [==========>] Loss 0.1757932012961511  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.12774625743855694  - accuracy: 0.875\n",
      "At: 412 [==========>] Loss 0.21904669902590124  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.15430873459355746  - accuracy: 0.84375\n",
      "At: 414 [==========>] Loss 0.2209431259304821  - accuracy: 0.75\n",
      "At: 415 [==========>] Loss 0.19174722759797935  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.2608083306153025  - accuracy: 0.71875\n",
      "At: 417 [==========>] Loss 0.21160689989796125  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.15952097982821298  - accuracy: 0.8125\n",
      "At: 419 [==========>] Loss 0.18730909389591055  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.2743616036957247  - accuracy: 0.71875\n",
      "At: 421 [==========>] Loss 0.14207578687719327  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.12424288471544165  - accuracy: 0.875\n",
      "At: 423 [==========>] Loss 0.16621840956083517  - accuracy: 0.8125\n",
      "At: 424 [==========>] Loss 0.304972486400393  - accuracy: 0.65625\n",
      "At: 425 [==========>] Loss 0.24141224122835808  - accuracy: 0.71875\n",
      "At: 426 [==========>] Loss 0.11963017134710802  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.2168263459124477  - accuracy: 0.75\n",
      "At: 428 [==========>] Loss 0.2842985204383165  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.24139230873047748  - accuracy: 0.71875\n",
      "At: 430 [==========>] Loss 0.14119127041956248  - accuracy: 0.84375\n",
      "At: 431 [==========>] Loss 0.1557365027307242  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.16629248095716848  - accuracy: 0.8125\n",
      "At: 433 [==========>] Loss 0.12006156447313251  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.15505995802992242  - accuracy: 0.84375\n",
      "At: 435 [==========>] Loss 0.2159771523124724  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.17045514405619155  - accuracy: 0.78125\n",
      "At: 437 [==========>] Loss 0.16875493530214947  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.19378534263661212  - accuracy: 0.78125\n",
      "At: 439 [==========>] Loss 0.13351676128232542  - accuracy: 0.875\n",
      "At: 440 [==========>] Loss 0.09362402123697738  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.23512497676595231  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.2391484268007939  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.13699707270661576  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.27377740781741156  - accuracy: 0.71875\n",
      "At: 445 [==========>] Loss 0.191028960506694  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.3007393759806105  - accuracy: 0.625\n",
      "At: 447 [==========>] Loss 0.19329059798337006  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.21458771143399238  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.15135343463664705  - accuracy: 0.8125\n",
      "At: 450 [==========>] Loss 0.23428075178663987  - accuracy: 0.71875\n",
      "At: 451 [==========>] Loss 0.2299641095843664  - accuracy: 0.75\n",
      "At: 452 [==========>] Loss 0.16409098265527794  - accuracy: 0.8125\n",
      "At: 453 [==========>] Loss 0.1949063359096619  - accuracy: 0.8125\n",
      "At: 454 [==========>] Loss 0.2703564590092359  - accuracy: 0.6875\n",
      "At: 455 [==========>] Loss 0.20583483237306904  - accuracy: 0.78125\n",
      "At: 456 [==========>] Loss 0.16514852187486728  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.16496475670799743  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.15574796183733286  - accuracy: 0.84375\n",
      "At: 459 [==========>] Loss 0.2902430103653252  - accuracy: 0.65625\n",
      "At: 460 [==========>] Loss 0.14981908623709783  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.3066470272364902  - accuracy: 0.65625\n",
      "At: 462 [==========>] Loss 0.16944252918802338  - accuracy: 0.75\n",
      "At: 463 [==========>] Loss 0.13296975454482723  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.2190005044906166  - accuracy: 0.6875\n",
      "At: 465 [==========>] Loss 0.22845253316148872  - accuracy: 0.6875\n",
      "At: 466 [==========>] Loss 0.15080443962335116  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.21820142880770443  - accuracy: 0.75\n",
      "At: 468 [==========>] Loss 0.1630570694363347  - accuracy: 0.8125\n",
      "At: 469 [==========>] Loss 0.1756263203106992  - accuracy: 0.78125\n",
      "At: 470 [==========>] Loss 0.14008721187288056  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.19551009496463756  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.10703402545376582  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.17300677803801207  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.15575998376548783  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.1730030209385953  - accuracy: 0.78125\n",
      "At: 476 [==========>] Loss 0.18238908282280283  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.15022905863923655  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.15463329983662774  - accuracy: 0.84375\n",
      "At: 479 [==========>] Loss 0.18761005228935385  - accuracy: 0.8125\n",
      "At: 480 [==========>] Loss 0.24162195608438142  - accuracy: 0.71875\n",
      "At: 481 [==========>] Loss 0.13048501159018933  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.14637274589554625  - accuracy: 0.84375\n",
      "At: 483 [==========>] Loss 0.16624088108709936  - accuracy: 0.84375\n",
      "At: 484 [==========>] Loss 0.07464451436976283  - accuracy: 0.90625\n",
      "At: 485 [==========>] Loss 0.16407438699165727  - accuracy: 0.8125\n",
      "At: 486 [==========>] Loss 0.23901676375927777  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.20687694880536492  - accuracy: 0.78125\n",
      "At: 488 [==========>] Loss 0.18466184072600958  - accuracy: 0.8125\n",
      "At: 489 [==========>] Loss 0.2025993111170976  - accuracy: 0.75\n",
      "At: 490 [==========>] Loss 0.19388192409720217  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.21838306795658793  - accuracy: 0.75\n",
      "At: 492 [==========>] Loss 0.23880902361063341  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.1536538263609251  - accuracy: 0.8125\n",
      "At: 494 [==========>] Loss 0.2852565674832763  - accuracy: 0.6875\n",
      "At: 495 [==========>] Loss 0.18276427979638393  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.1963961458405608  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.19865991319790946  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.10865865748394513  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.21115382581298306  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.20782095681573315  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.21306250482870667  - accuracy: 0.75\n",
      "At: 502 [==========>] Loss 0.14835767792468924  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.14308022658188305  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.16070163239815557  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.20184751045778132  - accuracy: 0.75\n",
      "At: 506 [==========>] Loss 0.35758162635709057  - accuracy: 0.5625\n",
      "At: 507 [==========>] Loss 0.13021402169217688  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.12268403268236323  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.2377622131968003  - accuracy: 0.6875\n",
      "At: 510 [==========>] Loss 0.225652176713092  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.15310985658419154  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.15960226296973984  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.27939480202606815  - accuracy: 0.65625\n",
      "At: 514 [==========>] Loss 0.25373547220670933  - accuracy: 0.71875\n",
      "At: 515 [==========>] Loss 0.1511336894951733  - accuracy: 0.84375\n",
      "At: 516 [==========>] Loss 0.20551233858033952  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.14924075630034375  - accuracy: 0.8125\n",
      "At: 518 [==========>] Loss 0.188787048935833  - accuracy: 0.78125\n",
      "At: 519 [==========>] Loss 0.16467436328897245  - accuracy: 0.8125\n",
      "At: 520 [==========>] Loss 0.18735695755511217  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.21039424801930623  - accuracy: 0.75\n",
      "At: 522 [==========>] Loss 0.15720452482407993  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.21339115492410532  - accuracy: 0.71875\n",
      "At: 524 [==========>] Loss 0.15435780533849192  - accuracy: 0.84375\n",
      "At: 525 [==========>] Loss 0.2579364232684188  - accuracy: 0.71875\n",
      "At: 526 [==========>] Loss 0.24026472136079602  - accuracy: 0.6875\n",
      "At: 527 [==========>] Loss 0.38742060260598543  - accuracy: 0.53125\n",
      "At: 528 [==========>] Loss 0.2381194422015872  - accuracy: 0.6875\n",
      "At: 529 [==========>] Loss 0.15528381412406542  - accuracy: 0.8125\n",
      "At: 530 [==========>] Loss 0.2382256102585079  - accuracy: 0.71875\n",
      "At: 531 [==========>] Loss 0.1883588661753594  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.11946132323110807  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.14424900173876665  - accuracy: 0.8125\n",
      "At: 534 [==========>] Loss 0.1857897767787117  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.20660979430965618  - accuracy: 0.75\n",
      "At: 536 [==========>] Loss 0.20410979510148514  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.15491953655718282  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.19712020401865277  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.148888578744024  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.22189867557771661  - accuracy: 0.71875\n",
      "At: 541 [==========>] Loss 0.23223861350350467  - accuracy: 0.6875\n",
      "At: 542 [==========>] Loss 0.16297790437548146  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.2589293230035806  - accuracy: 0.6875\n",
      "At: 544 [==========>] Loss 0.30027803294297634  - accuracy: 0.625\n",
      "At: 545 [==========>] Loss 0.1192348998904346  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.16412325552217183  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.20909376700767177  - accuracy: 0.75\n",
      "At: 548 [==========>] Loss 0.13030361095624318  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.18254979840483337  - accuracy: 0.71875\n",
      "At: 550 [==========>] Loss 0.1435889779103107  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.18425565287607273  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.2298174152185203  - accuracy: 0.71875\n",
      "At: 553 [==========>] Loss 0.17305110543262292  - accuracy: 0.8125\n",
      "At: 554 [==========>] Loss 0.12144561712484292  - accuracy: 0.84375\n",
      "At: 555 [==========>] Loss 0.23884501978134415  - accuracy: 0.75\n",
      "At: 556 [==========>] Loss 0.2033195782070301  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.1904697653929161  - accuracy: 0.78125\n",
      "At: 558 [==========>] Loss 0.23237838620742657  - accuracy: 0.75\n",
      "At: 559 [==========>] Loss 0.2827411569784908  - accuracy: 0.6875\n",
      "At: 560 [==========>] Loss 0.15744220845816004  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.1410723830950763  - accuracy: 0.78125\n",
      "At: 562 [==========>] Loss 0.06900030709534974  - accuracy: 0.9375\n",
      "At: 563 [==========>] Loss 0.1350932345713005  - accuracy: 0.84375\n",
      "At: 564 [==========>] Loss 0.1736799644253132  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.17111690876370977  - accuracy: 0.8125\n",
      "At: 566 [==========>] Loss 0.2752977667406885  - accuracy: 0.6875\n",
      "At: 567 [==========>] Loss 0.2721961122413737  - accuracy: 0.71875\n",
      "At: 568 [==========>] Loss 0.289715642753375  - accuracy: 0.59375\n",
      "At: 569 [==========>] Loss 0.21700657581861083  - accuracy: 0.75\n",
      "At: 570 [==========>] Loss 0.12332391194483924  - accuracy: 0.84375\n",
      "At: 571 [==========>] Loss 0.16453334338441128  - accuracy: 0.78125\n",
      "At: 572 [==========>] Loss 0.1553935207807115  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.12227463807092986  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.22671363889605822  - accuracy: 0.75\n",
      "At: 575 [==========>] Loss 0.16573265875351822  - accuracy: 0.8125\n",
      "At: 576 [==========>] Loss 0.10904264359804745  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.2331505806253368  - accuracy: 0.71875\n",
      "At: 578 [==========>] Loss 0.2315984550829515  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.1510611034684075  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.2086078822483632  - accuracy: 0.75\n",
      "At: 581 [==========>] Loss 0.22379756322492692  - accuracy: 0.75\n",
      "At: 582 [==========>] Loss 0.21660595872566565  - accuracy: 0.71875\n",
      "At: 583 [==========>] Loss 0.261938420977605  - accuracy: 0.59375\n",
      "At: 584 [==========>] Loss 0.12269462985098312  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.25370520672570324  - accuracy: 0.6875\n",
      "At: 586 [==========>] Loss 0.06171001092395998  - accuracy: 0.9375\n",
      "At: 587 [==========>] Loss 0.17705763904439542  - accuracy: 0.78125\n",
      "At: 588 [==========>] Loss 0.1489852741490384  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.21167093586145558  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.08699889304891362  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.1539044373013304  - accuracy: 0.84375\n",
      "At: 592 [==========>] Loss 0.1276139864642306  - accuracy: 0.84375\n",
      "At: 593 [==========>] Loss 0.314568963733599  - accuracy: 0.625\n",
      "At: 594 [==========>] Loss 0.17830883029473032  - accuracy: 0.78125\n",
      "At: 595 [==========>] Loss 0.18618838370852758  - accuracy: 0.78125\n",
      "At: 596 [==========>] Loss 0.14073165425416845  - accuracy: 0.84375\n",
      "At: 597 [==========>] Loss 0.22408724250941853  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.14747440004811002  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.16568443136714914  - accuracy: 0.75\n",
      "At: 600 [==========>] Loss 0.0625114931324207  - accuracy: 0.9375\n",
      "At: 601 [==========>] Loss 0.1672088634434612  - accuracy: 0.8125\n",
      "At: 602 [==========>] Loss 0.20801811263911751  - accuracy: 0.75\n",
      "At: 603 [==========>] Loss 0.2677204905523909  - accuracy: 0.65625\n",
      "At: 604 [==========>] Loss 0.32361946316916645  - accuracy: 0.59375\n",
      "At: 605 [==========>] Loss 0.11469319923602284  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.1790216332267885  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.10231876765723098  - accuracy: 0.875\n",
      "At: 608 [==========>] Loss 0.1632557910838278  - accuracy: 0.75\n",
      "At: 609 [==========>] Loss 0.13406478950140943  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.21041598989770038  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.12737541987474882  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.1577398359335449  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.24121388122670015  - accuracy: 0.71875\n",
      "At: 614 [==========>] Loss 0.12036339646848353  - accuracy: 0.875\n",
      "At: 615 [==========>] Loss 0.24094156720367116  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.20815652356249303  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.13101961522259126  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.27561116858920204  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.1577636103850535  - accuracy: 0.84375\n",
      "At: 620 [==========>] Loss 0.21116849194645537  - accuracy: 0.71875\n",
      "At: 621 [==========>] Loss 0.0891644331452633  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.17304586696104216  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.1458171602356818  - accuracy: 0.8125\n",
      "At: 624 [==========>] Loss 0.13459307420415745  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.21137876735358374  - accuracy: 0.75\n",
      "At: 626 [==========>] Loss 0.1834871954523507  - accuracy: 0.78125\n",
      "At: 627 [==========>] Loss 0.100039929297672  - accuracy: 0.875\n",
      "At: 628 [==========>] Loss 0.15564740665805846  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.1811495270464476  - accuracy: 0.71875\n",
      "At: 630 [==========>] Loss 0.33210407175785367  - accuracy: 0.59375\n",
      "At: 631 [==========>] Loss 0.1843381129990151  - accuracy: 0.78125\n",
      "At: 632 [==========>] Loss 0.20030326679836574  - accuracy: 0.78125\n",
      "At: 633 [==========>] Loss 0.19670155296827765  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.18244653725282048  - accuracy: 0.75\n",
      "At: 635 [==========>] Loss 0.1366387927599644  - accuracy: 0.78125\n",
      "At: 636 [==========>] Loss 0.16833223054744959  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.18437596261751796  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.14072588527603713  - accuracy: 0.8125\n",
      "At: 639 [==========>] Loss 0.16844516589719583  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.2905289493501356  - accuracy: 0.59375\n",
      "At: 641 [==========>] Loss 0.2675961377898949  - accuracy: 0.6875\n",
      "At: 642 [==========>] Loss 0.23075858434367968  - accuracy: 0.6875\n",
      "At: 643 [==========>] Loss 0.19472257253339575  - accuracy: 0.75\n",
      "At: 644 [==========>] Loss 0.11989945935709163  - accuracy: 0.8125\n",
      "At: 645 [==========>] Loss 0.1362993500760354  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.0932317426613182  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.19234598055065527  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.16406671528584552  - accuracy: 0.78125\n",
      "At: 649 [==========>] Loss 0.20534043524271461  - accuracy: 0.71875\n",
      "At: 650 [==========>] Loss 0.11883145916111723  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.22788638553314425  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.18636349920574133  - accuracy: 0.8125\n",
      "At: 653 [==========>] Loss 0.14646625025057  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.11489130250141955  - accuracy: 0.84375\n",
      "At: 655 [==========>] Loss 0.19819539420782975  - accuracy: 0.75\n",
      "At: 656 [==========>] Loss 0.1334315959894151  - accuracy: 0.8125\n",
      "At: 657 [==========>] Loss 0.19681948613850683  - accuracy: 0.75\n",
      "At: 658 [==========>] Loss 0.11542566648152539  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.1644938600440491  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.1757853866591784  - accuracy: 0.8125\n",
      "At: 661 [==========>] Loss 0.18616526501075878  - accuracy: 0.71875\n",
      "At: 662 [==========>] Loss 0.13589705030909416  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.10732618584984384  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.16477104293304015  - accuracy: 0.78125\n",
      "At: 665 [==========>] Loss 0.2749657808609719  - accuracy: 0.65625\n",
      "At: 666 [==========>] Loss 0.2225705988184126  - accuracy: 0.71875\n",
      "At: 667 [==========>] Loss 0.18248766316758896  - accuracy: 0.78125\n",
      "At: 668 [==========>] Loss 0.21936670767574257  - accuracy: 0.71875\n",
      "At: 669 [==========>] Loss 0.14708097840992862  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.2622921789705045  - accuracy: 0.65625\n",
      "At: 671 [==========>] Loss 0.1524832819050664  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.13826576536923416  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.08657705834360058  - accuracy: 0.90625\n",
      "At: 674 [==========>] Loss 0.12309147505214732  - accuracy: 0.84375\n",
      "At: 675 [==========>] Loss 0.1398054053149796  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.23484725917379426  - accuracy: 0.6875\n",
      "At: 677 [==========>] Loss 0.14612463933328873  - accuracy: 0.8125\n",
      "At: 678 [==========>] Loss 0.17559923266600008  - accuracy: 0.78125\n",
      "At: 679 [==========>] Loss 0.12046222867163377  - accuracy: 0.84375\n",
      "At: 680 [==========>] Loss 0.12470509550391404  - accuracy: 0.8125\n",
      "At: 681 [==========>] Loss 0.09166297905879325  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.1638042419111195  - accuracy: 0.8125\n",
      "At: 683 [==========>] Loss 0.20171285480725498  - accuracy: 0.75\n",
      "At: 684 [==========>] Loss 0.14656317750133432  - accuracy: 0.8125\n",
      "At: 685 [==========>] Loss 0.1560489604090253  - accuracy: 0.8125\n",
      "At: 686 [==========>] Loss 0.12153651394910883  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.08691697821766631  - accuracy: 0.9375\n",
      "At: 688 [==========>] Loss 0.12586111071030048  - accuracy: 0.84375\n",
      "At: 689 [==========>] Loss 0.20468470664375427  - accuracy: 0.75\n",
      "At: 690 [==========>] Loss 0.17636635726947486  - accuracy: 0.75\n",
      "At: 691 [==========>] Loss 0.13296940824295614  - accuracy: 0.84375\n",
      "At: 692 [==========>] Loss 0.1771732931869739  - accuracy: 0.78125\n",
      "At: 693 [==========>] Loss 0.17164523169462587  - accuracy: 0.75\n",
      "At: 694 [==========>] Loss 0.17444854885539268  - accuracy: 0.78125\n",
      "At: 695 [==========>] Loss 0.19217625500217173  - accuracy: 0.75\n",
      "At: 696 [==========>] Loss 0.20047032503490356  - accuracy: 0.6875\n",
      "At: 697 [==========>] Loss 0.2580865016603675  - accuracy: 0.65625\n",
      "At: 698 [==========>] Loss 0.15121089451108136  - accuracy: 0.8125\n",
      "At: 699 [==========>] Loss 0.1612878775581722  - accuracy: 0.78125\n",
      "At: 700 [==========>] Loss 0.13068024673331016  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.15916549093528945  - accuracy: 0.75\n",
      "At: 702 [==========>] Loss 0.1166920245128189  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.18208255863897346  - accuracy: 0.78125\n",
      "At: 704 [==========>] Loss 0.24247141251725463  - accuracy: 0.65625\n",
      "At: 705 [==========>] Loss 0.22835841494337436  - accuracy: 0.71875\n",
      "At: 706 [==========>] Loss 0.16419285493266195  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.14366529086748642  - accuracy: 0.84375\n",
      "At: 708 [==========>] Loss 0.24899955027906828  - accuracy: 0.65625\n",
      "At: 709 [==========>] Loss 0.14157203289827675  - accuracy: 0.8125\n",
      "At: 710 [==========>] Loss 0.1996402979218394  - accuracy: 0.75\n",
      "At: 711 [==========>] Loss 0.24642118480012704  - accuracy: 0.65625\n",
      "At: 712 [==========>] Loss 0.23361764734051219  - accuracy: 0.71875\n",
      "At: 713 [==========>] Loss 0.2109648484299434  - accuracy: 0.6875\n",
      "At: 714 [==========>] Loss 0.2505496213837332  - accuracy: 0.65625\n",
      "At: 715 [==========>] Loss 0.16525908560812028  - accuracy: 0.75\n",
      "At: 716 [==========>] Loss 0.14550709067386874  - accuracy: 0.8125\n",
      "At: 717 [==========>] Loss 0.0940696125379481  - accuracy: 0.90625\n",
      "At: 718 [==========>] Loss 0.22309697907059664  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.1589345615607083  - accuracy: 0.8125\n",
      "At: 720 [==========>] Loss 0.14480510465508936  - accuracy: 0.8125\n",
      "At: 721 [==========>] Loss 0.12688209466930683  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.18607229220069826  - accuracy: 0.6875\n",
      "At: 723 [==========>] Loss 0.15973224077094006  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.08183713707218276  - accuracy: 0.90625\n",
      "At: 725 [==========>] Loss 0.21198593794298348  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.25562115445876643  - accuracy: 0.6875\n",
      "At: 727 [==========>] Loss 0.21235836713277728  - accuracy: 0.75\n",
      "At: 728 [==========>] Loss 0.21352652512178294  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.24272822350347267  - accuracy: 0.625\n",
      "At: 730 [==========>] Loss 0.25217262094841936  - accuracy: 0.6875\n",
      "At: 731 [==========>] Loss 0.15460506173171398  - accuracy: 0.78125\n",
      "At: 732 [==========>] Loss 0.19595370074423984  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.13238776317919992  - accuracy: 0.8125\n",
      "At: 734 [==========>] Loss 0.21243496036541762  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.15383218871275867  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.12550199639719695  - accuracy: 0.84375\n",
      "At: 737 [==========>] Loss 0.22826607385377962  - accuracy: 0.75\n",
      "At: 738 [==========>] Loss 0.13665306625133314  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.1108604486101107  - accuracy: 0.90625\n",
      "At: 740 [==========>] Loss 0.21179409340620553  - accuracy: 0.75\n",
      "At: 741 [==========>] Loss 0.16301071074685042  - accuracy: 0.8125\n",
      "At: 742 [==========>] Loss 0.1478991626556157  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.16536581639002876  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.1325541707309421  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.3080212273243893  - accuracy: 0.59375\n",
      "At: 746 [==========>] Loss 0.1584465316920975  - accuracy: 0.78125\n",
      "At: 747 [==========>] Loss 0.1452456810171834  - accuracy: 0.8125\n",
      "At: 748 [==========>] Loss 0.17061932866717594  - accuracy: 0.75\n",
      "At: 749 [==========>] Loss 0.21215319967692875  - accuracy: 0.6875\n",
      "At: 750 [==========>] Loss 0.11710306581464042  - accuracy: 0.84375\n",
      "At: 751 [==========>] Loss 0.1909716768620957  - accuracy: 0.78125\n",
      "At: 752 [==========>] Loss 0.09441953052600457  - accuracy: 0.875\n",
      "At: 753 [==========>] Loss 0.23359108771574058  - accuracy: 0.65625\n",
      "At: 754 [==========>] Loss 0.14007730795218762  - accuracy: 0.78125\n",
      "At: 755 [==========>] Loss 0.10589337470250154  - accuracy: 0.875\n",
      "At: 756 [==========>] Loss 0.226827019284027  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.13636089590345996  - accuracy: 0.875\n",
      "At: 758 [==========>] Loss 0.15050448312521422  - accuracy: 0.875\n",
      "At: 759 [==========>] Loss 0.10616763057462283  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.26984601573986067  - accuracy: 0.65625\n",
      "At: 761 [==========>] Loss 0.1765482508901363  - accuracy: 0.78125\n",
      "At: 762 [==========>] Loss 0.1437141979107393  - accuracy: 0.84375\n",
      "At: 763 [==========>] Loss 0.09872556129382237  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.10693215702327437  - accuracy: 0.875\n",
      "At: 765 [==========>] Loss 0.17845319735238313  - accuracy: 0.78125\n",
      "At: 766 [==========>] Loss 0.16447789534261764  - accuracy: 0.8125\n",
      "At: 767 [==========>] Loss 0.07564563809008684  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.17637273558772296  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.15732276299282838  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.12784078857907966  - accuracy: 0.84375\n",
      "At: 771 [==========>] Loss 0.25594888023400175  - accuracy: 0.65625\n",
      "At: 772 [==========>] Loss 0.1516908861350012  - accuracy: 0.78125\n",
      "At: 773 [==========>] Loss 0.06488645066947638  - accuracy: 0.9375\n",
      "At: 774 [==========>] Loss 0.19680588821687417  - accuracy: 0.75\n",
      "At: 775 [==========>] Loss 0.2190055599934594  - accuracy: 0.6875\n",
      "At: 776 [==========>] Loss 0.17322036806289573  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.1289743448197883  - accuracy: 0.84375\n",
      "At: 778 [==========>] Loss 0.19698263825241852  - accuracy: 0.75\n",
      "At: 779 [==========>] Loss 0.12448141566504328  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.08927693246164897  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.18614447640443038  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.1719865173989862  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.21642675750309534  - accuracy: 0.71875\n",
      "At: 784 [==========>] Loss 0.2366382952850893  - accuracy: 0.6875\n",
      "At: 785 [==========>] Loss 0.25597248647209603  - accuracy: 0.65625\n",
      "At: 786 [==========>] Loss 0.13681135578800946  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.19651394181704088  - accuracy: 0.71875\n",
      "At: 788 [==========>] Loss 0.0832030586704378  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.19432205655186163  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.15824091053580008  - accuracy: 0.75\n",
      "At: 791 [==========>] Loss 0.22545826444910846  - accuracy: 0.6875\n",
      "At: 792 [==========>] Loss 0.20205479031433332  - accuracy: 0.71875\n",
      "At: 793 [==========>] Loss 0.12182513407783718  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.17592379821519338  - accuracy: 0.75\n",
      "At: 795 [==========>] Loss 0.1761954431419116  - accuracy: 0.75\n",
      "At: 796 [==========>] Loss 0.19947868921381548  - accuracy: 0.78125\n",
      "At: 797 [==========>] Loss 0.1901203520316315  - accuracy: 0.6875\n",
      "At: 798 [==========>] Loss 0.2234739209549129  - accuracy: 0.6875\n",
      "At: 799 [==========>] Loss 0.10237210645039635  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.12971647657048907  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.10034489410403533  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.2022712228905007  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.2040647440258878  - accuracy: 0.6875\n",
      "At: 804 [==========>] Loss 0.1326662962979983  - accuracy: 0.84375\n",
      "At: 805 [==========>] Loss 0.1543416572895823  - accuracy: 0.78125\n",
      "At: 806 [==========>] Loss 0.15367668091894304  - accuracy: 0.8125\n",
      "At: 807 [==========>] Loss 0.14655595893715856  - accuracy: 0.78125\n",
      "At: 808 [==========>] Loss 0.15826004026316265  - accuracy: 0.8125\n",
      "At: 809 [==========>] Loss 0.11753758869476498  - accuracy: 0.84375\n",
      "At: 810 [==========>] Loss 0.21076993393145066  - accuracy: 0.75\n",
      "At: 811 [==========>] Loss 0.18324343680942778  - accuracy: 0.75\n",
      "At: 812 [==========>] Loss 0.14213063410387122  - accuracy: 0.8125\n",
      "At: 813 [==========>] Loss 0.20070700850232673  - accuracy: 0.65625\n",
      "At: 814 [==========>] Loss 0.13955003613083528  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.1600790170059717  - accuracy: 0.84375\n",
      "At: 816 [==========>] Loss 0.16063832117922605  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.11214153281930207  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.15913671524415013  - accuracy: 0.8125\n",
      "At: 819 [==========>] Loss 0.1487793437454329  - accuracy: 0.84375\n",
      "At: 820 [==========>] Loss 0.12310822543762623  - accuracy: 0.875\n",
      "At: 821 [==========>] Loss 0.16743477620950076  - accuracy: 0.75\n",
      "At: 822 [==========>] Loss 0.2165382176789453  - accuracy: 0.71875\n",
      "At: 823 [==========>] Loss 0.14545135858191421  - accuracy: 0.84375\n",
      "At: 824 [==========>] Loss 0.1874818005495926  - accuracy: 0.75\n",
      "At: 825 [==========>] Loss 0.20258942691450674  - accuracy: 0.65625\n",
      "At: 826 [==========>] Loss 0.19070927667567789  - accuracy: 0.71875\n",
      "At: 827 [==========>] Loss 0.073550116752029  - accuracy: 0.9375\n",
      "At: 828 [==========>] Loss 0.0949411583514562  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.11110230539741513  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.13389221134200466  - accuracy: 0.8125\n",
      "At: 831 [==========>] Loss 0.11149039356015003  - accuracy: 0.875\n",
      "At: 832 [==========>] Loss 0.14056403572187925  - accuracy: 0.78125\n",
      "At: 833 [==========>] Loss 0.20595244046604658  - accuracy: 0.71875\n",
      "At: 834 [==========>] Loss 0.14630117088678238  - accuracy: 0.78125\n",
      "At: 835 [==========>] Loss 0.09614905841783414  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.13644833410508472  - accuracy: 0.8125\n",
      "At: 837 [==========>] Loss 0.16804565747890987  - accuracy: 0.78125\n",
      "At: 838 [==========>] Loss 0.1455391966272178  - accuracy: 0.78125\n",
      "At: 839 [==========>] Loss 0.15346680169924937  - accuracy: 0.78125\n",
      "At: 840 [==========>] Loss 0.15546354374404298  - accuracy: 0.75\n",
      "At: 841 [==========>] Loss 0.08878813437221794  - accuracy: 0.90625\n",
      "At: 842 [==========>] Loss 0.07597340398787118  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.22702451210268038  - accuracy: 0.6875\n",
      "At: 844 [==========>] Loss 0.1364796708384288  - accuracy: 0.78125\n",
      "At: 845 [==========>] Loss 0.18929350192440603  - accuracy: 0.78125\n",
      "At: 846 [==========>] Loss 0.1351833147088286  - accuracy: 0.875\n",
      "At: 847 [==========>] Loss 0.13628404483246268  - accuracy: 0.84375\n",
      "At: 848 [==========>] Loss 0.19708681858630303  - accuracy: 0.75\n",
      "At: 849 [==========>] Loss 0.21376742708508112  - accuracy: 0.65625\n",
      "At: 850 [==========>] Loss 0.06693832607681288  - accuracy: 0.9375\n",
      "At: 851 [==========>] Loss 0.1029274860421734  - accuracy: 0.84375\n",
      "At: 852 [==========>] Loss 0.1890612207151953  - accuracy: 0.71875\n",
      "At: 853 [==========>] Loss 0.16236228864731286  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.21078338991921663  - accuracy: 0.6875\n",
      "At: 855 [==========>] Loss 0.11470151161306183  - accuracy: 0.84375\n",
      "At: 856 [==========>] Loss 0.06883101747926712  - accuracy: 0.9375\n",
      "At: 857 [==========>] Loss 0.108791903366562  - accuracy: 0.90625\n",
      "At: 858 [==========>] Loss 0.24189729712412825  - accuracy: 0.65625\n",
      "At: 859 [==========>] Loss 0.13079578870699574  - accuracy: 0.78125\n",
      "At: 860 [==========>] Loss 0.14641160602475944  - accuracy: 0.8125\n",
      "At: 861 [==========>] Loss 0.10517494816403938  - accuracy: 0.78125\n",
      "At: 862 [==========>] Loss 0.15989564654108285  - accuracy: 0.8125\n",
      "At: 863 [==========>] Loss 0.1721497684722685  - accuracy: 0.71875\n",
      "At: 864 [==========>] Loss 0.22162240692754553  - accuracy: 0.71875\n",
      "At: 865 [==========>] Loss 0.19323484325937834  - accuracy: 0.6875\n",
      "At: 866 [==========>] Loss 0.17665937773689883  - accuracy: 0.75\n",
      "At: 867 [==========>] Loss 0.12763054884454875  - accuracy: 0.84375\n",
      "At: 868 [==========>] Loss 0.19421495463992186  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.1872208773801597  - accuracy: 0.75\n",
      "At: 870 [==========>] Loss 0.1490546772205815  - accuracy: 0.84375\n",
      "At: 871 [==========>] Loss 0.11093863354523432  - accuracy: 0.875\n",
      "At: 872 [==========>] Loss 0.1378621361888971  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.17165946506831128  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.18201029439067512  - accuracy: 0.71875\n",
      "At: 875 [==========>] Loss 0.13165884981334236  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.14606892438719787  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.2104596899550125  - accuracy: 0.71875\n",
      "At: 878 [==========>] Loss 0.1006396341435245  - accuracy: 0.875\n",
      "At: 879 [==========>] Loss 0.19143892229272275  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.1718572625401078  - accuracy: 0.78125\n",
      "At: 881 [==========>] Loss 0.17672695690307277  - accuracy: 0.75\n",
      "At: 882 [==========>] Loss 0.13571087966452283  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.18009794531988071  - accuracy: 0.71875\n",
      "At: 884 [==========>] Loss 0.1958583080783729  - accuracy: 0.71875\n",
      "At: 885 [==========>] Loss 0.14499999119612586  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.13007972494517642  - accuracy: 0.8125\n",
      "At: 887 [==========>] Loss 0.17608348181586397  - accuracy: 0.71875\n",
      "At: 888 [==========>] Loss 0.20252165373490055  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.10096300188619223  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.15056145123692535  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.13192615890979936  - accuracy: 0.84375\n",
      "At: 892 [==========>] Loss 0.13773770761671666  - accuracy: 0.84375\n",
      "At: 893 [==========>] Loss 0.1363296666106652  - accuracy: 0.8125\n",
      "At: 894 [==========>] Loss 0.11438238916800098  - accuracy: 0.84375\n",
      "At: 895 [==========>] Loss 0.19380765802766026  - accuracy: 0.84375\n",
      "At: 896 [==========>] Loss 0.18569712670326882  - accuracy: 0.6875\n",
      "At: 897 [==========>] Loss 0.15207811174885155  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.17398936214401461  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.11044054585629633  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.17032891218353488  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.18755732033438074  - accuracy: 0.75\n",
      "At: 902 [==========>] Loss 0.08906288806378315  - accuracy: 0.90625\n",
      "At: 903 [==========>] Loss 0.15220039665333235  - accuracy: 0.78125\n",
      "At: 904 [==========>] Loss 0.1323023122471756  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.136711117666413  - accuracy: 0.78125\n",
      "At: 906 [==========>] Loss 0.11577855605434625  - accuracy: 0.84375\n",
      "At: 907 [==========>] Loss 0.19309058619899505  - accuracy: 0.75\n",
      "At: 908 [==========>] Loss 0.1850548247532805  - accuracy: 0.71875\n",
      "At: 909 [==========>] Loss 0.12575570103506895  - accuracy: 0.78125\n",
      "At: 910 [==========>] Loss 0.13973037350477108  - accuracy: 0.75\n",
      "At: 911 [==========>] Loss 0.13651219204680176  - accuracy: 0.8125\n",
      "At: 912 [==========>] Loss 0.1545129962423626  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.10817677435378265  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.1369041316904575  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.16227854208912573  - accuracy: 0.75\n",
      "At: 916 [==========>] Loss 0.22967598900299424  - accuracy: 0.6875\n",
      "At: 917 [==========>] Loss 0.18225195883080497  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.20094777767146815  - accuracy: 0.6875\n",
      "At: 919 [==========>] Loss 0.12334371347342343  - accuracy: 0.875\n",
      "At: 920 [==========>] Loss 0.12451176999278102  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.16349338979668404  - accuracy: 0.8125\n",
      "At: 922 [==========>] Loss 0.14619094585584333  - accuracy: 0.75\n",
      "At: 923 [==========>] Loss 0.11337096800469937  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.1907820857743475  - accuracy: 0.71875\n",
      "At: 925 [==========>] Loss 0.13732601084003018  - accuracy: 0.875\n",
      "At: 926 [==========>] Loss 0.14218604465311008  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.14122807035648918  - accuracy: 0.78125\n",
      "At: 928 [==========>] Loss 0.11759716674048079  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.15359965137816584  - accuracy: 0.78125\n",
      "At: 930 [==========>] Loss 0.13198243622797307  - accuracy: 0.78125\n",
      "At: 931 [==========>] Loss 0.18198395218437252  - accuracy: 0.75\n",
      "At: 932 [==========>] Loss 0.10665154397766904  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.10672420273023031  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.16568425176236776  - accuracy: 0.75\n",
      "At: 935 [==========>] Loss 0.0797460438318634  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.1879475756378999  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.18670447718762656  - accuracy: 0.78125\n",
      "At: 938 [==========>] Loss 0.17200883126663644  - accuracy: 0.75\n",
      "At: 939 [==========>] Loss 0.1642700177008674  - accuracy: 0.78125\n",
      "At: 940 [==========>] Loss 0.24652258623150225  - accuracy: 0.5625\n",
      "At: 941 [==========>] Loss 0.1295900903674137  - accuracy: 0.8125\n",
      "At: 942 [==========>] Loss 0.18868348859466402  - accuracy: 0.65625\n",
      "At: 943 [==========>] Loss 0.12510818213734964  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.11045088639149189  - accuracy: 0.84375\n",
      "At: 945 [==========>] Loss 0.1298397024996057  - accuracy: 0.8125\n",
      "At: 946 [==========>] Loss 0.16656217227107914  - accuracy: 0.71875\n",
      "At: 947 [==========>] Loss 0.1855997340329591  - accuracy: 0.78125\n",
      "At: 948 [==========>] Loss 0.23394670733323392  - accuracy: 0.6875\n",
      "At: 949 [==========>] Loss 0.11944932143932008  - accuracy: 0.875\n",
      "At: 950 [==========>] Loss 0.15414667627319362  - accuracy: 0.75\n",
      "At: 951 [==========>] Loss 0.12529633912647248  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.1048663516235939  - accuracy: 0.875\n",
      "At: 953 [==========>] Loss 0.08676956606039377  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.1203955487232055  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.14205081275085443  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.12029744303158207  - accuracy: 0.8125\n",
      "At: 957 [==========>] Loss 0.20883848291718585  - accuracy: 0.75\n",
      "At: 958 [==========>] Loss 0.10993105445099007  - accuracy: 0.84375\n",
      "At: 959 [==========>] Loss 0.12381058624508415  - accuracy: 0.84375\n",
      "At: 960 [==========>] Loss 0.12861769648944582  - accuracy: 0.84375\n",
      "At: 961 [==========>] Loss 0.1399891720324738  - accuracy: 0.8125\n",
      "At: 962 [==========>] Loss 0.13834222156127163  - accuracy: 0.78125\n",
      "At: 963 [==========>] Loss 0.07992726840158183  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.200922898233357  - accuracy: 0.71875\n",
      "At: 965 [==========>] Loss 0.13782639370560246  - accuracy: 0.8125\n",
      "At: 966 [==========>] Loss 0.19713507596961427  - accuracy: 0.6875\n",
      "At: 967 [==========>] Loss 0.13495693825207197  - accuracy: 0.8125\n",
      "At: 968 [==========>] Loss 0.1610052234424346  - accuracy: 0.78125\n",
      "At: 969 [==========>] Loss 0.16661963028528268  - accuracy: 0.75\n",
      "At: 970 [==========>] Loss 0.16038296324838835  - accuracy: 0.78125\n",
      "At: 971 [==========>] Loss 0.13192438097478326  - accuracy: 0.78125\n",
      "At: 972 [==========>] Loss 0.09911467512819283  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.13273501001829643  - accuracy: 0.84375\n",
      "At: 974 [==========>] Loss 0.13421451723063554  - accuracy: 0.78125\n",
      "At: 975 [==========>] Loss 0.18735366387963742  - accuracy: 0.75\n",
      "At: 976 [==========>] Loss 0.1437536475736403  - accuracy: 0.8125\n",
      "At: 977 [==========>] Loss 0.15392926234880405  - accuracy: 0.78125\n",
      "At: 978 [==========>] Loss 0.19901726410076445  - accuracy: 0.71875\n",
      "At: 979 [==========>] Loss 0.09909927394624926  - accuracy: 0.90625\n",
      "At: 980 [==========>] Loss 0.16664873420809834  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.19340832900762275  - accuracy: 0.6875\n",
      "At: 982 [==========>] Loss 0.08628870693042454  - accuracy: 0.90625\n",
      "At: 983 [==========>] Loss 0.16236368539376192  - accuracy: 0.75\n",
      "At: 984 [==========>] Loss 0.13536972760896848  - accuracy: 0.78125\n",
      "At: 985 [==========>] Loss 0.20299253759188535  - accuracy: 0.75\n",
      "At: 986 [==========>] Loss 0.1450815505100086  - accuracy: 0.78125\n",
      "At: 987 [==========>] Loss 0.14795086274756117  - accuracy: 0.78125\n",
      "At: 988 [==========>] Loss 0.12205744728319376  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.14064572466667155  - accuracy: 0.78125\n",
      "At: 990 [==========>] Loss 0.1450512964720388  - accuracy: 0.71875\n",
      "At: 991 [==========>] Loss 0.1236606430299759  - accuracy: 0.8125\n",
      "At: 992 [==========>] Loss 0.25648611821422357  - accuracy: 0.625\n",
      "At: 993 [==========>] Loss 0.15310548764231144  - accuracy: 0.8125\n",
      "At: 994 [==========>] Loss 0.17982519240182593  - accuracy: 0.71875\n",
      "At: 995 [==========>] Loss 0.18863058172582509  - accuracy: 0.75\n",
      "At: 996 [==========>] Loss 0.11026156359742997  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.17734945863073184  - accuracy: 0.71875\n",
      "At: 998 [==========>] Loss 0.11762269699323313  - accuracy: 0.90625\n",
      "At: 999 [==========>] Loss 0.1750941382234457  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.21053876497769042  - accuracy: 0.71875\n",
      "At: 1001 [==========>] Loss 0.1651248550431404  - accuracy: 0.75\n",
      "At: 1002 [==========>] Loss 0.2114658457116117  - accuracy: 0.75\n",
      "At: 1003 [==========>] Loss 0.16074738463469856  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.1722867310778155  - accuracy: 0.78125\n",
      "At: 1005 [==========>] Loss 0.09314020289753418  - accuracy: 0.84375\n",
      "At: 1006 [==========>] Loss 0.15861809261852433  - accuracy: 0.8125\n",
      "At: 1007 [==========>] Loss 0.12473366659123175  - accuracy: 0.8125\n",
      "At: 1008 [==========>] Loss 0.19421579903067282  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.1429801432812504  - accuracy: 0.8125\n",
      "At: 1010 [==========>] Loss 0.20199503364251736  - accuracy: 0.6875\n",
      "At: 1011 [==========>] Loss 0.18765717871529944  - accuracy: 0.75\n",
      "At: 1012 [==========>] Loss 0.10934619583394897  - accuracy: 0.8125\n",
      "At: 1013 [==========>] Loss 0.10573317549440941  - accuracy: 0.84375\n",
      "At: 1014 [==========>] Loss 0.1154712496865227  - accuracy: 0.8125\n",
      "At: 1015 [==========>] Loss 0.2231009987832026  - accuracy: 0.625\n",
      "At: 1016 [==========>] Loss 0.18431533880840528  - accuracy: 0.78125\n",
      "At: 1017 [==========>] Loss 0.18918514572138215  - accuracy: 0.6875\n",
      "At: 1018 [==========>] Loss 0.18172956566824033  - accuracy: 0.8125\n",
      "At: 1019 [==========>] Loss 0.20031437646369588  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.16768409538921594  - accuracy: 0.71875\n",
      "At: 1021 [==========>] Loss 0.13062889066200978  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.1489824516671727  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.22359276282500778  - accuracy: 0.65625\n",
      "At: 1024 [==========>] Loss 0.19594520573603808  - accuracy: 0.65625\n",
      "At: 1025 [==========>] Loss 0.26375064824437183  - accuracy: 0.625\n",
      "At: 1026 [==========>] Loss 0.12403662447376806  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.13805470187797791  - accuracy: 0.84375\n",
      "At: 1028 [==========>] Loss 0.24693406953477778  - accuracy: 0.6875\n",
      "At: 1029 [==========>] Loss 0.09459405328703552  - accuracy: 0.90625\n",
      "At: 1030 [==========>] Loss 0.16438945422378134  - accuracy: 0.8125\n",
      "At: 1031 [==========>] Loss 0.14855291360654216  - accuracy: 0.75\n",
      "At: 1032 [==========>] Loss 0.1876113860474133  - accuracy: 0.71875\n",
      "At: 1033 [==========>] Loss 0.1356270099747008  - accuracy: 0.84375\n",
      "At: 1034 [==========>] Loss 0.14207161867152443  - accuracy: 0.8125\n",
      "At: 1035 [==========>] Loss 0.11429560421167809  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.20260171055260626  - accuracy: 0.6875\n",
      "At: 1037 [==========>] Loss 0.17118595913929918  - accuracy: 0.75\n",
      "At: 1038 [==========>] Loss 0.09876511200836277  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.11767685921098137  - accuracy: 0.875\n",
      "At: 1040 [==========>] Loss 0.15069897237525817  - accuracy: 0.8125\n",
      "At: 1041 [==========>] Loss 0.14143302464512306  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.11557230002236712  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.18145073645639218  - accuracy: 0.6875\n",
      "At: 1044 [==========>] Loss 0.1369948343586017  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.16919341301242197  - accuracy: 0.75\n",
      "At: 1046 [==========>] Loss 0.19458521442073512  - accuracy: 0.78125\n",
      "At: 1047 [==========>] Loss 0.14676452898712472  - accuracy: 0.78125\n",
      "At: 1048 [==========>] Loss 0.18035058559939088  - accuracy: 0.75\n",
      "At: 1049 [==========>] Loss 0.19636879285707315  - accuracy: 0.71875\n",
      "At: 1050 [==========>] Loss 0.1849787690767524  - accuracy: 0.71875\n",
      "At: 1051 [==========>] Loss 0.09042958281644808  - accuracy: 0.84375\n",
      "At: 1052 [==========>] Loss 0.12186890963571471  - accuracy: 0.875\n",
      "At: 1053 [==========>] Loss 0.10994183259441888  - accuracy: 0.90625\n",
      "At: 1054 [==========>] Loss 0.1322720275212162  - accuracy: 0.8125\n",
      "At: 1055 [==========>] Loss 0.20351606765683772  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.15075830863890005  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.1984243470869067  - accuracy: 0.6875\n",
      "At: 1058 [==========>] Loss 0.06167473295879453  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.11792464041566966  - accuracy: 0.8125\n",
      "At: 1060 [==========>] Loss 0.12495540139382291  - accuracy: 0.84375\n",
      "At: 1061 [==========>] Loss 0.09996393954321149  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.16842682836288003  - accuracy: 0.71875\n",
      "At: 1063 [==========>] Loss 0.15753388256357992  - accuracy: 0.78125\n",
      "At: 1064 [==========>] Loss 0.20103580559268605  - accuracy: 0.65625\n",
      "At: 1065 [==========>] Loss 0.11377772057108246  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.12665957803175906  - accuracy: 0.875\n",
      "At: 1067 [==========>] Loss 0.13645182608882592  - accuracy: 0.84375\n",
      "At: 1068 [==========>] Loss 0.10040507523691652  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.14889244783999023  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.14986184427600033  - accuracy: 0.8125\n",
      "At: 1071 [==========>] Loss 0.12651228980155363  - accuracy: 0.84375\n",
      "At: 1072 [==========>] Loss 0.1418051743607642  - accuracy: 0.8125\n",
      "At: 1073 [==========>] Loss 0.1661859097725237  - accuracy: 0.8125\n",
      "At: 1074 [==========>] Loss 0.20499896116924826  - accuracy: 0.71875\n",
      "At: 1075 [==========>] Loss 0.12382838040021049  - accuracy: 0.78125\n",
      "At: 1076 [==========>] Loss 0.18388469880887437  - accuracy: 0.8125\n",
      "At: 1077 [==========>] Loss 0.09533512380852655  - accuracy: 0.90625\n",
      "At: 1078 [==========>] Loss 0.10804894226820513  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.16855037673522277  - accuracy: 0.71875\n",
      "At: 1080 [==========>] Loss 0.1549481367381536  - accuracy: 0.71875\n",
      "At: 1081 [==========>] Loss 0.1192090344371263  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.14882801797577436  - accuracy: 0.75\n",
      "At: 1083 [==========>] Loss 0.10915059362404182  - accuracy: 0.84375\n",
      "At: 1084 [==========>] Loss 0.13434121701901622  - accuracy: 0.84375\n",
      "At: 1085 [==========>] Loss 0.10753536373942033  - accuracy: 0.84375\n",
      "At: 1086 [==========>] Loss 0.13668527411372816  - accuracy: 0.84375\n",
      "At: 1087 [==========>] Loss 0.15880996716025617  - accuracy: 0.78125\n",
      "At: 1088 [==========>] Loss 0.17995897365570895  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.1027830682766905  - accuracy: 0.84375\n",
      "At: 1090 [==========>] Loss 0.12455604777898177  - accuracy: 0.8125\n",
      "At: 1091 [==========>] Loss 0.1745087660613212  - accuracy: 0.78125\n",
      "At: 1092 [==========>] Loss 0.1113396163299801  - accuracy: 0.90625\n",
      "At: 1093 [==========>] Loss 0.15837854117639738  - accuracy: 0.8125\n",
      "At: 1094 [==========>] Loss 0.14520693462764434  - accuracy: 0.78125\n",
      "At: 1095 [==========>] Loss 0.14662339252163237  - accuracy: 0.78125\n",
      "At: 1096 [==========>] Loss 0.1407328278988664  - accuracy: 0.75\n",
      "At: 1097 [==========>] Loss 0.06557482946773568  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.15866099725963267  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.17229521389969712  - accuracy: 0.6875\n",
      "At: 1100 [==========>] Loss 0.1222687278971099  - accuracy: 0.84375\n",
      "At: 1101 [==========>] Loss 0.12320633579903814  - accuracy: 0.84375\n",
      "At: 1102 [==========>] Loss 0.15544096459075774  - accuracy: 0.78125\n",
      "At: 1103 [==========>] Loss 0.09712464160600749  - accuracy: 0.8125\n",
      "At: 1104 [==========>] Loss 0.08857085323476796  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.09279726089503001  - accuracy: 0.84375\n",
      "At: 1106 [==========>] Loss 0.09373632018571981  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.1980589748632805  - accuracy: 0.65625\n",
      "At: 1108 [==========>] Loss 0.13881507016169992  - accuracy: 0.8125\n",
      "At: 1109 [==========>] Loss 0.07011813531985109  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.14503929857153353  - accuracy: 0.78125\n",
      "At: 1111 [==========>] Loss 0.20330316158602157  - accuracy: 0.71875\n",
      "At: 1112 [==========>] Loss 0.2042201050888228  - accuracy: 0.6875\n",
      "At: 1113 [==========>] Loss 0.16396531957149857  - accuracy: 0.71875\n",
      "At: 1114 [==========>] Loss 0.11019536573560715  - accuracy: 0.875\n",
      "At: 1115 [==========>] Loss 0.09610303859151925  - accuracy: 0.875\n",
      "At: 1116 [==========>] Loss 0.12654954401894308  - accuracy: 0.8125\n",
      "At: 1117 [==========>] Loss 0.10366230425236106  - accuracy: 0.84375\n",
      "At: 1118 [==========>] Loss 0.12757843074033554  - accuracy: 0.78125\n",
      "At: 1119 [==========>] Loss 0.16068380506138896  - accuracy: 0.78125\n",
      "At: 1120 [==========>] Loss 0.1265072341106733  - accuracy: 0.8125\n",
      "At: 1121 [==========>] Loss 0.14569107878766274  - accuracy: 0.71875\n",
      "At: 1122 [==========>] Loss 0.1173768474453983  - accuracy: 0.84375\n",
      "At: 1123 [==========>] Loss 0.18629438451773989  - accuracy: 0.6875\n",
      "At: 1124 [==========>] Loss 0.14662567072676982  - accuracy: 0.78125\n",
      "At: 1125 [==========>] Loss 0.16025777608871117  - accuracy: 0.78125\n",
      "At: 1126 [==========>] Loss 0.13430890447710858  - accuracy: 0.875\n",
      "At: 1127 [==========>] Loss 0.13967664412490222  - accuracy: 0.8125\n",
      "At: 1128 [==========>] Loss 0.10428021047802978  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.15179013803542923  - accuracy: 0.8125\n",
      "At: 1130 [==========>] Loss 0.1427979165082061  - accuracy: 0.84375\n",
      "At: 1131 [==========>] Loss 0.14798595716051885  - accuracy: 0.8125\n",
      "At: 1132 [==========>] Loss 0.11667749197064903  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.15667610813700922  - accuracy: 0.78125\n",
      "At: 1134 [==========>] Loss 0.1189972308025667  - accuracy: 0.78125\n",
      "At: 1135 [==========>] Loss 0.15308803666710263  - accuracy: 0.78125\n",
      "At: 1136 [==========>] Loss 0.13647318201383707  - accuracy: 0.8125\n",
      "At: 1137 [==========>] Loss 0.1631750216078944  - accuracy: 0.75\n",
      "At: 1138 [==========>] Loss 0.11192592380623817  - accuracy: 0.8125\n",
      "At: 1139 [==========>] Loss 0.11350761308048174  - accuracy: 0.84375\n",
      "At: 1140 [==========>] Loss 0.21043409825035708  - accuracy: 0.65625\n",
      "At: 1141 [==========>] Loss 0.13858903392278202  - accuracy: 0.84375\n",
      "At: 1142 [==========>] Loss 0.17842225541296036  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.059612312565516176  - accuracy: 0.9375\n",
      "At: 1144 [==========>] Loss 0.09413486308557799  - accuracy: 0.90625\n",
      "At: 1145 [==========>] Loss 0.19054063146768602  - accuracy: 0.71875\n",
      "At: 1146 [==========>] Loss 0.13785567831914994  - accuracy: 0.8125\n",
      "At: 1147 [==========>] Loss 0.214106471634842  - accuracy: 0.6875\n",
      "At: 1148 [==========>] Loss 0.13369201354868898  - accuracy: 0.84375\n",
      "At: 1149 [==========>] Loss 0.11627098172045686  - accuracy: 0.875\n",
      "At: 1150 [==========>] Loss 0.16647740343601147  - accuracy: 0.75\n",
      "At: 1151 [==========>] Loss 0.18347919589995793  - accuracy: 0.71875\n",
      "At: 1152 [==========>] Loss 0.13797884622337353  - accuracy: 0.875\n",
      "At: 1153 [==========>] Loss 0.1737578914228475  - accuracy: 0.75\n",
      "At: 1154 [==========>] Loss 0.11053870457172967  - accuracy: 0.84375\n",
      "At: 1155 [==========>] Loss 0.1245947022817009  - accuracy: 0.8125\n",
      "At: 1156 [==========>] Loss 0.15551118514626844  - accuracy: 0.78125\n",
      "At: 1157 [==========>] Loss 0.15357427409397012  - accuracy: 0.75\n",
      "At: 1158 [==========>] Loss 0.16799213164687554  - accuracy: 0.84375\n",
      "At: 1159 [==========>] Loss 0.1694718091522354  - accuracy: 0.71875\n",
      "At: 1160 [==========>] Loss 0.11567563039234853  - accuracy: 0.71875\n",
      "At: 1161 [==========>] Loss 0.10346990225756085  - accuracy: 0.875\n",
      "At: 1162 [==========>] Loss 0.1636942650367702  - accuracy: 0.71875\n",
      "At: 1163 [==========>] Loss 0.19497045151426695  - accuracy: 0.65625\n",
      "At: 1164 [==========>] Loss 0.1003343505965765  - accuracy: 0.90625\n",
      "At: 1165 [==========>] Loss 0.17901872588644144  - accuracy: 0.78125\n",
      "At: 1166 [==========>] Loss 0.07901491251883097  - accuracy: 0.96875\n",
      "At: 1167 [==========>] Loss 0.15262088609869623  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.12497599148406432  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.10196785830615224  - accuracy: 0.8125\n",
      "At: 1170 [==========>] Loss 0.16148546108056772  - accuracy: 0.75\n",
      "At: 1171 [==========>] Loss 0.07644373885208347  - accuracy: 0.90625\n",
      "At: 1172 [==========>] Loss 0.11514133058329117  - accuracy: 0.84375\n",
      "At: 1173 [==========>] Loss 0.15951935280916463  - accuracy: 0.78125\n",
      "At: 1174 [==========>] Loss 0.2249698320992861  - accuracy: 0.6875\n",
      "At: 1175 [==========>] Loss 0.11399000214941986  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.12980036670374992  - accuracy: 0.78125\n",
      "At: 1177 [==========>] Loss 0.11563294852594755  - accuracy: 0.84375\n",
      "At: 1178 [==========>] Loss 0.17741710995511886  - accuracy: 0.71875\n",
      "At: 1179 [==========>] Loss 0.15099271153258803  - accuracy: 0.78125\n",
      "At: 1180 [==========>] Loss 0.2259942329363251  - accuracy: 0.65625\n",
      "At: 1181 [==========>] Loss 0.1489513383074872  - accuracy: 0.78125\n",
      "At: 1182 [==========>] Loss 0.12325266938449914  - accuracy: 0.78125\n",
      "At: 1183 [==========>] Loss 0.1835638506287987  - accuracy: 0.78125\n",
      "At: 1184 [==========>] Loss 0.18616784911573556  - accuracy: 0.75\n",
      "At: 1185 [==========>] Loss 0.12195676678733908  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.13232457680077547  - accuracy: 0.875\n",
      "At: 1187 [==========>] Loss 0.13182148737208255  - accuracy: 0.8125\n",
      "At: 1188 [==========>] Loss 0.0847014664513961  - accuracy: 0.9375\n",
      "At: 1189 [==========>] Loss 0.16147698599139843  - accuracy: 0.78125\n",
      "At: 1190 [==========>] Loss 0.11742496889033982  - accuracy: 0.84375\n",
      "At: 1191 [==========>] Loss 0.2010941673932187  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.0870515563874051  - accuracy: 0.875\n",
      "At: 1193 [==========>] Loss 0.19914952979542794  - accuracy: 0.6875\n",
      "At: 1194 [==========>] Loss 0.16080293452337285  - accuracy: 0.8125\n",
      "At: 1195 [==========>] Loss 0.14679942213355257  - accuracy: 0.84375\n",
      "At: 1196 [==========>] Loss 0.1519906692838977  - accuracy: 0.78125\n",
      "At: 1197 [==========>] Loss 0.09837432838192134  - accuracy: 0.8125\n",
      "At: 1198 [==========>] Loss 0.0988080346642719  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.22201976142440943  - accuracy: 0.625\n",
      "At: 1200 [==========>] Loss 0.11731643635978536  - accuracy: 0.84375\n",
      "At: 1201 [==========>] Loss 0.15194544939179275  - accuracy: 0.78125\n",
      "At: 1202 [==========>] Loss 0.1751975094758818  - accuracy: 0.65625\n",
      "At: 1203 [==========>] Loss 0.1731947775077022  - accuracy: 0.71875\n",
      "At: 1204 [==========>] Loss 0.08803310441684399  - accuracy: 0.90625\n",
      "At: 1205 [==========>] Loss 0.09434521653069824  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.1293498294699963  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.16424651844519472  - accuracy: 0.8125\n",
      "At: 1208 [==========>] Loss 0.12174203200721517  - accuracy: 0.84375\n",
      "At: 1209 [==========>] Loss 0.1439464028912885  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.17467000880997965  - accuracy: 0.75\n",
      "At: 1211 [==========>] Loss 0.17298620575262402  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.11461492011441862  - accuracy: 0.78125\n",
      "At: 1213 [==========>] Loss 0.21542146791960481  - accuracy: 0.59375\n",
      "At: 1214 [==========>] Loss 0.17253328452994732  - accuracy: 0.75\n",
      "At: 1215 [==========>] Loss 0.18836756976749663  - accuracy: 0.71875\n",
      "At: 1216 [==========>] Loss 0.12509464742229098  - accuracy: 0.84375\n",
      "At: 1217 [==========>] Loss 0.08820186174642902  - accuracy: 0.875\n",
      "At: 1218 [==========>] Loss 0.12759528946397958  - accuracy: 0.75\n",
      "At: 1219 [==========>] Loss 0.14897511243749925  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.18151903759949742  - accuracy: 0.71875\n",
      "At: 1221 [==========>] Loss 0.09891396806081668  - accuracy: 0.9375\n",
      "At: 1222 [==========>] Loss 0.2022986499795401  - accuracy: 0.65625\n",
      "At: 1223 [==========>] Loss 0.1107065499003407  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.1001751548146496  - accuracy: 0.875\n",
      "At: 1225 [==========>] Loss 0.12194565721831799  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.11785195237407267  - accuracy: 0.875\n",
      "At: 1227 [==========>] Loss 0.16028920579190414  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.1473099808032427  - accuracy: 0.78125\n",
      "At: 1229 [==========>] Loss 0.14768777752971052  - accuracy: 0.75\n",
      "At: 1230 [==========>] Loss 0.1885348163328695  - accuracy: 0.71875\n",
      "At: 1231 [==========>] Loss 0.17127434684638354  - accuracy: 0.8125\n",
      "At: 1232 [==========>] Loss 0.07621260705998312  - accuracy: 0.9375\n",
      "At: 1233 [==========>] Loss 0.15863856443215554  - accuracy: 0.71875\n",
      "At: 1234 [==========>] Loss 0.13274349189818763  - accuracy: 0.78125\n",
      "At: 1235 [==========>] Loss 0.09473452579943889  - accuracy: 0.875\n",
      "At: 1236 [==========>] Loss 0.14504909738644375  - accuracy: 0.8125\n",
      "At: 1237 [==========>] Loss 0.08954539054000296  - accuracy: 0.84375\n",
      "At: 1238 [==========>] Loss 0.1278407025059088  - accuracy: 0.84375\n",
      "At: 1239 [==========>] Loss 0.18114495234614247  - accuracy: 0.75\n",
      "At: 1240 [==========>] Loss 0.10891704533768384  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.13538895197507383  - accuracy: 0.78125\n",
      "At: 1242 [==========>] Loss 0.1301169798062433  - accuracy: 0.8125\n",
      "At: 1243 [==========>] Loss 0.15431609975073302  - accuracy: 0.78125\n",
      "At: 1244 [==========>] Loss 0.1762194750126795  - accuracy: 0.71875\n",
      "At: 1245 [==========>] Loss 0.1596742311094673  - accuracy: 0.75\n",
      "At: 1246 [==========>] Loss 0.07697189257711068  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.18223705787575478  - accuracy: 0.71875\n",
      "At: 1248 [==========>] Loss 0.10260644287890264  - accuracy: 0.9375\n",
      "At: 1249 [==========>] Loss 0.1336147941372234  - accuracy: 0.78125\n",
      "At: 1250 [==========>] Loss 0.1267381219624812  - accuracy: 0.875\n",
      "At: 1251 [==========>] Loss 0.14166408596810365  - accuracy: 0.75\n",
      "At: 1252 [==========>] Loss 0.08563340461645337  - accuracy: 0.875\n",
      "At: 1253 [==========>] Loss 0.11589570335282132  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.18266749592542558  - accuracy: 0.71875\n",
      "At: 1255 [==========>] Loss 0.11688287386726236  - accuracy: 0.84375\n",
      "At: 1256 [==========>] Loss 0.15745001638007705  - accuracy: 0.78125\n",
      "At: 1257 [==========>] Loss 0.12175383241772894  - accuracy: 0.84375\n",
      "At: 1258 [==========>] Loss 0.09547254359354115  - accuracy: 0.875\n",
      "At: 1259 [==========>] Loss 0.1366336567155771  - accuracy: 0.8125\n",
      "At: 1260 [==========>] Loss 0.15469939215783435  - accuracy: 0.75\n",
      "At: 1261 [==========>] Loss 0.13491286619677434  - accuracy: 0.8125\n",
      "At: 1262 [==========>] Loss 0.14072119610259592  - accuracy: 0.75\n",
      "At: 1263 [==========>] Loss 0.12058039322008055  - accuracy: 0.8125\n",
      "At: 1264 [==========>] Loss 0.09281983241482997  - accuracy: 0.875\n",
      "At: 1265 [==========>] Loss 0.1715213871363074  - accuracy: 0.6875\n",
      "At: 1266 [==========>] Loss 0.15421106576963295  - accuracy: 0.75\n",
      "At: 1267 [==========>] Loss 0.1751979628785355  - accuracy: 0.71875\n",
      "At: 1268 [==========>] Loss 0.1582046603898159  - accuracy: 0.75\n",
      "At: 1269 [==========>] Loss 0.14223793282177394  - accuracy: 0.8125\n",
      "At: 1270 [==========>] Loss 0.14478144340534738  - accuracy: 0.8125\n",
      "At: 1271 [==========>] Loss 0.15057925567026684  - accuracy: 0.78125\n",
      "At: 1272 [==========>] Loss 0.06705489865237356  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.1803395024860655  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.1459050717222753  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.11906872031113874  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.09618070890589578  - accuracy: 0.875\n",
      "At: 1277 [==========>] Loss 0.08432830678823523  - accuracy: 0.875\n",
      "At: 1278 [==========>] Loss 0.16751931739433834  - accuracy: 0.75\n",
      "At: 1279 [==========>] Loss 0.11175944990178908  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.12120947775999531  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.17589257072096803  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.17104273015576688  - accuracy: 0.75\n",
      "At: 1283 [==========>] Loss 0.1610357517139661  - accuracy: 0.78125\n",
      "At: 1284 [==========>] Loss 0.1691213862830629  - accuracy: 0.78125\n",
      "At: 1285 [==========>] Loss 0.08265203112391797  - accuracy: 0.84375\n",
      "At: 1286 [==========>] Loss 0.12672187732177637  - accuracy: 0.78125\n",
      "At: 1287 [==========>] Loss 0.10810461772402022  - accuracy: 0.90625\n",
      "At: 1288 [==========>] Loss 0.15504468873977517  - accuracy: 0.78125\n",
      "At: 1289 [==========>] Loss 0.09471622002080772  - accuracy: 0.875\n",
      "At: 1290 [==========>] Loss 0.15610717414991396  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.18404549273966114  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.10901526644113935  - accuracy: 0.84375\n",
      "At: 1293 [==========>] Loss 0.18648256379232103  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.1398037057565809  - accuracy: 0.84375\n",
      "At: 1295 [==========>] Loss 0.1686069701479845  - accuracy: 0.71875\n",
      "At: 1296 [==========>] Loss 0.15725133942031222  - accuracy: 0.8125\n",
      "At: 1297 [==========>] Loss 0.12648957337386624  - accuracy: 0.875\n",
      "At: 1298 [==========>] Loss 0.11835609677284162  - accuracy: 0.8125\n",
      "At: 1299 [==========>] Loss 0.17736852073533693  - accuracy: 0.75\n",
      "At: 1300 [==========>] Loss 0.13744518069332515  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.1370817123745928  - accuracy: 0.8125\n",
      "At: 1302 [==========>] Loss 0.09384089845148857  - accuracy: 0.90625\n",
      "At: 1303 [==========>] Loss 0.13110434276019578  - accuracy: 0.8125\n",
      "At: 1304 [==========>] Loss 0.11830416407087803  - accuracy: 0.84375\n",
      "At: 1305 [==========>] Loss 0.1562330139952175  - accuracy: 0.75\n",
      "At: 1306 [==========>] Loss 0.12110174743706432  - accuracy: 0.84375\n",
      "At: 1307 [==========>] Loss 0.15554127436845921  - accuracy: 0.8125\n",
      "At: 1308 [==========>] Loss 0.08103181132495402  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.1905424277295173  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.15864542680361352  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.1462113913102102  - accuracy: 0.8125\n",
      "At: 1312 [==========>] Loss 0.08424416919712206  - accuracy: 0.96875\n",
      "At: 1313 [==========>] Loss 0.14939699528341738  - accuracy: 0.8125\n",
      "At: 1314 [==========>] Loss 0.06858822165102421  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.16888684129145126  - accuracy: 0.71875\n",
      "At: 1316 [==========>] Loss 0.1583186823103589  - accuracy: 0.75\n",
      "At: 1317 [==========>] Loss 0.14946956463401168  - accuracy: 0.75\n",
      "At: 1318 [==========>] Loss 0.11611955043461603  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.1389646662496273  - accuracy: 0.78125\n",
      "At: 1320 [==========>] Loss 0.1573808855377527  - accuracy: 0.75\n",
      "At: 1321 [==========>] Loss 0.08847562195696049  - accuracy: 0.90625\n",
      "At: 1322 [==========>] Loss 0.17351948112899745  - accuracy: 0.78125\n",
      "At: 1323 [==========>] Loss 0.13719930731370894  - accuracy: 0.8125\n",
      "At: 1324 [==========>] Loss 0.1826477394639115  - accuracy: 0.71875\n",
      "At: 1325 [==========>] Loss 0.0895765871505482  - accuracy: 0.90625\n",
      "At: 1326 [==========>] Loss 0.07843387832139892  - accuracy: 0.9375\n",
      "At: 1327 [==========>] Loss 0.14610662746221723  - accuracy: 0.84375\n",
      "At: 1328 [==========>] Loss 0.12128997255759287  - accuracy: 0.78125\n",
      "At: 1329 [==========>] Loss 0.11082578016704689  - accuracy: 0.84375\n",
      "At: 1330 [==========>] Loss 0.14277299415699737  - accuracy: 0.8125\n",
      "At: 1331 [==========>] Loss 0.19373174430108486  - accuracy: 0.75\n",
      "At: 1332 [==========>] Loss 0.14409497054998016  - accuracy: 0.75\n",
      "At: 1333 [==========>] Loss 0.15705858441559573  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.12543053458833575  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.14244204984224165  - accuracy: 0.8125\n",
      "At: 1336 [==========>] Loss 0.10301231437414304  - accuracy: 0.8125\n",
      "At: 1337 [==========>] Loss 0.1778364248175343  - accuracy: 0.78125\n",
      "At: 1338 [==========>] Loss 0.14972866230623538  - accuracy: 0.78125\n",
      "At: 1339 [==========>] Loss 0.13466887933566016  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.142931196111471  - accuracy: 0.78125\n",
      "At: 1341 [==========>] Loss 0.10401286372709267  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.11829699571476951  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.21024449210180485  - accuracy: 0.65625\n",
      "At: 1344 [==========>] Loss 0.16472976532628164  - accuracy: 0.78125\n",
      "At: 1345 [==========>] Loss 0.0970631460080644  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.10621358679148617  - accuracy: 0.90625\n",
      "At: 1347 [==========>] Loss 0.13983710159594107  - accuracy: 0.8125\n",
      "At: 1348 [==========>] Loss 0.11128485603576121  - accuracy: 0.8125\n",
      "At: 1349 [==========>] Loss 0.15916637972359426  - accuracy: 0.71875\n",
      "At: 1350 [==========>] Loss 0.14813487921609575  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.12085284792227542  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.09851211021766779  - accuracy: 0.90625\n",
      "At: 1353 [==========>] Loss 0.1627978652365909  - accuracy: 0.75\n",
      "At: 1354 [==========>] Loss 0.1919827438961565  - accuracy: 0.6875\n",
      "At: 1355 [==========>] Loss 0.0785965245892761  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.1168025499114679  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.1100910957391146  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.14314571501213527  - accuracy: 0.75\n",
      "At: 1359 [==========>] Loss 0.10564945107105125  - accuracy: 0.84375\n",
      "At: 1360 [==========>] Loss 0.18484233986942056  - accuracy: 0.75\n",
      "At: 1361 [==========>] Loss 0.10192611628929762  - accuracy: 0.8125\n",
      "At: 1362 [==========>] Loss 0.12412215120434072  - accuracy: 0.8125\n",
      "At: 1363 [==========>] Loss 0.15600856937092486  - accuracy: 0.8125\n",
      "At: 1364 [==========>] Loss 0.1241964233257208  - accuracy: 0.84375\n",
      "At: 1365 [==========>] Loss 0.13195811592379647  - accuracy: 0.78125\n",
      "At: 1366 [==========>] Loss 0.11130858653392403  - accuracy: 0.8125\n",
      "At: 1367 [==========>] Loss 0.09261475701713133  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.17791585488330913  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.0855030044032199  - accuracy: 0.90625\n",
      "At: 1370 [==========>] Loss 0.143923234603046  - accuracy: 0.75\n",
      "At: 1371 [==========>] Loss 0.20343269323068972  - accuracy: 0.71875\n",
      "At: 1372 [==========>] Loss 0.13658870227120112  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.14822431670978306  - accuracy: 0.78125\n",
      "At: 1374 [==========>] Loss 0.1687127166447546  - accuracy: 0.71875\n",
      "At: 1375 [==========>] Loss 0.1372311078745514  - accuracy: 0.78125\n",
      "At: 1376 [==========>] Loss 0.1349052762710769  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.18484046488941563  - accuracy: 0.75\n",
      "At: 1378 [==========>] Loss 0.1385689115046009  - accuracy: 0.8125\n",
      "At: 1379 [==========>] Loss 0.18558214228591718  - accuracy: 0.6875\n",
      "At: 1380 [==========>] Loss 0.14564440855011057  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.10161649018745533  - accuracy: 0.90625\n",
      "At: 1382 [==========>] Loss 0.15207736740665934  - accuracy: 0.75\n",
      "At: 1383 [==========>] Loss 0.1271857063150243  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.11319493332207056  - accuracy: 0.8125\n",
      "At: 1385 [==========>] Loss 0.1900038123932104  - accuracy: 0.6875\n",
      "At: 1386 [==========>] Loss 0.2119467121031468  - accuracy: 0.71875\n",
      "At: 1387 [==========>] Loss 0.06805926593022693  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.15877922462408828  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.1253437523179518  - accuracy: 0.84375\n",
      "At: 1390 [==========>] Loss 0.16462031783765854  - accuracy: 0.75\n",
      "At: 1391 [==========>] Loss 0.12725011088547555  - accuracy: 0.875\n",
      "At: 1392 [==========>] Loss 0.13391225902320683  - accuracy: 0.8125\n",
      "At: 1393 [==========>] Loss 0.1592712502279412  - accuracy: 0.78125\n",
      "At: 1394 [==========>] Loss 0.09006699612562202  - accuracy: 0.90625\n",
      "At: 1395 [==========>] Loss 0.2645834823363763  - accuracy: 0.5625\n",
      "At: 1396 [==========>] Loss 0.07379789791805434  - accuracy: 0.90625\n",
      "At: 1397 [==========>] Loss 0.13786560539552556  - accuracy: 0.78125\n",
      "At: 1398 [==========>] Loss 0.11672366424368576  - accuracy: 0.8125\n",
      "At: 1399 [==========>] Loss 0.14744505183617052  - accuracy: 0.75\n",
      "At: 1400 [==========>] Loss 0.15604476846446313  - accuracy: 0.8125\n",
      "At: 1401 [==========>] Loss 0.10240375743779337  - accuracy: 0.84375\n",
      "At: 1402 [==========>] Loss 0.18829958490151844  - accuracy: 0.6875\n",
      "At: 1403 [==========>] Loss 0.14473415191062644  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.11458183783692606  - accuracy: 0.8125\n",
      "At: 1405 [==========>] Loss 0.081045334485122  - accuracy: 0.9375\n",
      "At: 1406 [==========>] Loss 0.1654195209096532  - accuracy: 0.71875\n",
      "At: 1407 [==========>] Loss 0.13812500578113568  - accuracy: 0.8125\n",
      "At: 1408 [==========>] Loss 0.18620976894398306  - accuracy: 0.6875\n",
      "At: 1409 [==========>] Loss 0.03524600014944839  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.14029920798685858  - accuracy: 0.8125\n",
      "At: 1411 [==========>] Loss 0.14772901573741737  - accuracy: 0.75\n",
      "At: 1412 [==========>] Loss 0.1438505082520958  - accuracy: 0.8125\n",
      "At: 1413 [==========>] Loss 0.1334564195986673  - accuracy: 0.78125\n",
      "At: 1414 [==========>] Loss 0.20041022745277437  - accuracy: 0.6875\n",
      "At: 1415 [==========>] Loss 0.10422017500908845  - accuracy: 0.875\n",
      "At: 1416 [==========>] Loss 0.19912952628423253  - accuracy: 0.71875\n",
      "At: 1417 [==========>] Loss 0.1467110337430581  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.15161160786208253  - accuracy: 0.75\n",
      "At: 1419 [==========>] Loss 0.1073982455151262  - accuracy: 0.84375\n",
      "At: 1420 [==========>] Loss 0.09818105107822905  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.1147260382111671  - accuracy: 0.875\n",
      "At: 1422 [==========>] Loss 0.14670595418629362  - accuracy: 0.75\n",
      "At: 1423 [==========>] Loss 0.15862424038620992  - accuracy: 0.75\n",
      "At: 1424 [==========>] Loss 0.15023348887264953  - accuracy: 0.78125\n",
      "At: 1425 [==========>] Loss 0.08543738227747777  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.14429318276091505  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.12648748314439223  - accuracy: 0.9375\n",
      "At: 1428 [==========>] Loss 0.09164568290879949  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.14431076741737395  - accuracy: 0.8125\n",
      "At: 1430 [==========>] Loss 0.0982112225095363  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.10778745140823107  - accuracy: 0.875\n",
      "At: 1432 [==========>] Loss 0.10052338899536646  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.11286022785978003  - accuracy: 0.90625\n",
      "At: 1434 [==========>] Loss 0.16694316963411976  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.13684606031868357  - accuracy: 0.84375\n",
      "At: 1436 [==========>] Loss 0.08860954417267909  - accuracy: 0.9375\n",
      "At: 1437 [==========>] Loss 0.11599858192719159  - accuracy: 0.90625\n",
      "At: 1438 [==========>] Loss 0.19027762463203615  - accuracy: 0.75\n",
      "At: 1439 [==========>] Loss 0.13366109491190797  - accuracy: 0.84375\n",
      "At: 1440 [==========>] Loss 0.11202780900074036  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.0927531399504557  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.1163862151549345  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.13315359927468667  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.1380041599632286  - accuracy: 0.78125\n",
      "At: 1445 [==========>] Loss 0.18049030142295186  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.2115645515867845  - accuracy: 0.625\n",
      "At: 1447 [==========>] Loss 0.17849897935663403  - accuracy: 0.71875\n",
      "At: 1448 [==========>] Loss 0.09719814609552413  - accuracy: 0.84375\n",
      "At: 1449 [==========>] Loss 0.14423969384404506  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.1282186200605362  - accuracy: 0.8125\n",
      "At: 1451 [==========>] Loss 0.11025432361364765  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.12250462351743269  - accuracy: 0.8125\n",
      "At: 1453 [==========>] Loss 0.06521235315666872  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.15883849162347508  - accuracy: 0.78125\n",
      "At: 1455 [==========>] Loss 0.14324988575454975  - accuracy: 0.84375\n",
      "At: 1456 [==========>] Loss 0.1031718857242083  - accuracy: 0.84375\n",
      "At: 1457 [==========>] Loss 0.12291962490264988  - accuracy: 0.8125\n",
      "At: 1458 [==========>] Loss 0.16482339472948318  - accuracy: 0.75\n",
      "At: 1459 [==========>] Loss 0.12901044717375046  - accuracy: 0.875\n",
      "At: 1460 [==========>] Loss 0.177363456807424  - accuracy: 0.75\n",
      "At: 1461 [==========>] Loss 0.13885523799262944  - accuracy: 0.78125\n",
      "At: 1462 [==========>] Loss 0.18412524169396427  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.08800921608407734  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.17553258826560322  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.13073555458699693  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.0939811848911685  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.18614184272859538  - accuracy: 0.65625\n",
      "At: 1468 [==========>] Loss 0.19873956994218345  - accuracy: 0.71875\n",
      "At: 1469 [==========>] Loss 0.1785743981293554  - accuracy: 0.75\n",
      "At: 1470 [==========>] Loss 0.14411834223606834  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.15896769058178684  - accuracy: 0.71875\n",
      "At: 1472 [==========>] Loss 0.09684482165665191  - accuracy: 0.84375\n",
      "At: 1473 [==========>] Loss 0.12662746074425968  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.19304455794457298  - accuracy: 0.6875\n",
      "At: 1475 [==========>] Loss 0.17138791279193236  - accuracy: 0.71875\n",
      "At: 1476 [==========>] Loss 0.15191471129833825  - accuracy: 0.75\n",
      "At: 1477 [==========>] Loss 0.0986756835530012  - accuracy: 0.84375\n",
      "At: 1478 [==========>] Loss 0.12249676775591398  - accuracy: 0.8125\n",
      "At: 1479 [==========>] Loss 0.15568578960385088  - accuracy: 0.8125\n",
      "At: 1480 [==========>] Loss 0.10461961966238749  - accuracy: 0.8125\n",
      "At: 1481 [==========>] Loss 0.18400600936579847  - accuracy: 0.71875\n",
      "At: 1482 [==========>] Loss 0.11483311981828939  - accuracy: 0.875\n",
      "At: 1483 [==========>] Loss 0.1970719213698483  - accuracy: 0.65625\n",
      "At: 1484 [==========>] Loss 0.1497748807790947  - accuracy: 0.75\n",
      "At: 1485 [==========>] Loss 0.17843085941714687  - accuracy: 0.75\n",
      "At: 1486 [==========>] Loss 0.09115525235588909  - accuracy: 0.84375\n",
      "At: 1487 [==========>] Loss 0.09382874479205287  - accuracy: 0.8125\n",
      "At: 1488 [==========>] Loss 0.13576319672157255  - accuracy: 0.84375\n",
      "At: 1489 [==========>] Loss 0.19784553403233668  - accuracy: 0.71875\n",
      "At: 1490 [==========>] Loss 0.10950434952952542  - accuracy: 0.78125\n",
      "At: 1491 [==========>] Loss 0.142647917869431  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.13827786243734294  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.1794017871036703  - accuracy: 0.71875\n",
      "At: 1494 [==========>] Loss 0.16235843847673542  - accuracy: 0.75\n",
      "At: 1495 [==========>] Loss 0.1418243337197483  - accuracy: 0.78125\n",
      "At: 1496 [==========>] Loss 0.10128030587505774  - accuracy: 0.875\n",
      "At: 1497 [==========>] Loss 0.15411973486288638  - accuracy: 0.78125\n",
      "At: 1498 [==========>] Loss 0.18216999734279637  - accuracy: 0.75\n",
      "At: 1499 [==========>] Loss 0.13467742397892418  - accuracy: 0.78125\n",
      "At: 1500 [==========>] Loss 0.09915833402416643  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.10443297680880446  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.13851525139652934  - accuracy: 0.78125\n",
      "At: 1503 [==========>] Loss 0.12437275483644986  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.1792643232271795  - accuracy: 0.75\n",
      "At: 1505 [==========>] Loss 0.1623449892259973  - accuracy: 0.78125\n",
      "At: 1506 [==========>] Loss 0.17216739154354643  - accuracy: 0.65625\n",
      "At: 1507 [==========>] Loss 0.14829282489457452  - accuracy: 0.78125\n",
      "At: 1508 [==========>] Loss 0.23660111174939982  - accuracy: 0.625\n",
      "At: 1509 [==========>] Loss 0.11339086858358724  - accuracy: 0.8125\n",
      "At: 1510 [==========>] Loss 0.1319080341651787  - accuracy: 0.71875\n",
      "At: 1511 [==========>] Loss 0.12295558514569908  - accuracy: 0.84375\n",
      "At: 1512 [==========>] Loss 0.0843088573942193  - accuracy: 0.9375\n",
      "At: 1513 [==========>] Loss 0.15981993772560404  - accuracy: 0.8125\n",
      "At: 1514 [==========>] Loss 0.15171243446545868  - accuracy: 0.78125\n",
      "At: 1515 [==========>] Loss 0.14647875589679543  - accuracy: 0.84375\n",
      "At: 1516 [==========>] Loss 0.15396650313460497  - accuracy: 0.8125\n",
      "At: 1517 [==========>] Loss 0.15912386780622528  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.13471005018792212  - accuracy: 0.75\n",
      "At: 1519 [==========>] Loss 0.13523992948040608  - accuracy: 0.875\n",
      "At: 1520 [==========>] Loss 0.11933076094923054  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.08849962105485762  - accuracy: 0.8125\n",
      "At: 1522 [==========>] Loss 0.2043721105050316  - accuracy: 0.71875\n",
      "At: 1523 [==========>] Loss 0.1339136569844343  - accuracy: 0.78125\n",
      "At: 1524 [==========>] Loss 0.1430000457394774  - accuracy: 0.8125\n",
      "At: 1525 [==========>] Loss 0.15554794347015227  - accuracy: 0.84375\n",
      "At: 1526 [==========>] Loss 0.1342758972567763  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.14605739060265277  - accuracy: 0.78125\n",
      "At: 1528 [==========>] Loss 0.15978380727847052  - accuracy: 0.78125\n",
      "At: 1529 [==========>] Loss 0.10585872641224485  - accuracy: 0.84375\n",
      "At: 1530 [==========>] Loss 0.07620399825795704  - accuracy: 0.9375\n",
      "At: 1531 [==========>] Loss 0.1608522969548335  - accuracy: 0.8125\n",
      "At: 1532 [==========>] Loss 0.1853851090529125  - accuracy: 0.6875\n",
      "At: 1533 [==========>] Loss 0.14693090108841583  - accuracy: 0.8125\n",
      "At: 1534 [==========>] Loss 0.1089770522363255  - accuracy: 0.90625\n",
      "At: 1535 [==========>] Loss 0.12900796366172035  - accuracy: 0.78125\n",
      "At: 1536 [==========>] Loss 0.1427813763967302  - accuracy: 0.84375\n",
      "At: 1537 [==========>] Loss 0.10895705824172969  - accuracy: 0.84375\n",
      "At: 1538 [==========>] Loss 0.15141097536428572  - accuracy: 0.75\n",
      "At: 1539 [==========>] Loss 0.11133524125964209  - accuracy: 0.84375\n",
      "At: 1540 [==========>] Loss 0.15040613722672566  - accuracy: 0.75\n",
      "At: 1541 [==========>] Loss 0.12379265720285959  - accuracy: 0.84375\n",
      "At: 1542 [==========>] Loss 0.08313201272292148  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.16576036714028922  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.15941439975628677  - accuracy: 0.75\n",
      "At: 1545 [==========>] Loss 0.21392062805549683  - accuracy: 0.75\n",
      "At: 1546 [==========>] Loss 0.13965158120034737  - accuracy: 0.8125\n",
      "At: 1547 [==========>] Loss 0.16617135650056022  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.13518473877391649  - accuracy: 0.8125\n",
      "At: 1549 [==========>] Loss 0.16002034248291963  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.09574036945560728  - accuracy: 0.84375\n",
      "At: 1551 [==========>] Loss 0.19102979066249898  - accuracy: 0.75\n",
      "At: 1552 [==========>] Loss 0.12603712150135105  - accuracy: 0.875\n",
      "At: 1553 [==========>] Loss 0.07482499491135142  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.1527776249607144  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.12619725315964941  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.1619425207568611  - accuracy: 0.75\n",
      "At: 1557 [==========>] Loss 0.09993522274703465  - accuracy: 0.84375\n",
      "At: 1558 [==========>] Loss 0.14669409685295604  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.09322622731508494  - accuracy: 0.90625\n",
      "At: 1560 [==========>] Loss 0.15574620213035695  - accuracy: 0.75\n",
      "At: 1561 [==========>] Loss 0.16438254297462782  - accuracy: 0.75\n",
      "At: 1562 [==========>] Loss 0.13684174478315914  - accuracy: 0.8125\n",
      "At: 1563 [==========>] Loss 0.1162848186993172  - accuracy: 0.875\n",
      "At: 1564 [==========>] Loss 0.12418280110375883  - accuracy: 0.8125\n",
      "At: 1565 [==========>] Loss 0.15082491020829764  - accuracy: 0.84375\n",
      "At: 1566 [==========>] Loss 0.14571592964097835  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.15467464274653117  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.11502778842355701  - accuracy: 0.78125\n",
      "At: 1569 [==========>] Loss 0.10292992095215692  - accuracy: 0.90625\n",
      "At: 1570 [==========>] Loss 0.1249046116677484  - accuracy: 0.8125\n",
      "At: 1571 [==========>] Loss 0.16055462749359042  - accuracy: 0.75\n",
      "At: 1572 [==========>] Loss 0.12970103335008976  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.05038790897192922  - accuracy: 0.9375\n",
      "At: 1574 [==========>] Loss 0.16512548544246275  - accuracy: 0.78125\n",
      "At: 1575 [==========>] Loss 0.12932989757210495  - accuracy: 0.8125\n",
      "At: 1576 [==========>] Loss 0.13912771227068244  - accuracy: 0.78125\n",
      "At: 1577 [==========>] Loss 0.08536968918505804  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.06704324129803721  - accuracy: 0.96875\n",
      "At: 1579 [==========>] Loss 0.11006028741306503  - accuracy: 0.84375\n",
      "At: 1580 [==========>] Loss 0.1396688032513969  - accuracy: 0.84375\n",
      "At: 1581 [==========>] Loss 0.10836572058341887  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.16927977851177442  - accuracy: 0.8125\n",
      "At: 1583 [==========>] Loss 0.08947622953829068  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.12985598940178544  - accuracy: 0.90625\n",
      "At: 1585 [==========>] Loss 0.10715530177276769  - accuracy: 0.875\n",
      "At: 1586 [==========>] Loss 0.16921602887947823  - accuracy: 0.78125\n",
      "At: 1587 [==========>] Loss 0.11582743697801118  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.13784245703197878  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.17134823698166254  - accuracy: 0.75\n",
      "At: 1590 [==========>] Loss 0.1267792617477568  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.11321695051446205  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.08082821019868063  - accuracy: 0.90625\n",
      "At: 1593 [==========>] Loss 0.1629271652813676  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.12953004351041947  - accuracy: 0.84375\n",
      "At: 1595 [==========>] Loss 0.16164198213690908  - accuracy: 0.8125\n",
      "At: 1596 [==========>] Loss 0.17488265100911143  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.16613473690532632  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.18216383133420952  - accuracy: 0.75\n",
      "At: 1599 [==========>] Loss 0.22668715863405464  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.16402547535614106  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.10387700124800284  - accuracy: 0.875\n",
      "At: 1602 [==========>] Loss 0.13433271910914546  - accuracy: 0.78125\n",
      "At: 1603 [==========>] Loss 0.17474004951416583  - accuracy: 0.6875\n",
      "At: 1604 [==========>] Loss 0.25477138618890643  - accuracy: 0.5625\n",
      "At: 1605 [==========>] Loss 0.09789092346881534  - accuracy: 0.84375\n",
      "At: 1606 [==========>] Loss 0.13252703011839434  - accuracy: 0.875\n",
      "At: 1607 [==========>] Loss 0.21763683972596592  - accuracy: 0.65625\n",
      "At: 1608 [==========>] Loss 0.13546601254954643  - accuracy: 0.90625\n",
      "At: 1609 [==========>] Loss 0.16960854336264775  - accuracy: 0.71875\n",
      "At: 1610 [==========>] Loss 0.1865439315876337  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.08177644032271524  - accuracy: 0.9375\n",
      "At: 1612 [==========>] Loss 0.08615442344274685  - accuracy: 0.875\n",
      "At: 1613 [==========>] Loss 0.15779476046682334  - accuracy: 0.84375\n",
      "At: 1614 [==========>] Loss 0.15388937029715946  - accuracy: 0.75\n",
      "At: 1615 [==========>] Loss 0.0911321645468377  - accuracy: 0.90625\n",
      "At: 1616 [==========>] Loss 0.13722133793048366  - accuracy: 0.78125\n",
      "At: 1617 [==========>] Loss 0.09758486670535743  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.1270881797583529  - accuracy: 0.8125\n",
      "At: 1619 [==========>] Loss 0.2123478137687311  - accuracy: 0.59375\n",
      "At: 1620 [==========>] Loss 0.0956708637922461  - accuracy: 0.90625\n",
      "At: 1621 [==========>] Loss 0.09588744288692395  - accuracy: 0.90625\n",
      "At: 1622 [==========>] Loss 0.16914795310106395  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.08891852060175673  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.14230722037546323  - accuracy: 0.75\n",
      "At: 1625 [==========>] Loss 0.15762016296127435  - accuracy: 0.78125\n",
      "At: 1626 [==========>] Loss 0.12001901448241332  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.13240087238279585  - accuracy: 0.8125\n",
      "At: 1628 [==========>] Loss 0.15996516852088613  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.15500353157346675  - accuracy: 0.78125\n",
      "At: 1630 [==========>] Loss 0.10630354043781237  - accuracy: 0.84375\n",
      "At: 1631 [==========>] Loss 0.10430839479802573  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.09160672184230632  - accuracy: 0.84375\n",
      "At: 1633 [==========>] Loss 0.08241077590187632  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.1410085726337092  - accuracy: 0.8125\n",
      "At: 1635 [==========>] Loss 0.2154593105495506  - accuracy: 0.75\n",
      "At: 1636 [==========>] Loss 0.1307676756518436  - accuracy: 0.78125\n",
      "At: 1637 [==========>] Loss 0.08591867869055586  - accuracy: 0.9375\n",
      "At: 1638 [==========>] Loss 0.1760814002731979  - accuracy: 0.75\n",
      "At: 1639 [==========>] Loss 0.150024460984072  - accuracy: 0.75\n",
      "At: 1640 [==========>] Loss 0.15331863137853868  - accuracy: 0.78125\n",
      "At: 1641 [==========>] Loss 0.11626739001843295  - accuracy: 0.84375\n",
      "At: 1642 [==========>] Loss 0.1177638765021238  - accuracy: 0.90625\n",
      "At: 1643 [==========>] Loss 0.13359032043361754  - accuracy: 0.75\n",
      "At: 1644 [==========>] Loss 0.12298725242786174  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.06860541109341062  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.16862624234589726  - accuracy: 0.71875\n",
      "At: 1647 [==========>] Loss 0.16477124756169786  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.1480135842915699  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.10024382440588396  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.09791634227726193  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.1326643096411052  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.09353394921050957  - accuracy: 0.875\n",
      "At: 1653 [==========>] Loss 0.09190641163719376  - accuracy: 0.90625\n",
      "At: 1654 [==========>] Loss 0.08793078869039418  - accuracy: 0.90625\n",
      "At: 1655 [==========>] Loss 0.20245749069760863  - accuracy: 0.75\n",
      "At: 1656 [==========>] Loss 0.12682201991927125  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.13396302217024297  - accuracy: 0.75\n",
      "At: 1658 [==========>] Loss 0.20106323261940717  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.09920881213906671  - accuracy: 0.9375\n",
      "At: 1660 [==========>] Loss 0.07545675469170146  - accuracy: 0.9375\n",
      "At: 1661 [==========>] Loss 0.11257191796833146  - accuracy: 0.8125\n",
      "At: 1662 [==========>] Loss 0.1198094889912899  - accuracy: 0.8125\n",
      "At: 1663 [==========>] Loss 0.1813218992052984  - accuracy: 0.6875\n",
      "At: 1664 [==========>] Loss 0.11331845425056984  - accuracy: 0.8125\n",
      "At: 1665 [==========>] Loss 0.1141445795727922  - accuracy: 0.8125\n",
      "At: 1666 [==========>] Loss 0.09027356055277207  - accuracy: 0.875\n",
      "At: 1667 [==========>] Loss 0.14637426125020275  - accuracy: 0.8125\n",
      "At: 1668 [==========>] Loss 0.13690545823277034  - accuracy: 0.84375\n",
      "At: 1669 [==========>] Loss 0.1381667372559493  - accuracy: 0.78125\n",
      "At: 1670 [==========>] Loss 0.10923990790306748  - accuracy: 0.84375\n",
      "At: 1671 [==========>] Loss 0.12292604518277965  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.14026845299523497  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.10529759134112321  - accuracy: 0.84375\n",
      "At: 1674 [==========>] Loss 0.17357420950894623  - accuracy: 0.78125\n",
      "At: 1675 [==========>] Loss 0.20126662859934683  - accuracy: 0.75\n",
      "At: 1676 [==========>] Loss 0.22247353179467644  - accuracy: 0.625\n",
      "At: 1677 [==========>] Loss 0.11702069928422985  - accuracy: 0.875\n",
      "At: 1678 [==========>] Loss 0.15612190354390526  - accuracy: 0.8125\n",
      "At: 1679 [==========>] Loss 0.11857688288162777  - accuracy: 0.875\n",
      "At: 1680 [==========>] Loss 0.1641677942693734  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.14946873848180206  - accuracy: 0.84375\n",
      "At: 1682 [==========>] Loss 0.0820013351575369  - accuracy: 0.84375\n",
      "At: 1683 [==========>] Loss 0.1685302006786445  - accuracy: 0.8125\n",
      "At: 1684 [==========>] Loss 0.1437909845401345  - accuracy: 0.78125\n",
      "At: 1685 [==========>] Loss 0.11698997280659801  - accuracy: 0.875\n",
      "At: 1686 [==========>] Loss 0.13134867299562475  - accuracy: 0.84375\n",
      "At: 1687 [==========>] Loss 0.1479239246000152  - accuracy: 0.75\n",
      "At: 1688 [==========>] Loss 0.06364254641518584  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.12976211786023964  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.13402609888068928  - accuracy: 0.8125\n",
      "At: 1691 [==========>] Loss 0.10367424120720593  - accuracy: 0.84375\n",
      "At: 1692 [==========>] Loss 0.2057234009133126  - accuracy: 0.71875\n",
      "At: 1693 [==========>] Loss 0.09617717029251771  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.09189653559812105  - accuracy: 0.90625\n",
      "At: 1695 [==========>] Loss 0.1386944355976355  - accuracy: 0.75\n",
      "At: 1696 [==========>] Loss 0.13858010010708807  - accuracy: 0.8125\n",
      "At: 1697 [==========>] Loss 0.12205128293412679  - accuracy: 0.84375\n",
      "At: 1698 [==========>] Loss 0.09548122650249205  - accuracy: 0.875\n",
      "At: 1699 [==========>] Loss 0.11488487861482073  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.14866856209092705  - accuracy: 0.78125\n",
      "At: 1701 [==========>] Loss 0.10351435178594973  - accuracy: 0.8125\n",
      "At: 1702 [==========>] Loss 0.10774465227479887  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.19869815725444964  - accuracy: 0.65625\n",
      "At: 1704 [==========>] Loss 0.09266967175545873  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.12115568067961463  - accuracy: 0.78125\n",
      "At: 1706 [==========>] Loss 0.171179619021801  - accuracy: 0.75\n",
      "At: 1707 [==========>] Loss 0.18649941025222058  - accuracy: 0.75\n",
      "At: 1708 [==========>] Loss 0.0990296380523275  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.197213855100127  - accuracy: 0.6875\n",
      "At: 1710 [==========>] Loss 0.18021090127293957  - accuracy: 0.78125\n",
      "At: 1711 [==========>] Loss 0.09301232448095745  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.1401747938975074  - accuracy: 0.71875\n",
      "At: 1713 [==========>] Loss 0.09575585831010064  - accuracy: 0.875\n",
      "At: 1714 [==========>] Loss 0.18643240713698142  - accuracy: 0.71875\n",
      "At: 1715 [==========>] Loss 0.13442339064342607  - accuracy: 0.78125\n",
      "At: 1716 [==========>] Loss 0.06296493229791382  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.11431383323775676  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.15132499507163494  - accuracy: 0.8125\n",
      "At: 1719 [==========>] Loss 0.1371304887237615  - accuracy: 0.75\n",
      "At: 1720 [==========>] Loss 0.0629737654059342  - accuracy: 0.90625\n",
      "At: 1721 [==========>] Loss 0.1887626713054224  - accuracy: 0.6875\n",
      "At: 1722 [==========>] Loss 0.04975607770096012  - accuracy: 1.0\n",
      "At: 1723 [==========>] Loss 0.2340647222767412  - accuracy: 0.65625\n",
      "At: 1724 [==========>] Loss 0.0915337177329428  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.15535637187116275  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.13724442830508887  - accuracy: 0.75\n",
      "At: 1727 [==========>] Loss 0.13522274991713235  - accuracy: 0.78125\n",
      "At: 1728 [==========>] Loss 0.11472066447078072  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.20458310557161885  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.17335482269597016  - accuracy: 0.75\n",
      "At: 1731 [==========>] Loss 0.14682745671673714  - accuracy: 0.75\n",
      "At: 1732 [==========>] Loss 0.09527852458848557  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.16883363782937666  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.11455894388813209  - accuracy: 0.8125\n",
      "At: 1735 [==========>] Loss 0.1646700231154496  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.14734067039635854  - accuracy: 0.78125\n",
      "At: 1737 [==========>] Loss 0.16980415463316612  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.1315870192352777  - accuracy: 0.78125\n",
      "At: 1739 [==========>] Loss 0.1398225318444713  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.15857569283939177  - accuracy: 0.75\n",
      "At: 1741 [==========>] Loss 0.14921708097836514  - accuracy: 0.8125\n",
      "At: 1742 [==========>] Loss 0.05870868833058682  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.15179960167909262  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.1116095448571569  - accuracy: 0.875\n",
      "At: 1745 [==========>] Loss 0.12841066699021386  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.15300033606941077  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.11522600479983486  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.11887932162603004  - accuracy: 0.84375\n",
      "At: 1749 [==========>] Loss 0.10599233621701008  - accuracy: 0.84375\n",
      "At: 1750 [==========>] Loss 0.07823547905254971  - accuracy: 0.90625\n",
      "At: 1751 [==========>] Loss 0.18169233641665664  - accuracy: 0.71875\n",
      "At: 1752 [==========>] Loss 0.1159419097957088  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.10866619515324734  - accuracy: 0.875\n",
      "At: 1754 [==========>] Loss 0.11888059924322116  - accuracy: 0.84375\n",
      "At: 1755 [==========>] Loss 0.08025650403465155  - accuracy: 0.9375\n",
      "At: 1756 [==========>] Loss 0.15599277111455134  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.19431600204031296  - accuracy: 0.6875\n",
      "At: 1758 [==========>] Loss 0.08606017604898067  - accuracy: 0.90625\n",
      "At: 1759 [==========>] Loss 0.11585526939032137  - accuracy: 0.875\n",
      "At: 1760 [==========>] Loss 0.08599242116632463  - accuracy: 0.875\n",
      "At: 1761 [==========>] Loss 0.1312882241159133  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.1627483345865703  - accuracy: 0.78125\n",
      "At: 1763 [==========>] Loss 0.11402625951359042  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.10076880176503648  - accuracy: 0.875\n",
      "At: 1765 [==========>] Loss 0.16018993890055427  - accuracy: 0.75\n",
      "At: 1766 [==========>] Loss 0.11507095387457021  - accuracy: 0.8125\n",
      "At: 1767 [==========>] Loss 0.10913355384372367  - accuracy: 0.84375\n",
      "At: 1768 [==========>] Loss 0.11845607448745592  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.07612793928826195  - accuracy: 0.90625\n",
      "At: 1770 [==========>] Loss 0.08169491512524005  - accuracy: 0.84375\n",
      "At: 1771 [==========>] Loss 0.1417391540117445  - accuracy: 0.8125\n",
      "At: 1772 [==========>] Loss 0.14306128354027986  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.08845347106258972  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.16443793580375068  - accuracy: 0.84375\n",
      "At: 1775 [==========>] Loss 0.1001071939685548  - accuracy: 0.875\n",
      "At: 1776 [==========>] Loss 0.12150094723167723  - accuracy: 0.84375\n",
      "At: 1777 [==========>] Loss 0.12783372543151636  - accuracy: 0.84375\n",
      "At: 1778 [==========>] Loss 0.11191921495544052  - accuracy: 0.78125\n",
      "At: 1779 [==========>] Loss 0.10669958130046199  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.13122997384341145  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.19972528807242557  - accuracy: 0.71875\n",
      "At: 1782 [==========>] Loss 0.12513581513913818  - accuracy: 0.78125\n",
      "At: 1783 [==========>] Loss 0.11299450748853582  - accuracy: 0.8125\n",
      "At: 1784 [==========>] Loss 0.0836709400713809  - accuracy: 0.9375\n",
      "At: 1785 [==========>] Loss 0.09729203536441272  - accuracy: 0.875\n",
      "At: 1786 [==========>] Loss 0.13654537548229595  - accuracy: 0.8125\n",
      "At: 1787 [==========>] Loss 0.13154786958232384  - accuracy: 0.75\n",
      "At: 1788 [==========>] Loss 0.0984831686379066  - accuracy: 0.8125\n",
      "At: 1789 [==========>] Loss 0.10406239665845282  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.18161917432482705  - accuracy: 0.71875\n",
      "At: 1791 [==========>] Loss 0.06913889833024245  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.12031888751602926  - accuracy: 0.90625\n",
      "At: 1793 [==========>] Loss 0.08743177169424872  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.20741767522346805  - accuracy: 0.71875\n",
      "At: 1795 [==========>] Loss 0.10029111449720957  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.14680410763839508  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.15083495146985104  - accuracy: 0.78125\n",
      "At: 1798 [==========>] Loss 0.14005076502233893  - accuracy: 0.8125\n",
      "At: 1799 [==========>] Loss 0.08344928436884158  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.1417284845546174  - accuracy: 0.78125\n",
      "At: 1801 [==========>] Loss 0.20323294658073504  - accuracy: 0.6875\n",
      "At: 1802 [==========>] Loss 0.12404024095286596  - accuracy: 0.875\n",
      "At: 1803 [==========>] Loss 0.17322502629449832  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.14405538829100445  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.05447666054423062  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.17606300552917967  - accuracy: 0.78125\n",
      "At: 1807 [==========>] Loss 0.18909513605841777  - accuracy: 0.71875\n",
      "At: 1808 [==========>] Loss 0.1721517052889885  - accuracy: 0.75\n",
      "At: 1809 [==========>] Loss 0.09384670126741615  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.11546111534611347  - accuracy: 0.84375\n",
      "At: 1811 [==========>] Loss 0.12744551266505447  - accuracy: 0.8125\n",
      "At: 1812 [==========>] Loss 0.09520272173766892  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.13365591497404342  - accuracy: 0.8125\n",
      "At: 1814 [==========>] Loss 0.12300237639269275  - accuracy: 0.78125\n",
      "At: 1815 [==========>] Loss 0.18318448593925893  - accuracy: 0.6875\n",
      "At: 1816 [==========>] Loss 0.04398179530350367  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.1454621897144721  - accuracy: 0.75\n",
      "At: 1818 [==========>] Loss 0.13333161835630114  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.17167215194139085  - accuracy: 0.75\n",
      "At: 1820 [==========>] Loss 0.11287874814989823  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.12319910184758052  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.16139083384350733  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.17936693006251386  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.17355879883947795  - accuracy: 0.75\n",
      "At: 1825 [==========>] Loss 0.1232192701828637  - accuracy: 0.75\n",
      "At: 1826 [==========>] Loss 0.07689234913721163  - accuracy: 0.90625\n",
      "At: 1827 [==========>] Loss 0.11897088317012301  - accuracy: 0.875\n",
      "At: 1828 [==========>] Loss 0.16580816294854883  - accuracy: 0.78125\n",
      "At: 1829 [==========>] Loss 0.15700542464147044  - accuracy: 0.71875\n",
      "At: 1830 [==========>] Loss 0.15571682032900885  - accuracy: 0.75\n",
      "At: 1831 [==========>] Loss 0.12406744862372851  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.15291658291217963  - accuracy: 0.6875\n",
      "At: 1833 [==========>] Loss 0.13260749575109  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.0688675660235504  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.1590156606146848  - accuracy: 0.8125\n",
      "At: 1836 [==========>] Loss 0.12413633454093198  - accuracy: 0.8125\n",
      "At: 1837 [==========>] Loss 0.057563909213105786  - accuracy: 0.90625\n",
      "At: 1838 [==========>] Loss 0.11192174168435537  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.09603419932663106  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.13096984445370108  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.12511998469343322  - accuracy: 0.84375\n",
      "At: 1842 [==========>] Loss 0.12237600201878474  - accuracy: 0.84375\n",
      "At: 1843 [==========>] Loss 0.13481666212132715  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.11840774473727915  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.19512479111515776  - accuracy: 0.75\n",
      "At: 1846 [==========>] Loss 0.15680254935273838  - accuracy: 0.78125\n",
      "At: 1847 [==========>] Loss 0.0881625382562497  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.07173118164098441  - accuracy: 0.90625\n",
      "At: 1849 [==========>] Loss 0.18431017773483038  - accuracy: 0.8125\n",
      "At: 1850 [==========>] Loss 0.054538386422302285  - accuracy: 0.9375\n",
      "At: 1851 [==========>] Loss 0.2034989520613156  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.07528041182839418  - accuracy: 0.9375\n",
      "At: 1853 [==========>] Loss 0.1368779265073588  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.13001898667951345  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.2075674097067236  - accuracy: 0.71875\n",
      "At: 1856 [==========>] Loss 0.13068914642028373  - accuracy: 0.8125\n",
      "At: 1857 [==========>] Loss 0.19086124617442302  - accuracy: 0.75\n",
      "At: 1858 [==========>] Loss 0.11362293395003795  - accuracy: 0.90625\n",
      "At: 1859 [==========>] Loss 0.1404790040540923  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.15653611573080423  - accuracy: 0.8125\n",
      "At: 1861 [==========>] Loss 0.1099783142166548  - accuracy: 0.84375\n",
      "At: 1862 [==========>] Loss 0.1737847321253639  - accuracy: 0.78125\n",
      "At: 1863 [==========>] Loss 0.14177466755934032  - accuracy: 0.78125\n",
      "At: 1864 [==========>] Loss 0.13081645980338297  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.09414014331928285  - accuracy: 0.90625\n",
      "At: 1866 [==========>] Loss 0.18284807218757637  - accuracy: 0.71875\n",
      "At: 1867 [==========>] Loss 0.12858681323649246  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.15444541954409363  - accuracy: 0.8125\n",
      "At: 1869 [==========>] Loss 0.14658858660975066  - accuracy: 0.71875\n",
      "At: 1870 [==========>] Loss 0.10300118963977087  - accuracy: 0.90625\n",
      "At: 1871 [==========>] Loss 0.13175329955621448  - accuracy: 0.84375\n",
      "At: 1872 [==========>] Loss 0.16858346598834734  - accuracy: 0.71875\n",
      "At: 1873 [==========>] Loss 0.10468481664240614  - accuracy: 0.875\n",
      "At: 1874 [==========>] Loss 0.15641332277005812  - accuracy: 0.8125\n",
      "At: 1875 [==========>] Loss 0.09034576616637442  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.19608341008914776  - accuracy: 0.75\n",
      "At: 1877 [==========>] Loss 0.1323010329129912  - accuracy: 0.78125\n",
      "At: 1878 [==========>] Loss 0.12153257632578673  - accuracy: 0.8125\n",
      "At: 1879 [==========>] Loss 0.1408819138890874  - accuracy: 0.75\n",
      "At: 1880 [==========>] Loss 0.09344936589809855  - accuracy: 0.90625\n",
      "At: 1881 [==========>] Loss 0.08357554357657933  - accuracy: 0.9375\n",
      "At: 1882 [==========>] Loss 0.1439160138107699  - accuracy: 0.8125\n",
      "At: 1883 [==========>] Loss 0.16477685406427964  - accuracy: 0.75\n",
      "At: 1884 [==========>] Loss 0.104429730377711  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.10356742170829177  - accuracy: 0.875\n",
      "At: 1886 [==========>] Loss 0.15277818350991978  - accuracy: 0.71875\n",
      "At: 1887 [==========>] Loss 0.07875990021789586  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.14240238978055134  - accuracy: 0.75\n",
      "At: 1889 [==========>] Loss 0.12397893207001162  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.16867615784644552  - accuracy: 0.75\n",
      "At: 1891 [==========>] Loss 0.09552074267064106  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.10015406378784658  - accuracy: 0.875\n",
      "At: 1893 [==========>] Loss 0.11583043870958133  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.11942513715240738  - accuracy: 0.875\n",
      "At: 1895 [==========>] Loss 0.08404068786987708  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.12274732609389315  - accuracy: 0.875\n",
      "At: 1897 [==========>] Loss 0.07691334571825797  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.12937298328791702  - accuracy: 0.78125\n",
      "At: 1899 [==========>] Loss 0.11083368455586617  - accuracy: 0.84375\n",
      "At: 1900 [==========>] Loss 0.14727815279640444  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.13393126914822157  - accuracy: 0.84375\n",
      "At: 1902 [==========>] Loss 0.15092821283722568  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.1215558342469644  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.06334372268581197  - accuracy: 0.90625\n",
      "At: 1905 [==========>] Loss 0.12424702278665734  - accuracy: 0.8125\n",
      "At: 1906 [==========>] Loss 0.10530925690521342  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.08055384876075145  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.13367951304819845  - accuracy: 0.78125\n",
      "At: 1909 [==========>] Loss 0.10918014317204243  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.06930716071952041  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.13304846043647559  - accuracy: 0.8125\n",
      "At: 1912 [==========>] Loss 0.12755383600500608  - accuracy: 0.8125\n",
      "At: 1913 [==========>] Loss 0.17426984343045251  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.12135888714091  - accuracy: 0.84375\n",
      "At: 1915 [==========>] Loss 0.11955669194965769  - accuracy: 0.84375\n",
      "At: 1916 [==========>] Loss 0.14885289861567308  - accuracy: 0.84375\n",
      "At: 1917 [==========>] Loss 0.1934804488723127  - accuracy: 0.71875\n",
      "At: 1918 [==========>] Loss 0.16615012160915843  - accuracy: 0.78125\n",
      "At: 1919 [==========>] Loss 0.11773268503077594  - accuracy: 0.84375\n",
      "At: 1920 [==========>] Loss 0.12019879772773526  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.1350998065972423  - accuracy: 0.84375\n",
      "At: 1922 [==========>] Loss 0.11786544961157897  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.17189675184973802  - accuracy: 0.71875\n",
      "At: 1924 [==========>] Loss 0.13695189666237506  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.14138070267091746  - accuracy: 0.8125\n",
      "At: 1926 [==========>] Loss 0.09078617946406829  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.10912076142331434  - accuracy: 0.84375\n",
      "At: 1928 [==========>] Loss 0.14333870134546042  - accuracy: 0.78125\n",
      "At: 1929 [==========>] Loss 0.16846721177564838  - accuracy: 0.8125\n",
      "At: 1930 [==========>] Loss 0.1801365474223389  - accuracy: 0.6875\n",
      "At: 1931 [==========>] Loss 0.09816658398678907  - accuracy: 0.90625\n",
      "At: 1932 [==========>] Loss 0.14914199585339324  - accuracy: 0.84375\n",
      "At: 1933 [==========>] Loss 0.10539235173195653  - accuracy: 0.84375\n",
      "At: 1934 [==========>] Loss 0.1580556391590957  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.1451222722999523  - accuracy: 0.8125\n",
      "At: 1936 [==========>] Loss 0.1056357263260282  - accuracy: 0.90625\n",
      "At: 1937 [==========>] Loss 0.13474447988237176  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.1698480080509488  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.09678470846962295  - accuracy: 0.90625\n",
      "At: 1940 [==========>] Loss 0.1164840765443721  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.13713562633887746  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.16410093358552452  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.1537091696268149  - accuracy: 0.8125\n",
      "At: 1944 [==========>] Loss 0.11281715969793706  - accuracy: 0.875\n",
      "At: 1945 [==========>] Loss 0.16808130049801812  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.1097546098175996  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.12055597461697785  - accuracy: 0.8125\n",
      "At: 1948 [==========>] Loss 0.11225792948033758  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.08587921911730362  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.15559422969490894  - accuracy: 0.75\n",
      "At: 1951 [==========>] Loss 0.15355759385146558  - accuracy: 0.8125\n",
      "At: 1952 [==========>] Loss 0.0797859087832976  - accuracy: 0.875\n",
      "At: 1953 [==========>] Loss 0.07919921690936921  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.21945010871530507  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.07505070939511574  - accuracy: 0.875\n",
      "At: 1956 [==========>] Loss 0.11958901754984269  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.10176936772038145  - accuracy: 0.84375\n",
      "At: 1958 [==========>] Loss 0.09785394062772369  - accuracy: 0.875\n",
      "At: 1959 [==========>] Loss 0.1292575084496792  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.06270998865887809  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.18000247807355002  - accuracy: 0.6875\n",
      "At: 1962 [==========>] Loss 0.20774427358813854  - accuracy: 0.75\n",
      "At: 1963 [==========>] Loss 0.11327345918051307  - accuracy: 0.84375\n",
      "At: 1964 [==========>] Loss 0.18869384974037803  - accuracy: 0.71875\n",
      "At: 1965 [==========>] Loss 0.13757663948992116  - accuracy: 0.84375\n",
      "At: 1966 [==========>] Loss 0.11403780473166548  - accuracy: 0.84375\n",
      "At: 1967 [==========>] Loss 0.12910647046982637  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.2097162515660412  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.14562592659757867  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.09837959649096895  - accuracy: 0.84375\n",
      "At: 1971 [==========>] Loss 0.20087869914561157  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.0802944940727054  - accuracy: 0.90625\n",
      "At: 1973 [==========>] Loss 0.13976871127220758  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.12526592299043238  - accuracy: 0.78125\n",
      "At: 1975 [==========>] Loss 0.15095717497737926  - accuracy: 0.78125\n",
      "At: 1976 [==========>] Loss 0.08350278120254472  - accuracy: 0.875\n",
      "At: 1977 [==========>] Loss 0.09086916527226825  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.1275430173279463  - accuracy: 0.78125\n",
      "At: 1979 [==========>] Loss 0.12838740276730104  - accuracy: 0.8125\n",
      "At: 1980 [==========>] Loss 0.1265809853775643  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.1586488352885397  - accuracy: 0.71875\n",
      "At: 1982 [==========>] Loss 0.09385986748792176  - accuracy: 0.90625\n",
      "At: 1983 [==========>] Loss 0.13271711506260392  - accuracy: 0.78125\n",
      "At: 1984 [==========>] Loss 0.1004453512959285  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.12244478941079115  - accuracy: 0.875\n",
      "At: 1986 [==========>] Loss 0.1813564609563263  - accuracy: 0.8125\n",
      "At: 1987 [==========>] Loss 0.10640838049469681  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.11938995626130768  - accuracy: 0.875\n",
      "At: 1989 [==========>] Loss 0.1090634389353109  - accuracy: 0.875\n",
      "At: 1990 [==========>] Loss 0.14170027189965229  - accuracy: 0.78125\n",
      "At: 1991 [==========>] Loss 0.14118925686591732  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.09995112857720193  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.15060211005696805  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.12187904331300099  - accuracy: 0.875\n",
      "At: 1995 [==========>] Loss 0.17147216600873408  - accuracy: 0.75\n",
      "At: 1996 [==========>] Loss 0.13092751801960834  - accuracy: 0.78125\n",
      "At: 1997 [==========>] Loss 0.18814397193550336  - accuracy: 0.71875\n",
      "At: 1998 [==========>] Loss 0.16196461545463192  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.11447496714273737  - accuracy: 0.875\n",
      "At: 2000 [==========>] Loss 0.14707600789433214  - accuracy: 0.75\n",
      "At: 2001 [==========>] Loss 0.0974513798524975  - accuracy: 0.8125\n",
      "At: 2002 [==========>] Loss 0.08145425374853835  - accuracy: 0.90625\n",
      "At: 2003 [==========>] Loss 0.15137253310649443  - accuracy: 0.75\n",
      "At: 2004 [==========>] Loss 0.1586637723546263  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.14042008999116654  - accuracy: 0.71875\n",
      "At: 2006 [==========>] Loss 0.1335078569515044  - accuracy: 0.78125\n",
      "At: 2007 [==========>] Loss 0.12715614427589844  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.12087289159484088  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.15652503961801423  - accuracy: 0.8125\n",
      "At: 2010 [==========>] Loss 0.12119916537292438  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.12300272634366076  - accuracy: 0.8125\n",
      "At: 2012 [==========>] Loss 0.10552864324510088  - accuracy: 0.84375\n",
      "At: 2013 [==========>] Loss 0.10240887761124487  - accuracy: 0.875\n",
      "At: 2014 [==========>] Loss 0.20680970776287802  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.05995207109344494  - accuracy: 0.96875\n",
      "At: 2016 [==========>] Loss 0.12670615206924696  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.09984845496812494  - accuracy: 0.84375\n",
      "At: 2018 [==========>] Loss 0.10542495529497264  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.13004451946053358  - accuracy: 0.78125\n",
      "At: 2020 [==========>] Loss 0.08667806353085386  - accuracy: 0.875\n",
      "At: 2021 [==========>] Loss 0.11462027380062248  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.12843043004992583  - accuracy: 0.78125\n",
      "At: 2023 [==========>] Loss 0.07355369451632972  - accuracy: 0.90625\n",
      "At: 2024 [==========>] Loss 0.1313319398568335  - accuracy: 0.78125\n",
      "At: 2025 [==========>] Loss 0.1371194285545217  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.10654802775397072  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.1589116831093444  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.11652297477189272  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.13284718937499163  - accuracy: 0.8125\n",
      "At: 2030 [==========>] Loss 0.15628835275258468  - accuracy: 0.78125\n",
      "At: 2031 [==========>] Loss 0.1342062002915654  - accuracy: 0.75\n",
      "At: 2032 [==========>] Loss 0.147933588773916  - accuracy: 0.71875\n",
      "At: 2033 [==========>] Loss 0.15333330245653864  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.23484127423170964  - accuracy: 0.625\n",
      "At: 2035 [==========>] Loss 0.1265869195110848  - accuracy: 0.8125\n",
      "At: 2036 [==========>] Loss 0.13291368683399185  - accuracy: 0.8125\n",
      "At: 2037 [==========>] Loss 0.1430841314878923  - accuracy: 0.8125\n",
      "At: 2038 [==========>] Loss 0.11514058830973699  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.09694416231093972  - accuracy: 0.875\n",
      "At: 2040 [==========>] Loss 0.11355744893690903  - accuracy: 0.8125\n",
      "At: 2041 [==========>] Loss 0.05509478412674891  - accuracy: 0.9375\n",
      "At: 2042 [==========>] Loss 0.10783395162653683  - accuracy: 0.84375\n",
      "At: 2043 [==========>] Loss 0.12919234294785023  - accuracy: 0.84375\n",
      "At: 2044 [==========>] Loss 0.09885822775362074  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.23117170195990494  - accuracy: 0.59375\n",
      "At: 2046 [==========>] Loss 0.06308106293290872  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.08826083497377665  - accuracy: 0.90625\n",
      "At: 2048 [==========>] Loss 0.10502572991845806  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.16109768895207321  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.16351378096386926  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.1769368288593764  - accuracy: 0.8125\n",
      "At: 2052 [==========>] Loss 0.07184984375130363  - accuracy: 0.9375\n",
      "At: 2053 [==========>] Loss 0.13328156156084317  - accuracy: 0.75\n",
      "At: 2054 [==========>] Loss 0.12272833840167235  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.08562303143875463  - accuracy: 0.9375\n",
      "At: 2056 [==========>] Loss 0.12427194595972027  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.13651751216084784  - accuracy: 0.84375\n",
      "At: 2058 [==========>] Loss 0.15454913623713148  - accuracy: 0.75\n",
      "At: 2059 [==========>] Loss 0.22083284737940964  - accuracy: 0.6875\n",
      "At: 2060 [==========>] Loss 0.11771356242579128  - accuracy: 0.84375\n",
      "At: 2061 [==========>] Loss 0.12777678513517393  - accuracy: 0.875\n",
      "At: 2062 [==========>] Loss 0.12377580317554818  - accuracy: 0.875\n",
      "At: 2063 [==========>] Loss 0.09006763555692751  - accuracy: 0.9375\n",
      "At: 2064 [==========>] Loss 0.19629575071147543  - accuracy: 0.78125\n",
      "At: 2065 [==========>] Loss 0.04313457262214733  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.13356927909700006  - accuracy: 0.8125\n",
      "At: 2067 [==========>] Loss 0.09339822173533169  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.08221195121932819  - accuracy: 0.84375\n",
      "At: 2069 [==========>] Loss 0.11361832039719169  - accuracy: 0.84375\n",
      "At: 2070 [==========>] Loss 0.13619828780696552  - accuracy: 0.84375\n",
      "At: 2071 [==========>] Loss 0.10839760670705642  - accuracy: 0.84375\n",
      "At: 2072 [==========>] Loss 0.07919045215429707  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.10829221537353965  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.09992840455579514  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.10499596655428005  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.12374607874707627  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.16257030730879243  - accuracy: 0.75\n",
      "At: 2078 [==========>] Loss 0.09859758877968358  - accuracy: 0.84375\n",
      "At: 2079 [==========>] Loss 0.07495911130216894  - accuracy: 0.90625\n",
      "At: 2080 [==========>] Loss 0.12157817213467725  - accuracy: 0.84375\n",
      "At: 2081 [==========>] Loss 0.13322627523445812  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.12024985670649671  - accuracy: 0.875\n",
      "At: 2083 [==========>] Loss 0.19341792313681921  - accuracy: 0.6875\n",
      "At: 2084 [==========>] Loss 0.10942201116215089  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.1007060088586201  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.09769380785013373  - accuracy: 0.875\n",
      "At: 2087 [==========>] Loss 0.17780494976586628  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.09400678891110266  - accuracy: 0.84375\n",
      "At: 2089 [==========>] Loss 0.13336260849752457  - accuracy: 0.75\n",
      "At: 2090 [==========>] Loss 0.09804832012034641  - accuracy: 0.875\n",
      "At: 2091 [==========>] Loss 0.16587316308097783  - accuracy: 0.71875\n",
      "At: 2092 [==========>] Loss 0.0879770796554604  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.1559951558272068  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.11411975047410915  - accuracy: 0.84375\n",
      "At: 2095 [==========>] Loss 0.13209516066425697  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.1319884676441787  - accuracy: 0.8125\n",
      "At: 2097 [==========>] Loss 0.14654709101684169  - accuracy: 0.8125\n",
      "At: 2098 [==========>] Loss 0.13139049709346162  - accuracy: 0.84375\n",
      "At: 2099 [==========>] Loss 0.1363549327266359  - accuracy: 0.75\n",
      "At: 2100 [==========>] Loss 0.07503685382460888  - accuracy: 0.875\n",
      "At: 2101 [==========>] Loss 0.16637695403779973  - accuracy: 0.75\n",
      "At: 2102 [==========>] Loss 0.0982342726127841  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.1348980562854527  - accuracy: 0.875\n",
      "At: 2104 [==========>] Loss 0.09719483022871889  - accuracy: 0.875\n",
      "At: 2105 [==========>] Loss 0.14583090398216822  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.163939160791446  - accuracy: 0.75\n",
      "At: 2107 [==========>] Loss 0.12130813546419829  - accuracy: 0.84375\n",
      "At: 2108 [==========>] Loss 0.13770071663181738  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.11925546798785248  - accuracy: 0.84375\n",
      "At: 2110 [==========>] Loss 0.07250607407137558  - accuracy: 0.96875\n",
      "At: 2111 [==========>] Loss 0.14479461676270194  - accuracy: 0.8125\n",
      "At: 2112 [==========>] Loss 0.10941858742038979  - accuracy: 0.90625\n",
      "At: 2113 [==========>] Loss 0.1061804143541963  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.119574597861168  - accuracy: 0.875\n",
      "At: 2115 [==========>] Loss 0.11394978646698765  - accuracy: 0.8125\n",
      "At: 2116 [==========>] Loss 0.1264400198386111  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.14787449001775244  - accuracy: 0.84375\n",
      "At: 2118 [==========>] Loss 0.14887247995255026  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.07215850779526792  - accuracy: 0.9375\n",
      "At: 2120 [==========>] Loss 0.12132390103258779  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.14822706519168996  - accuracy: 0.8125\n",
      "At: 2122 [==========>] Loss 0.15444391108304473  - accuracy: 0.8125\n",
      "At: 2123 [==========>] Loss 0.19351939250535294  - accuracy: 0.71875\n",
      "At: 2124 [==========>] Loss 0.12313747381129661  - accuracy: 0.84375\n",
      "At: 2125 [==========>] Loss 0.10666719742449712  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.07203503356833751  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.08041478130925901  - accuracy: 0.9375\n",
      "At: 2128 [==========>] Loss 0.1303843735280738  - accuracy: 0.8125\n",
      "At: 2129 [==========>] Loss 0.16818658242160148  - accuracy: 0.8125\n",
      "At: 2130 [==========>] Loss 0.07952325992972531  - accuracy: 0.875\n",
      "At: 2131 [==========>] Loss 0.10223225892642204  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.2050837271065811  - accuracy: 0.78125\n",
      "At: 2133 [==========>] Loss 0.1710417438877039  - accuracy: 0.78125\n",
      "At: 2134 [==========>] Loss 0.14777491892016234  - accuracy: 0.78125\n",
      "At: 2135 [==========>] Loss 0.08528732901749508  - accuracy: 0.90625\n",
      "At: 2136 [==========>] Loss 0.14395244270296353  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.12796919677599425  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.11529060686614997  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.1762386354112259  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.11179251100353299  - accuracy: 0.84375\n",
      "At: 2141 [==========>] Loss 0.14095666507643773  - accuracy: 0.8125\n",
      "At: 2142 [==========>] Loss 0.12323864574532747  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.09137631127948574  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.07446777905087429  - accuracy: 0.875\n",
      "At: 2145 [==========>] Loss 0.11495600089257772  - accuracy: 0.84375\n",
      "At: 2146 [==========>] Loss 0.15656216477467283  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.13348793481967147  - accuracy: 0.875\n",
      "At: 2148 [==========>] Loss 0.18041573531204272  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.118069085630609  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.12048763374765031  - accuracy: 0.84375\n",
      "At: 2151 [==========>] Loss 0.1146350206353545  - accuracy: 0.875\n",
      "At: 2152 [==========>] Loss 0.21026034498007076  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.15414100061184743  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.1771614298973933  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.15234644824098822  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.1280387729702056  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.12841544135852329  - accuracy: 0.8125\n",
      "At: 2158 [==========>] Loss 0.16451117395321735  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.08736876467030777  - accuracy: 0.90625\n",
      "At: 2160 [==========>] Loss 0.1305709221881287  - accuracy: 0.8125\n",
      "At: 2161 [==========>] Loss 0.1008710121018268  - accuracy: 0.90625\n",
      "At: 2162 [==========>] Loss 0.11697250667816309  - accuracy: 0.8125\n",
      "At: 2163 [==========>] Loss 0.15194752967626438  - accuracy: 0.78125\n",
      "At: 2164 [==========>] Loss 0.14839581396607504  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.11286119692974814  - accuracy: 0.84375\n",
      "At: 2166 [==========>] Loss 0.10031487951472347  - accuracy: 0.90625\n",
      "At: 2167 [==========>] Loss 0.09116335988066351  - accuracy: 0.90625\n",
      "At: 2168 [==========>] Loss 0.09547477337142528  - accuracy: 0.8125\n",
      "At: 2169 [==========>] Loss 0.1252220318912461  - accuracy: 0.78125\n",
      "At: 2170 [==========>] Loss 0.11746262208792005  - accuracy: 0.875\n",
      "At: 2171 [==========>] Loss 0.17275385307312274  - accuracy: 0.78125\n",
      "At: 2172 [==========>] Loss 0.1185072398476199  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.1584304909362892  - accuracy: 0.78125\n",
      "At: 2174 [==========>] Loss 0.1232270098308575  - accuracy: 0.90625\n",
      "At: 2175 [==========>] Loss 0.1507425956602062  - accuracy: 0.84375\n",
      "At: 2176 [==========>] Loss 0.14012714711500293  - accuracy: 0.78125\n",
      "At: 2177 [==========>] Loss 0.18480603157388434  - accuracy: 0.8125\n",
      "At: 2178 [==========>] Loss 0.09428414540913498  - accuracy: 0.84375\n",
      "At: 2179 [==========>] Loss 0.1129272684799573  - accuracy: 0.8125\n",
      "At: 2180 [==========>] Loss 0.12463595466144216  - accuracy: 0.8125\n",
      "At: 2181 [==========>] Loss 0.1734529605250557  - accuracy: 0.6875\n",
      "At: 2182 [==========>] Loss 0.11292859414996231  - accuracy: 0.84375\n",
      "At: 2183 [==========>] Loss 0.2545142478252278  - accuracy: 0.59375\n",
      "At: 2184 [==========>] Loss 0.0906489366093981  - accuracy: 0.84375\n",
      "At: 2185 [==========>] Loss 0.12290802932168447  - accuracy: 0.78125\n",
      "At: 2186 [==========>] Loss 0.15285426740032865  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.1929156410968761  - accuracy: 0.71875\n",
      "At: 2188 [==========>] Loss 0.0818810773800659  - accuracy: 0.9375\n",
      "At: 2189 [==========>] Loss 0.07916461547654607  - accuracy: 0.84375\n",
      "At: 2190 [==========>] Loss 0.11109207430149728  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.13136667038541697  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.09335574524497191  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.17139216907764804  - accuracy: 0.6875\n",
      "At: 2194 [==========>] Loss 0.12617345957256576  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.20119363052109662  - accuracy: 0.71875\n",
      "At: 2196 [==========>] Loss 0.14866157060226093  - accuracy: 0.75\n",
      "At: 2197 [==========>] Loss 0.08461124344682101  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.09746037768398501  - accuracy: 0.90625\n",
      "At: 2199 [==========>] Loss 0.04685266757525164  - accuracy: 0.96875\n",
      "At: 2200 [==========>] Loss 0.05018623636236203  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.11306894172122994  - accuracy: 0.9375\n",
      "At: 2202 [==========>] Loss 0.09391676969682251  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.09182881074264371  - accuracy: 0.90625\n",
      "At: 2204 [==========>] Loss 0.12350752438650098  - accuracy: 0.84375\n",
      "At: 2205 [==========>] Loss 0.11733166838331457  - accuracy: 0.84375\n",
      "At: 2206 [==========>] Loss 0.10660096475706325  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.09144013844557727  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.17146347087168273  - accuracy: 0.75\n",
      "At: 2209 [==========>] Loss 0.17191048838733072  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.12003730722137672  - accuracy: 0.8125\n",
      "At: 2211 [==========>] Loss 0.13472343022935485  - accuracy: 0.84375\n",
      "At: 2212 [==========>] Loss 0.13167198115802992  - accuracy: 0.8125\n",
      "At: 2213 [==========>] Loss 0.12122890172408217  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.11779393997567672  - accuracy: 0.84375\n",
      "At: 2215 [==========>] Loss 0.14292899444560403  - accuracy: 0.8125\n",
      "At: 2216 [==========>] Loss 0.1515103180254963  - accuracy: 0.8125\n",
      "At: 2217 [==========>] Loss 0.13905951494854007  - accuracy: 0.78125\n",
      "At: 2218 [==========>] Loss 0.17057805831815387  - accuracy: 0.78125\n",
      "At: 2219 [==========>] Loss 0.10650826757878228  - accuracy: 0.875\n",
      "At: 2220 [==========>] Loss 0.13645233595387113  - accuracy: 0.78125\n",
      "At: 2221 [==========>] Loss 0.18292639359068447  - accuracy: 0.71875\n",
      "At: 2222 [==========>] Loss 0.11991187708525916  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.12626585460610343  - accuracy: 0.78125\n",
      "At: 2224 [==========>] Loss 0.1338686502870088  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.12062778079770055  - accuracy: 0.875\n",
      "At: 2226 [==========>] Loss 0.13269275390818938  - accuracy: 0.78125\n",
      "At: 2227 [==========>] Loss 0.18892673720276482  - accuracy: 0.65625\n",
      "At: 2228 [==========>] Loss 0.09558952023908567  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.18277446712969847  - accuracy: 0.71875\n",
      "At: 2230 [==========>] Loss 0.10975607537903095  - accuracy: 0.875\n",
      "At: 2231 [==========>] Loss 0.16765130153303698  - accuracy: 0.78125\n",
      "At: 2232 [==========>] Loss 0.17746498936006025  - accuracy: 0.71875\n",
      "At: 2233 [==========>] Loss 0.13839426359884188  - accuracy: 0.84375\n",
      "At: 2234 [==========>] Loss 0.1582415728225215  - accuracy: 0.71875\n",
      "At: 2235 [==========>] Loss 0.14128915880716755  - accuracy: 0.84375\n",
      "At: 2236 [==========>] Loss 0.08409205112396036  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.13123262239684158  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.15181238867048766  - accuracy: 0.8125\n",
      "At: 2239 [==========>] Loss 0.16601934833686902  - accuracy: 0.78125\n",
      "At: 2240 [==========>] Loss 0.14100304090557675  - accuracy: 0.8125\n",
      "At: 2241 [==========>] Loss 0.1437767825134645  - accuracy: 0.78125\n",
      "At: 2242 [==========>] Loss 0.16666377387223008  - accuracy: 0.875\n",
      "At: 2243 [==========>] Loss 0.07660129435374634  - accuracy: 0.875\n",
      "At: 2244 [==========>] Loss 0.10551318473454821  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.10857274491736421  - accuracy: 0.8125\n",
      "At: 2246 [==========>] Loss 0.1430129257547319  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.0932008057558795  - accuracy: 0.90625\n",
      "At: 2248 [==========>] Loss 0.1821123754748131  - accuracy: 0.78125\n",
      "At: 2249 [==========>] Loss 0.09398719012093562  - accuracy: 0.84375\n",
      "At: 2250 [==========>] Loss 0.08348152681630633  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.08531582319275627  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.1487258542312955  - accuracy: 0.75\n",
      "At: 2253 [==========>] Loss 0.13119769457378602  - accuracy: 0.75\n",
      "At: 2254 [==========>] Loss 0.1309989268410294  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.1377869681996739  - accuracy: 0.84375\n",
      "At: 2256 [==========>] Loss 0.13455158379826626  - accuracy: 0.78125\n",
      "At: 2257 [==========>] Loss 0.11463900796230786  - accuracy: 0.8125\n",
      "At: 2258 [==========>] Loss 0.12214458506322329  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.1314627023390589  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.15976270078274607  - accuracy: 0.78125\n",
      "At: 2261 [==========>] Loss 0.1179774315062748  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.15562786977973386  - accuracy: 0.78125\n",
      "At: 2263 [==========>] Loss 0.13955030829190473  - accuracy: 0.84375\n",
      "At: 2264 [==========>] Loss 0.07782708726587166  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.08779350031009006  - accuracy: 0.90625\n",
      "At: 2266 [==========>] Loss 0.1270575491639082  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.0676486866818759  - accuracy: 0.9375\n",
      "At: 2268 [==========>] Loss 0.11027844415386817  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.04347643879391763  - accuracy: 1.0\n",
      "At: 2270 [==========>] Loss 0.11281714390872012  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.123904820254019  - accuracy: 0.78125\n",
      "At: 2272 [==========>] Loss 0.08336069486431817  - accuracy: 0.875\n",
      "At: 2273 [==========>] Loss 0.12855045267673876  - accuracy: 0.84375\n",
      "At: 2274 [==========>] Loss 0.0824690245627954  - accuracy: 0.90625\n",
      "At: 2275 [==========>] Loss 0.09938978173151905  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.09935174095185194  - accuracy: 0.84375\n",
      "At: 2277 [==========>] Loss 0.1288458946910454  - accuracy: 0.8125\n",
      "At: 2278 [==========>] Loss 0.09593392505729799  - accuracy: 0.90625\n",
      "At: 2279 [==========>] Loss 0.12446047548596735  - accuracy: 0.84375\n",
      "At: 2280 [==========>] Loss 0.15078159266098173  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.11243794316526821  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.07313345644396008  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.15597149592173953  - accuracy: 0.6875\n",
      "At: 2284 [==========>] Loss 0.11839227327704555  - accuracy: 0.75\n",
      "At: 2285 [==========>] Loss 0.11230114831458272  - accuracy: 0.84375\n",
      "At: 2286 [==========>] Loss 0.11409131714677015  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.13677000251803395  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.09403272964227666  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.12755371652720712  - accuracy: 0.84375\n",
      "At: 2290 [==========>] Loss 0.09942486956797783  - accuracy: 0.84375\n",
      "At: 2291 [==========>] Loss 0.12071962208766951  - accuracy: 0.875\n",
      "At: 2292 [==========>] Loss 0.08688036627017517  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.056207259523893  - accuracy: 0.9375\n",
      "At: 2294 [==========>] Loss 0.061905846177848195  - accuracy: 0.875\n",
      "At: 2295 [==========>] Loss 0.14375920688665095  - accuracy: 0.71875\n",
      "At: 2296 [==========>] Loss 0.178250352069454  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.08637719233599707  - accuracy: 0.90625\n",
      "At: 2298 [==========>] Loss 0.10795857692079859  - accuracy: 0.90625\n",
      "At: 2299 [==========>] Loss 0.1299245155098256  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.14469278383901335  - accuracy: 0.71875\n",
      "At: 2301 [==========>] Loss 0.23427729271533485  - accuracy: 0.59375\n",
      "At: 2302 [==========>] Loss 0.17195954515949824  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.08180344245629458  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.09679778624289478  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10637618040128498  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.14580865795992107  - accuracy: 0.78125\n",
      "At: 2307 [==========>] Loss 0.16449716558558608  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.20501907942301453  - accuracy: 0.6875\n",
      "At: 2309 [==========>] Loss 0.12660000434550248  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.14558621359795026  - accuracy: 0.8125\n",
      "At: 2311 [==========>] Loss 0.12489123962181312  - accuracy: 0.8125\n",
      "At: 2312 [==========>] Loss 0.11449564489793618  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.07934550724228433  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.1228964734078684  - accuracy: 0.78125\n",
      "At: 2315 [==========>] Loss 0.12033414223886382  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.1278563851576579  - accuracy: 0.8125\n",
      "At: 2317 [==========>] Loss 0.2047613907998986  - accuracy: 0.6875\n",
      "At: 2318 [==========>] Loss 0.17320826442966097  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.11028579445676043  - accuracy: 0.90625\n",
      "At: 2320 [==========>] Loss 0.10667929998541906  - accuracy: 0.84375\n",
      "At: 2321 [==========>] Loss 0.14993962307918773  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.19829626472455592  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.14536226408050237  - accuracy: 0.71875\n",
      "At: 2324 [==========>] Loss 0.1592308542915393  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.13157050625069516  - accuracy: 0.8125\n",
      "At: 2326 [==========>] Loss 0.08737178603218065  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.07653767593797352  - accuracy: 0.90625\n",
      "At: 2328 [==========>] Loss 0.12632397914946106  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.0965294598099878  - accuracy: 0.84375\n",
      "At: 2330 [==========>] Loss 0.15527211625868215  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.10535792210986106  - accuracy: 0.84375\n",
      "At: 2332 [==========>] Loss 0.145992811711113  - accuracy: 0.75\n",
      "At: 2333 [==========>] Loss 0.11048819862153308  - accuracy: 0.78125\n",
      "At: 2334 [==========>] Loss 0.1723096703962093  - accuracy: 0.71875\n",
      "At: 2335 [==========>] Loss 0.11193220985750674  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.11221783846781735  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.17068308468477283  - accuracy: 0.75\n",
      "At: 2338 [==========>] Loss 0.09979706798705391  - accuracy: 0.875\n",
      "At: 2339 [==========>] Loss 0.10991305899042805  - accuracy: 0.8125\n",
      "At: 2340 [==========>] Loss 0.15386013613350408  - accuracy: 0.78125\n",
      "At: 2341 [==========>] Loss 0.14319987981826213  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.1921179651826348  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.09025648142352563  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.18236460005488186  - accuracy: 0.8125\n",
      "At: 2345 [==========>] Loss 0.1530270424442301  - accuracy: 0.78125\n",
      "At: 2346 [==========>] Loss 0.08962513906722433  - accuracy: 0.90625\n",
      "At: 2347 [==========>] Loss 0.12038964069026192  - accuracy: 0.84375\n",
      "At: 2348 [==========>] Loss 0.06741942164944083  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.11129658860456923  - accuracy: 0.8125\n",
      "At: 2350 [==========>] Loss 0.11611777266414142  - accuracy: 0.84375\n",
      "At: 2351 [==========>] Loss 0.10290412440020455  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.09468351164727087  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.09626058927748714  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.16550146725053405  - accuracy: 0.78125\n",
      "At: 2355 [==========>] Loss 0.07039290809354441  - accuracy: 0.90625\n",
      "At: 2356 [==========>] Loss 0.12341132846411348  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.15846563154609472  - accuracy: 0.84375\n",
      "At: 2358 [==========>] Loss 0.17736638363207435  - accuracy: 0.75\n",
      "At: 2359 [==========>] Loss 0.19802897105684136  - accuracy: 0.65625\n",
      "At: 2360 [==========>] Loss 0.1184078531867757  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.1655708370285725  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.07716270198679559  - accuracy: 0.875\n",
      "At: 2363 [==========>] Loss 0.19734029063731393  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.0836928430516492  - accuracy: 0.9375\n",
      "At: 2365 [==========>] Loss 0.07590564164312161  - accuracy: 0.9375\n",
      "At: 2366 [==========>] Loss 0.15001967509851194  - accuracy: 0.75\n",
      "At: 2367 [==========>] Loss 0.09823802422324554  - accuracy: 0.875\n",
      "At: 2368 [==========>] Loss 0.0941901430232805  - accuracy: 0.90625\n",
      "At: 2369 [==========>] Loss 0.11795513037576608  - accuracy: 0.78125\n",
      "At: 2370 [==========>] Loss 0.09137029860239218  - accuracy: 0.84375\n",
      "At: 2371 [==========>] Loss 0.10664544137983122  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.11781677582486888  - accuracy: 0.8125\n",
      "At: 2373 [==========>] Loss 0.11517534123866688  - accuracy: 0.78125\n",
      "At: 2374 [==========>] Loss 0.10292497249720203  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.08184852561087848  - accuracy: 0.90625\n",
      "At: 2376 [==========>] Loss 0.11242550686071179  - accuracy: 0.875\n",
      "At: 2377 [==========>] Loss 0.10699787737342698  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.13225337410496452  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.16619197872077668  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.07986637076891054  - accuracy: 0.875\n",
      "At: 2381 [==========>] Loss 0.08766158595808965  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.08301665002027803  - accuracy: 0.90625\n",
      "At: 2383 [==========>] Loss 0.13575887380765606  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.11601162571178553  - accuracy: 0.8125\n",
      "At: 2385 [==========>] Loss 0.13851813351263154  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.12367670620622509  - accuracy: 0.8125\n",
      "At: 2387 [==========>] Loss 0.09254794343628192  - accuracy: 0.84375\n",
      "At: 2388 [==========>] Loss 0.11850367433588513  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.05680457977484797  - accuracy: 0.9375\n",
      "At: 2390 [==========>] Loss 0.09236409312083826  - accuracy: 0.90625\n",
      "At: 2391 [==========>] Loss 0.17475112884198613  - accuracy: 0.71875\n",
      "At: 2392 [==========>] Loss 0.1386385762464916  - accuracy: 0.84375\n",
      "At: 2393 [==========>] Loss 0.09843060000129994  - accuracy: 0.875\n",
      "At: 2394 [==========>] Loss 0.07771025754156811  - accuracy: 0.9375\n",
      "At: 2395 [==========>] Loss 0.10986842590978  - accuracy: 0.84375\n",
      "At: 2396 [==========>] Loss 0.06730033313374488  - accuracy: 0.9375\n",
      "At: 2397 [==========>] Loss 0.09713690192937624  - accuracy: 0.90625\n",
      "At: 2398 [==========>] Loss 0.10805825882858228  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.1334339049674639  - accuracy: 0.84375\n",
      "At: 2400 [==========>] Loss 0.10617245367321371  - accuracy: 0.875\n",
      "At: 2401 [==========>] Loss 0.1039867778467402  - accuracy: 0.8125\n",
      "At: 2402 [==========>] Loss 0.11242964997402037  - accuracy: 0.84375\n",
      "At: 2403 [==========>] Loss 0.18877698913900887  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.14655251339906578  - accuracy: 0.78125\n",
      "At: 2405 [==========>] Loss 0.06724181265562454  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.14546327834054634  - accuracy: 0.78125\n",
      "At: 2407 [==========>] Loss 0.11682691999999713  - accuracy: 0.84375\n",
      "At: 2408 [==========>] Loss 0.08755376316098082  - accuracy: 0.90625\n",
      "At: 2409 [==========>] Loss 0.17596622074753557  - accuracy: 0.71875\n",
      "At: 2410 [==========>] Loss 0.16430742869932313  - accuracy: 0.8125\n",
      "At: 2411 [==========>] Loss 0.1089129844944017  - accuracy: 0.90625\n",
      "At: 2412 [==========>] Loss 0.09696952982281605  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.10128229431849373  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.07871755448016057  - accuracy: 0.84375\n",
      "At: 2415 [==========>] Loss 0.11482269666661846  - accuracy: 0.875\n",
      "At: 2416 [==========>] Loss 0.1250299691864431  - accuracy: 0.8125\n",
      "At: 2417 [==========>] Loss 0.1365656067930112  - accuracy: 0.875\n",
      "At: 2418 [==========>] Loss 0.12491673116885343  - accuracy: 0.84375\n",
      "At: 2419 [==========>] Loss 0.14613405320746686  - accuracy: 0.8125\n",
      "At: 2420 [==========>] Loss 0.1386888150955322  - accuracy: 0.8125\n",
      "At: 2421 [==========>] Loss 0.12115392759112337  - accuracy: 0.8125\n",
      "At: 2422 [==========>] Loss 0.1478292262027977  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.1340178042615695  - accuracy: 0.84375\n",
      "At: 2424 [==========>] Loss 0.1032713427328079  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.09452708485151023  - accuracy: 0.90625\n",
      "At: 2426 [==========>] Loss 0.21087660272183245  - accuracy: 0.625\n",
      "At: 2427 [==========>] Loss 0.12871885274348446  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.08966899834572892  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.12859274282253486  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.14717908445414574  - accuracy: 0.8125\n",
      "At: 2431 [==========>] Loss 0.15355432117566464  - accuracy: 0.75\n",
      "At: 2432 [==========>] Loss 0.0761037844840916  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.07568642122673791  - accuracy: 0.90625\n",
      "At: 2434 [==========>] Loss 0.04516472970483529  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.15389660420671034  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.12337635949992609  - accuracy: 0.875\n",
      "At: 2437 [==========>] Loss 0.1722365912270031  - accuracy: 0.75\n",
      "At: 2438 [==========>] Loss 0.10501789873325446  - accuracy: 0.84375\n",
      "At: 2439 [==========>] Loss 0.14743601065340314  - accuracy: 0.75\n",
      "At: 2440 [==========>] Loss 0.14355683513676032  - accuracy: 0.78125\n",
      "At: 2441 [==========>] Loss 0.12131327149420709  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.13530084537811657  - accuracy: 0.78125\n",
      "At: 2443 [==========>] Loss 0.09319567317247207  - accuracy: 0.875\n",
      "At: 2444 [==========>] Loss 0.0970684265199103  - accuracy: 0.84375\n",
      "At: 2445 [==========>] Loss 0.06878402989180463  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.20976664384758964  - accuracy: 0.71875\n",
      "At: 2447 [==========>] Loss 0.18284737637570653  - accuracy: 0.78125\n",
      "At: 2448 [==========>] Loss 0.14716119636824018  - accuracy: 0.8125\n",
      "At: 2449 [==========>] Loss 0.09299743541090831  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.07132841660183402  - accuracy: 0.875\n",
      "At: 2451 [==========>] Loss 0.04778102588432431  - accuracy: 0.9375\n",
      "At: 2452 [==========>] Loss 0.12538982166769969  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.1385829257163113  - accuracy: 0.84375\n",
      "At: 2454 [==========>] Loss 0.15031962957101536  - accuracy: 0.78125\n",
      "At: 2455 [==========>] Loss 0.13670342240603744  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.15484816397342255  - accuracy: 0.75\n",
      "At: 2457 [==========>] Loss 0.17545097639328172  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.0908147964806015  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.1356173291205841  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.07113235808645489  - accuracy: 0.90625\n",
      "At: 2461 [==========>] Loss 0.06422149434333847  - accuracy: 0.96875\n",
      "At: 2462 [==========>] Loss 0.17457324066125932  - accuracy: 0.71875\n",
      "At: 2463 [==========>] Loss 0.0981315619694175  - accuracy: 0.875\n",
      "At: 2464 [==========>] Loss 0.17489753278279474  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.16017482436435695  - accuracy: 0.75\n",
      "At: 2466 [==========>] Loss 0.1100509157069224  - accuracy: 0.8125\n",
      "At: 2467 [==========>] Loss 0.11229039266510787  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.10594002531414588  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.15338763033401775  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.12833199926527683  - accuracy: 0.8125\n",
      "At: 2471 [==========>] Loss 0.13105679535988596  - accuracy: 0.84375\n",
      "At: 2472 [==========>] Loss 0.09342780600234274  - accuracy: 0.90625\n",
      "At: 2473 [==========>] Loss 0.12523301651177848  - accuracy: 0.78125\n",
      "At: 2474 [==========>] Loss 0.07321744902523156  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.11770271734998036  - accuracy: 0.84375\n",
      "At: 2476 [==========>] Loss 0.08332087132869512  - accuracy: 0.90625\n",
      "At: 2477 [==========>] Loss 0.14612750040222916  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.10517805717587664  - accuracy: 0.8125\n",
      "At: 2479 [==========>] Loss 0.07821832368870435  - accuracy: 0.90625\n",
      "At: 2480 [==========>] Loss 0.13137751908615508  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.07677424200874373  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.18899855825234874  - accuracy: 0.75\n",
      "At: 2483 [==========>] Loss 0.1036484864290531  - accuracy: 0.84375\n",
      "At: 2484 [==========>] Loss 0.09858835460147812  - accuracy: 0.84375\n",
      "At: 2485 [==========>] Loss 0.10785365822141325  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.1402001488850779  - accuracy: 0.84375\n",
      "At: 2487 [==========>] Loss 0.11505820433642117  - accuracy: 0.90625\n",
      "At: 2488 [==========>] Loss 0.164708778578871  - accuracy: 0.78125\n",
      "At: 2489 [==========>] Loss 0.2005088990813683  - accuracy: 0.75\n",
      "At: 2490 [==========>] Loss 0.1042902414462894  - accuracy: 0.84375\n",
      "At: 2491 [==========>] Loss 0.1413350282191077  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.11886815298593151  - accuracy: 0.78125\n",
      "At: 2493 [==========>] Loss 0.09606042493109526  - accuracy: 0.84375\n",
      "At: 2494 [==========>] Loss 0.10499831869332764  - accuracy: 0.84375\n",
      "At: 2495 [==========>] Loss 0.09229627038276592  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.09087206653810445  - accuracy: 0.875\n",
      "At: 2497 [==========>] Loss 0.22946800490323782  - accuracy: 0.625\n",
      "At: 2498 [==========>] Loss 0.11450382397395163  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.055124416908868296  - accuracy: 0.9375\n",
      "At: 2500 [==========>] Loss 0.17265504898540718  - accuracy: 0.8125\n",
      "At: 2501 [==========>] Loss 0.15009106672681227  - accuracy: 0.71875\n",
      "At: 2502 [==========>] Loss 0.10844876821761176  - accuracy: 0.875\n",
      "At: 2503 [==========>] Loss 0.12569764311287757  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.13763861752145579  - accuracy: 0.84375\n",
      "At: 2505 [==========>] Loss 0.12169960823844264  - accuracy: 0.78125\n",
      "At: 2506 [==========>] Loss 0.11330248194875063  - accuracy: 0.875\n",
      "At: 2507 [==========>] Loss 0.1462830106245906  - accuracy: 0.78125\n",
      "At: 2508 [==========>] Loss 0.13565602695051984  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.15585701469183402  - accuracy: 0.75\n",
      "At: 2510 [==========>] Loss 0.11667594843819128  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.1666327467527488  - accuracy: 0.78125\n",
      "At: 2512 [==========>] Loss 0.11185241864532881  - accuracy: 0.84375\n",
      "At: 2513 [==========>] Loss 0.15047768910724205  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.12683469430773153  - accuracy: 0.8125\n",
      "At: 2515 [==========>] Loss 0.20586505815195505  - accuracy: 0.71875\n",
      "At: 2516 [==========>] Loss 0.1958992309296579  - accuracy: 0.65625\n",
      "At: 2517 [==========>] Loss 0.09854791922439982  - accuracy: 0.875\n",
      "At: 2518 [==========>] Loss 0.1289246012244351  - accuracy: 0.8125\n",
      "At: 2519 [==========>] Loss 0.08327855891669615  - accuracy: 0.875\n",
      "At: 2520 [==========>] Loss 0.14861891132384272  - accuracy: 0.84375\n",
      "At: 2521 [==========>] Loss 0.1095393053981752  - accuracy: 0.8125\n",
      "At: 2522 [==========>] Loss 0.23559966256729853  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.11886354862545163  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.1784815479459383  - accuracy: 0.71875\n",
      "At: 2525 [==========>] Loss 0.05949782883512647  - accuracy: 1.0\n",
      "At: 2526 [==========>] Loss 0.11739177872968648  - accuracy: 0.875\n",
      "At: 2527 [==========>] Loss 0.1274290626441173  - accuracy: 0.78125\n",
      "At: 2528 [==========>] Loss 0.08930259617055113  - accuracy: 0.84375\n",
      "At: 2529 [==========>] Loss 0.15697968876099921  - accuracy: 0.8125\n",
      "At: 2530 [==========>] Loss 0.12233868475811861  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.052930764021050986  - accuracy: 0.9375\n",
      "At: 2532 [==========>] Loss 0.1458365971470335  - accuracy: 0.78125\n",
      "At: 2533 [==========>] Loss 0.10906086184588341  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.07477432220073729  - accuracy: 0.90625\n",
      "At: 2535 [==========>] Loss 0.07639582533506072  - accuracy: 0.96875\n",
      "At: 2536 [==========>] Loss 0.16424556202286908  - accuracy: 0.78125\n",
      "At: 2537 [==========>] Loss 0.0973860883348185  - accuracy: 0.84375\n",
      "At: 2538 [==========>] Loss 0.1901685158729149  - accuracy: 0.6875\n",
      "At: 2539 [==========>] Loss 0.10767728499164766  - accuracy: 0.875\n",
      "At: 2540 [==========>] Loss 0.1393482530614237  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.05822561877536839  - accuracy: 0.90625\n",
      "At: 2542 [==========>] Loss 0.04819284497192199  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.17196620363513856  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.15580623697443985  - accuracy: 0.75\n",
      "At: 2545 [==========>] Loss 0.06017642080940831  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.13908014339683183  - accuracy: 0.78125\n",
      "At: 2547 [==========>] Loss 0.09794485390566948  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.09613856200375064  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.11148822061461117  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.1376613611350719  - accuracy: 0.78125\n",
      "At: 2551 [==========>] Loss 0.12060160274231327  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.10743609652598565  - accuracy: 0.90625\n",
      "At: 2553 [==========>] Loss 0.10276554856845246  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.15776237212775063  - accuracy: 0.8125\n",
      "At: 2555 [==========>] Loss 0.17176674568163885  - accuracy: 0.75\n",
      "At: 2556 [==========>] Loss 0.0971201673981898  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.1597741089439291  - accuracy: 0.78125\n",
      "At: 2558 [==========>] Loss 0.08775912190760664  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.11205427000442862  - accuracy: 0.8125\n",
      "At: 2560 [==========>] Loss 0.1626457702880493  - accuracy: 0.78125\n",
      "At: 2561 [==========>] Loss 0.11315682492027787  - accuracy: 0.8125\n",
      "At: 2562 [==========>] Loss 0.12333455705652807  - accuracy: 0.78125\n",
      "At: 2563 [==========>] Loss 0.11825069663964966  - accuracy: 0.78125\n",
      "At: 2564 [==========>] Loss 0.09416822684496398  - accuracy: 0.84375\n",
      "At: 2565 [==========>] Loss 0.10744632742468395  - accuracy: 0.84375\n",
      "At: 2566 [==========>] Loss 0.17482701449479227  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.12740722587921186  - accuracy: 0.875\n",
      "At: 2568 [==========>] Loss 0.09976476971998714  - accuracy: 0.84375\n",
      "At: 2569 [==========>] Loss 0.0766887024101899  - accuracy: 0.9375\n",
      "At: 2570 [==========>] Loss 0.1881982042103163  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.10406502029035425  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.18855677489440775  - accuracy: 0.75\n",
      "At: 2573 [==========>] Loss 0.18244263803010222  - accuracy: 0.6875\n",
      "At: 2574 [==========>] Loss 0.1239238403215569  - accuracy: 0.84375\n",
      "At: 2575 [==========>] Loss 0.048805778679946205  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.11370238912205823  - accuracy: 0.84375\n",
      "At: 2577 [==========>] Loss 0.2288308686275263  - accuracy: 0.65625\n",
      "At: 2578 [==========>] Loss 0.20627828987555719  - accuracy: 0.6875\n",
      "At: 2579 [==========>] Loss 0.21776330147780115  - accuracy: 0.65625\n",
      "At: 2580 [==========>] Loss 0.1623453111677726  - accuracy: 0.71875\n",
      "At: 2581 [==========>] Loss 0.06820895414756781  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.19635635545149133  - accuracy: 0.78125\n",
      "At: 2583 [==========>] Loss 0.08755685848252868  - accuracy: 0.875\n",
      "At: 2584 [==========>] Loss 0.2017067194507603  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.10440860217265  - accuracy: 0.84375\n",
      "At: 2586 [==========>] Loss 0.09671696156446596  - accuracy: 0.84375\n",
      "At: 2587 [==========>] Loss 0.16920425300542594  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.14222430923206647  - accuracy: 0.84375\n",
      "At: 2589 [==========>] Loss 0.12531651528123935  - accuracy: 0.75\n",
      "At: 2590 [==========>] Loss 0.21651865359947603  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.15189523715097275  - accuracy: 0.8125\n",
      "At: 2592 [==========>] Loss 0.1268299660480198  - accuracy: 0.75\n",
      "At: 2593 [==========>] Loss 0.086249903821348  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.08755061417762394  - accuracy: 0.875\n",
      "At: 2595 [==========>] Loss 0.07272950021537289  - accuracy: 0.9375\n",
      "At: 2596 [==========>] Loss 0.1088270115274218  - accuracy: 0.8125\n",
      "At: 2597 [==========>] Loss 0.07679634524751543  - accuracy: 0.9375\n",
      "At: 2598 [==========>] Loss 0.11060427714280124  - accuracy: 0.875\n",
      "At: 2599 [==========>] Loss 0.15856629683037715  - accuracy: 0.75\n",
      "At: 2600 [==========>] Loss 0.15809045636227476  - accuracy: 0.75\n",
      "At: 2601 [==========>] Loss 0.15363939054173534  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.07649373882734463  - accuracy: 0.875\n",
      "At: 2603 [==========>] Loss 0.158939877132172  - accuracy: 0.78125\n",
      "At: 2604 [==========>] Loss 0.12767488810506636  - accuracy: 0.8125\n",
      "At: 2605 [==========>] Loss 0.13625761524065916  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.12932415847153478  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.14888548646874802  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.07978407393280264  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.11380738626851296  - accuracy: 0.84375\n",
      "At: 2610 [==========>] Loss 0.15189223301185692  - accuracy: 0.75\n",
      "At: 2611 [==========>] Loss 0.14125478813389974  - accuracy: 0.8125\n",
      "At: 2612 [==========>] Loss 0.09662366188563458  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.0884160187701882  - accuracy: 0.84375\n",
      "At: 2614 [==========>] Loss 0.14667683060663345  - accuracy: 0.71875\n",
      "At: 2615 [==========>] Loss 0.07408482016456393  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.05465933270393778  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.0920611691626034  - accuracy: 0.84375\n",
      "At: 2618 [==========>] Loss 0.07571755224579131  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.09807609759281419  - accuracy: 0.84375\n",
      "At: 2620 [==========>] Loss 0.11544739508204474  - accuracy: 0.84375\n",
      "At: 2621 [==========>] Loss 0.12622225997031492  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.1178269843802387  - accuracy: 0.8125\n",
      "At: 2623 [==========>] Loss 0.09278236573576584  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.1382466025101874  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.0681262177814797  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.047187985514776126  - accuracy: 0.9375\n",
      "At: 2627 [==========>] Loss 0.14357761280758574  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.09052728246329173  - accuracy: 0.9375\n",
      "At: 2629 [==========>] Loss 0.09777318895185759  - accuracy: 0.78125\n",
      "At: 2630 [==========>] Loss 0.1067142241991727  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.14569755439793866  - accuracy: 0.75\n",
      "At: 2632 [==========>] Loss 0.15564771467167812  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.10284137428909107  - accuracy: 0.84375\n",
      "At: 2634 [==========>] Loss 0.08100672801145875  - accuracy: 0.90625\n",
      "At: 2635 [==========>] Loss 0.22621226829713786  - accuracy: 0.625\n",
      "At: 2636 [==========>] Loss 0.11488745196093492  - accuracy: 0.84375\n",
      "At: 2637 [==========>] Loss 0.15593695871999136  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.08908276759313873  - accuracy: 0.90625\n",
      "At: 2639 [==========>] Loss 0.08572623641710489  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.13036067401509374  - accuracy: 0.8125\n",
      "At: 2641 [==========>] Loss 0.14663601709376342  - accuracy: 0.78125\n",
      "At: 2642 [==========>] Loss 0.14742009761253722  - accuracy: 0.78125\n",
      "At: 2643 [==========>] Loss 0.06969908911942999  - accuracy: 0.90625\n",
      "At: 2644 [==========>] Loss 0.16984955431781895  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.08885093941788219  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.13972007910663656  - accuracy: 0.8125\n",
      "At: 2647 [==========>] Loss 0.12705671184641548  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.15549458797181565  - accuracy: 0.78125\n",
      "At: 2649 [==========>] Loss 0.10767064993774432  - accuracy: 0.84375\n",
      "At: 2650 [==========>] Loss 0.10821803860489898  - accuracy: 0.84375\n",
      "At: 2651 [==========>] Loss 0.18107110284865496  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.10592239571903245  - accuracy: 0.84375\n",
      "At: 2653 [==========>] Loss 0.13243033800072934  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.11720459283574813  - accuracy: 0.875\n",
      "At: 2655 [==========>] Loss 0.2351438480180464  - accuracy: 0.625\n",
      "At: 2656 [==========>] Loss 0.024689886586016688  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.11272592661936665  - accuracy: 0.875\n",
      "At: 2658 [==========>] Loss 0.09375936989990394  - accuracy: 0.875\n",
      "At: 2659 [==========>] Loss 0.07246824740506033  - accuracy: 0.875\n",
      "At: 2660 [==========>] Loss 0.10522453199154815  - accuracy: 0.8125\n",
      "At: 2661 [==========>] Loss 0.08369999414143664  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.0946729298029017  - accuracy: 0.875\n",
      "At: 2663 [==========>] Loss 0.09089905236895764  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.08900798599354134  - accuracy: 0.875\n",
      "At: 2665 [==========>] Loss 0.09049165617123828  - accuracy: 0.9375\n",
      "At: 2666 [==========>] Loss 0.1405890956453406  - accuracy: 0.75\n",
      "At: 2667 [==========>] Loss 0.1223170263834351  - accuracy: 0.875\n",
      "At: 2668 [==========>] Loss 0.14748089134015696  - accuracy: 0.75\n",
      "At: 2669 [==========>] Loss 0.15112155530775495  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.10526593437612836  - accuracy: 0.84375\n",
      "At: 2671 [==========>] Loss 0.09005755228201673  - accuracy: 0.90625\n",
      "At: 2672 [==========>] Loss 0.09266760400893755  - accuracy: 0.90625\n",
      "At: 2673 [==========>] Loss 0.11958739250291087  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.0936953043003182  - accuracy: 0.90625\n",
      "At: 2675 [==========>] Loss 0.13708614850047562  - accuracy: 0.8125\n",
      "At: 2676 [==========>] Loss 0.14600112172465954  - accuracy: 0.78125\n",
      "At: 2677 [==========>] Loss 0.11682270981309276  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.049693506037626185  - accuracy: 1.0\n",
      "At: 2679 [==========>] Loss 0.09222885087558258  - accuracy: 0.875\n",
      "At: 2680 [==========>] Loss 0.10572729653152627  - accuracy: 0.8125\n",
      "At: 2681 [==========>] Loss 0.09836750401585415  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.15342541544705368  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.1950762127085594  - accuracy: 0.78125\n",
      "At: 2684 [==========>] Loss 0.08960699240392565  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.12758093835709206  - accuracy: 0.78125\n",
      "At: 2686 [==========>] Loss 0.08433260281663396  - accuracy: 0.875\n",
      "At: 2687 [==========>] Loss 0.14354044866338  - accuracy: 0.8125\n",
      "At: 2688 [==========>] Loss 0.1691964872342199  - accuracy: 0.84375\n",
      "At: 2689 [==========>] Loss 0.10647105622593761  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.1348445778452674  - accuracy: 0.75\n",
      "Epochs  9 / 10\n",
      "At: 1 [==========>] Loss 0.13226141823191911  - accuracy: 0.8125\n",
      "At: 2 [==========>] Loss 0.22498114724732712  - accuracy: 0.75\n",
      "At: 3 [==========>] Loss 0.13500741987776094  - accuracy: 0.84375\n",
      "At: 4 [==========>] Loss 0.21261384480155643  - accuracy: 0.71875\n",
      "At: 5 [==========>] Loss 0.07947804311335255  - accuracy: 0.875\n",
      "At: 6 [==========>] Loss 0.17220281275740557  - accuracy: 0.75\n",
      "At: 7 [==========>] Loss 0.22251471364227038  - accuracy: 0.65625\n",
      "At: 8 [==========>] Loss 0.26581753015305404  - accuracy: 0.6875\n",
      "At: 9 [==========>] Loss 0.24707317991294464  - accuracy: 0.65625\n",
      "At: 10 [==========>] Loss 0.21500024080416438  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.2196650823213176  - accuracy: 0.6875\n",
      "At: 12 [==========>] Loss 0.159952445175039  - accuracy: 0.78125\n",
      "At: 13 [==========>] Loss 0.14700046070511963  - accuracy: 0.8125\n",
      "At: 14 [==========>] Loss 0.07914244414232184  - accuracy: 0.875\n",
      "At: 15 [==========>] Loss 0.1662984372215427  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.20765563718168067  - accuracy: 0.71875\n",
      "At: 17 [==========>] Loss 0.1707337569154716  - accuracy: 0.8125\n",
      "At: 18 [==========>] Loss 0.26975735803650236  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.18752248067496494  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.10285308881247222  - accuracy: 0.875\n",
      "At: 21 [==========>] Loss 0.23221578621702  - accuracy: 0.71875\n",
      "At: 22 [==========>] Loss 0.17656668263084757  - accuracy: 0.8125\n",
      "At: 23 [==========>] Loss 0.09132356917553133  - accuracy: 0.9375\n",
      "At: 24 [==========>] Loss 0.2438926737392538  - accuracy: 0.6875\n",
      "At: 25 [==========>] Loss 0.220973582248784  - accuracy: 0.75\n",
      "At: 26 [==========>] Loss 0.22417406404237433  - accuracy: 0.71875\n",
      "At: 27 [==========>] Loss 0.22554991943258731  - accuracy: 0.71875\n",
      "At: 28 [==========>] Loss 0.15521602158137832  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.19794611565025663  - accuracy: 0.75\n",
      "At: 30 [==========>] Loss 0.1978951839735765  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.256856098143211  - accuracy: 0.625\n",
      "At: 32 [==========>] Loss 0.2138813911708068  - accuracy: 0.75\n",
      "At: 33 [==========>] Loss 0.1046572029731119  - accuracy: 0.875\n",
      "At: 34 [==========>] Loss 0.17331041010506099  - accuracy: 0.75\n",
      "At: 35 [==========>] Loss 0.17950288627040337  - accuracy: 0.8125\n",
      "At: 36 [==========>] Loss 0.2203105650139559  - accuracy: 0.75\n",
      "At: 37 [==========>] Loss 0.24741024184644833  - accuracy: 0.6875\n",
      "At: 38 [==========>] Loss 0.25436328038644074  - accuracy: 0.71875\n",
      "At: 39 [==========>] Loss 0.1775015868991764  - accuracy: 0.78125\n",
      "At: 40 [==========>] Loss 0.24041879767034532  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.07394434790350617  - accuracy: 0.9375\n",
      "At: 42 [==========>] Loss 0.17752605916779388  - accuracy: 0.75\n",
      "At: 43 [==========>] Loss 0.18033550992394165  - accuracy: 0.78125\n",
      "At: 44 [==========>] Loss 0.162797219936637  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.0871846755654112  - accuracy: 0.9375\n",
      "At: 46 [==========>] Loss 0.23187504620621385  - accuracy: 0.6875\n",
      "At: 47 [==========>] Loss 0.17317060908188578  - accuracy: 0.78125\n",
      "At: 48 [==========>] Loss 0.12308869129606775  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.13511296078873875  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.1743113839988133  - accuracy: 0.78125\n",
      "At: 51 [==========>] Loss 0.2284596031127286  - accuracy: 0.71875\n",
      "At: 52 [==========>] Loss 0.3176411005576647  - accuracy: 0.59375\n",
      "At: 53 [==========>] Loss 0.17065460084673012  - accuracy: 0.8125\n",
      "At: 54 [==========>] Loss 0.15866235439741083  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.18082170584865476  - accuracy: 0.78125\n",
      "At: 56 [==========>] Loss 0.16420438048070599  - accuracy: 0.8125\n",
      "At: 57 [==========>] Loss 0.1552667051705451  - accuracy: 0.8125\n",
      "At: 58 [==========>] Loss 0.19523448993028308  - accuracy: 0.75\n",
      "At: 59 [==========>] Loss 0.19822495299882878  - accuracy: 0.6875\n",
      "At: 60 [==========>] Loss 0.24070143591832652  - accuracy: 0.6875\n",
      "At: 61 [==========>] Loss 0.26364695017444456  - accuracy: 0.625\n",
      "At: 62 [==========>] Loss 0.1866284669782021  - accuracy: 0.75\n",
      "At: 63 [==========>] Loss 0.19225758794161057  - accuracy: 0.78125\n",
      "At: 64 [==========>] Loss 0.2140315902281943  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.29571260325437076  - accuracy: 0.625\n",
      "At: 66 [==========>] Loss 0.21508286460743226  - accuracy: 0.6875\n",
      "At: 67 [==========>] Loss 0.24703857688239514  - accuracy: 0.71875\n",
      "At: 68 [==========>] Loss 0.12472255348706476  - accuracy: 0.84375\n",
      "At: 69 [==========>] Loss 0.1633782039185898  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.19541445177666034  - accuracy: 0.75\n",
      "At: 71 [==========>] Loss 0.183579853680022  - accuracy: 0.78125\n",
      "At: 72 [==========>] Loss 0.16989105075566366  - accuracy: 0.78125\n",
      "At: 73 [==========>] Loss 0.15038659562477394  - accuracy: 0.8125\n",
      "At: 74 [==========>] Loss 0.20965228324694374  - accuracy: 0.6875\n",
      "At: 75 [==========>] Loss 0.23417104269086242  - accuracy: 0.71875\n",
      "At: 76 [==========>] Loss 0.30077544747087215  - accuracy: 0.625\n",
      "At: 77 [==========>] Loss 0.21922093834300746  - accuracy: 0.71875\n",
      "At: 78 [==========>] Loss 0.1423060713426238  - accuracy: 0.84375\n",
      "At: 79 [==========>] Loss 0.17846180675988088  - accuracy: 0.78125\n",
      "At: 80 [==========>] Loss 0.18301711235794058  - accuracy: 0.78125\n",
      "At: 81 [==========>] Loss 0.18942828391979483  - accuracy: 0.78125\n",
      "At: 82 [==========>] Loss 0.23938003292445625  - accuracy: 0.71875\n",
      "At: 83 [==========>] Loss 0.17667947701308306  - accuracy: 0.71875\n",
      "At: 84 [==========>] Loss 0.2397851618540432  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.22503678204864314  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.19105671840102828  - accuracy: 0.78125\n",
      "At: 87 [==========>] Loss 0.16511058857130886  - accuracy: 0.78125\n",
      "At: 88 [==========>] Loss 0.3564364909686381  - accuracy: 0.5625\n",
      "At: 89 [==========>] Loss 0.2018235057749068  - accuracy: 0.75\n",
      "At: 90 [==========>] Loss 0.23170146595797686  - accuracy: 0.75\n",
      "At: 91 [==========>] Loss 0.18194891557195342  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.07289533088956315  - accuracy: 0.9375\n",
      "At: 93 [==========>] Loss 0.1786871763163429  - accuracy: 0.8125\n",
      "At: 94 [==========>] Loss 0.157673106197599  - accuracy: 0.8125\n",
      "At: 95 [==========>] Loss 0.16379551304227613  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.12920122292142033  - accuracy: 0.875\n",
      "At: 97 [==========>] Loss 0.0950634393499476  - accuracy: 0.90625\n",
      "At: 98 [==========>] Loss 0.2664663213385896  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.14064543970760407  - accuracy: 0.8125\n",
      "At: 100 [==========>] Loss 0.14261713737103004  - accuracy: 0.84375\n",
      "At: 101 [==========>] Loss 0.14828508042084426  - accuracy: 0.84375\n",
      "At: 102 [==========>] Loss 0.20760798197086713  - accuracy: 0.75\n",
      "At: 103 [==========>] Loss 0.13334709771913228  - accuracy: 0.8125\n",
      "At: 104 [==========>] Loss 0.1454598926791035  - accuracy: 0.84375\n",
      "At: 105 [==========>] Loss 0.17387830786643121  - accuracy: 0.75\n",
      "At: 106 [==========>] Loss 0.2547181594875543  - accuracy: 0.6875\n",
      "At: 107 [==========>] Loss 0.17992986198825103  - accuracy: 0.78125\n",
      "At: 108 [==========>] Loss 0.20243511905720474  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.12141430534714923  - accuracy: 0.90625\n",
      "At: 110 [==========>] Loss 0.2013537765798376  - accuracy: 0.6875\n",
      "At: 111 [==========>] Loss 0.10660123417843367  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.18944755057172094  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.1898360020416871  - accuracy: 0.75\n",
      "At: 114 [==========>] Loss 0.15538237443007544  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.1877693478257402  - accuracy: 0.75\n",
      "At: 116 [==========>] Loss 0.20011761654093746  - accuracy: 0.78125\n",
      "At: 117 [==========>] Loss 0.16672400124302583  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.25072140345811084  - accuracy: 0.6875\n",
      "At: 119 [==========>] Loss 0.13411913807508108  - accuracy: 0.84375\n",
      "At: 120 [==========>] Loss 0.15814983922384956  - accuracy: 0.75\n",
      "At: 121 [==========>] Loss 0.14834689741986742  - accuracy: 0.84375\n",
      "At: 122 [==========>] Loss 0.19480149723399615  - accuracy: 0.78125\n",
      "At: 123 [==========>] Loss 0.23640030227210626  - accuracy: 0.6875\n",
      "At: 124 [==========>] Loss 0.1758165118385383  - accuracy: 0.75\n",
      "At: 125 [==========>] Loss 0.17799800775969749  - accuracy: 0.8125\n",
      "At: 126 [==========>] Loss 0.23265406366288255  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.22103046299528584  - accuracy: 0.75\n",
      "At: 128 [==========>] Loss 0.26895807933910953  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.09183146672359613  - accuracy: 0.875\n",
      "At: 130 [==========>] Loss 0.23254410310378054  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.22338016726379314  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.2871324774764844  - accuracy: 0.59375\n",
      "At: 133 [==========>] Loss 0.17496186919094436  - accuracy: 0.75\n",
      "At: 134 [==========>] Loss 0.17178388749593376  - accuracy: 0.78125\n",
      "At: 135 [==========>] Loss 0.21137296784941068  - accuracy: 0.75\n",
      "At: 136 [==========>] Loss 0.16980053766585138  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.1273025014406588  - accuracy: 0.84375\n",
      "At: 138 [==========>] Loss 0.19304612875090238  - accuracy: 0.75\n",
      "At: 139 [==========>] Loss 0.11801558989351053  - accuracy: 0.875\n",
      "At: 140 [==========>] Loss 0.15533994761034708  - accuracy: 0.84375\n",
      "At: 141 [==========>] Loss 0.2830349016162754  - accuracy: 0.59375\n",
      "At: 142 [==========>] Loss 0.16754127718779316  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.20258885807784444  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.14233419167061334  - accuracy: 0.84375\n",
      "At: 145 [==========>] Loss 0.12327554311633773  - accuracy: 0.84375\n",
      "At: 146 [==========>] Loss 0.13616972273881794  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.18692216055974076  - accuracy: 0.75\n",
      "At: 148 [==========>] Loss 0.14279422773158462  - accuracy: 0.84375\n",
      "At: 149 [==========>] Loss 0.1876490659544136  - accuracy: 0.75\n",
      "At: 150 [==========>] Loss 0.15653982946633302  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.2068765177246735  - accuracy: 0.71875\n",
      "At: 152 [==========>] Loss 0.18063126841908414  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.23533079970465665  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.19339165067813233  - accuracy: 0.75\n",
      "At: 155 [==========>] Loss 0.2106085105757613  - accuracy: 0.75\n",
      "At: 156 [==========>] Loss 0.1438959436949643  - accuracy: 0.78125\n",
      "At: 157 [==========>] Loss 0.2661607544483652  - accuracy: 0.65625\n",
      "At: 158 [==========>] Loss 0.15998904010989484  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.16653932133074978  - accuracy: 0.8125\n",
      "At: 160 [==========>] Loss 0.1668371872283314  - accuracy: 0.84375\n",
      "At: 161 [==========>] Loss 0.1330763672597753  - accuracy: 0.875\n",
      "At: 162 [==========>] Loss 0.1817189187206757  - accuracy: 0.71875\n",
      "At: 163 [==========>] Loss 0.1915312365288595  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.18786757877262372  - accuracy: 0.78125\n",
      "At: 165 [==========>] Loss 0.2521922755178621  - accuracy: 0.6875\n",
      "At: 166 [==========>] Loss 0.21934051438702018  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.13393639015952521  - accuracy: 0.8125\n",
      "At: 168 [==========>] Loss 0.19800202041447285  - accuracy: 0.75\n",
      "At: 169 [==========>] Loss 0.1606897485116335  - accuracy: 0.78125\n",
      "At: 170 [==========>] Loss 0.15364601257181115  - accuracy: 0.78125\n",
      "At: 171 [==========>] Loss 0.23613726362084714  - accuracy: 0.71875\n",
      "At: 172 [==========>] Loss 0.21939629240946007  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.30108351946603706  - accuracy: 0.65625\n",
      "At: 174 [==========>] Loss 0.18093230212204886  - accuracy: 0.75\n",
      "At: 175 [==========>] Loss 0.12854224819405843  - accuracy: 0.78125\n",
      "At: 176 [==========>] Loss 0.16508673914257527  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.11473619911095673  - accuracy: 0.90625\n",
      "At: 178 [==========>] Loss 0.21611858856832672  - accuracy: 0.71875\n",
      "At: 179 [==========>] Loss 0.12404632848845412  - accuracy: 0.75\n",
      "At: 180 [==========>] Loss 0.1897047062068704  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.06574381366986784  - accuracy: 0.96875\n",
      "At: 182 [==========>] Loss 0.19944446618248174  - accuracy: 0.75\n",
      "At: 183 [==========>] Loss 0.20783338262662654  - accuracy: 0.78125\n",
      "At: 184 [==========>] Loss 0.19183849771009476  - accuracy: 0.75\n",
      "At: 185 [==========>] Loss 0.13271032128316468  - accuracy: 0.875\n",
      "At: 186 [==========>] Loss 0.18116175633108406  - accuracy: 0.75\n",
      "At: 187 [==========>] Loss 0.16455342142952167  - accuracy: 0.78125\n",
      "At: 188 [==========>] Loss 0.2092677732511756  - accuracy: 0.71875\n",
      "At: 189 [==========>] Loss 0.22797189761723907  - accuracy: 0.6875\n",
      "At: 190 [==========>] Loss 0.1188949570829817  - accuracy: 0.84375\n",
      "At: 191 [==========>] Loss 0.2914342593315594  - accuracy: 0.625\n",
      "At: 192 [==========>] Loss 0.14407421492080885  - accuracy: 0.84375\n",
      "At: 193 [==========>] Loss 0.25476703926291194  - accuracy: 0.6875\n",
      "At: 194 [==========>] Loss 0.18919861770172985  - accuracy: 0.75\n",
      "At: 195 [==========>] Loss 0.1966354409905642  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.20412322104095018  - accuracy: 0.78125\n",
      "At: 197 [==========>] Loss 0.143416390654557  - accuracy: 0.84375\n",
      "At: 198 [==========>] Loss 0.14779483341941096  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.11265980515415755  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.2171774537434602  - accuracy: 0.71875\n",
      "At: 201 [==========>] Loss 0.1437341348628254  - accuracy: 0.84375\n",
      "At: 202 [==========>] Loss 0.1334546364955943  - accuracy: 0.875\n",
      "At: 203 [==========>] Loss 0.18681987222179547  - accuracy: 0.75\n",
      "At: 204 [==========>] Loss 0.1756413550535069  - accuracy: 0.78125\n",
      "At: 205 [==========>] Loss 0.16302590582637694  - accuracy: 0.8125\n",
      "At: 206 [==========>] Loss 0.09407621377684103  - accuracy: 0.875\n",
      "At: 207 [==========>] Loss 0.10669973653572838  - accuracy: 0.84375\n",
      "At: 208 [==========>] Loss 0.25237261395653326  - accuracy: 0.6875\n",
      "At: 209 [==========>] Loss 0.18681509579451877  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.12010677242775712  - accuracy: 0.84375\n",
      "At: 211 [==========>] Loss 0.14434118882483005  - accuracy: 0.8125\n",
      "At: 212 [==========>] Loss 0.215620327585766  - accuracy: 0.75\n",
      "At: 213 [==========>] Loss 0.20327580513543955  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.21482441651156056  - accuracy: 0.6875\n",
      "At: 215 [==========>] Loss 0.10569091363602648  - accuracy: 0.84375\n",
      "At: 216 [==========>] Loss 0.20816535177466688  - accuracy: 0.75\n",
      "At: 217 [==========>] Loss 0.22711531302491392  - accuracy: 0.6875\n",
      "At: 218 [==========>] Loss 0.16008337882937385  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.2075769950829653  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.2527826537851166  - accuracy: 0.6875\n",
      "At: 221 [==========>] Loss 0.14878734395411386  - accuracy: 0.78125\n",
      "At: 222 [==========>] Loss 0.1321526020958252  - accuracy: 0.8125\n",
      "At: 223 [==========>] Loss 0.2216426318088996  - accuracy: 0.6875\n",
      "At: 224 [==========>] Loss 0.18272315972691716  - accuracy: 0.8125\n",
      "At: 225 [==========>] Loss 0.14750505086384283  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.204285628662716  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.1968647495823702  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.18946642473968167  - accuracy: 0.78125\n",
      "At: 229 [==========>] Loss 0.16970171000813178  - accuracy: 0.75\n",
      "At: 230 [==========>] Loss 0.16272995134537524  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.23730052315205824  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.23077215954156716  - accuracy: 0.6875\n",
      "At: 233 [==========>] Loss 0.1901425470168971  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.15432989500718317  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.22853343085759592  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.20853797024920045  - accuracy: 0.71875\n",
      "At: 237 [==========>] Loss 0.08548116067847814  - accuracy: 0.90625\n",
      "At: 238 [==========>] Loss 0.10658100161536475  - accuracy: 0.84375\n",
      "At: 239 [==========>] Loss 0.17833799019189178  - accuracy: 0.8125\n",
      "At: 240 [==========>] Loss 0.18866990415182483  - accuracy: 0.75\n",
      "At: 241 [==========>] Loss 0.189745050361734  - accuracy: 0.78125\n",
      "At: 242 [==========>] Loss 0.17176212045105935  - accuracy: 0.75\n",
      "At: 243 [==========>] Loss 0.1267201085495304  - accuracy: 0.875\n",
      "At: 244 [==========>] Loss 0.1448734105397882  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.16750545051060278  - accuracy: 0.8125\n",
      "At: 246 [==========>] Loss 0.16381104173763786  - accuracy: 0.8125\n",
      "At: 247 [==========>] Loss 0.13462189990413787  - accuracy: 0.84375\n",
      "At: 248 [==========>] Loss 0.13635608191517007  - accuracy: 0.84375\n",
      "At: 249 [==========>] Loss 0.09016754771194244  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.23527630418405876  - accuracy: 0.65625\n",
      "At: 251 [==========>] Loss 0.21217684530654723  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.10449723028471913  - accuracy: 0.875\n",
      "At: 253 [==========>] Loss 0.16274318647328523  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.10156138649475206  - accuracy: 0.90625\n",
      "At: 255 [==========>] Loss 0.15672000240246828  - accuracy: 0.84375\n",
      "At: 256 [==========>] Loss 0.20064960371142343  - accuracy: 0.75\n",
      "At: 257 [==========>] Loss 0.10939837710105983  - accuracy: 0.875\n",
      "At: 258 [==========>] Loss 0.1750792620485528  - accuracy: 0.75\n",
      "At: 259 [==========>] Loss 0.16873437008741182  - accuracy: 0.8125\n",
      "At: 260 [==========>] Loss 0.11661297622067554  - accuracy: 0.78125\n",
      "At: 261 [==========>] Loss 0.0804387211559877  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.16110400418273918  - accuracy: 0.84375\n",
      "At: 263 [==========>] Loss 0.10215099014492479  - accuracy: 0.84375\n",
      "At: 264 [==========>] Loss 0.12889458388620442  - accuracy: 0.875\n",
      "At: 265 [==========>] Loss 0.2118215385470834  - accuracy: 0.75\n",
      "At: 266 [==========>] Loss 0.2865797517944296  - accuracy: 0.625\n",
      "At: 267 [==========>] Loss 0.16085631283878257  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.21204203367999264  - accuracy: 0.71875\n",
      "At: 269 [==========>] Loss 0.14089228376062474  - accuracy: 0.84375\n",
      "At: 270 [==========>] Loss 0.28883164025415536  - accuracy: 0.5625\n",
      "At: 271 [==========>] Loss 0.20099043758885532  - accuracy: 0.75\n",
      "At: 272 [==========>] Loss 0.09507818981492983  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.18657972922144206  - accuracy: 0.78125\n",
      "At: 274 [==========>] Loss 0.1758917258238936  - accuracy: 0.75\n",
      "At: 275 [==========>] Loss 0.06715299463465635  - accuracy: 0.9375\n",
      "At: 276 [==========>] Loss 0.22126934858623304  - accuracy: 0.6875\n",
      "At: 277 [==========>] Loss 0.11400872404438257  - accuracy: 0.8125\n",
      "At: 278 [==========>] Loss 0.13645519331310835  - accuracy: 0.8125\n",
      "At: 279 [==========>] Loss 0.15304928044829325  - accuracy: 0.8125\n",
      "At: 280 [==========>] Loss 0.15976592352498264  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.14823067270952947  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.248797696068455  - accuracy: 0.6875\n",
      "At: 283 [==========>] Loss 0.1626791802234964  - accuracy: 0.78125\n",
      "At: 284 [==========>] Loss 0.09860478842191082  - accuracy: 0.875\n",
      "At: 285 [==========>] Loss 0.14339451692637045  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.07007644856152627  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.23425464787165895  - accuracy: 0.71875\n",
      "At: 288 [==========>] Loss 0.1302277718756175  - accuracy: 0.84375\n",
      "At: 289 [==========>] Loss 0.11025067428606312  - accuracy: 0.875\n",
      "At: 290 [==========>] Loss 0.12856509109009237  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.16576273300681255  - accuracy: 0.8125\n",
      "At: 292 [==========>] Loss 0.15088741119481058  - accuracy: 0.8125\n",
      "At: 293 [==========>] Loss 0.23867799734809533  - accuracy: 0.65625\n",
      "At: 294 [==========>] Loss 0.16141031085593782  - accuracy: 0.8125\n",
      "At: 295 [==========>] Loss 0.14011649404154888  - accuracy: 0.8125\n",
      "At: 296 [==========>] Loss 0.10684457448559184  - accuracy: 0.90625\n",
      "At: 297 [==========>] Loss 0.11725808846498102  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.14776833398290362  - accuracy: 0.84375\n",
      "At: 299 [==========>] Loss 0.23720070447451788  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.17859025736510895  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.16553509981350958  - accuracy: 0.78125\n",
      "At: 302 [==========>] Loss 0.1417014167466491  - accuracy: 0.875\n",
      "At: 303 [==========>] Loss 0.10717504115762735  - accuracy: 0.90625\n",
      "At: 304 [==========>] Loss 0.15014967503609103  - accuracy: 0.78125\n",
      "At: 305 [==========>] Loss 0.23299236613649538  - accuracy: 0.65625\n",
      "At: 306 [==========>] Loss 0.1538956838415948  - accuracy: 0.8125\n",
      "At: 307 [==========>] Loss 0.2518480089669246  - accuracy: 0.65625\n",
      "At: 308 [==========>] Loss 0.1702215150612363  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.1213499317899199  - accuracy: 0.875\n",
      "At: 310 [==========>] Loss 0.2027076255510449  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.10648828443405875  - accuracy: 0.84375\n",
      "At: 312 [==========>] Loss 0.14976617210527743  - accuracy: 0.8125\n",
      "At: 313 [==========>] Loss 0.13352262609714177  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.23838976116518723  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.15652222004562955  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.17985447762697868  - accuracy: 0.75\n",
      "At: 317 [==========>] Loss 0.2829218749740806  - accuracy: 0.59375\n",
      "At: 318 [==========>] Loss 0.14462332079012893  - accuracy: 0.84375\n",
      "At: 319 [==========>] Loss 0.08376921483414082  - accuracy: 0.90625\n",
      "At: 320 [==========>] Loss 0.2439786970311047  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.2019549708711004  - accuracy: 0.75\n",
      "At: 322 [==========>] Loss 0.08533064502162807  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.10400488450358791  - accuracy: 0.90625\n",
      "At: 324 [==========>] Loss 0.15107349013290583  - accuracy: 0.8125\n",
      "At: 325 [==========>] Loss 0.12395360189723752  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.19984596495639118  - accuracy: 0.75\n",
      "At: 327 [==========>] Loss 0.12978431645945837  - accuracy: 0.875\n",
      "At: 328 [==========>] Loss 0.1320137072481011  - accuracy: 0.875\n",
      "At: 329 [==========>] Loss 0.13336432533963488  - accuracy: 0.84375\n",
      "At: 330 [==========>] Loss 0.16308033009495831  - accuracy: 0.78125\n",
      "At: 331 [==========>] Loss 0.18530575544860964  - accuracy: 0.75\n",
      "At: 332 [==========>] Loss 0.26515465972664914  - accuracy: 0.5625\n",
      "At: 333 [==========>] Loss 0.1616781214934539  - accuracy: 0.78125\n",
      "At: 334 [==========>] Loss 0.10556926077250355  - accuracy: 0.875\n",
      "At: 335 [==========>] Loss 0.16170681097886852  - accuracy: 0.8125\n",
      "At: 336 [==========>] Loss 0.13883040298956278  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.20510208781254768  - accuracy: 0.75\n",
      "At: 338 [==========>] Loss 0.15223301041249926  - accuracy: 0.84375\n",
      "At: 339 [==========>] Loss 0.14547812304358648  - accuracy: 0.8125\n",
      "At: 340 [==========>] Loss 0.12098367393718096  - accuracy: 0.875\n",
      "At: 341 [==========>] Loss 0.10406329010908524  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.14026059839233315  - accuracy: 0.84375\n",
      "At: 343 [==========>] Loss 0.26992559004075234  - accuracy: 0.625\n",
      "At: 344 [==========>] Loss 0.1512688669977721  - accuracy: 0.78125\n",
      "At: 345 [==========>] Loss 0.1946150494205912  - accuracy: 0.6875\n",
      "At: 346 [==========>] Loss 0.14133080614019883  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.1157468730588981  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.14439126228241028  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.19929548166528263  - accuracy: 0.71875\n",
      "At: 350 [==========>] Loss 0.1058053993218288  - accuracy: 0.84375\n",
      "At: 351 [==========>] Loss 0.2639408165113798  - accuracy: 0.625\n",
      "At: 352 [==========>] Loss 0.15853275775119008  - accuracy: 0.8125\n",
      "At: 353 [==========>] Loss 0.1472834159045375  - accuracy: 0.84375\n",
      "At: 354 [==========>] Loss 0.22763490552596677  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.07505701549129787  - accuracy: 0.90625\n",
      "At: 356 [==========>] Loss 0.1993191794563973  - accuracy: 0.71875\n",
      "At: 357 [==========>] Loss 0.12310473662780187  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.18177478180520704  - accuracy: 0.78125\n",
      "At: 359 [==========>] Loss 0.10502618883204098  - accuracy: 0.90625\n",
      "At: 360 [==========>] Loss 0.16899476691658985  - accuracy: 0.71875\n",
      "At: 361 [==========>] Loss 0.10890402458444101  - accuracy: 0.9375\n",
      "At: 362 [==========>] Loss 0.1464401790460066  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.10048859920584197  - accuracy: 0.875\n",
      "At: 364 [==========>] Loss 0.20414351853048518  - accuracy: 0.75\n",
      "At: 365 [==========>] Loss 0.1358852881187699  - accuracy: 0.8125\n",
      "At: 366 [==========>] Loss 0.1914637542084362  - accuracy: 0.6875\n",
      "At: 367 [==========>] Loss 0.18549065644974694  - accuracy: 0.75\n",
      "At: 368 [==========>] Loss 0.18485211439850635  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.14223437352399118  - accuracy: 0.8125\n",
      "At: 370 [==========>] Loss 0.18387085143670104  - accuracy: 0.71875\n",
      "At: 371 [==========>] Loss 0.08335934878938558  - accuracy: 0.90625\n",
      "At: 372 [==========>] Loss 0.13700498429811853  - accuracy: 0.84375\n",
      "At: 373 [==========>] Loss 0.19823091710521512  - accuracy: 0.71875\n",
      "At: 374 [==========>] Loss 0.10057695591028096  - accuracy: 0.90625\n",
      "At: 375 [==========>] Loss 0.16353201793285643  - accuracy: 0.78125\n",
      "At: 376 [==========>] Loss 0.09428529085742018  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.23221795435171447  - accuracy: 0.75\n",
      "At: 378 [==========>] Loss 0.1653317570019786  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.15972147214642954  - accuracy: 0.8125\n",
      "At: 380 [==========>] Loss 0.142549275652064  - accuracy: 0.8125\n",
      "At: 381 [==========>] Loss 0.18046064928625657  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.09705850517928624  - accuracy: 0.875\n",
      "At: 383 [==========>] Loss 0.23045582551246097  - accuracy: 0.6875\n",
      "At: 384 [==========>] Loss 0.14217670410883546  - accuracy: 0.8125\n",
      "At: 385 [==========>] Loss 0.12739751598433952  - accuracy: 0.84375\n",
      "At: 386 [==========>] Loss 0.20141457934596904  - accuracy: 0.71875\n",
      "At: 387 [==========>] Loss 0.09207997446007843  - accuracy: 0.84375\n",
      "At: 388 [==========>] Loss 0.21328513722378512  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.19790600850777287  - accuracy: 0.71875\n",
      "At: 390 [==========>] Loss 0.1125951446996172  - accuracy: 0.78125\n",
      "At: 391 [==========>] Loss 0.16373247707491206  - accuracy: 0.78125\n",
      "At: 392 [==========>] Loss 0.1618879186019993  - accuracy: 0.8125\n",
      "At: 393 [==========>] Loss 0.26163690071411205  - accuracy: 0.6875\n",
      "At: 394 [==========>] Loss 0.1068037633695032  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.17154426351463903  - accuracy: 0.78125\n",
      "At: 396 [==========>] Loss 0.17338327540667314  - accuracy: 0.78125\n",
      "At: 397 [==========>] Loss 0.10132130518312286  - accuracy: 0.84375\n",
      "At: 398 [==========>] Loss 0.18838269089344306  - accuracy: 0.71875\n",
      "At: 399 [==========>] Loss 0.22842454449709038  - accuracy: 0.71875\n",
      "At: 400 [==========>] Loss 0.1853487101431715  - accuracy: 0.71875\n",
      "At: 401 [==========>] Loss 0.12711048985705029  - accuracy: 0.875\n",
      "At: 402 [==========>] Loss 0.09292481355233587  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.044173270258481806  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.11908734335351419  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.16781303548154458  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.16806233731839432  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.15512872881310136  - accuracy: 0.78125\n",
      "At: 408 [==========>] Loss 0.20470541316768665  - accuracy: 0.71875\n",
      "At: 409 [==========>] Loss 0.21552447311982506  - accuracy: 0.65625\n",
      "At: 410 [==========>] Loss 0.1447418142215502  - accuracy: 0.8125\n",
      "At: 411 [==========>] Loss 0.07584793012360619  - accuracy: 0.9375\n",
      "At: 412 [==========>] Loss 0.1685619723736111  - accuracy: 0.75\n",
      "At: 413 [==========>] Loss 0.11297327016045894  - accuracy: 0.875\n",
      "At: 414 [==========>] Loss 0.16165179755526266  - accuracy: 0.78125\n",
      "At: 415 [==========>] Loss 0.18967644270512476  - accuracy: 0.78125\n",
      "At: 416 [==========>] Loss 0.19999957827088993  - accuracy: 0.75\n",
      "At: 417 [==========>] Loss 0.15938078537765873  - accuracy: 0.78125\n",
      "At: 418 [==========>] Loss 0.13961979474633496  - accuracy: 0.8125\n",
      "At: 419 [==========>] Loss 0.11097404792419391  - accuracy: 0.84375\n",
      "At: 420 [==========>] Loss 0.2036992574421238  - accuracy: 0.6875\n",
      "At: 421 [==========>] Loss 0.11828898621140696  - accuracy: 0.84375\n",
      "At: 422 [==========>] Loss 0.14416195900876028  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.1208147487128903  - accuracy: 0.875\n",
      "At: 424 [==========>] Loss 0.2278192074172827  - accuracy: 0.6875\n",
      "At: 425 [==========>] Loss 0.22611889240355346  - accuracy: 0.65625\n",
      "At: 426 [==========>] Loss 0.13909899718026414  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.21248676686495682  - accuracy: 0.71875\n",
      "At: 428 [==========>] Loss 0.21527472240800077  - accuracy: 0.6875\n",
      "At: 429 [==========>] Loss 0.17492281529489076  - accuracy: 0.75\n",
      "At: 430 [==========>] Loss 0.12505121079158907  - accuracy: 0.875\n",
      "At: 431 [==========>] Loss 0.09750961158268873  - accuracy: 0.9375\n",
      "At: 432 [==========>] Loss 0.16742031878137065  - accuracy: 0.78125\n",
      "At: 433 [==========>] Loss 0.11757788589086017  - accuracy: 0.84375\n",
      "At: 434 [==========>] Loss 0.13494579605388857  - accuracy: 0.8125\n",
      "At: 435 [==========>] Loss 0.20352445443604672  - accuracy: 0.75\n",
      "At: 436 [==========>] Loss 0.18430937561230593  - accuracy: 0.75\n",
      "At: 437 [==========>] Loss 0.16416260244570346  - accuracy: 0.8125\n",
      "At: 438 [==========>] Loss 0.1480068997843721  - accuracy: 0.8125\n",
      "At: 439 [==========>] Loss 0.11906668004415137  - accuracy: 0.90625\n",
      "At: 440 [==========>] Loss 0.0919130445854762  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.17965005052423438  - accuracy: 0.75\n",
      "At: 442 [==========>] Loss 0.20568294419072877  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.14889327997715301  - accuracy: 0.78125\n",
      "At: 444 [==========>] Loss 0.21279442021579031  - accuracy: 0.65625\n",
      "At: 445 [==========>] Loss 0.13600336272159053  - accuracy: 0.78125\n",
      "At: 446 [==========>] Loss 0.2467647278872453  - accuracy: 0.65625\n",
      "At: 447 [==========>] Loss 0.18368574307148666  - accuracy: 0.78125\n",
      "At: 448 [==========>] Loss 0.18781775122848487  - accuracy: 0.71875\n",
      "At: 449 [==========>] Loss 0.15606513080304568  - accuracy: 0.75\n",
      "At: 450 [==========>] Loss 0.1832967877372076  - accuracy: 0.75\n",
      "At: 451 [==========>] Loss 0.13636524506631637  - accuracy: 0.78125\n",
      "At: 452 [==========>] Loss 0.15855872252689113  - accuracy: 0.78125\n",
      "At: 453 [==========>] Loss 0.11219344408679519  - accuracy: 0.84375\n",
      "At: 454 [==========>] Loss 0.20100861006765608  - accuracy: 0.71875\n",
      "At: 455 [==========>] Loss 0.18196610149999703  - accuracy: 0.8125\n",
      "At: 456 [==========>] Loss 0.13559493564229103  - accuracy: 0.84375\n",
      "At: 457 [==========>] Loss 0.15709134753428783  - accuracy: 0.78125\n",
      "At: 458 [==========>] Loss 0.08978349344422536  - accuracy: 0.90625\n",
      "At: 459 [==========>] Loss 0.23372164485167207  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.15607471625709313  - accuracy: 0.78125\n",
      "At: 461 [==========>] Loss 0.2216075371925356  - accuracy: 0.71875\n",
      "At: 462 [==========>] Loss 0.1438737596452334  - accuracy: 0.8125\n",
      "At: 463 [==========>] Loss 0.13592110893160508  - accuracy: 0.84375\n",
      "At: 464 [==========>] Loss 0.178172092593416  - accuracy: 0.8125\n",
      "At: 465 [==========>] Loss 0.22830266832595855  - accuracy: 0.65625\n",
      "At: 466 [==========>] Loss 0.1391089533014373  - accuracy: 0.8125\n",
      "At: 467 [==========>] Loss 0.17954172006279  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.13797329029079392  - accuracy: 0.84375\n",
      "At: 469 [==========>] Loss 0.1518757984988441  - accuracy: 0.8125\n",
      "At: 470 [==========>] Loss 0.13143850828287668  - accuracy: 0.84375\n",
      "At: 471 [==========>] Loss 0.1941332695854152  - accuracy: 0.75\n",
      "At: 472 [==========>] Loss 0.12202878428896448  - accuracy: 0.875\n",
      "At: 473 [==========>] Loss 0.15054665684195348  - accuracy: 0.84375\n",
      "At: 474 [==========>] Loss 0.17668801428209466  - accuracy: 0.8125\n",
      "At: 475 [==========>] Loss 0.1186186304289528  - accuracy: 0.84375\n",
      "At: 476 [==========>] Loss 0.1616521305987837  - accuracy: 0.78125\n",
      "At: 477 [==========>] Loss 0.11465149818815887  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.1068644732324926  - accuracy: 0.90625\n",
      "At: 479 [==========>] Loss 0.22695212131222917  - accuracy: 0.6875\n",
      "At: 480 [==========>] Loss 0.1791988676535527  - accuracy: 0.75\n",
      "At: 481 [==========>] Loss 0.12834278253046397  - accuracy: 0.875\n",
      "At: 482 [==========>] Loss 0.10148258183243829  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.11749031976449023  - accuracy: 0.875\n",
      "At: 484 [==========>] Loss 0.08340446416821974  - accuracy: 0.90625\n",
      "At: 485 [==========>] Loss 0.16543053820740858  - accuracy: 0.8125\n",
      "At: 486 [==========>] Loss 0.16346031318512164  - accuracy: 0.8125\n",
      "At: 487 [==========>] Loss 0.14590038813387907  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.1095381209587051  - accuracy: 0.84375\n",
      "At: 489 [==========>] Loss 0.1538018225696856  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.14263800848478675  - accuracy: 0.8125\n",
      "At: 491 [==========>] Loss 0.16682238757176127  - accuracy: 0.78125\n",
      "At: 492 [==========>] Loss 0.21797857056630568  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.16548523933883053  - accuracy: 0.8125\n",
      "At: 494 [==========>] Loss 0.17045877785790708  - accuracy: 0.78125\n",
      "At: 495 [==========>] Loss 0.1420848077641002  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.16625294859686476  - accuracy: 0.8125\n",
      "At: 497 [==========>] Loss 0.1635542956581846  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.07970967603807688  - accuracy: 0.90625\n",
      "At: 499 [==========>] Loss 0.1392777243813924  - accuracy: 0.8125\n",
      "At: 500 [==========>] Loss 0.14718870677225865  - accuracy: 0.8125\n",
      "At: 501 [==========>] Loss 0.1703143880398786  - accuracy: 0.75\n",
      "At: 502 [==========>] Loss 0.12743459162068388  - accuracy: 0.84375\n",
      "At: 503 [==========>] Loss 0.11584142359769918  - accuracy: 0.84375\n",
      "At: 504 [==========>] Loss 0.13042524865945035  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.17252465373711973  - accuracy: 0.78125\n",
      "At: 506 [==========>] Loss 0.23981227641096425  - accuracy: 0.625\n",
      "At: 507 [==========>] Loss 0.09726141966799214  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.11201314581373215  - accuracy: 0.875\n",
      "At: 509 [==========>] Loss 0.2195841760099253  - accuracy: 0.71875\n",
      "At: 510 [==========>] Loss 0.17975847614478885  - accuracy: 0.71875\n",
      "At: 511 [==========>] Loss 0.14940101050223098  - accuracy: 0.78125\n",
      "At: 512 [==========>] Loss 0.14486237311403669  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.21634443263963388  - accuracy: 0.75\n",
      "At: 514 [==========>] Loss 0.21354602038564469  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.09713211406034511  - accuracy: 0.90625\n",
      "At: 516 [==========>] Loss 0.1958220166598599  - accuracy: 0.71875\n",
      "At: 517 [==========>] Loss 0.1321750706294808  - accuracy: 0.84375\n",
      "At: 518 [==========>] Loss 0.15426868586989384  - accuracy: 0.84375\n",
      "At: 519 [==========>] Loss 0.11730014037958378  - accuracy: 0.78125\n",
      "At: 520 [==========>] Loss 0.11747909783376978  - accuracy: 0.875\n",
      "At: 521 [==========>] Loss 0.14976263399590545  - accuracy: 0.84375\n",
      "At: 522 [==========>] Loss 0.14834033236253027  - accuracy: 0.8125\n",
      "At: 523 [==========>] Loss 0.17620805435649542  - accuracy: 0.78125\n",
      "At: 524 [==========>] Loss 0.11208500121760467  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.1659477763951738  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.14906579807439718  - accuracy: 0.8125\n",
      "At: 527 [==========>] Loss 0.24936918203211927  - accuracy: 0.59375\n",
      "At: 528 [==========>] Loss 0.18523191156207486  - accuracy: 0.75\n",
      "At: 529 [==========>] Loss 0.12520404640528238  - accuracy: 0.84375\n",
      "At: 530 [==========>] Loss 0.19637864796713522  - accuracy: 0.75\n",
      "At: 531 [==========>] Loss 0.13739264917450592  - accuracy: 0.78125\n",
      "At: 532 [==========>] Loss 0.12466580200976801  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.1437746174646658  - accuracy: 0.78125\n",
      "At: 534 [==========>] Loss 0.20331808350173602  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.13462863697216554  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.17120245665277617  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.10566278576457414  - accuracy: 0.875\n",
      "At: 538 [==========>] Loss 0.14175403891441807  - accuracy: 0.78125\n",
      "At: 539 [==========>] Loss 0.13535098688944014  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.28551759600188864  - accuracy: 0.59375\n",
      "At: 541 [==========>] Loss 0.18181467329299783  - accuracy: 0.71875\n",
      "At: 542 [==========>] Loss 0.14487494132804402  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.15435104064981875  - accuracy: 0.84375\n",
      "At: 544 [==========>] Loss 0.21352828843599186  - accuracy: 0.6875\n",
      "At: 545 [==========>] Loss 0.08831965849248542  - accuracy: 0.9375\n",
      "At: 546 [==========>] Loss 0.15484854085094846  - accuracy: 0.8125\n",
      "At: 547 [==========>] Loss 0.15273715114928135  - accuracy: 0.75\n",
      "At: 548 [==========>] Loss 0.0925674045583351  - accuracy: 0.90625\n",
      "At: 549 [==========>] Loss 0.129803120355661  - accuracy: 0.78125\n",
      "At: 550 [==========>] Loss 0.08370145343251564  - accuracy: 0.90625\n",
      "At: 551 [==========>] Loss 0.11277004292190476  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.1726370890156646  - accuracy: 0.78125\n",
      "At: 553 [==========>] Loss 0.10651683172830062  - accuracy: 0.875\n",
      "At: 554 [==========>] Loss 0.06323497896473278  - accuracy: 0.9375\n",
      "At: 555 [==========>] Loss 0.15116257725717386  - accuracy: 0.78125\n",
      "At: 556 [==========>] Loss 0.13955026924851915  - accuracy: 0.8125\n",
      "At: 557 [==========>] Loss 0.15875316115965898  - accuracy: 0.75\n",
      "At: 558 [==========>] Loss 0.12903217567373465  - accuracy: 0.84375\n",
      "At: 559 [==========>] Loss 0.18568748509857966  - accuracy: 0.75\n",
      "At: 560 [==========>] Loss 0.11827856748926424  - accuracy: 0.84375\n",
      "At: 561 [==========>] Loss 0.14008322563062517  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.08596619909237461  - accuracy: 0.90625\n",
      "At: 563 [==========>] Loss 0.11594012355187348  - accuracy: 0.875\n",
      "At: 564 [==========>] Loss 0.1857881907525612  - accuracy: 0.6875\n",
      "At: 565 [==========>] Loss 0.10868213749293901  - accuracy: 0.875\n",
      "At: 566 [==========>] Loss 0.18255633682677874  - accuracy: 0.75\n",
      "At: 567 [==========>] Loss 0.1779347527615846  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.21279966932339905  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.17879633756634167  - accuracy: 0.75\n",
      "At: 570 [==========>] Loss 0.09818834070173876  - accuracy: 0.84375\n",
      "At: 571 [==========>] Loss 0.11363014502417622  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.12163460256315893  - accuracy: 0.84375\n",
      "At: 573 [==========>] Loss 0.0910162585060684  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.14800808400002874  - accuracy: 0.78125\n",
      "At: 575 [==========>] Loss 0.1278079196709037  - accuracy: 0.84375\n",
      "At: 576 [==========>] Loss 0.09415246834839805  - accuracy: 0.875\n",
      "At: 577 [==========>] Loss 0.17610250412026313  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.1566399794348904  - accuracy: 0.71875\n",
      "At: 579 [==========>] Loss 0.09821785853593218  - accuracy: 0.84375\n",
      "At: 580 [==========>] Loss 0.13694076689943507  - accuracy: 0.84375\n",
      "At: 581 [==========>] Loss 0.11369315090782227  - accuracy: 0.875\n",
      "At: 582 [==========>] Loss 0.1556840661233657  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.17337537703873912  - accuracy: 0.75\n",
      "At: 584 [==========>] Loss 0.12473072755713419  - accuracy: 0.8125\n",
      "At: 585 [==========>] Loss 0.19000945863785373  - accuracy: 0.65625\n",
      "At: 586 [==========>] Loss 0.08822244298715895  - accuracy: 0.875\n",
      "At: 587 [==========>] Loss 0.1724319843292585  - accuracy: 0.75\n",
      "At: 588 [==========>] Loss 0.16179824158456507  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.1685965231848237  - accuracy: 0.75\n",
      "At: 590 [==========>] Loss 0.09632988189925745  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.17432383992166184  - accuracy: 0.78125\n",
      "At: 592 [==========>] Loss 0.08764662074584519  - accuracy: 0.9375\n",
      "At: 593 [==========>] Loss 0.16124954383848383  - accuracy: 0.71875\n",
      "At: 594 [==========>] Loss 0.16772332800124587  - accuracy: 0.75\n",
      "At: 595 [==========>] Loss 0.11843830920851556  - accuracy: 0.875\n",
      "At: 596 [==========>] Loss 0.09830177118501313  - accuracy: 0.9375\n",
      "At: 597 [==========>] Loss 0.1852281402876107  - accuracy: 0.75\n",
      "At: 598 [==========>] Loss 0.15235555960254624  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.13383776514189483  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.09786998305524225  - accuracy: 0.875\n",
      "At: 601 [==========>] Loss 0.1352623444429446  - accuracy: 0.875\n",
      "At: 602 [==========>] Loss 0.13771485287698088  - accuracy: 0.84375\n",
      "At: 603 [==========>] Loss 0.14513898823571073  - accuracy: 0.8125\n",
      "At: 604 [==========>] Loss 0.25566219388430533  - accuracy: 0.5625\n",
      "At: 605 [==========>] Loss 0.10197471177343453  - accuracy: 0.90625\n",
      "At: 606 [==========>] Loss 0.17493800433705728  - accuracy: 0.71875\n",
      "At: 607 [==========>] Loss 0.14289819779392948  - accuracy: 0.8125\n",
      "At: 608 [==========>] Loss 0.1541444141397641  - accuracy: 0.75\n",
      "At: 609 [==========>] Loss 0.1073172714227629  - accuracy: 0.875\n",
      "At: 610 [==========>] Loss 0.17326568008744841  - accuracy: 0.71875\n",
      "At: 611 [==========>] Loss 0.10861516591809267  - accuracy: 0.84375\n",
      "At: 612 [==========>] Loss 0.15999733632015697  - accuracy: 0.8125\n",
      "At: 613 [==========>] Loss 0.15098928421067442  - accuracy: 0.78125\n",
      "At: 614 [==========>] Loss 0.11959174698189468  - accuracy: 0.90625\n",
      "At: 615 [==========>] Loss 0.17893162375335242  - accuracy: 0.8125\n",
      "At: 616 [==========>] Loss 0.16426101385594183  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.15629629695748357  - accuracy: 0.78125\n",
      "At: 618 [==========>] Loss 0.19531169132268317  - accuracy: 0.6875\n",
      "At: 619 [==========>] Loss 0.1374938907071803  - accuracy: 0.8125\n",
      "At: 620 [==========>] Loss 0.14587807305625172  - accuracy: 0.8125\n",
      "At: 621 [==========>] Loss 0.08965613436819347  - accuracy: 0.9375\n",
      "At: 622 [==========>] Loss 0.16433145568176985  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.13652109120254968  - accuracy: 0.84375\n",
      "At: 624 [==========>] Loss 0.11096218041718738  - accuracy: 0.875\n",
      "At: 625 [==========>] Loss 0.14677944039742985  - accuracy: 0.75\n",
      "At: 626 [==========>] Loss 0.12711879737528653  - accuracy: 0.84375\n",
      "At: 627 [==========>] Loss 0.09754984312272322  - accuracy: 0.875\n",
      "At: 628 [==========>] Loss 0.1297370911307293  - accuracy: 0.8125\n",
      "At: 629 [==========>] Loss 0.19911688636819236  - accuracy: 0.6875\n",
      "At: 630 [==========>] Loss 0.23099731731323309  - accuracy: 0.6875\n",
      "At: 631 [==========>] Loss 0.1537723977607587  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.14813354351292235  - accuracy: 0.78125\n",
      "At: 633 [==========>] Loss 0.18373227630702288  - accuracy: 0.75\n",
      "At: 634 [==========>] Loss 0.16017400941552834  - accuracy: 0.8125\n",
      "At: 635 [==========>] Loss 0.1141633595130977  - accuracy: 0.84375\n",
      "At: 636 [==========>] Loss 0.1320513391407328  - accuracy: 0.84375\n",
      "At: 637 [==========>] Loss 0.1600356426146971  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.11340498400382681  - accuracy: 0.84375\n",
      "At: 639 [==========>] Loss 0.11865338388848727  - accuracy: 0.8125\n",
      "At: 640 [==========>] Loss 0.22245786513548632  - accuracy: 0.6875\n",
      "At: 641 [==========>] Loss 0.14134481551687095  - accuracy: 0.78125\n",
      "At: 642 [==========>] Loss 0.1778248512788151  - accuracy: 0.71875\n",
      "At: 643 [==========>] Loss 0.17697309430276986  - accuracy: 0.6875\n",
      "At: 644 [==========>] Loss 0.11095621820506339  - accuracy: 0.84375\n",
      "At: 645 [==========>] Loss 0.13823574497871896  - accuracy: 0.84375\n",
      "At: 646 [==========>] Loss 0.13229690829396812  - accuracy: 0.84375\n",
      "At: 647 [==========>] Loss 0.13270803367835854  - accuracy: 0.78125\n",
      "At: 648 [==========>] Loss 0.1451079515228327  - accuracy: 0.78125\n",
      "At: 649 [==========>] Loss 0.1762413725307466  - accuracy: 0.8125\n",
      "At: 650 [==========>] Loss 0.11373814618414385  - accuracy: 0.84375\n",
      "At: 651 [==========>] Loss 0.1566888332747547  - accuracy: 0.78125\n",
      "At: 652 [==========>] Loss 0.15336300551416035  - accuracy: 0.78125\n",
      "At: 653 [==========>] Loss 0.1203521359430825  - accuracy: 0.8125\n",
      "At: 654 [==========>] Loss 0.11637754682310414  - accuracy: 0.875\n",
      "At: 655 [==========>] Loss 0.15680331265048778  - accuracy: 0.8125\n",
      "At: 656 [==========>] Loss 0.12847914457518972  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.18667071353938108  - accuracy: 0.6875\n",
      "At: 658 [==========>] Loss 0.11662701386791521  - accuracy: 0.875\n",
      "At: 659 [==========>] Loss 0.14270457518562227  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.0947479157723379  - accuracy: 0.90625\n",
      "At: 661 [==========>] Loss 0.127672672969468  - accuracy: 0.84375\n",
      "At: 662 [==========>] Loss 0.11143995505833462  - accuracy: 0.84375\n",
      "At: 663 [==========>] Loss 0.1173066985827623  - accuracy: 0.84375\n",
      "At: 664 [==========>] Loss 0.13128271579221193  - accuracy: 0.75\n",
      "At: 665 [==========>] Loss 0.18164585781381626  - accuracy: 0.71875\n",
      "At: 666 [==========>] Loss 0.1749697587961351  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.12898067840441196  - accuracy: 0.8125\n",
      "At: 668 [==========>] Loss 0.1693840514081148  - accuracy: 0.75\n",
      "At: 669 [==========>] Loss 0.1338140000399151  - accuracy: 0.84375\n",
      "At: 670 [==========>] Loss 0.18932016675142832  - accuracy: 0.6875\n",
      "At: 671 [==========>] Loss 0.10977068299827057  - accuracy: 0.84375\n",
      "At: 672 [==========>] Loss 0.13642331437142585  - accuracy: 0.84375\n",
      "At: 673 [==========>] Loss 0.07869143846878689  - accuracy: 0.9375\n",
      "At: 674 [==========>] Loss 0.11393173761168365  - accuracy: 0.875\n",
      "At: 675 [==========>] Loss 0.12513037685783948  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.15312343994752364  - accuracy: 0.75\n",
      "At: 677 [==========>] Loss 0.15718003338532965  - accuracy: 0.78125\n",
      "At: 678 [==========>] Loss 0.12534887567850372  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.11093761226260489  - accuracy: 0.8125\n",
      "At: 680 [==========>] Loss 0.1259552930771644  - accuracy: 0.875\n",
      "At: 681 [==========>] Loss 0.13252974384385866  - accuracy: 0.78125\n",
      "At: 682 [==========>] Loss 0.1482769465946171  - accuracy: 0.75\n",
      "At: 683 [==========>] Loss 0.18595078665526144  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.11485390884156708  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.13690279248584147  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.09892511730590316  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.07256543467872005  - accuracy: 0.90625\n",
      "At: 688 [==========>] Loss 0.09520114869381399  - accuracy: 0.90625\n",
      "At: 689 [==========>] Loss 0.18830439986160025  - accuracy: 0.75\n",
      "At: 690 [==========>] Loss 0.14546860126256272  - accuracy: 0.78125\n",
      "At: 691 [==========>] Loss 0.10816199595846585  - accuracy: 0.8125\n",
      "At: 692 [==========>] Loss 0.13022684143878133  - accuracy: 0.8125\n",
      "At: 693 [==========>] Loss 0.1424059309387185  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.2027793428003837  - accuracy: 0.71875\n",
      "At: 695 [==========>] Loss 0.16204476895422285  - accuracy: 0.78125\n",
      "At: 696 [==========>] Loss 0.16321651147311403  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.20824344490429642  - accuracy: 0.71875\n",
      "At: 698 [==========>] Loss 0.12248760542226282  - accuracy: 0.875\n",
      "At: 699 [==========>] Loss 0.11618095089801928  - accuracy: 0.84375\n",
      "At: 700 [==========>] Loss 0.10836977731278138  - accuracy: 0.875\n",
      "At: 701 [==========>] Loss 0.15014982612141062  - accuracy: 0.8125\n",
      "At: 702 [==========>] Loss 0.09921179628157006  - accuracy: 0.90625\n",
      "At: 703 [==========>] Loss 0.1429832500115448  - accuracy: 0.75\n",
      "At: 704 [==========>] Loss 0.1475182823850819  - accuracy: 0.78125\n",
      "At: 705 [==========>] Loss 0.20378607241919752  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.1362759806544624  - accuracy: 0.78125\n",
      "At: 707 [==========>] Loss 0.11496696091442732  - accuracy: 0.8125\n",
      "At: 708 [==========>] Loss 0.15742618536153175  - accuracy: 0.75\n",
      "At: 709 [==========>] Loss 0.2034690521280912  - accuracy: 0.71875\n",
      "At: 710 [==========>] Loss 0.14750130386203963  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.21840051297386298  - accuracy: 0.71875\n",
      "At: 712 [==========>] Loss 0.18335924959474187  - accuracy: 0.75\n",
      "At: 713 [==========>] Loss 0.1928909746325546  - accuracy: 0.65625\n",
      "At: 714 [==========>] Loss 0.1948083971676523  - accuracy: 0.78125\n",
      "At: 715 [==========>] Loss 0.13460176239032434  - accuracy: 0.78125\n",
      "At: 716 [==========>] Loss 0.09596020665095037  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.08317134166824525  - accuracy: 0.9375\n",
      "At: 718 [==========>] Loss 0.18803928970350647  - accuracy: 0.75\n",
      "At: 719 [==========>] Loss 0.09622606871081832  - accuracy: 0.90625\n",
      "At: 720 [==========>] Loss 0.1336261735140254  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.13513623312666517  - accuracy: 0.84375\n",
      "At: 722 [==========>] Loss 0.1735195365687865  - accuracy: 0.75\n",
      "At: 723 [==========>] Loss 0.1433423967071898  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.05837270934113212  - accuracy: 1.0\n",
      "At: 725 [==========>] Loss 0.17440503342118535  - accuracy: 0.75\n",
      "At: 726 [==========>] Loss 0.18659904816199893  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.17830001782981117  - accuracy: 0.6875\n",
      "At: 728 [==========>] Loss 0.12663606874074315  - accuracy: 0.8125\n",
      "At: 729 [==========>] Loss 0.20354148229921173  - accuracy: 0.65625\n",
      "At: 730 [==========>] Loss 0.14659119621951294  - accuracy: 0.78125\n",
      "At: 731 [==========>] Loss 0.14345958330669784  - accuracy: 0.875\n",
      "At: 732 [==========>] Loss 0.1408919645664887  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.12643752643694717  - accuracy: 0.8125\n",
      "At: 734 [==========>] Loss 0.1409910091151629  - accuracy: 0.8125\n",
      "At: 735 [==========>] Loss 0.11029294192264934  - accuracy: 0.875\n",
      "At: 736 [==========>] Loss 0.11717807267514908  - accuracy: 0.875\n",
      "At: 737 [==========>] Loss 0.1798695536477927  - accuracy: 0.8125\n",
      "At: 738 [==========>] Loss 0.11747935453085223  - accuracy: 0.875\n",
      "At: 739 [==========>] Loss 0.1014950749653916  - accuracy: 0.875\n",
      "At: 740 [==========>] Loss 0.15233456159696876  - accuracy: 0.71875\n",
      "At: 741 [==========>] Loss 0.10969159240832017  - accuracy: 0.84375\n",
      "At: 742 [==========>] Loss 0.13649909731309506  - accuracy: 0.84375\n",
      "At: 743 [==========>] Loss 0.15301156057331294  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.15813168835400093  - accuracy: 0.84375\n",
      "At: 745 [==========>] Loss 0.2021788065825481  - accuracy: 0.65625\n",
      "At: 746 [==========>] Loss 0.11815044344226079  - accuracy: 0.84375\n",
      "At: 747 [==========>] Loss 0.14637891906949066  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.15753548728611214  - accuracy: 0.71875\n",
      "At: 749 [==========>] Loss 0.16346181221188233  - accuracy: 0.71875\n",
      "At: 750 [==========>] Loss 0.09410788316181963  - accuracy: 0.90625\n",
      "At: 751 [==========>] Loss 0.16738027473502692  - accuracy: 0.8125\n",
      "At: 752 [==========>] Loss 0.10136714447547328  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.1713354654992973  - accuracy: 0.75\n",
      "At: 754 [==========>] Loss 0.17921388918506817  - accuracy: 0.75\n",
      "At: 755 [==========>] Loss 0.09519068297046121  - accuracy: 0.90625\n",
      "At: 756 [==========>] Loss 0.21454955503994677  - accuracy: 0.71875\n",
      "At: 757 [==========>] Loss 0.10782775177817058  - accuracy: 0.90625\n",
      "At: 758 [==========>] Loss 0.1386028439126585  - accuracy: 0.8125\n",
      "At: 759 [==========>] Loss 0.08314923036035252  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.18131207442737635  - accuracy: 0.71875\n",
      "At: 761 [==========>] Loss 0.14615124352765313  - accuracy: 0.8125\n",
      "At: 762 [==========>] Loss 0.1404792177757071  - accuracy: 0.8125\n",
      "At: 763 [==========>] Loss 0.12081099792299757  - accuracy: 0.8125\n",
      "At: 764 [==========>] Loss 0.14705257216214268  - accuracy: 0.78125\n",
      "At: 765 [==========>] Loss 0.14814729808570998  - accuracy: 0.8125\n",
      "At: 766 [==========>] Loss 0.13188112042269576  - accuracy: 0.875\n",
      "At: 767 [==========>] Loss 0.13082965555459186  - accuracy: 0.84375\n",
      "At: 768 [==========>] Loss 0.14154664657040678  - accuracy: 0.84375\n",
      "At: 769 [==========>] Loss 0.14970175911688527  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.13885261621616224  - accuracy: 0.78125\n",
      "At: 771 [==========>] Loss 0.2229089163027101  - accuracy: 0.65625\n",
      "At: 772 [==========>] Loss 0.11952514336607938  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.07308847361033623  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.13981688986280258  - accuracy: 0.78125\n",
      "At: 775 [==========>] Loss 0.2112889866474712  - accuracy: 0.65625\n",
      "At: 776 [==========>] Loss 0.1991884517249985  - accuracy: 0.71875\n",
      "At: 777 [==========>] Loss 0.08162370758455012  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.18268357885441897  - accuracy: 0.6875\n",
      "At: 779 [==========>] Loss 0.15081261843346713  - accuracy: 0.75\n",
      "At: 780 [==========>] Loss 0.08129146869666606  - accuracy: 0.90625\n",
      "At: 781 [==========>] Loss 0.1501791234252321  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.17324020857657355  - accuracy: 0.71875\n",
      "At: 783 [==========>] Loss 0.18037203226696177  - accuracy: 0.75\n",
      "At: 784 [==========>] Loss 0.19610794676977034  - accuracy: 0.71875\n",
      "At: 785 [==========>] Loss 0.22076032305546467  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.14188787136961512  - accuracy: 0.8125\n",
      "At: 787 [==========>] Loss 0.12074445170876769  - accuracy: 0.8125\n",
      "At: 788 [==========>] Loss 0.07884868916410741  - accuracy: 0.90625\n",
      "At: 789 [==========>] Loss 0.13501158357697318  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.14156892810577015  - accuracy: 0.84375\n",
      "At: 791 [==========>] Loss 0.15634548639901827  - accuracy: 0.78125\n",
      "At: 792 [==========>] Loss 0.17328150068309806  - accuracy: 0.8125\n",
      "At: 793 [==========>] Loss 0.11337935644536098  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.12520508825301627  - accuracy: 0.84375\n",
      "At: 795 [==========>] Loss 0.12619325674903148  - accuracy: 0.78125\n",
      "At: 796 [==========>] Loss 0.16444716425311254  - accuracy: 0.84375\n",
      "At: 797 [==========>] Loss 0.15819944885806558  - accuracy: 0.8125\n",
      "At: 798 [==========>] Loss 0.17362647070776954  - accuracy: 0.75\n",
      "At: 799 [==========>] Loss 0.10875173177640671  - accuracy: 0.875\n",
      "At: 800 [==========>] Loss 0.13416791118873778  - accuracy: 0.8125\n",
      "At: 801 [==========>] Loss 0.1322198273341832  - accuracy: 0.8125\n",
      "At: 802 [==========>] Loss 0.17232614194638807  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.1739257882035934  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.13159341371635694  - accuracy: 0.8125\n",
      "At: 805 [==========>] Loss 0.14836822134883976  - accuracy: 0.75\n",
      "At: 806 [==========>] Loss 0.07723117120301012  - accuracy: 0.90625\n",
      "At: 807 [==========>] Loss 0.10202001058261152  - accuracy: 0.90625\n",
      "At: 808 [==========>] Loss 0.13637176708889914  - accuracy: 0.84375\n",
      "At: 809 [==========>] Loss 0.10537807096388732  - accuracy: 0.875\n",
      "At: 810 [==========>] Loss 0.19000615277589245  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.1228226114753674  - accuracy: 0.84375\n",
      "At: 812 [==========>] Loss 0.15230893057815031  - accuracy: 0.78125\n",
      "At: 813 [==========>] Loss 0.1926031716798949  - accuracy: 0.75\n",
      "At: 814 [==========>] Loss 0.17255769774441704  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.15074286961537037  - accuracy: 0.8125\n",
      "At: 816 [==========>] Loss 0.14272078127490762  - accuracy: 0.8125\n",
      "At: 817 [==========>] Loss 0.04553815762899975  - accuracy: 0.96875\n",
      "At: 818 [==========>] Loss 0.1356609847838623  - accuracy: 0.8125\n",
      "At: 819 [==========>] Loss 0.13697092132565186  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.14285673678580157  - accuracy: 0.8125\n",
      "At: 821 [==========>] Loss 0.19579304776093195  - accuracy: 0.78125\n",
      "At: 822 [==========>] Loss 0.14536095574052993  - accuracy: 0.78125\n",
      "At: 823 [==========>] Loss 0.1449637165756818  - accuracy: 0.8125\n",
      "At: 824 [==========>] Loss 0.1186152525753573  - accuracy: 0.84375\n",
      "At: 825 [==========>] Loss 0.17223428383181788  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.1449430550701981  - accuracy: 0.78125\n",
      "At: 827 [==========>] Loss 0.09666620872297527  - accuracy: 0.875\n",
      "At: 828 [==========>] Loss 0.07090979915601714  - accuracy: 0.9375\n",
      "At: 829 [==========>] Loss 0.09158410228015003  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.12604984356918125  - accuracy: 0.8125\n",
      "At: 831 [==========>] Loss 0.0811708943567965  - accuracy: 0.9375\n",
      "At: 832 [==========>] Loss 0.11016694016477913  - accuracy: 0.84375\n",
      "At: 833 [==========>] Loss 0.19447874523792835  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.1223564436184825  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.1319761624010699  - accuracy: 0.8125\n",
      "At: 836 [==========>] Loss 0.14322386034920342  - accuracy: 0.78125\n",
      "At: 837 [==========>] Loss 0.13183274868758382  - accuracy: 0.8125\n",
      "At: 838 [==========>] Loss 0.11535093584071779  - accuracy: 0.8125\n",
      "At: 839 [==========>] Loss 0.14517595051905566  - accuracy: 0.8125\n",
      "At: 840 [==========>] Loss 0.14368323873442868  - accuracy: 0.78125\n",
      "At: 841 [==========>] Loss 0.0896688225075743  - accuracy: 0.875\n",
      "At: 842 [==========>] Loss 0.08289011165131493  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.1556178395519953  - accuracy: 0.8125\n",
      "At: 844 [==========>] Loss 0.13568409347837365  - accuracy: 0.8125\n",
      "At: 845 [==========>] Loss 0.16435599548434182  - accuracy: 0.8125\n",
      "At: 846 [==========>] Loss 0.14680490138471627  - accuracy: 0.8125\n",
      "At: 847 [==========>] Loss 0.07935072597163254  - accuracy: 0.9375\n",
      "At: 848 [==========>] Loss 0.1535241842766316  - accuracy: 0.84375\n",
      "At: 849 [==========>] Loss 0.13995035666559702  - accuracy: 0.71875\n",
      "At: 850 [==========>] Loss 0.08634742956497632  - accuracy: 0.9375\n",
      "At: 851 [==========>] Loss 0.11927375866802774  - accuracy: 0.8125\n",
      "At: 852 [==========>] Loss 0.1200935323202903  - accuracy: 0.78125\n",
      "At: 853 [==========>] Loss 0.16233449340929862  - accuracy: 0.8125\n",
      "At: 854 [==========>] Loss 0.21418460871259554  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.07911818759243397  - accuracy: 0.9375\n",
      "At: 856 [==========>] Loss 0.09622918269739292  - accuracy: 0.90625\n",
      "At: 857 [==========>] Loss 0.06598191159141661  - accuracy: 0.96875\n",
      "At: 858 [==========>] Loss 0.22576678079278953  - accuracy: 0.71875\n",
      "At: 859 [==========>] Loss 0.13218598099338585  - accuracy: 0.8125\n",
      "At: 860 [==========>] Loss 0.10798430063838067  - accuracy: 0.90625\n",
      "At: 861 [==========>] Loss 0.08817513598960644  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.08201600268379368  - accuracy: 0.875\n",
      "At: 863 [==========>] Loss 0.14338074212472296  - accuracy: 0.8125\n",
      "At: 864 [==========>] Loss 0.18337368815640542  - accuracy: 0.6875\n",
      "At: 865 [==========>] Loss 0.2210572571824182  - accuracy: 0.65625\n",
      "At: 866 [==========>] Loss 0.1796574343751155  - accuracy: 0.71875\n",
      "At: 867 [==========>] Loss 0.06803014187352024  - accuracy: 0.9375\n",
      "At: 868 [==========>] Loss 0.1488705445893189  - accuracy: 0.71875\n",
      "At: 869 [==========>] Loss 0.1750073216136822  - accuracy: 0.71875\n",
      "At: 870 [==========>] Loss 0.14466046755658996  - accuracy: 0.78125\n",
      "At: 871 [==========>] Loss 0.07507720905543873  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.10947775399770202  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.17176298256140368  - accuracy: 0.78125\n",
      "At: 874 [==========>] Loss 0.15335490374084393  - accuracy: 0.75\n",
      "At: 875 [==========>] Loss 0.11005515103955057  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.11095479484421765  - accuracy: 0.875\n",
      "At: 877 [==========>] Loss 0.17413357091341045  - accuracy: 0.71875\n",
      "At: 878 [==========>] Loss 0.07575535939140352  - accuracy: 0.9375\n",
      "At: 879 [==========>] Loss 0.1745532564840482  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.13917340852475948  - accuracy: 0.8125\n",
      "At: 881 [==========>] Loss 0.14441231429503673  - accuracy: 0.875\n",
      "At: 882 [==========>] Loss 0.11575670642884615  - accuracy: 0.84375\n",
      "At: 883 [==========>] Loss 0.1602504273709476  - accuracy: 0.78125\n",
      "At: 884 [==========>] Loss 0.13122434002340816  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.1289728573650215  - accuracy: 0.78125\n",
      "At: 886 [==========>] Loss 0.10825605043838547  - accuracy: 0.8125\n",
      "At: 887 [==========>] Loss 0.13825916622362047  - accuracy: 0.78125\n",
      "At: 888 [==========>] Loss 0.12646090260401108  - accuracy: 0.78125\n",
      "At: 889 [==========>] Loss 0.10227500880240745  - accuracy: 0.8125\n",
      "At: 890 [==========>] Loss 0.14988584050719095  - accuracy: 0.78125\n",
      "At: 891 [==========>] Loss 0.10698032847937566  - accuracy: 0.875\n",
      "At: 892 [==========>] Loss 0.14370675528873578  - accuracy: 0.75\n",
      "At: 893 [==========>] Loss 0.16283418829348617  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.15224486141050594  - accuracy: 0.84375\n",
      "At: 895 [==========>] Loss 0.12019782388104581  - accuracy: 0.8125\n",
      "At: 896 [==========>] Loss 0.1225223095535737  - accuracy: 0.84375\n",
      "At: 897 [==========>] Loss 0.14767935751357286  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.1653377202152696  - accuracy: 0.78125\n",
      "At: 899 [==========>] Loss 0.08910441214727449  - accuracy: 0.875\n",
      "At: 900 [==========>] Loss 0.15370978912719369  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.17152679306183527  - accuracy: 0.75\n",
      "At: 902 [==========>] Loss 0.09180801261186708  - accuracy: 0.875\n",
      "At: 903 [==========>] Loss 0.14848010968161987  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.10907920989651879  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.11083971929598187  - accuracy: 0.84375\n",
      "At: 906 [==========>] Loss 0.10857801620482349  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.1688061776927647  - accuracy: 0.78125\n",
      "At: 908 [==========>] Loss 0.10548602445624865  - accuracy: 0.90625\n",
      "At: 909 [==========>] Loss 0.10007210110636432  - accuracy: 0.875\n",
      "At: 910 [==========>] Loss 0.13430488286897585  - accuracy: 0.75\n",
      "At: 911 [==========>] Loss 0.11442705182417953  - accuracy: 0.875\n",
      "At: 912 [==========>] Loss 0.1634340813422055  - accuracy: 0.78125\n",
      "At: 913 [==========>] Loss 0.12745514888687293  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.1285956741032982  - accuracy: 0.84375\n",
      "At: 915 [==========>] Loss 0.14501200855665447  - accuracy: 0.84375\n",
      "At: 916 [==========>] Loss 0.17435323172343112  - accuracy: 0.75\n",
      "At: 917 [==========>] Loss 0.16371905667254147  - accuracy: 0.78125\n",
      "At: 918 [==========>] Loss 0.17086973869887465  - accuracy: 0.75\n",
      "At: 919 [==========>] Loss 0.08278079518888712  - accuracy: 0.875\n",
      "At: 920 [==========>] Loss 0.11883612953155422  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.144939468095329  - accuracy: 0.84375\n",
      "At: 922 [==========>] Loss 0.13042642438965324  - accuracy: 0.84375\n",
      "At: 923 [==========>] Loss 0.10280296314574744  - accuracy: 0.875\n",
      "At: 924 [==========>] Loss 0.17759000341115777  - accuracy: 0.75\n",
      "At: 925 [==========>] Loss 0.1438811800465643  - accuracy: 0.8125\n",
      "At: 926 [==========>] Loss 0.14789107613665722  - accuracy: 0.78125\n",
      "At: 927 [==========>] Loss 0.12594715774641885  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.13697150641463907  - accuracy: 0.8125\n",
      "At: 929 [==========>] Loss 0.15622769325092895  - accuracy: 0.78125\n",
      "At: 930 [==========>] Loss 0.11716802122726627  - accuracy: 0.875\n",
      "At: 931 [==========>] Loss 0.16373417569852888  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.10133887765284702  - accuracy: 0.875\n",
      "At: 933 [==========>] Loss 0.09854412418013805  - accuracy: 0.84375\n",
      "At: 934 [==========>] Loss 0.10350170405854714  - accuracy: 0.875\n",
      "At: 935 [==========>] Loss 0.06558643476988879  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.16052118071961258  - accuracy: 0.75\n",
      "At: 937 [==========>] Loss 0.16470762966670677  - accuracy: 0.75\n",
      "At: 938 [==========>] Loss 0.13205818104340067  - accuracy: 0.8125\n",
      "At: 939 [==========>] Loss 0.12768003340616724  - accuracy: 0.84375\n",
      "At: 940 [==========>] Loss 0.20289022363468723  - accuracy: 0.71875\n",
      "At: 941 [==========>] Loss 0.09891910408214993  - accuracy: 0.875\n",
      "At: 942 [==========>] Loss 0.18076034086970288  - accuracy: 0.6875\n",
      "At: 943 [==========>] Loss 0.09735414248115391  - accuracy: 0.84375\n",
      "At: 944 [==========>] Loss 0.13040172418632223  - accuracy: 0.8125\n",
      "At: 945 [==========>] Loss 0.1046849278169251  - accuracy: 0.875\n",
      "At: 946 [==========>] Loss 0.0906679691867191  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.1527404360484469  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.19773886862977505  - accuracy: 0.78125\n",
      "At: 949 [==========>] Loss 0.07131603470067588  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.10987555580834922  - accuracy: 0.84375\n",
      "At: 951 [==========>] Loss 0.11831536298085307  - accuracy: 0.84375\n",
      "At: 952 [==========>] Loss 0.10015926880679679  - accuracy: 0.84375\n",
      "At: 953 [==========>] Loss 0.07406408621609051  - accuracy: 0.9375\n",
      "At: 954 [==========>] Loss 0.09719141056069461  - accuracy: 0.90625\n",
      "At: 955 [==========>] Loss 0.12797700988196972  - accuracy: 0.84375\n",
      "At: 956 [==========>] Loss 0.10749741242237898  - accuracy: 0.8125\n",
      "At: 957 [==========>] Loss 0.17583220304782254  - accuracy: 0.6875\n",
      "At: 958 [==========>] Loss 0.08348747575730481  - accuracy: 0.9375\n",
      "At: 959 [==========>] Loss 0.15802497733689014  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.1218738804608231  - accuracy: 0.875\n",
      "At: 961 [==========>] Loss 0.10091816955921146  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.12251984423049901  - accuracy: 0.78125\n",
      "At: 963 [==========>] Loss 0.08258773251037838  - accuracy: 0.90625\n",
      "At: 964 [==========>] Loss 0.15748436032441226  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.15954868779456033  - accuracy: 0.78125\n",
      "At: 966 [==========>] Loss 0.20063735556177323  - accuracy: 0.6875\n",
      "At: 967 [==========>] Loss 0.10369288981520855  - accuracy: 0.84375\n",
      "At: 968 [==========>] Loss 0.12744970579338397  - accuracy: 0.84375\n",
      "At: 969 [==========>] Loss 0.14474358627270337  - accuracy: 0.78125\n",
      "At: 970 [==========>] Loss 0.10960308042359329  - accuracy: 0.8125\n",
      "At: 971 [==========>] Loss 0.09994703505067529  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.0734885451845671  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.15437512140656495  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.09151334812469494  - accuracy: 0.90625\n",
      "At: 975 [==========>] Loss 0.15721937167710787  - accuracy: 0.78125\n",
      "At: 976 [==========>] Loss 0.10999966878771705  - accuracy: 0.84375\n",
      "At: 977 [==========>] Loss 0.1108836492778164  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.17849967785421084  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.11494300523258909  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.153918047339507  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.20628557173206197  - accuracy: 0.71875\n",
      "At: 982 [==========>] Loss 0.08454875531828078  - accuracy: 0.875\n",
      "At: 983 [==========>] Loss 0.11278402568591209  - accuracy: 0.90625\n",
      "At: 984 [==========>] Loss 0.10917343156150397  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.12705580647249393  - accuracy: 0.84375\n",
      "At: 986 [==========>] Loss 0.12698849842729878  - accuracy: 0.8125\n",
      "At: 987 [==========>] Loss 0.11564216303243069  - accuracy: 0.84375\n",
      "At: 988 [==========>] Loss 0.13713474696695405  - accuracy: 0.84375\n",
      "At: 989 [==========>] Loss 0.12496786020525387  - accuracy: 0.875\n",
      "At: 990 [==========>] Loss 0.10491887152908189  - accuracy: 0.78125\n",
      "At: 991 [==========>] Loss 0.1177945962791419  - accuracy: 0.875\n",
      "At: 992 [==========>] Loss 0.1864201744601873  - accuracy: 0.78125\n",
      "At: 993 [==========>] Loss 0.10430839349928456  - accuracy: 0.84375\n",
      "At: 994 [==========>] Loss 0.1360122204855339  - accuracy: 0.84375\n",
      "At: 995 [==========>] Loss 0.15396770359555761  - accuracy: 0.8125\n",
      "At: 996 [==========>] Loss 0.07716647243604971  - accuracy: 0.9375\n",
      "At: 997 [==========>] Loss 0.09741178994536687  - accuracy: 0.875\n",
      "At: 998 [==========>] Loss 0.12443703241451522  - accuracy: 0.84375\n",
      "At: 999 [==========>] Loss 0.1353681378737069  - accuracy: 0.78125\n",
      "At: 1000 [==========>] Loss 0.21407253555507316  - accuracy: 0.75\n",
      "At: 1001 [==========>] Loss 0.14899808111897783  - accuracy: 0.78125\n",
      "At: 1002 [==========>] Loss 0.20634214196707634  - accuracy: 0.65625\n",
      "At: 1003 [==========>] Loss 0.14076405792729996  - accuracy: 0.8125\n",
      "At: 1004 [==========>] Loss 0.1375831503244119  - accuracy: 0.8125\n",
      "At: 1005 [==========>] Loss 0.0923116900058352  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.11174236549253352  - accuracy: 0.875\n",
      "At: 1007 [==========>] Loss 0.12910459214545034  - accuracy: 0.84375\n",
      "At: 1008 [==========>] Loss 0.20376624428613677  - accuracy: 0.71875\n",
      "At: 1009 [==========>] Loss 0.1147997017988574  - accuracy: 0.875\n",
      "At: 1010 [==========>] Loss 0.17297645894486183  - accuracy: 0.75\n",
      "At: 1011 [==========>] Loss 0.1524111506299336  - accuracy: 0.78125\n",
      "At: 1012 [==========>] Loss 0.0986650854608281  - accuracy: 0.90625\n",
      "At: 1013 [==========>] Loss 0.0800758332878207  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.10958472304592803  - accuracy: 0.875\n",
      "At: 1015 [==========>] Loss 0.19744945694096175  - accuracy: 0.75\n",
      "At: 1016 [==========>] Loss 0.14315719851235165  - accuracy: 0.8125\n",
      "At: 1017 [==========>] Loss 0.13905100140020035  - accuracy: 0.78125\n",
      "At: 1018 [==========>] Loss 0.1630082510634438  - accuracy: 0.75\n",
      "At: 1019 [==========>] Loss 0.1935723090373906  - accuracy: 0.6875\n",
      "At: 1020 [==========>] Loss 0.1462586003915879  - accuracy: 0.75\n",
      "At: 1021 [==========>] Loss 0.12183062055611642  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.13721434101311467  - accuracy: 0.78125\n",
      "At: 1023 [==========>] Loss 0.18655331740273418  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.20078231045440587  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.18660130598389832  - accuracy: 0.71875\n",
      "At: 1026 [==========>] Loss 0.12268038790763439  - accuracy: 0.8125\n",
      "At: 1027 [==========>] Loss 0.13540721243344755  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.2580687087390432  - accuracy: 0.5625\n",
      "At: 1029 [==========>] Loss 0.08945425443053269  - accuracy: 0.875\n",
      "At: 1030 [==========>] Loss 0.08517557561007613  - accuracy: 0.9375\n",
      "At: 1031 [==========>] Loss 0.16421550539093327  - accuracy: 0.75\n",
      "At: 1032 [==========>] Loss 0.12181344642639134  - accuracy: 0.8125\n",
      "At: 1033 [==========>] Loss 0.1493484593389326  - accuracy: 0.78125\n",
      "At: 1034 [==========>] Loss 0.09670222806471246  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.08004173521697489  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.17518198066697727  - accuracy: 0.71875\n",
      "At: 1037 [==========>] Loss 0.1613998710663898  - accuracy: 0.78125\n",
      "At: 1038 [==========>] Loss 0.09461412712416348  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.09149783030809773  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.12722815780250113  - accuracy: 0.84375\n",
      "At: 1041 [==========>] Loss 0.12282237300020943  - accuracy: 0.8125\n",
      "At: 1042 [==========>] Loss 0.1162629204571734  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.2044703367174852  - accuracy: 0.59375\n",
      "At: 1044 [==========>] Loss 0.11071674043614017  - accuracy: 0.875\n",
      "At: 1045 [==========>] Loss 0.14550176494775285  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.15957612896838627  - accuracy: 0.8125\n",
      "At: 1047 [==========>] Loss 0.1134158266256191  - accuracy: 0.875\n",
      "At: 1048 [==========>] Loss 0.1765624352880744  - accuracy: 0.78125\n",
      "At: 1049 [==========>] Loss 0.15577197320800895  - accuracy: 0.78125\n",
      "At: 1050 [==========>] Loss 0.1376729174281386  - accuracy: 0.8125\n",
      "At: 1051 [==========>] Loss 0.07968898894913776  - accuracy: 0.90625\n",
      "At: 1052 [==========>] Loss 0.1325763366563999  - accuracy: 0.84375\n",
      "At: 1053 [==========>] Loss 0.12706197452872742  - accuracy: 0.84375\n",
      "At: 1054 [==========>] Loss 0.09038144334051523  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.19872826061207788  - accuracy: 0.625\n",
      "At: 1056 [==========>] Loss 0.11592229292355469  - accuracy: 0.875\n",
      "At: 1057 [==========>] Loss 0.1337360564445612  - accuracy: 0.8125\n",
      "At: 1058 [==========>] Loss 0.07614734797770827  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.08176220810833652  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.1100683962862079  - accuracy: 0.78125\n",
      "At: 1061 [==========>] Loss 0.07949116342000673  - accuracy: 0.90625\n",
      "At: 1062 [==========>] Loss 0.1750635430015111  - accuracy: 0.75\n",
      "At: 1063 [==========>] Loss 0.12361028841887295  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.15138535004330006  - accuracy: 0.78125\n",
      "At: 1065 [==========>] Loss 0.0919394902948305  - accuracy: 0.90625\n",
      "At: 1066 [==========>] Loss 0.1024099530575173  - accuracy: 0.84375\n",
      "At: 1067 [==========>] Loss 0.1130930849756205  - accuracy: 0.8125\n",
      "At: 1068 [==========>] Loss 0.10992918799158544  - accuracy: 0.84375\n",
      "At: 1069 [==========>] Loss 0.14010888379251135  - accuracy: 0.75\n",
      "At: 1070 [==========>] Loss 0.10010870952835843  - accuracy: 0.90625\n",
      "At: 1071 [==========>] Loss 0.12613695991879906  - accuracy: 0.8125\n",
      "At: 1072 [==========>] Loss 0.10535289216985597  - accuracy: 0.875\n",
      "At: 1073 [==========>] Loss 0.17243866021634838  - accuracy: 0.75\n",
      "At: 1074 [==========>] Loss 0.18017962249413574  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.09966975904754685  - accuracy: 0.875\n",
      "At: 1076 [==========>] Loss 0.1436657037767453  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.0929113717109171  - accuracy: 0.84375\n",
      "At: 1078 [==========>] Loss 0.11335739121178638  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.14409422326218538  - accuracy: 0.8125\n",
      "At: 1080 [==========>] Loss 0.1760644103918645  - accuracy: 0.71875\n",
      "At: 1081 [==========>] Loss 0.09602628727258758  - accuracy: 0.84375\n",
      "At: 1082 [==========>] Loss 0.11045180256638927  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.14279831915408714  - accuracy: 0.71875\n",
      "At: 1084 [==========>] Loss 0.10634664674091676  - accuracy: 0.875\n",
      "At: 1085 [==========>] Loss 0.11115946216253281  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.0880321106903482  - accuracy: 0.875\n",
      "At: 1087 [==========>] Loss 0.13805662704892369  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.1686474015963253  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.10884952736974146  - accuracy: 0.875\n",
      "At: 1090 [==========>] Loss 0.09112627854305841  - accuracy: 0.84375\n",
      "At: 1091 [==========>] Loss 0.21580031586250886  - accuracy: 0.75\n",
      "At: 1092 [==========>] Loss 0.10484735229435357  - accuracy: 0.875\n",
      "At: 1093 [==========>] Loss 0.13124551896107656  - accuracy: 0.875\n",
      "At: 1094 [==========>] Loss 0.1493784149852066  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.12607237912861796  - accuracy: 0.75\n",
      "At: 1096 [==========>] Loss 0.08954380184545939  - accuracy: 0.875\n",
      "At: 1097 [==========>] Loss 0.05676838789365852  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.142859192423339  - accuracy: 0.78125\n",
      "At: 1099 [==========>] Loss 0.172437046024894  - accuracy: 0.71875\n",
      "At: 1100 [==========>] Loss 0.12491830049479775  - accuracy: 0.84375\n",
      "At: 1101 [==========>] Loss 0.0927455648658267  - accuracy: 0.90625\n",
      "At: 1102 [==========>] Loss 0.10932710154343328  - accuracy: 0.875\n",
      "At: 1103 [==========>] Loss 0.10723276058480988  - accuracy: 0.8125\n",
      "At: 1104 [==========>] Loss 0.0556612447095534  - accuracy: 0.9375\n",
      "At: 1105 [==========>] Loss 0.07228536126221956  - accuracy: 0.9375\n",
      "At: 1106 [==========>] Loss 0.08317447807247626  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.19036116668148884  - accuracy: 0.75\n",
      "At: 1108 [==========>] Loss 0.11413681184199302  - accuracy: 0.8125\n",
      "At: 1109 [==========>] Loss 0.057135677029510445  - accuracy: 0.96875\n",
      "At: 1110 [==========>] Loss 0.09995599335589239  - accuracy: 0.90625\n",
      "At: 1111 [==========>] Loss 0.20665619246712522  - accuracy: 0.75\n",
      "At: 1112 [==========>] Loss 0.14748543644522513  - accuracy: 0.8125\n",
      "At: 1113 [==========>] Loss 0.13980155959081322  - accuracy: 0.84375\n",
      "At: 1114 [==========>] Loss 0.13830419643006048  - accuracy: 0.84375\n",
      "At: 1115 [==========>] Loss 0.13108564103104936  - accuracy: 0.84375\n",
      "At: 1116 [==========>] Loss 0.11265460480007744  - accuracy: 0.84375\n",
      "At: 1117 [==========>] Loss 0.07642023971091083  - accuracy: 0.90625\n",
      "At: 1118 [==========>] Loss 0.110499372254488  - accuracy: 0.875\n",
      "At: 1119 [==========>] Loss 0.13333976076932635  - accuracy: 0.78125\n",
      "At: 1120 [==========>] Loss 0.09806742284398388  - accuracy: 0.90625\n",
      "At: 1121 [==========>] Loss 0.09684880899048295  - accuracy: 0.875\n",
      "At: 1122 [==========>] Loss 0.07552337667232245  - accuracy: 0.9375\n",
      "At: 1123 [==========>] Loss 0.1259495036902316  - accuracy: 0.78125\n",
      "At: 1124 [==========>] Loss 0.1135664917563959  - accuracy: 0.8125\n",
      "At: 1125 [==========>] Loss 0.164666076486033  - accuracy: 0.71875\n",
      "At: 1126 [==========>] Loss 0.10592268104989569  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.14933741864733785  - accuracy: 0.8125\n",
      "At: 1128 [==========>] Loss 0.06208672619278231  - accuracy: 0.9375\n",
      "At: 1129 [==========>] Loss 0.15471915180439408  - accuracy: 0.75\n",
      "At: 1130 [==========>] Loss 0.10860225716561817  - accuracy: 0.90625\n",
      "At: 1131 [==========>] Loss 0.10904132896622837  - accuracy: 0.875\n",
      "At: 1132 [==========>] Loss 0.0876715687039443  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.1384538112041725  - accuracy: 0.75\n",
      "At: 1134 [==========>] Loss 0.09599208329973982  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.09115556634669675  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.17558482119715668  - accuracy: 0.78125\n",
      "At: 1137 [==========>] Loss 0.12258362850241217  - accuracy: 0.8125\n",
      "At: 1138 [==========>] Loss 0.1047912194854448  - accuracy: 0.8125\n",
      "At: 1139 [==========>] Loss 0.12045495810829424  - accuracy: 0.78125\n",
      "At: 1140 [==========>] Loss 0.1356867358462764  - accuracy: 0.75\n",
      "At: 1141 [==========>] Loss 0.13243870750214054  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.14556824534534957  - accuracy: 0.78125\n",
      "At: 1143 [==========>] Loss 0.03942238605382467  - accuracy: 0.96875\n",
      "At: 1144 [==========>] Loss 0.10216115746300866  - accuracy: 0.84375\n",
      "At: 1145 [==========>] Loss 0.1456996798113181  - accuracy: 0.8125\n",
      "At: 1146 [==========>] Loss 0.1356912542427875  - accuracy: 0.78125\n",
      "At: 1147 [==========>] Loss 0.17703596968108823  - accuracy: 0.71875\n",
      "At: 1148 [==========>] Loss 0.08765897766771522  - accuracy: 0.90625\n",
      "At: 1149 [==========>] Loss 0.12029427520515132  - accuracy: 0.78125\n",
      "At: 1150 [==========>] Loss 0.13777906668309658  - accuracy: 0.84375\n",
      "At: 1151 [==========>] Loss 0.18330037285861678  - accuracy: 0.75\n",
      "At: 1152 [==========>] Loss 0.10025886971063627  - accuracy: 0.90625\n",
      "At: 1153 [==========>] Loss 0.17344425672182467  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.08748260901951033  - accuracy: 0.90625\n",
      "At: 1155 [==========>] Loss 0.11416102067753527  - accuracy: 0.8125\n",
      "At: 1156 [==========>] Loss 0.15432615399363317  - accuracy: 0.78125\n",
      "At: 1157 [==========>] Loss 0.14121317136256487  - accuracy: 0.8125\n",
      "At: 1158 [==========>] Loss 0.14094229241608747  - accuracy: 0.8125\n",
      "At: 1159 [==========>] Loss 0.11253420342569502  - accuracy: 0.84375\n",
      "At: 1160 [==========>] Loss 0.08004052591040826  - accuracy: 0.875\n",
      "At: 1161 [==========>] Loss 0.10114302325771726  - accuracy: 0.8125\n",
      "At: 1162 [==========>] Loss 0.1342263432768041  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.1393866409179052  - accuracy: 0.8125\n",
      "At: 1164 [==========>] Loss 0.09118638225052772  - accuracy: 0.875\n",
      "At: 1165 [==========>] Loss 0.12669896167709993  - accuracy: 0.84375\n",
      "At: 1166 [==========>] Loss 0.06825753838886148  - accuracy: 0.9375\n",
      "At: 1167 [==========>] Loss 0.15580187422571343  - accuracy: 0.75\n",
      "At: 1168 [==========>] Loss 0.12850928708400106  - accuracy: 0.84375\n",
      "At: 1169 [==========>] Loss 0.11149157957507952  - accuracy: 0.875\n",
      "At: 1170 [==========>] Loss 0.1621807131720963  - accuracy: 0.78125\n",
      "At: 1171 [==========>] Loss 0.09919655088108534  - accuracy: 0.875\n",
      "At: 1172 [==========>] Loss 0.13004978968306358  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.12583885899535086  - accuracy: 0.84375\n",
      "At: 1174 [==========>] Loss 0.14857806145033697  - accuracy: 0.8125\n",
      "At: 1175 [==========>] Loss 0.12960323251871889  - accuracy: 0.8125\n",
      "At: 1176 [==========>] Loss 0.11333681153529576  - accuracy: 0.90625\n",
      "At: 1177 [==========>] Loss 0.05910249700810029  - accuracy: 0.9375\n",
      "At: 1178 [==========>] Loss 0.17933583403423098  - accuracy: 0.75\n",
      "At: 1179 [==========>] Loss 0.1256684491523547  - accuracy: 0.84375\n",
      "At: 1180 [==========>] Loss 0.17994457040219886  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.13780680729023032  - accuracy: 0.8125\n",
      "At: 1182 [==========>] Loss 0.1142535602482962  - accuracy: 0.8125\n",
      "At: 1183 [==========>] Loss 0.15504326851076042  - accuracy: 0.75\n",
      "At: 1184 [==========>] Loss 0.11605840062525315  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.08501970842618424  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.15677558143802742  - accuracy: 0.75\n",
      "At: 1187 [==========>] Loss 0.14189466677261367  - accuracy: 0.78125\n",
      "At: 1188 [==========>] Loss 0.09421565505860517  - accuracy: 0.875\n",
      "At: 1189 [==========>] Loss 0.13077356891190178  - accuracy: 0.84375\n",
      "At: 1190 [==========>] Loss 0.1287323851148802  - accuracy: 0.78125\n",
      "At: 1191 [==========>] Loss 0.17853880568985425  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.07839916767752017  - accuracy: 0.875\n",
      "At: 1193 [==========>] Loss 0.13051930095812853  - accuracy: 0.78125\n",
      "At: 1194 [==========>] Loss 0.11013211977411838  - accuracy: 0.8125\n",
      "At: 1195 [==========>] Loss 0.1380345205813896  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.12890365046738514  - accuracy: 0.8125\n",
      "At: 1197 [==========>] Loss 0.11298766525012477  - accuracy: 0.78125\n",
      "At: 1198 [==========>] Loss 0.10336002417484391  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.1753600150889243  - accuracy: 0.71875\n",
      "At: 1200 [==========>] Loss 0.0701150466124736  - accuracy: 0.9375\n",
      "At: 1201 [==========>] Loss 0.11580211710006144  - accuracy: 0.84375\n",
      "At: 1202 [==========>] Loss 0.15296642587552062  - accuracy: 0.78125\n",
      "At: 1203 [==========>] Loss 0.1121674043179339  - accuracy: 0.84375\n",
      "At: 1204 [==========>] Loss 0.057952754146846086  - accuracy: 0.96875\n",
      "At: 1205 [==========>] Loss 0.08988651932201781  - accuracy: 0.875\n",
      "At: 1206 [==========>] Loss 0.1113329664945087  - accuracy: 0.84375\n",
      "At: 1207 [==========>] Loss 0.15421083619182513  - accuracy: 0.71875\n",
      "At: 1208 [==========>] Loss 0.10224921284256633  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.13589165125367678  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.12296202995130699  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.15079402337204253  - accuracy: 0.78125\n",
      "At: 1212 [==========>] Loss 0.10888163000525869  - accuracy: 0.8125\n",
      "At: 1213 [==========>] Loss 0.20141462015545497  - accuracy: 0.6875\n",
      "At: 1214 [==========>] Loss 0.15355739039376237  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.1680654923563536  - accuracy: 0.78125\n",
      "At: 1216 [==========>] Loss 0.1066966210092491  - accuracy: 0.78125\n",
      "At: 1217 [==========>] Loss 0.08553631895638886  - accuracy: 0.84375\n",
      "At: 1218 [==========>] Loss 0.11208326664568405  - accuracy: 0.90625\n",
      "At: 1219 [==========>] Loss 0.15485869184430948  - accuracy: 0.78125\n",
      "At: 1220 [==========>] Loss 0.12621143026015394  - accuracy: 0.90625\n",
      "At: 1221 [==========>] Loss 0.08683724214171323  - accuracy: 0.90625\n",
      "At: 1222 [==========>] Loss 0.18588412996883352  - accuracy: 0.71875\n",
      "At: 1223 [==========>] Loss 0.11117474111172422  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.08334691048330134  - accuracy: 0.875\n",
      "At: 1225 [==========>] Loss 0.0950870693577783  - accuracy: 0.875\n",
      "At: 1226 [==========>] Loss 0.109670899488097  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.14949252985726902  - accuracy: 0.8125\n",
      "At: 1228 [==========>] Loss 0.14891003296006644  - accuracy: 0.75\n",
      "At: 1229 [==========>] Loss 0.13503146211011002  - accuracy: 0.84375\n",
      "At: 1230 [==========>] Loss 0.15264775150643567  - accuracy: 0.78125\n",
      "At: 1231 [==========>] Loss 0.1418112096592265  - accuracy: 0.84375\n",
      "At: 1232 [==========>] Loss 0.08024851063959698  - accuracy: 0.96875\n",
      "At: 1233 [==========>] Loss 0.13794072818405698  - accuracy: 0.75\n",
      "At: 1234 [==========>] Loss 0.1302775475710521  - accuracy: 0.75\n",
      "At: 1235 [==========>] Loss 0.07772035273343625  - accuracy: 0.90625\n",
      "At: 1236 [==========>] Loss 0.13663065085644685  - accuracy: 0.84375\n",
      "At: 1237 [==========>] Loss 0.09326303279083278  - accuracy: 0.875\n",
      "At: 1238 [==========>] Loss 0.10530108061989557  - accuracy: 0.875\n",
      "At: 1239 [==========>] Loss 0.14361094871727623  - accuracy: 0.78125\n",
      "At: 1240 [==========>] Loss 0.1021431930680273  - accuracy: 0.84375\n",
      "At: 1241 [==========>] Loss 0.15203597600496516  - accuracy: 0.75\n",
      "At: 1242 [==========>] Loss 0.1594432880409264  - accuracy: 0.78125\n",
      "At: 1243 [==========>] Loss 0.14242745787496147  - accuracy: 0.78125\n",
      "At: 1244 [==========>] Loss 0.15474581542272722  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.1280397281698902  - accuracy: 0.8125\n",
      "At: 1246 [==========>] Loss 0.07160512213878435  - accuracy: 0.90625\n",
      "At: 1247 [==========>] Loss 0.13793464965759328  - accuracy: 0.78125\n",
      "At: 1248 [==========>] Loss 0.11259717802755624  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.1204977077570727  - accuracy: 0.8125\n",
      "At: 1250 [==========>] Loss 0.12616457233034345  - accuracy: 0.8125\n",
      "At: 1251 [==========>] Loss 0.13955040599502178  - accuracy: 0.8125\n",
      "At: 1252 [==========>] Loss 0.08828490140367495  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.10355757981474363  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.16567067891709056  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.11750438773557657  - accuracy: 0.875\n",
      "At: 1256 [==========>] Loss 0.13630045924966844  - accuracy: 0.8125\n",
      "At: 1257 [==========>] Loss 0.14497476949998855  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.09652598637748497  - accuracy: 0.9375\n",
      "At: 1259 [==========>] Loss 0.13089656763028884  - accuracy: 0.75\n",
      "At: 1260 [==========>] Loss 0.13133072420815578  - accuracy: 0.78125\n",
      "At: 1261 [==========>] Loss 0.13935625045511751  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.13247556056421173  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.13277974302231832  - accuracy: 0.78125\n",
      "At: 1264 [==========>] Loss 0.08673603617531074  - accuracy: 0.90625\n",
      "At: 1265 [==========>] Loss 0.12734857923909532  - accuracy: 0.78125\n",
      "At: 1266 [==========>] Loss 0.12036490154267374  - accuracy: 0.875\n",
      "At: 1267 [==========>] Loss 0.1488301899649449  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.17540443858948263  - accuracy: 0.75\n",
      "At: 1269 [==========>] Loss 0.12110335250487757  - accuracy: 0.8125\n",
      "At: 1270 [==========>] Loss 0.1169746306426799  - accuracy: 0.84375\n",
      "At: 1271 [==========>] Loss 0.15099248687954603  - accuracy: 0.8125\n",
      "At: 1272 [==========>] Loss 0.08317783217789282  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.19162953212524664  - accuracy: 0.75\n",
      "At: 1274 [==========>] Loss 0.1525251109642922  - accuracy: 0.71875\n",
      "At: 1275 [==========>] Loss 0.08492646110131266  - accuracy: 0.9375\n",
      "At: 1276 [==========>] Loss 0.11687265472934527  - accuracy: 0.84375\n",
      "At: 1277 [==========>] Loss 0.06365326979234509  - accuracy: 0.9375\n",
      "At: 1278 [==========>] Loss 0.12103570579399556  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.10497279071687557  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.0984987013266605  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.15288456743882425  - accuracy: 0.78125\n",
      "At: 1282 [==========>] Loss 0.1296586652092522  - accuracy: 0.84375\n",
      "At: 1283 [==========>] Loss 0.14682768905150612  - accuracy: 0.84375\n",
      "At: 1284 [==========>] Loss 0.16631482762273356  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.07644290177059078  - accuracy: 0.90625\n",
      "At: 1286 [==========>] Loss 0.10359446471237004  - accuracy: 0.8125\n",
      "At: 1287 [==========>] Loss 0.12652065437545973  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.17490698379988914  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.10674178976640339  - accuracy: 0.875\n",
      "At: 1290 [==========>] Loss 0.14601748339134374  - accuracy: 0.75\n",
      "At: 1291 [==========>] Loss 0.16299611004712  - accuracy: 0.75\n",
      "At: 1292 [==========>] Loss 0.11568013610068272  - accuracy: 0.875\n",
      "At: 1293 [==========>] Loss 0.14859912895040164  - accuracy: 0.75\n",
      "At: 1294 [==========>] Loss 0.1359573587634903  - accuracy: 0.8125\n",
      "At: 1295 [==========>] Loss 0.15950554000990055  - accuracy: 0.84375\n",
      "At: 1296 [==========>] Loss 0.1413124503944027  - accuracy: 0.8125\n",
      "At: 1297 [==========>] Loss 0.12217735213780162  - accuracy: 0.8125\n",
      "At: 1298 [==========>] Loss 0.1187861962826711  - accuracy: 0.84375\n",
      "At: 1299 [==========>] Loss 0.13557056824788782  - accuracy: 0.78125\n",
      "At: 1300 [==========>] Loss 0.13551383214186286  - accuracy: 0.84375\n",
      "At: 1301 [==========>] Loss 0.12699365150803435  - accuracy: 0.84375\n",
      "At: 1302 [==========>] Loss 0.07271337314487172  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.08998803782130532  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.14049686841253078  - accuracy: 0.78125\n",
      "At: 1305 [==========>] Loss 0.14952570840736262  - accuracy: 0.8125\n",
      "At: 1306 [==========>] Loss 0.06532529981772386  - accuracy: 0.9375\n",
      "At: 1307 [==========>] Loss 0.14720352994042277  - accuracy: 0.78125\n",
      "At: 1308 [==========>] Loss 0.06875308263032133  - accuracy: 0.9375\n",
      "At: 1309 [==========>] Loss 0.1657564897337029  - accuracy: 0.75\n",
      "At: 1310 [==========>] Loss 0.17210869372119997  - accuracy: 0.75\n",
      "At: 1311 [==========>] Loss 0.13346443792199003  - accuracy: 0.75\n",
      "At: 1312 [==========>] Loss 0.08929400620249771  - accuracy: 0.84375\n",
      "At: 1313 [==========>] Loss 0.14406932010570614  - accuracy: 0.75\n",
      "At: 1314 [==========>] Loss 0.06160440538760164  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.17248525912824625  - accuracy: 0.71875\n",
      "At: 1316 [==========>] Loss 0.16783928990844565  - accuracy: 0.78125\n",
      "At: 1317 [==========>] Loss 0.10417000695607453  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.12497973718697043  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.12365359962275572  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.1191129205315701  - accuracy: 0.8125\n",
      "At: 1321 [==========>] Loss 0.07744807474269906  - accuracy: 0.90625\n",
      "At: 1322 [==========>] Loss 0.1464211402294628  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.12003639982657291  - accuracy: 0.84375\n",
      "At: 1324 [==========>] Loss 0.1599418211568982  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.08158824019052067  - accuracy: 0.9375\n",
      "At: 1326 [==========>] Loss 0.09595290406882336  - accuracy: 0.90625\n",
      "At: 1327 [==========>] Loss 0.1412540737766496  - accuracy: 0.75\n",
      "At: 1328 [==========>] Loss 0.1288586964008319  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.06745603680132038  - accuracy: 0.9375\n",
      "At: 1330 [==========>] Loss 0.14660294688231187  - accuracy: 0.78125\n",
      "At: 1331 [==========>] Loss 0.18771870717054126  - accuracy: 0.71875\n",
      "At: 1332 [==========>] Loss 0.12288189062890958  - accuracy: 0.8125\n",
      "At: 1333 [==========>] Loss 0.13118741567261022  - accuracy: 0.8125\n",
      "At: 1334 [==========>] Loss 0.08353406542439684  - accuracy: 0.90625\n",
      "At: 1335 [==========>] Loss 0.13982095111487924  - accuracy: 0.8125\n",
      "At: 1336 [==========>] Loss 0.09864844125808318  - accuracy: 0.875\n",
      "At: 1337 [==========>] Loss 0.16021906333384667  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.13322053578387236  - accuracy: 0.8125\n",
      "At: 1339 [==========>] Loss 0.12402474221115686  - accuracy: 0.8125\n",
      "At: 1340 [==========>] Loss 0.12977408200386817  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.1082810017628  - accuracy: 0.875\n",
      "At: 1342 [==========>] Loss 0.1069603642023664  - accuracy: 0.90625\n",
      "At: 1343 [==========>] Loss 0.21149676125900113  - accuracy: 0.71875\n",
      "At: 1344 [==========>] Loss 0.16649148965492427  - accuracy: 0.75\n",
      "At: 1345 [==========>] Loss 0.09949863637479592  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.11878898390683594  - accuracy: 0.84375\n",
      "At: 1347 [==========>] Loss 0.12160904053497071  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.11664460234856736  - accuracy: 0.875\n",
      "At: 1349 [==========>] Loss 0.14232392385993647  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.11823123914418888  - accuracy: 0.8125\n",
      "At: 1351 [==========>] Loss 0.12117541780059345  - accuracy: 0.8125\n",
      "At: 1352 [==========>] Loss 0.0887248895452794  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.16851988985139876  - accuracy: 0.75\n",
      "At: 1354 [==========>] Loss 0.1648442843193556  - accuracy: 0.78125\n",
      "At: 1355 [==========>] Loss 0.08667356267911255  - accuracy: 0.9375\n",
      "At: 1356 [==========>] Loss 0.1070993183750525  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.11616957101800056  - accuracy: 0.875\n",
      "At: 1358 [==========>] Loss 0.15312453646363164  - accuracy: 0.8125\n",
      "At: 1359 [==========>] Loss 0.050415634940457726  - accuracy: 0.9375\n",
      "At: 1360 [==========>] Loss 0.16753782937231534  - accuracy: 0.75\n",
      "At: 1361 [==========>] Loss 0.09344218934769322  - accuracy: 0.875\n",
      "At: 1362 [==========>] Loss 0.15177444656286748  - accuracy: 0.78125\n",
      "At: 1363 [==========>] Loss 0.1134369585624729  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.13682407351915557  - accuracy: 0.84375\n",
      "At: 1365 [==========>] Loss 0.11691863903909078  - accuracy: 0.84375\n",
      "At: 1366 [==========>] Loss 0.166175236113393  - accuracy: 0.78125\n",
      "At: 1367 [==========>] Loss 0.10305089629847798  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.18987246487472653  - accuracy: 0.71875\n",
      "At: 1369 [==========>] Loss 0.06957832979105216  - accuracy: 0.96875\n",
      "At: 1370 [==========>] Loss 0.10703190187494299  - accuracy: 0.78125\n",
      "At: 1371 [==========>] Loss 0.18470354091986707  - accuracy: 0.75\n",
      "At: 1372 [==========>] Loss 0.11279986899768463  - accuracy: 0.84375\n",
      "At: 1373 [==========>] Loss 0.11330190651957164  - accuracy: 0.875\n",
      "At: 1374 [==========>] Loss 0.13197918555568772  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.11862357518755913  - accuracy: 0.84375\n",
      "At: 1376 [==========>] Loss 0.1003491639580323  - accuracy: 0.84375\n",
      "At: 1377 [==========>] Loss 0.16698658262423205  - accuracy: 0.78125\n",
      "At: 1378 [==========>] Loss 0.1369579809255501  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.13186337364612755  - accuracy: 0.78125\n",
      "At: 1380 [==========>] Loss 0.14162181203758906  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.10750828999670645  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.15326729322898613  - accuracy: 0.84375\n",
      "At: 1383 [==========>] Loss 0.08985440985458554  - accuracy: 0.875\n",
      "At: 1384 [==========>] Loss 0.10187002640758533  - accuracy: 0.875\n",
      "At: 1385 [==========>] Loss 0.17395276441937163  - accuracy: 0.71875\n",
      "At: 1386 [==========>] Loss 0.18796276312843624  - accuracy: 0.71875\n",
      "At: 1387 [==========>] Loss 0.08232330866927781  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.14640026976851064  - accuracy: 0.8125\n",
      "At: 1389 [==========>] Loss 0.08889951502774822  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.14022880023891385  - accuracy: 0.78125\n",
      "At: 1391 [==========>] Loss 0.12371773939772272  - accuracy: 0.84375\n",
      "At: 1392 [==========>] Loss 0.0878155021003788  - accuracy: 0.90625\n",
      "At: 1393 [==========>] Loss 0.1091873908450835  - accuracy: 0.84375\n",
      "At: 1394 [==========>] Loss 0.09776123902647818  - accuracy: 0.875\n",
      "At: 1395 [==========>] Loss 0.23928704348710744  - accuracy: 0.625\n",
      "At: 1396 [==========>] Loss 0.07146907396551468  - accuracy: 0.90625\n",
      "At: 1397 [==========>] Loss 0.12074161155803022  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.12909049933609396  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.12840905108701522  - accuracy: 0.8125\n",
      "At: 1400 [==========>] Loss 0.17449072229372195  - accuracy: 0.75\n",
      "At: 1401 [==========>] Loss 0.08792133612985409  - accuracy: 0.875\n",
      "At: 1402 [==========>] Loss 0.15174991735655846  - accuracy: 0.78125\n",
      "At: 1403 [==========>] Loss 0.10881970773715297  - accuracy: 0.8125\n",
      "At: 1404 [==========>] Loss 0.1256111291416948  - accuracy: 0.84375\n",
      "At: 1405 [==========>] Loss 0.09346996043822531  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.17822192656378072  - accuracy: 0.75\n",
      "At: 1407 [==========>] Loss 0.09433187516695062  - accuracy: 0.90625\n",
      "At: 1408 [==========>] Loss 0.14066496656204872  - accuracy: 0.75\n",
      "At: 1409 [==========>] Loss 0.032377796457566325  - accuracy: 1.0\n",
      "At: 1410 [==========>] Loss 0.13512506458135642  - accuracy: 0.8125\n",
      "At: 1411 [==========>] Loss 0.12169619376544201  - accuracy: 0.84375\n",
      "At: 1412 [==========>] Loss 0.15563895870966574  - accuracy: 0.78125\n",
      "At: 1413 [==========>] Loss 0.08899764567915236  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.17279876204218925  - accuracy: 0.8125\n",
      "At: 1415 [==========>] Loss 0.05862788421146402  - accuracy: 0.96875\n",
      "At: 1416 [==========>] Loss 0.1785605943526435  - accuracy: 0.78125\n",
      "At: 1417 [==========>] Loss 0.1047388097956668  - accuracy: 0.84375\n",
      "At: 1418 [==========>] Loss 0.1385144542233229  - accuracy: 0.78125\n",
      "At: 1419 [==========>] Loss 0.12805572909489774  - accuracy: 0.875\n",
      "At: 1420 [==========>] Loss 0.10221839256536566  - accuracy: 0.875\n",
      "At: 1421 [==========>] Loss 0.07604031237049239  - accuracy: 0.96875\n",
      "At: 1422 [==========>] Loss 0.14066336250937483  - accuracy: 0.8125\n",
      "At: 1423 [==========>] Loss 0.1437471867521662  - accuracy: 0.78125\n",
      "At: 1424 [==========>] Loss 0.13955777371853845  - accuracy: 0.8125\n",
      "At: 1425 [==========>] Loss 0.09034781987876485  - accuracy: 0.84375\n",
      "At: 1426 [==========>] Loss 0.14263685878581928  - accuracy: 0.8125\n",
      "At: 1427 [==========>] Loss 0.12752642290047866  - accuracy: 0.84375\n",
      "At: 1428 [==========>] Loss 0.11427450357521293  - accuracy: 0.84375\n",
      "At: 1429 [==========>] Loss 0.14382115118127636  - accuracy: 0.71875\n",
      "At: 1430 [==========>] Loss 0.07980080392021245  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.14709536503123422  - accuracy: 0.84375\n",
      "At: 1432 [==========>] Loss 0.09909647895944039  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.1098229147100162  - accuracy: 0.84375\n",
      "At: 1434 [==========>] Loss 0.15358160407151247  - accuracy: 0.75\n",
      "At: 1435 [==========>] Loss 0.12273333882324328  - accuracy: 0.8125\n",
      "At: 1436 [==========>] Loss 0.07645211801646286  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.1455940809859816  - accuracy: 0.84375\n",
      "At: 1438 [==========>] Loss 0.14028792048824773  - accuracy: 0.84375\n",
      "At: 1439 [==========>] Loss 0.1049277157955544  - accuracy: 0.875\n",
      "At: 1440 [==========>] Loss 0.08655652781986782  - accuracy: 0.875\n",
      "At: 1441 [==========>] Loss 0.07910025591165923  - accuracy: 0.875\n",
      "At: 1442 [==========>] Loss 0.11623634258874385  - accuracy: 0.84375\n",
      "At: 1443 [==========>] Loss 0.153144958701948  - accuracy: 0.8125\n",
      "At: 1444 [==========>] Loss 0.08989081157419976  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.16957629058540374  - accuracy: 0.75\n",
      "At: 1446 [==========>] Loss 0.17563368364890103  - accuracy: 0.71875\n",
      "At: 1447 [==========>] Loss 0.1503394207652161  - accuracy: 0.8125\n",
      "At: 1448 [==========>] Loss 0.0971128709715087  - accuracy: 0.875\n",
      "At: 1449 [==========>] Loss 0.14052398892867643  - accuracy: 0.78125\n",
      "At: 1450 [==========>] Loss 0.13762868774070658  - accuracy: 0.8125\n",
      "At: 1451 [==========>] Loss 0.13308549121739233  - accuracy: 0.78125\n",
      "At: 1452 [==========>] Loss 0.10513101877594307  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.033229594662833865  - accuracy: 1.0\n",
      "At: 1454 [==========>] Loss 0.17337393165560827  - accuracy: 0.75\n",
      "At: 1455 [==========>] Loss 0.11386899303200182  - accuracy: 0.90625\n",
      "At: 1456 [==========>] Loss 0.09541340717174962  - accuracy: 0.90625\n",
      "At: 1457 [==========>] Loss 0.08625794590346607  - accuracy: 0.90625\n",
      "At: 1458 [==========>] Loss 0.14279792436247923  - accuracy: 0.78125\n",
      "At: 1459 [==========>] Loss 0.13425881939751133  - accuracy: 0.84375\n",
      "At: 1460 [==========>] Loss 0.16841030206917557  - accuracy: 0.8125\n",
      "At: 1461 [==========>] Loss 0.10685329348906653  - accuracy: 0.875\n",
      "At: 1462 [==========>] Loss 0.18498550626394036  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.07197314840895538  - accuracy: 0.90625\n",
      "At: 1464 [==========>] Loss 0.1841661940792022  - accuracy: 0.75\n",
      "At: 1465 [==========>] Loss 0.12534387853916523  - accuracy: 0.8125\n",
      "At: 1466 [==========>] Loss 0.08989176077745792  - accuracy: 0.90625\n",
      "At: 1467 [==========>] Loss 0.2052995442954107  - accuracy: 0.6875\n",
      "At: 1468 [==========>] Loss 0.1536322933709773  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.15776749130888634  - accuracy: 0.78125\n",
      "At: 1470 [==========>] Loss 0.16701145187072391  - accuracy: 0.75\n",
      "At: 1471 [==========>] Loss 0.1663886290512292  - accuracy: 0.71875\n",
      "At: 1472 [==========>] Loss 0.08062813269004285  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.12783167649806426  - accuracy: 0.84375\n",
      "At: 1474 [==========>] Loss 0.23692266574819187  - accuracy: 0.6875\n",
      "At: 1475 [==========>] Loss 0.15995740074290032  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.13341185076491063  - accuracy: 0.78125\n",
      "At: 1477 [==========>] Loss 0.08574539590617568  - accuracy: 0.875\n",
      "At: 1478 [==========>] Loss 0.08476117862380056  - accuracy: 0.9375\n",
      "At: 1479 [==========>] Loss 0.12799971563040158  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.08389524408692751  - accuracy: 0.875\n",
      "At: 1481 [==========>] Loss 0.11949632916110786  - accuracy: 0.75\n",
      "At: 1482 [==========>] Loss 0.1210073927746888  - accuracy: 0.8125\n",
      "At: 1483 [==========>] Loss 0.22245678829355398  - accuracy: 0.71875\n",
      "At: 1484 [==========>] Loss 0.12955425677704446  - accuracy: 0.84375\n",
      "At: 1485 [==========>] Loss 0.15366538314120293  - accuracy: 0.71875\n",
      "At: 1486 [==========>] Loss 0.09808936824770964  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.08506753469702175  - accuracy: 0.8125\n",
      "At: 1488 [==========>] Loss 0.14699757911492872  - accuracy: 0.78125\n",
      "At: 1489 [==========>] Loss 0.21061287956924687  - accuracy: 0.6875\n",
      "At: 1490 [==========>] Loss 0.10456899909942133  - accuracy: 0.875\n",
      "At: 1491 [==========>] Loss 0.17387802109005487  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.12310099439396124  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.1858444602405638  - accuracy: 0.75\n",
      "At: 1494 [==========>] Loss 0.1580005054953445  - accuracy: 0.78125\n",
      "At: 1495 [==========>] Loss 0.1238024514016358  - accuracy: 0.84375\n",
      "At: 1496 [==========>] Loss 0.10429980501200715  - accuracy: 0.84375\n",
      "At: 1497 [==========>] Loss 0.17751980682485297  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.12862003166860664  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.11373015024341684  - accuracy: 0.84375\n",
      "At: 1500 [==========>] Loss 0.10958929060575215  - accuracy: 0.84375\n",
      "At: 1501 [==========>] Loss 0.09455883589676947  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.14821719961211594  - accuracy: 0.8125\n",
      "At: 1503 [==========>] Loss 0.1257247675449878  - accuracy: 0.84375\n",
      "At: 1504 [==========>] Loss 0.13118597926053455  - accuracy: 0.875\n",
      "At: 1505 [==========>] Loss 0.1279026400100553  - accuracy: 0.8125\n",
      "At: 1506 [==========>] Loss 0.15833321704772346  - accuracy: 0.71875\n",
      "At: 1507 [==========>] Loss 0.13978075181413235  - accuracy: 0.8125\n",
      "At: 1508 [==========>] Loss 0.20555079507161794  - accuracy: 0.71875\n",
      "At: 1509 [==========>] Loss 0.08853545396699991  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.1284795384038075  - accuracy: 0.8125\n",
      "At: 1511 [==========>] Loss 0.09648194534304458  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.08402058487233088  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.13616119686410655  - accuracy: 0.84375\n",
      "At: 1514 [==========>] Loss 0.16100659587475946  - accuracy: 0.8125\n",
      "At: 1515 [==========>] Loss 0.13174297189864984  - accuracy: 0.8125\n",
      "At: 1516 [==========>] Loss 0.08440837899525452  - accuracy: 0.9375\n",
      "At: 1517 [==========>] Loss 0.14172033399302839  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.11136864997144778  - accuracy: 0.875\n",
      "At: 1519 [==========>] Loss 0.15234368117193975  - accuracy: 0.75\n",
      "At: 1520 [==========>] Loss 0.10835754162515651  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.09585493521936345  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.1796806037706807  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.10381797062020726  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.1416525832343042  - accuracy: 0.78125\n",
      "At: 1525 [==========>] Loss 0.13987043988877376  - accuracy: 0.78125\n",
      "At: 1526 [==========>] Loss 0.11652869982000605  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.13260002505302276  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.1559312146046141  - accuracy: 0.75\n",
      "At: 1529 [==========>] Loss 0.058364607885313356  - accuracy: 0.9375\n",
      "At: 1530 [==========>] Loss 0.05616803559800326  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.11663126569717959  - accuracy: 0.875\n",
      "At: 1532 [==========>] Loss 0.1654372682189686  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.12341436260155847  - accuracy: 0.8125\n",
      "At: 1534 [==========>] Loss 0.1105311049562014  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.15489607597737007  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.14718605894384817  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.09905129217978731  - accuracy: 0.90625\n",
      "At: 1538 [==========>] Loss 0.12286355298548589  - accuracy: 0.8125\n",
      "At: 1539 [==========>] Loss 0.09569663425446334  - accuracy: 0.875\n",
      "At: 1540 [==========>] Loss 0.14857069262365627  - accuracy: 0.75\n",
      "At: 1541 [==========>] Loss 0.13567021851006242  - accuracy: 0.875\n",
      "At: 1542 [==========>] Loss 0.10804805665084824  - accuracy: 0.84375\n",
      "At: 1543 [==========>] Loss 0.11532954713845958  - accuracy: 0.875\n",
      "At: 1544 [==========>] Loss 0.15985449871544866  - accuracy: 0.8125\n",
      "At: 1545 [==========>] Loss 0.21509587746998762  - accuracy: 0.6875\n",
      "At: 1546 [==========>] Loss 0.1424388725091714  - accuracy: 0.71875\n",
      "At: 1547 [==========>] Loss 0.12972486173696934  - accuracy: 0.8125\n",
      "At: 1548 [==========>] Loss 0.11596300479971758  - accuracy: 0.90625\n",
      "At: 1549 [==========>] Loss 0.1340563163713275  - accuracy: 0.84375\n",
      "At: 1550 [==========>] Loss 0.08156745300672884  - accuracy: 0.9375\n",
      "At: 1551 [==========>] Loss 0.1392180298078487  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.10330469447343954  - accuracy: 0.84375\n",
      "At: 1553 [==========>] Loss 0.07629279296276437  - accuracy: 0.9375\n",
      "At: 1554 [==========>] Loss 0.15661209069811313  - accuracy: 0.8125\n",
      "At: 1555 [==========>] Loss 0.11772041063622124  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.14560462633470944  - accuracy: 0.8125\n",
      "At: 1557 [==========>] Loss 0.08022254135813936  - accuracy: 0.90625\n",
      "At: 1558 [==========>] Loss 0.12484440487479002  - accuracy: 0.8125\n",
      "At: 1559 [==========>] Loss 0.09581712094651686  - accuracy: 0.875\n",
      "At: 1560 [==========>] Loss 0.1317001931053135  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.15601840288219895  - accuracy: 0.78125\n",
      "At: 1562 [==========>] Loss 0.11504358238079272  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.10960318661458576  - accuracy: 0.875\n",
      "At: 1564 [==========>] Loss 0.11266824836049424  - accuracy: 0.8125\n",
      "At: 1565 [==========>] Loss 0.12982663902405076  - accuracy: 0.875\n",
      "At: 1566 [==========>] Loss 0.17911631385801569  - accuracy: 0.75\n",
      "At: 1567 [==========>] Loss 0.1516288606788264  - accuracy: 0.78125\n",
      "At: 1568 [==========>] Loss 0.07083380267641191  - accuracy: 0.90625\n",
      "At: 1569 [==========>] Loss 0.11478464579167719  - accuracy: 0.8125\n",
      "At: 1570 [==========>] Loss 0.09049928488577255  - accuracy: 0.90625\n",
      "At: 1571 [==========>] Loss 0.13878362711275785  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.12587990475249788  - accuracy: 0.875\n",
      "At: 1573 [==========>] Loss 0.038778051575788126  - accuracy: 0.96875\n",
      "At: 1574 [==========>] Loss 0.11733226532497952  - accuracy: 0.84375\n",
      "At: 1575 [==========>] Loss 0.10605817691865654  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.13545671851336644  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.0809386189883281  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.06692992806196787  - accuracy: 0.90625\n",
      "At: 1579 [==========>] Loss 0.10463406756751856  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.12573656705596997  - accuracy: 0.8125\n",
      "At: 1581 [==========>] Loss 0.10486914774587772  - accuracy: 0.875\n",
      "At: 1582 [==========>] Loss 0.17477619073444123  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.07632458350305865  - accuracy: 0.875\n",
      "At: 1584 [==========>] Loss 0.12916858803759262  - accuracy: 0.8125\n",
      "At: 1585 [==========>] Loss 0.08903516038816076  - accuracy: 0.90625\n",
      "At: 1586 [==========>] Loss 0.11399766334643496  - accuracy: 0.84375\n",
      "At: 1587 [==========>] Loss 0.10429152724966528  - accuracy: 0.875\n",
      "At: 1588 [==========>] Loss 0.15608290034165828  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.1522286878538304  - accuracy: 0.8125\n",
      "At: 1590 [==========>] Loss 0.1524797684619501  - accuracy: 0.8125\n",
      "At: 1591 [==========>] Loss 0.1283657730849967  - accuracy: 0.78125\n",
      "At: 1592 [==========>] Loss 0.0669116900318027  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.1984664753420594  - accuracy: 0.75\n",
      "At: 1594 [==========>] Loss 0.07548330366766216  - accuracy: 0.9375\n",
      "At: 1595 [==========>] Loss 0.11391163128842458  - accuracy: 0.875\n",
      "At: 1596 [==========>] Loss 0.17803309695217198  - accuracy: 0.71875\n",
      "At: 1597 [==========>] Loss 0.14476824330581467  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.15387280754507351  - accuracy: 0.84375\n",
      "At: 1599 [==========>] Loss 0.2026643920441144  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.15960769183979945  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.08894731941062722  - accuracy: 0.90625\n",
      "At: 1602 [==========>] Loss 0.11305673453246232  - accuracy: 0.84375\n",
      "At: 1603 [==========>] Loss 0.1838891552750367  - accuracy: 0.75\n",
      "At: 1604 [==========>] Loss 0.23674123785867873  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.10012850057021166  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.08705005069568483  - accuracy: 0.90625\n",
      "At: 1607 [==========>] Loss 0.20890990656248276  - accuracy: 0.6875\n",
      "At: 1608 [==========>] Loss 0.1307119281445568  - accuracy: 0.84375\n",
      "At: 1609 [==========>] Loss 0.15595822308468296  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.1597169243484602  - accuracy: 0.8125\n",
      "At: 1611 [==========>] Loss 0.10079784509655623  - accuracy: 0.8125\n",
      "At: 1612 [==========>] Loss 0.0633869282450387  - accuracy: 0.90625\n",
      "At: 1613 [==========>] Loss 0.12008830860895908  - accuracy: 0.875\n",
      "At: 1614 [==========>] Loss 0.13806020829572752  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.09500519453563841  - accuracy: 0.875\n",
      "At: 1616 [==========>] Loss 0.10998836605728977  - accuracy: 0.84375\n",
      "At: 1617 [==========>] Loss 0.09833167968682933  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.1332647093685588  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.15455184912283668  - accuracy: 0.78125\n",
      "At: 1620 [==========>] Loss 0.0849881061552965  - accuracy: 0.875\n",
      "At: 1621 [==========>] Loss 0.10726261635634292  - accuracy: 0.84375\n",
      "At: 1622 [==========>] Loss 0.1670522376518731  - accuracy: 0.75\n",
      "At: 1623 [==========>] Loss 0.0978884813899352  - accuracy: 0.84375\n",
      "At: 1624 [==========>] Loss 0.1759885892100344  - accuracy: 0.78125\n",
      "At: 1625 [==========>] Loss 0.16133228077953138  - accuracy: 0.84375\n",
      "At: 1626 [==========>] Loss 0.08437275756423906  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.11559050125274599  - accuracy: 0.875\n",
      "At: 1628 [==========>] Loss 0.17401691512239426  - accuracy: 0.71875\n",
      "At: 1629 [==========>] Loss 0.15207442813642777  - accuracy: 0.75\n",
      "At: 1630 [==========>] Loss 0.11521274078082963  - accuracy: 0.84375\n",
      "At: 1631 [==========>] Loss 0.11497308351588006  - accuracy: 0.84375\n",
      "At: 1632 [==========>] Loss 0.09442050259371806  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.0926701995657464  - accuracy: 0.84375\n",
      "At: 1634 [==========>] Loss 0.1219880184869492  - accuracy: 0.78125\n",
      "At: 1635 [==========>] Loss 0.19558214691457132  - accuracy: 0.71875\n",
      "At: 1636 [==========>] Loss 0.1350414823419645  - accuracy: 0.78125\n",
      "At: 1637 [==========>] Loss 0.0900078283425656  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.13627977535425395  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.15521605102975589  - accuracy: 0.71875\n",
      "At: 1640 [==========>] Loss 0.12354853299132953  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.09318028830730961  - accuracy: 0.8125\n",
      "At: 1642 [==========>] Loss 0.10863432321471264  - accuracy: 0.875\n",
      "At: 1643 [==========>] Loss 0.09824042650723455  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.11955984317689781  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.053355330034743076  - accuracy: 0.96875\n",
      "At: 1646 [==========>] Loss 0.13414819790011534  - accuracy: 0.84375\n",
      "At: 1647 [==========>] Loss 0.16532586726810986  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.15294861150180847  - accuracy: 0.8125\n",
      "At: 1649 [==========>] Loss 0.08538299114729599  - accuracy: 0.84375\n",
      "At: 1650 [==========>] Loss 0.09451212250336301  - accuracy: 0.875\n",
      "At: 1651 [==========>] Loss 0.12850929145376516  - accuracy: 0.84375\n",
      "At: 1652 [==========>] Loss 0.07741153177195569  - accuracy: 0.875\n",
      "At: 1653 [==========>] Loss 0.07611274366952504  - accuracy: 0.875\n",
      "At: 1654 [==========>] Loss 0.07807627250745137  - accuracy: 0.875\n",
      "At: 1655 [==========>] Loss 0.17194132754493968  - accuracy: 0.75\n",
      "At: 1656 [==========>] Loss 0.10147637003945958  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.12849039050140268  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.23059078267283858  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.08588799456994126  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.05609800251977451  - accuracy: 0.9375\n",
      "At: 1661 [==========>] Loss 0.09712405262751833  - accuracy: 0.875\n",
      "At: 1662 [==========>] Loss 0.0998194888115675  - accuracy: 0.90625\n",
      "At: 1663 [==========>] Loss 0.15745871224850433  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.11243118202113074  - accuracy: 0.875\n",
      "At: 1665 [==========>] Loss 0.09472040931562009  - accuracy: 0.875\n",
      "At: 1666 [==========>] Loss 0.10886386019394836  - accuracy: 0.8125\n",
      "At: 1667 [==========>] Loss 0.17194449913463278  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.17535122408901238  - accuracy: 0.71875\n",
      "At: 1669 [==========>] Loss 0.13457766134065818  - accuracy: 0.8125\n",
      "At: 1670 [==========>] Loss 0.06987899614791224  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.13192985808211605  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.15109959659099526  - accuracy: 0.75\n",
      "At: 1673 [==========>] Loss 0.0667268191082152  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.14810462558852525  - accuracy: 0.71875\n",
      "At: 1675 [==========>] Loss 0.17424801191969902  - accuracy: 0.78125\n",
      "At: 1676 [==========>] Loss 0.20550659215325134  - accuracy: 0.75\n",
      "At: 1677 [==========>] Loss 0.12616486480590416  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.1815550176220282  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.11891803766546719  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.13809380869650104  - accuracy: 0.8125\n",
      "At: 1681 [==========>] Loss 0.14009933089941295  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.09061055925568684  - accuracy: 0.90625\n",
      "At: 1683 [==========>] Loss 0.17157283411589824  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.12724784415875473  - accuracy: 0.8125\n",
      "At: 1685 [==========>] Loss 0.11580598487685467  - accuracy: 0.84375\n",
      "At: 1686 [==========>] Loss 0.1205564406130028  - accuracy: 0.90625\n",
      "At: 1687 [==========>] Loss 0.1406277602192687  - accuracy: 0.8125\n",
      "At: 1688 [==========>] Loss 0.055289948588884  - accuracy: 0.9375\n",
      "At: 1689 [==========>] Loss 0.11429936171270189  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.1362487431773521  - accuracy: 0.84375\n",
      "At: 1691 [==========>] Loss 0.10168634177317079  - accuracy: 0.875\n",
      "At: 1692 [==========>] Loss 0.17514150381676938  - accuracy: 0.6875\n",
      "At: 1693 [==========>] Loss 0.09244690086968466  - accuracy: 0.9375\n",
      "At: 1694 [==========>] Loss 0.08736112374751888  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.1312081796909854  - accuracy: 0.78125\n",
      "At: 1696 [==========>] Loss 0.14604524470207014  - accuracy: 0.75\n",
      "At: 1697 [==========>] Loss 0.094574687718536  - accuracy: 0.9375\n",
      "At: 1698 [==========>] Loss 0.07624935361637519  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.09971022470269776  - accuracy: 0.84375\n",
      "At: 1700 [==========>] Loss 0.11521054563892393  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.1042630416317853  - accuracy: 0.875\n",
      "At: 1702 [==========>] Loss 0.10932474097455419  - accuracy: 0.875\n",
      "At: 1703 [==========>] Loss 0.16403757477727413  - accuracy: 0.78125\n",
      "At: 1704 [==========>] Loss 0.08554941719345432  - accuracy: 0.875\n",
      "At: 1705 [==========>] Loss 0.10536810690389234  - accuracy: 0.9375\n",
      "At: 1706 [==========>] Loss 0.1698967307994078  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.19338124586438168  - accuracy: 0.8125\n",
      "At: 1708 [==========>] Loss 0.0906088273005868  - accuracy: 0.84375\n",
      "At: 1709 [==========>] Loss 0.15167063520357407  - accuracy: 0.78125\n",
      "At: 1710 [==========>] Loss 0.12804984898839528  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.07066340145895925  - accuracy: 0.9375\n",
      "At: 1712 [==========>] Loss 0.10322281207157609  - accuracy: 0.875\n",
      "At: 1713 [==========>] Loss 0.08844615276307448  - accuracy: 0.90625\n",
      "At: 1714 [==========>] Loss 0.14500595636089625  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.12518202770504133  - accuracy: 0.875\n",
      "At: 1716 [==========>] Loss 0.06360352216252588  - accuracy: 0.9375\n",
      "At: 1717 [==========>] Loss 0.08237956689253238  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.13555984909718893  - accuracy: 0.84375\n",
      "At: 1719 [==========>] Loss 0.11760848511627886  - accuracy: 0.84375\n",
      "At: 1720 [==========>] Loss 0.07033692109671008  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.17781232215308834  - accuracy: 0.75\n",
      "At: 1722 [==========>] Loss 0.057270054317593154  - accuracy: 0.9375\n",
      "At: 1723 [==========>] Loss 0.20547157191285437  - accuracy: 0.6875\n",
      "At: 1724 [==========>] Loss 0.09085559137225847  - accuracy: 0.90625\n",
      "At: 1725 [==========>] Loss 0.14052492643237596  - accuracy: 0.8125\n",
      "At: 1726 [==========>] Loss 0.12289159995637522  - accuracy: 0.84375\n",
      "At: 1727 [==========>] Loss 0.13203886951870358  - accuracy: 0.8125\n",
      "At: 1728 [==========>] Loss 0.10751490652063803  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.1775898693034988  - accuracy: 0.71875\n",
      "At: 1730 [==========>] Loss 0.13852005874482878  - accuracy: 0.75\n",
      "At: 1731 [==========>] Loss 0.12039777929941983  - accuracy: 0.8125\n",
      "At: 1732 [==========>] Loss 0.08664216016336082  - accuracy: 0.875\n",
      "At: 1733 [==========>] Loss 0.16064235185119372  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.09859354589433644  - accuracy: 0.90625\n",
      "At: 1735 [==========>] Loss 0.15562062227025789  - accuracy: 0.78125\n",
      "At: 1736 [==========>] Loss 0.11825438208425756  - accuracy: 0.8125\n",
      "At: 1737 [==========>] Loss 0.17548709872419418  - accuracy: 0.71875\n",
      "At: 1738 [==========>] Loss 0.10750746373702141  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.1305185312690667  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.11944628804811085  - accuracy: 0.84375\n",
      "At: 1741 [==========>] Loss 0.11900290991168988  - accuracy: 0.875\n",
      "At: 1742 [==========>] Loss 0.036419921806351986  - accuracy: 0.96875\n",
      "At: 1743 [==========>] Loss 0.12991159154366833  - accuracy: 0.8125\n",
      "At: 1744 [==========>] Loss 0.09583432518948276  - accuracy: 0.90625\n",
      "At: 1745 [==========>] Loss 0.13379948023819555  - accuracy: 0.78125\n",
      "At: 1746 [==========>] Loss 0.15721304241379808  - accuracy: 0.78125\n",
      "At: 1747 [==========>] Loss 0.1237506497825496  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.1220450319163562  - accuracy: 0.8125\n",
      "At: 1749 [==========>] Loss 0.09021293782709994  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.10688406683078663  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.18216051390005775  - accuracy: 0.75\n",
      "At: 1752 [==========>] Loss 0.08250061712632772  - accuracy: 0.875\n",
      "At: 1753 [==========>] Loss 0.09556793715951167  - accuracy: 0.90625\n",
      "At: 1754 [==========>] Loss 0.11828616985642305  - accuracy: 0.875\n",
      "At: 1755 [==========>] Loss 0.08281986591734719  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.14951887403280756  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.15326067441795846  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.07303720111139478  - accuracy: 0.875\n",
      "At: 1759 [==========>] Loss 0.09246485614393836  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.08887257639132845  - accuracy: 0.84375\n",
      "At: 1761 [==========>] Loss 0.12227959555264394  - accuracy: 0.78125\n",
      "At: 1762 [==========>] Loss 0.15451455082395638  - accuracy: 0.75\n",
      "At: 1763 [==========>] Loss 0.08564200603943106  - accuracy: 0.90625\n",
      "At: 1764 [==========>] Loss 0.13254734466530915  - accuracy: 0.78125\n",
      "At: 1765 [==========>] Loss 0.11967975677272605  - accuracy: 0.84375\n",
      "At: 1766 [==========>] Loss 0.07800533544651148  - accuracy: 0.9375\n",
      "At: 1767 [==========>] Loss 0.10006244235867406  - accuracy: 0.875\n",
      "At: 1768 [==========>] Loss 0.0975533056119086  - accuracy: 0.875\n",
      "At: 1769 [==========>] Loss 0.06783665726704424  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.06125222730685051  - accuracy: 0.96875\n",
      "At: 1771 [==========>] Loss 0.17145768622788293  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.13052516977288844  - accuracy: 0.8125\n",
      "At: 1773 [==========>] Loss 0.09576618430992345  - accuracy: 0.875\n",
      "At: 1774 [==========>] Loss 0.1358036415101369  - accuracy: 0.8125\n",
      "At: 1775 [==========>] Loss 0.09642508696306691  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.11564693982608479  - accuracy: 0.875\n",
      "At: 1777 [==========>] Loss 0.11883403197604622  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.10306817840661951  - accuracy: 0.875\n",
      "At: 1779 [==========>] Loss 0.08765312096378663  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.134617703614416  - accuracy: 0.84375\n",
      "At: 1781 [==========>] Loss 0.18063453328555806  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.10583962369821048  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.10457811886011494  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.07014771789144715  - accuracy: 0.96875\n",
      "At: 1785 [==========>] Loss 0.09128522336265046  - accuracy: 0.84375\n",
      "At: 1786 [==========>] Loss 0.12309473645742164  - accuracy: 0.84375\n",
      "At: 1787 [==========>] Loss 0.12462229111103791  - accuracy: 0.84375\n",
      "At: 1788 [==========>] Loss 0.11467814682336389  - accuracy: 0.84375\n",
      "At: 1789 [==========>] Loss 0.11343412203194034  - accuracy: 0.90625\n",
      "At: 1790 [==========>] Loss 0.17124137212876053  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.0668118318117448  - accuracy: 0.9375\n",
      "At: 1792 [==========>] Loss 0.12124650533153408  - accuracy: 0.875\n",
      "At: 1793 [==========>] Loss 0.09505604475320394  - accuracy: 0.875\n",
      "At: 1794 [==========>] Loss 0.15734659610592286  - accuracy: 0.78125\n",
      "At: 1795 [==========>] Loss 0.08676184823821267  - accuracy: 0.90625\n",
      "At: 1796 [==========>] Loss 0.15059502948168108  - accuracy: 0.78125\n",
      "At: 1797 [==========>] Loss 0.11922199768362143  - accuracy: 0.90625\n",
      "At: 1798 [==========>] Loss 0.1281854481010334  - accuracy: 0.8125\n",
      "At: 1799 [==========>] Loss 0.08225002303794295  - accuracy: 0.90625\n",
      "At: 1800 [==========>] Loss 0.12237443072702095  - accuracy: 0.84375\n",
      "At: 1801 [==========>] Loss 0.1871109029229382  - accuracy: 0.6875\n",
      "At: 1802 [==========>] Loss 0.11010197809268119  - accuracy: 0.8125\n",
      "At: 1803 [==========>] Loss 0.17281379711420047  - accuracy: 0.78125\n",
      "At: 1804 [==========>] Loss 0.13159909164577338  - accuracy: 0.8125\n",
      "At: 1805 [==========>] Loss 0.047719763477271554  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.16289751233570715  - accuracy: 0.8125\n",
      "At: 1807 [==========>] Loss 0.14337358431130992  - accuracy: 0.8125\n",
      "At: 1808 [==========>] Loss 0.16877842748504088  - accuracy: 0.75\n",
      "At: 1809 [==========>] Loss 0.09410973542222874  - accuracy: 0.90625\n",
      "At: 1810 [==========>] Loss 0.15582850453133631  - accuracy: 0.75\n",
      "At: 1811 [==========>] Loss 0.1245968834372305  - accuracy: 0.84375\n",
      "At: 1812 [==========>] Loss 0.09967895546409226  - accuracy: 0.875\n",
      "At: 1813 [==========>] Loss 0.13013522714365183  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.12897742782173774  - accuracy: 0.8125\n",
      "At: 1815 [==========>] Loss 0.16451542424383164  - accuracy: 0.75\n",
      "At: 1816 [==========>] Loss 0.044865390327997676  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.15821512629482354  - accuracy: 0.71875\n",
      "At: 1818 [==========>] Loss 0.13104240226289585  - accuracy: 0.84375\n",
      "At: 1819 [==========>] Loss 0.19640427398616553  - accuracy: 0.65625\n",
      "At: 1820 [==========>] Loss 0.10904397726152397  - accuracy: 0.875\n",
      "At: 1821 [==========>] Loss 0.09166951460925954  - accuracy: 0.90625\n",
      "At: 1822 [==========>] Loss 0.14815035682908134  - accuracy: 0.8125\n",
      "At: 1823 [==========>] Loss 0.17121354614143014  - accuracy: 0.71875\n",
      "At: 1824 [==========>] Loss 0.15463210570772914  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.12260887296012524  - accuracy: 0.8125\n",
      "At: 1826 [==========>] Loss 0.06346366139014796  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.11299677546022616  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.13490983581212113  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.14335017937993094  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.12925457753719755  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.14596933306375268  - accuracy: 0.78125\n",
      "At: 1832 [==========>] Loss 0.1260455654431027  - accuracy: 0.8125\n",
      "At: 1833 [==========>] Loss 0.14645614378742222  - accuracy: 0.8125\n",
      "At: 1834 [==========>] Loss 0.07429025984120922  - accuracy: 0.90625\n",
      "At: 1835 [==========>] Loss 0.1763229832199179  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.12289495656211571  - accuracy: 0.875\n",
      "At: 1837 [==========>] Loss 0.04558692499982542  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.10717117443601763  - accuracy: 0.84375\n",
      "At: 1839 [==========>] Loss 0.10118462629231684  - accuracy: 0.84375\n",
      "At: 1840 [==========>] Loss 0.12492183430567264  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.10931069665173804  - accuracy: 0.875\n",
      "At: 1842 [==========>] Loss 0.1295210236073267  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.09491375735111623  - accuracy: 0.875\n",
      "At: 1844 [==========>] Loss 0.09211576408233574  - accuracy: 0.90625\n",
      "At: 1845 [==========>] Loss 0.17011110674503635  - accuracy: 0.6875\n",
      "At: 1846 [==========>] Loss 0.16346284016513785  - accuracy: 0.84375\n",
      "At: 1847 [==========>] Loss 0.06045051615624178  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.05653272267787633  - accuracy: 0.96875\n",
      "At: 1849 [==========>] Loss 0.18031894209482488  - accuracy: 0.78125\n",
      "At: 1850 [==========>] Loss 0.055138934592297184  - accuracy: 0.9375\n",
      "At: 1851 [==========>] Loss 0.16370878547560025  - accuracy: 0.75\n",
      "At: 1852 [==========>] Loss 0.09931960381226587  - accuracy: 0.90625\n",
      "At: 1853 [==========>] Loss 0.11498303166929566  - accuracy: 0.8125\n",
      "At: 1854 [==========>] Loss 0.12716991577007272  - accuracy: 0.84375\n",
      "At: 1855 [==========>] Loss 0.185366645918972  - accuracy: 0.75\n",
      "At: 1856 [==========>] Loss 0.10832761235297036  - accuracy: 0.84375\n",
      "At: 1857 [==========>] Loss 0.1642537180700856  - accuracy: 0.78125\n",
      "At: 1858 [==========>] Loss 0.10365347881173126  - accuracy: 0.875\n",
      "At: 1859 [==========>] Loss 0.1653993296716675  - accuracy: 0.78125\n",
      "At: 1860 [==========>] Loss 0.1859283781083385  - accuracy: 0.6875\n",
      "At: 1861 [==========>] Loss 0.08202085100924661  - accuracy: 0.9375\n",
      "At: 1862 [==========>] Loss 0.17224982796007413  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.14831277423914274  - accuracy: 0.78125\n",
      "At: 1864 [==========>] Loss 0.12541604545641177  - accuracy: 0.8125\n",
      "At: 1865 [==========>] Loss 0.09730867910362181  - accuracy: 0.875\n",
      "At: 1866 [==========>] Loss 0.21917344894106272  - accuracy: 0.65625\n",
      "At: 1867 [==========>] Loss 0.11543672316001656  - accuracy: 0.875\n",
      "At: 1868 [==========>] Loss 0.13411545851494777  - accuracy: 0.8125\n",
      "At: 1869 [==========>] Loss 0.17511537281596834  - accuracy: 0.75\n",
      "At: 1870 [==========>] Loss 0.10713386595976641  - accuracy: 0.84375\n",
      "At: 1871 [==========>] Loss 0.13177014715460095  - accuracy: 0.875\n",
      "At: 1872 [==========>] Loss 0.12654384275741967  - accuracy: 0.8125\n",
      "At: 1873 [==========>] Loss 0.08213041540186101  - accuracy: 0.875\n",
      "At: 1874 [==========>] Loss 0.1601913634615578  - accuracy: 0.78125\n",
      "At: 1875 [==========>] Loss 0.09238404396853407  - accuracy: 0.90625\n",
      "At: 1876 [==========>] Loss 0.19961158514355665  - accuracy: 0.6875\n",
      "At: 1877 [==========>] Loss 0.09456649082355656  - accuracy: 0.84375\n",
      "At: 1878 [==========>] Loss 0.09384405807654826  - accuracy: 0.90625\n",
      "At: 1879 [==========>] Loss 0.1059816477357051  - accuracy: 0.875\n",
      "At: 1880 [==========>] Loss 0.10841647911042879  - accuracy: 0.84375\n",
      "At: 1881 [==========>] Loss 0.09986135397885526  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.10359510569514752  - accuracy: 0.84375\n",
      "At: 1883 [==========>] Loss 0.13551983814302948  - accuracy: 0.84375\n",
      "At: 1884 [==========>] Loss 0.08502377141482739  - accuracy: 0.875\n",
      "At: 1885 [==========>] Loss 0.09933869193000221  - accuracy: 0.875\n",
      "At: 1886 [==========>] Loss 0.133506770039509  - accuracy: 0.78125\n",
      "At: 1887 [==========>] Loss 0.08113321195744105  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.1485296222373457  - accuracy: 0.84375\n",
      "At: 1889 [==========>] Loss 0.11313249281547483  - accuracy: 0.8125\n",
      "At: 1890 [==========>] Loss 0.16681979123718366  - accuracy: 0.78125\n",
      "At: 1891 [==========>] Loss 0.06362144140210406  - accuracy: 0.9375\n",
      "At: 1892 [==========>] Loss 0.0916088138641209  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.09533793305326306  - accuracy: 0.90625\n",
      "At: 1894 [==========>] Loss 0.09703301138736069  - accuracy: 0.90625\n",
      "At: 1895 [==========>] Loss 0.0760096983462614  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.11809992344463434  - accuracy: 0.84375\n",
      "At: 1897 [==========>] Loss 0.07050390428045615  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.107773561032734  - accuracy: 0.90625\n",
      "At: 1899 [==========>] Loss 0.08138474580930838  - accuracy: 0.875\n",
      "At: 1900 [==========>] Loss 0.11762433130766259  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.10806062626169019  - accuracy: 0.875\n",
      "At: 1902 [==========>] Loss 0.13068640238771795  - accuracy: 0.8125\n",
      "At: 1903 [==========>] Loss 0.14309675679364062  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.049766844132062914  - accuracy: 0.96875\n",
      "At: 1905 [==========>] Loss 0.14875775517617104  - accuracy: 0.75\n",
      "At: 1906 [==========>] Loss 0.10096790264370664  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.08531589215472449  - accuracy: 0.875\n",
      "At: 1908 [==========>] Loss 0.07113208357637923  - accuracy: 0.90625\n",
      "At: 1909 [==========>] Loss 0.09956340482819329  - accuracy: 0.875\n",
      "At: 1910 [==========>] Loss 0.06762422304297945  - accuracy: 0.9375\n",
      "At: 1911 [==========>] Loss 0.11814184413343022  - accuracy: 0.75\n",
      "At: 1912 [==========>] Loss 0.1137382424957937  - accuracy: 0.8125\n",
      "At: 1913 [==========>] Loss 0.1821962628841537  - accuracy: 0.75\n",
      "At: 1914 [==========>] Loss 0.0841235971272615  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.09563839851487725  - accuracy: 0.90625\n",
      "At: 1916 [==========>] Loss 0.13887776490501705  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.16312271586603316  - accuracy: 0.75\n",
      "At: 1918 [==========>] Loss 0.13666314189120377  - accuracy: 0.75\n",
      "At: 1919 [==========>] Loss 0.11809604192673179  - accuracy: 0.84375\n",
      "At: 1920 [==========>] Loss 0.12289243683273605  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.12530082407322415  - accuracy: 0.8125\n",
      "At: 1922 [==========>] Loss 0.128571003749782  - accuracy: 0.6875\n",
      "At: 1923 [==========>] Loss 0.215561523646928  - accuracy: 0.65625\n",
      "At: 1924 [==========>] Loss 0.13304542986728723  - accuracy: 0.8125\n",
      "At: 1925 [==========>] Loss 0.1793121298706818  - accuracy: 0.71875\n",
      "At: 1926 [==========>] Loss 0.10968571562409003  - accuracy: 0.84375\n",
      "At: 1927 [==========>] Loss 0.12269233317890033  - accuracy: 0.8125\n",
      "At: 1928 [==========>] Loss 0.13307777107918214  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.18913890199086766  - accuracy: 0.71875\n",
      "At: 1930 [==========>] Loss 0.18257879792503778  - accuracy: 0.6875\n",
      "At: 1931 [==========>] Loss 0.11525252533117575  - accuracy: 0.84375\n",
      "At: 1932 [==========>] Loss 0.16299068144197948  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.1240092993351449  - accuracy: 0.78125\n",
      "At: 1934 [==========>] Loss 0.15685475987399122  - accuracy: 0.78125\n",
      "At: 1935 [==========>] Loss 0.11849998844917663  - accuracy: 0.84375\n",
      "At: 1936 [==========>] Loss 0.11578126994269683  - accuracy: 0.875\n",
      "At: 1937 [==========>] Loss 0.1594440333322524  - accuracy: 0.75\n",
      "At: 1938 [==========>] Loss 0.18834089301652518  - accuracy: 0.71875\n",
      "At: 1939 [==========>] Loss 0.1113948239300906  - accuracy: 0.875\n",
      "At: 1940 [==========>] Loss 0.12226990757740039  - accuracy: 0.8125\n",
      "At: 1941 [==========>] Loss 0.13548548697484328  - accuracy: 0.8125\n",
      "At: 1942 [==========>] Loss 0.14481761803601273  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.13601941641088175  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.1450610969161611  - accuracy: 0.8125\n",
      "At: 1945 [==========>] Loss 0.16165281505249812  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.1112870019801223  - accuracy: 0.8125\n",
      "At: 1947 [==========>] Loss 0.13477175161237462  - accuracy: 0.78125\n",
      "At: 1948 [==========>] Loss 0.12146878438285416  - accuracy: 0.84375\n",
      "At: 1949 [==========>] Loss 0.0783169545079132  - accuracy: 0.90625\n",
      "At: 1950 [==========>] Loss 0.14661700569755906  - accuracy: 0.8125\n",
      "At: 1951 [==========>] Loss 0.12751363676438104  - accuracy: 0.84375\n",
      "At: 1952 [==========>] Loss 0.080815211115639  - accuracy: 0.90625\n",
      "At: 1953 [==========>] Loss 0.06741170957331272  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.15980937247309202  - accuracy: 0.75\n",
      "At: 1955 [==========>] Loss 0.07947632004855992  - accuracy: 0.90625\n",
      "At: 1956 [==========>] Loss 0.10900284523302563  - accuracy: 0.84375\n",
      "At: 1957 [==========>] Loss 0.05579005698391404  - accuracy: 0.96875\n",
      "At: 1958 [==========>] Loss 0.09649297322946963  - accuracy: 0.90625\n",
      "At: 1959 [==========>] Loss 0.1358065552428535  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.05678468601993965  - accuracy: 0.9375\n",
      "At: 1961 [==========>] Loss 0.1973497485508856  - accuracy: 0.75\n",
      "At: 1962 [==========>] Loss 0.1849512368237251  - accuracy: 0.75\n",
      "At: 1963 [==========>] Loss 0.061258915860330665  - accuracy: 0.9375\n",
      "At: 1964 [==========>] Loss 0.19629443180521472  - accuracy: 0.65625\n",
      "At: 1965 [==========>] Loss 0.15350812524747964  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.13139611490520015  - accuracy: 0.8125\n",
      "At: 1967 [==========>] Loss 0.14104997261794705  - accuracy: 0.78125\n",
      "At: 1968 [==========>] Loss 0.1810164230264582  - accuracy: 0.78125\n",
      "At: 1969 [==========>] Loss 0.16635253592736132  - accuracy: 0.78125\n",
      "At: 1970 [==========>] Loss 0.11327160487347696  - accuracy: 0.875\n",
      "At: 1971 [==========>] Loss 0.20965536611367186  - accuracy: 0.71875\n",
      "At: 1972 [==========>] Loss 0.0861685632230877  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.1184847930530149  - accuracy: 0.8125\n",
      "At: 1974 [==========>] Loss 0.1314988380515095  - accuracy: 0.8125\n",
      "At: 1975 [==========>] Loss 0.15665139521199967  - accuracy: 0.75\n",
      "At: 1976 [==========>] Loss 0.08406704803161465  - accuracy: 0.875\n",
      "At: 1977 [==========>] Loss 0.08158124876542772  - accuracy: 0.875\n",
      "At: 1978 [==========>] Loss 0.15403145800627085  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.12268974272225948  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.11637159322575819  - accuracy: 0.84375\n",
      "At: 1981 [==========>] Loss 0.15583352913527837  - accuracy: 0.71875\n",
      "At: 1982 [==========>] Loss 0.07852004700270522  - accuracy: 0.90625\n",
      "At: 1983 [==========>] Loss 0.1533223754592095  - accuracy: 0.78125\n",
      "At: 1984 [==========>] Loss 0.09405267826874675  - accuracy: 0.90625\n",
      "At: 1985 [==========>] Loss 0.12743835239312623  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.16720817756718281  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.10727576232687189  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.06916761728817218  - accuracy: 0.90625\n",
      "At: 1989 [==========>] Loss 0.12113411767836249  - accuracy: 0.8125\n",
      "At: 1990 [==========>] Loss 0.09805833897329383  - accuracy: 0.875\n",
      "At: 1991 [==========>] Loss 0.13972966297532843  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.13461208191957907  - accuracy: 0.8125\n",
      "At: 1993 [==========>] Loss 0.14793416922449631  - accuracy: 0.78125\n",
      "At: 1994 [==========>] Loss 0.11284314003196377  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.20780641325638372  - accuracy: 0.6875\n",
      "At: 1996 [==========>] Loss 0.11690226345318924  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.19074854620553255  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.15162885037381219  - accuracy: 0.71875\n",
      "At: 1999 [==========>] Loss 0.10197792589654368  - accuracy: 0.875\n",
      "At: 2000 [==========>] Loss 0.12970472284144102  - accuracy: 0.8125\n",
      "At: 2001 [==========>] Loss 0.0737143003993167  - accuracy: 0.90625\n",
      "At: 2002 [==========>] Loss 0.060362939726932935  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.11811929464948429  - accuracy: 0.8125\n",
      "At: 2004 [==========>] Loss 0.13154087516864768  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.1232699500227917  - accuracy: 0.8125\n",
      "At: 2006 [==========>] Loss 0.13404026317143936  - accuracy: 0.8125\n",
      "At: 2007 [==========>] Loss 0.1083364161843524  - accuracy: 0.84375\n",
      "At: 2008 [==========>] Loss 0.1277544322502075  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13752903224655252  - accuracy: 0.8125\n",
      "At: 2010 [==========>] Loss 0.11051949029294339  - accuracy: 0.8125\n",
      "At: 2011 [==========>] Loss 0.11336252249308998  - accuracy: 0.875\n",
      "At: 2012 [==========>] Loss 0.08805204196048715  - accuracy: 0.875\n",
      "At: 2013 [==========>] Loss 0.10985250737058669  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.23723437722981208  - accuracy: 0.65625\n",
      "At: 2015 [==========>] Loss 0.06346018051013438  - accuracy: 0.9375\n",
      "At: 2016 [==========>] Loss 0.10714134936620891  - accuracy: 0.90625\n",
      "At: 2017 [==========>] Loss 0.07288300593215696  - accuracy: 0.9375\n",
      "At: 2018 [==========>] Loss 0.10093307179739841  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.11603447788420748  - accuracy: 0.84375\n",
      "At: 2020 [==========>] Loss 0.07000362190664473  - accuracy: 0.9375\n",
      "At: 2021 [==========>] Loss 0.09357735566276289  - accuracy: 0.875\n",
      "At: 2022 [==========>] Loss 0.12488667248896587  - accuracy: 0.84375\n",
      "At: 2023 [==========>] Loss 0.1079920230898238  - accuracy: 0.84375\n",
      "At: 2024 [==========>] Loss 0.07295983038127778  - accuracy: 0.96875\n",
      "At: 2025 [==========>] Loss 0.1620165553775177  - accuracy: 0.8125\n",
      "At: 2026 [==========>] Loss 0.09952763650085071  - accuracy: 0.875\n",
      "At: 2027 [==========>] Loss 0.15549825144035725  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.10420913789974814  - accuracy: 0.8125\n",
      "At: 2029 [==========>] Loss 0.09538661230010022  - accuracy: 0.9375\n",
      "At: 2030 [==========>] Loss 0.1560474763154272  - accuracy: 0.75\n",
      "At: 2031 [==========>] Loss 0.13876339364729873  - accuracy: 0.84375\n",
      "At: 2032 [==========>] Loss 0.1415809282283347  - accuracy: 0.78125\n",
      "At: 2033 [==========>] Loss 0.1514818369091856  - accuracy: 0.78125\n",
      "At: 2034 [==========>] Loss 0.18987102026585748  - accuracy: 0.6875\n",
      "At: 2035 [==========>] Loss 0.10804611153700415  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.12826410255450932  - accuracy: 0.84375\n",
      "At: 2037 [==========>] Loss 0.11248244283626771  - accuracy: 0.8125\n",
      "At: 2038 [==========>] Loss 0.07867183600703972  - accuracy: 0.9375\n",
      "At: 2039 [==========>] Loss 0.08869858179969915  - accuracy: 0.90625\n",
      "At: 2040 [==========>] Loss 0.10500764508469826  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.05451211032313586  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.0941236604389819  - accuracy: 0.875\n",
      "At: 2043 [==========>] Loss 0.08513992842646866  - accuracy: 0.90625\n",
      "At: 2044 [==========>] Loss 0.09551470015582395  - accuracy: 0.84375\n",
      "At: 2045 [==========>] Loss 0.20427346131198565  - accuracy: 0.71875\n",
      "At: 2046 [==========>] Loss 0.052869063844960026  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.07579008947977219  - accuracy: 0.84375\n",
      "At: 2048 [==========>] Loss 0.0990641257105748  - accuracy: 0.875\n",
      "At: 2049 [==========>] Loss 0.15818415690959703  - accuracy: 0.78125\n",
      "At: 2050 [==========>] Loss 0.14855773691571966  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.18287473914236552  - accuracy: 0.6875\n",
      "At: 2052 [==========>] Loss 0.051324254700891414  - accuracy: 1.0\n",
      "At: 2053 [==========>] Loss 0.14603255698895795  - accuracy: 0.78125\n",
      "At: 2054 [==========>] Loss 0.11928484835122671  - accuracy: 0.78125\n",
      "At: 2055 [==========>] Loss 0.07692774939628788  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.13224135555438993  - accuracy: 0.84375\n",
      "At: 2057 [==========>] Loss 0.1527511286856384  - accuracy: 0.78125\n",
      "At: 2058 [==========>] Loss 0.12286468580888835  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.18649253913080713  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.12495973042644742  - accuracy: 0.8125\n",
      "At: 2061 [==========>] Loss 0.13495199881156805  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.14608018655255872  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.09183858668830694  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.1692815917387832  - accuracy: 0.78125\n",
      "At: 2065 [==========>] Loss 0.032849280900851305  - accuracy: 1.0\n",
      "At: 2066 [==========>] Loss 0.16491014672313506  - accuracy: 0.71875\n",
      "At: 2067 [==========>] Loss 0.09844676509982965  - accuracy: 0.875\n",
      "At: 2068 [==========>] Loss 0.09896933284770043  - accuracy: 0.875\n",
      "At: 2069 [==========>] Loss 0.08295892284882457  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.160547721372301  - accuracy: 0.78125\n",
      "At: 2071 [==========>] Loss 0.13998586566412374  - accuracy: 0.8125\n",
      "At: 2072 [==========>] Loss 0.07304233348822677  - accuracy: 0.90625\n",
      "At: 2073 [==========>] Loss 0.1082694380771348  - accuracy: 0.84375\n",
      "At: 2074 [==========>] Loss 0.08484998566850456  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.13212482151915586  - accuracy: 0.84375\n",
      "At: 2076 [==========>] Loss 0.1223571234504833  - accuracy: 0.8125\n",
      "At: 2077 [==========>] Loss 0.16134426567072097  - accuracy: 0.78125\n",
      "At: 2078 [==========>] Loss 0.11415613600602818  - accuracy: 0.8125\n",
      "At: 2079 [==========>] Loss 0.06009197088759773  - accuracy: 0.9375\n",
      "At: 2080 [==========>] Loss 0.09419981112186462  - accuracy: 0.875\n",
      "At: 2081 [==========>] Loss 0.12945553242572763  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.12503810552364092  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.2175333517912969  - accuracy: 0.65625\n",
      "At: 2084 [==========>] Loss 0.10766743378871091  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.10019135065127287  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.10227597850868692  - accuracy: 0.90625\n",
      "At: 2087 [==========>] Loss 0.14906681268947475  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.11502911134816231  - accuracy: 0.8125\n",
      "At: 2089 [==========>] Loss 0.13141176232959745  - accuracy: 0.84375\n",
      "At: 2090 [==========>] Loss 0.09498328836307113  - accuracy: 0.84375\n",
      "At: 2091 [==========>] Loss 0.14799392973931896  - accuracy: 0.78125\n",
      "At: 2092 [==========>] Loss 0.09294820193598197  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.148484742845763  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.11522037842548255  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.09378857627240236  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.15629140519632098  - accuracy: 0.71875\n",
      "At: 2097 [==========>] Loss 0.10119656826159201  - accuracy: 0.84375\n",
      "At: 2098 [==========>] Loss 0.13822193365775287  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.07369510342982019  - accuracy: 0.90625\n",
      "At: 2100 [==========>] Loss 0.05858646393559802  - accuracy: 0.9375\n",
      "At: 2101 [==========>] Loss 0.12457540251620572  - accuracy: 0.84375\n",
      "At: 2102 [==========>] Loss 0.08912914157827766  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.14047281428520508  - accuracy: 0.875\n",
      "At: 2104 [==========>] Loss 0.11483310432843902  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.17267283701307568  - accuracy: 0.71875\n",
      "At: 2106 [==========>] Loss 0.13201459676406452  - accuracy: 0.8125\n",
      "At: 2107 [==========>] Loss 0.09094322688934536  - accuracy: 0.90625\n",
      "At: 2108 [==========>] Loss 0.14633436799242117  - accuracy: 0.84375\n",
      "At: 2109 [==========>] Loss 0.13296969096068872  - accuracy: 0.78125\n",
      "At: 2110 [==========>] Loss 0.06092790779400733  - accuracy: 0.875\n",
      "At: 2111 [==========>] Loss 0.12706989279236486  - accuracy: 0.84375\n",
      "At: 2112 [==========>] Loss 0.11184165111443567  - accuracy: 0.875\n",
      "At: 2113 [==========>] Loss 0.09458940465575286  - accuracy: 0.90625\n",
      "At: 2114 [==========>] Loss 0.13908177229466664  - accuracy: 0.8125\n",
      "At: 2115 [==========>] Loss 0.11302513250317006  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.09287470882337695  - accuracy: 0.875\n",
      "At: 2117 [==========>] Loss 0.12816867012102473  - accuracy: 0.84375\n",
      "At: 2118 [==========>] Loss 0.1313763463621736  - accuracy: 0.84375\n",
      "At: 2119 [==========>] Loss 0.09263606918489649  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.12787649007421784  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.13251858638799882  - accuracy: 0.84375\n",
      "At: 2122 [==========>] Loss 0.14134371786620054  - accuracy: 0.71875\n",
      "At: 2123 [==========>] Loss 0.18873506888636163  - accuracy: 0.75\n",
      "At: 2124 [==========>] Loss 0.1341286546407859  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.08813588061000943  - accuracy: 0.90625\n",
      "At: 2126 [==========>] Loss 0.04069192934465245  - accuracy: 0.96875\n",
      "At: 2127 [==========>] Loss 0.10713580213930488  - accuracy: 0.875\n",
      "At: 2128 [==========>] Loss 0.11951986917928087  - accuracy: 0.8125\n",
      "At: 2129 [==========>] Loss 0.13825342297387017  - accuracy: 0.875\n",
      "At: 2130 [==========>] Loss 0.06399450231634093  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.09441622922231287  - accuracy: 0.90625\n",
      "At: 2132 [==========>] Loss 0.2082980787790445  - accuracy: 0.71875\n",
      "At: 2133 [==========>] Loss 0.16018016808873928  - accuracy: 0.71875\n",
      "At: 2134 [==========>] Loss 0.1331848866512862  - accuracy: 0.8125\n",
      "At: 2135 [==========>] Loss 0.10163999002882668  - accuracy: 0.875\n",
      "At: 2136 [==========>] Loss 0.09939874977292673  - accuracy: 0.875\n",
      "At: 2137 [==========>] Loss 0.10827850697951451  - accuracy: 0.875\n",
      "At: 2138 [==========>] Loss 0.11357616878984406  - accuracy: 0.84375\n",
      "At: 2139 [==========>] Loss 0.15823597018706687  - accuracy: 0.78125\n",
      "At: 2140 [==========>] Loss 0.09588620116811039  - accuracy: 0.875\n",
      "At: 2141 [==========>] Loss 0.13376491054726541  - accuracy: 0.8125\n",
      "At: 2142 [==========>] Loss 0.10409775792656484  - accuracy: 0.84375\n",
      "At: 2143 [==========>] Loss 0.08944459650624517  - accuracy: 0.90625\n",
      "At: 2144 [==========>] Loss 0.06964826203625417  - accuracy: 0.90625\n",
      "At: 2145 [==========>] Loss 0.10722350346763301  - accuracy: 0.8125\n",
      "At: 2146 [==========>] Loss 0.16366595880512652  - accuracy: 0.78125\n",
      "At: 2147 [==========>] Loss 0.1103868095402809  - accuracy: 0.875\n",
      "At: 2148 [==========>] Loss 0.17133259416749297  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.11635999585571656  - accuracy: 0.8125\n",
      "At: 2150 [==========>] Loss 0.11393285002532771  - accuracy: 0.875\n",
      "At: 2151 [==========>] Loss 0.11827262369829042  - accuracy: 0.84375\n",
      "At: 2152 [==========>] Loss 0.2396155339997587  - accuracy: 0.65625\n",
      "At: 2153 [==========>] Loss 0.17619752440970665  - accuracy: 0.71875\n",
      "At: 2154 [==========>] Loss 0.13042890667372922  - accuracy: 0.78125\n",
      "At: 2155 [==========>] Loss 0.11136721322117858  - accuracy: 0.90625\n",
      "At: 2156 [==========>] Loss 0.1122465390416962  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.11225513675807217  - accuracy: 0.84375\n",
      "At: 2158 [==========>] Loss 0.14223462018317673  - accuracy: 0.71875\n",
      "At: 2159 [==========>] Loss 0.07298822135647152  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.12857353210720857  - accuracy: 0.78125\n",
      "At: 2161 [==========>] Loss 0.10837566686257817  - accuracy: 0.84375\n",
      "At: 2162 [==========>] Loss 0.08719166698404754  - accuracy: 0.9375\n",
      "At: 2163 [==========>] Loss 0.12031234012006976  - accuracy: 0.8125\n",
      "At: 2164 [==========>] Loss 0.16826769883806694  - accuracy: 0.75\n",
      "At: 2165 [==========>] Loss 0.08040301780308154  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.10362158624772957  - accuracy: 0.875\n",
      "At: 2167 [==========>] Loss 0.10251701765138274  - accuracy: 0.84375\n",
      "At: 2168 [==========>] Loss 0.0871891152455422  - accuracy: 0.90625\n",
      "At: 2169 [==========>] Loss 0.10222292611656862  - accuracy: 0.90625\n",
      "At: 2170 [==========>] Loss 0.12982809831877545  - accuracy: 0.84375\n",
      "At: 2171 [==========>] Loss 0.13063629800482418  - accuracy: 0.8125\n",
      "At: 2172 [==========>] Loss 0.12675461263364088  - accuracy: 0.84375\n",
      "At: 2173 [==========>] Loss 0.14630340005701314  - accuracy: 0.75\n",
      "At: 2174 [==========>] Loss 0.1165098340614821  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.11777564764343705  - accuracy: 0.8125\n",
      "At: 2176 [==========>] Loss 0.12310611653952953  - accuracy: 0.8125\n",
      "At: 2177 [==========>] Loss 0.12540881427336778  - accuracy: 0.84375\n",
      "At: 2178 [==========>] Loss 0.10077656146229455  - accuracy: 0.8125\n",
      "At: 2179 [==========>] Loss 0.11696021823776628  - accuracy: 0.84375\n",
      "At: 2180 [==========>] Loss 0.0993551401025807  - accuracy: 0.8125\n",
      "At: 2181 [==========>] Loss 0.1746164223728198  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.1015974456434596  - accuracy: 0.84375\n",
      "At: 2183 [==========>] Loss 0.23426947947615148  - accuracy: 0.59375\n",
      "At: 2184 [==========>] Loss 0.0912791905632308  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.11808411475658165  - accuracy: 0.8125\n",
      "At: 2186 [==========>] Loss 0.17010284380339274  - accuracy: 0.78125\n",
      "At: 2187 [==========>] Loss 0.17085951501280738  - accuracy: 0.75\n",
      "At: 2188 [==========>] Loss 0.0861758462667474  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.08283711078691242  - accuracy: 0.875\n",
      "At: 2190 [==========>] Loss 0.10610661371312438  - accuracy: 0.78125\n",
      "At: 2191 [==========>] Loss 0.11194066634869608  - accuracy: 0.8125\n",
      "At: 2192 [==========>] Loss 0.12326710727967263  - accuracy: 0.8125\n",
      "At: 2193 [==========>] Loss 0.16271227862674337  - accuracy: 0.75\n",
      "At: 2194 [==========>] Loss 0.1335133739407022  - accuracy: 0.8125\n",
      "At: 2195 [==========>] Loss 0.19113852937473297  - accuracy: 0.625\n",
      "At: 2196 [==========>] Loss 0.14148013097819065  - accuracy: 0.84375\n",
      "At: 2197 [==========>] Loss 0.10533099427812972  - accuracy: 0.84375\n",
      "At: 2198 [==========>] Loss 0.07023419892054905  - accuracy: 0.9375\n",
      "At: 2199 [==========>] Loss 0.06499680390583497  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.04261581739629303  - accuracy: 0.9375\n",
      "At: 2201 [==========>] Loss 0.09253005888472894  - accuracy: 0.90625\n",
      "At: 2202 [==========>] Loss 0.09054541069910402  - accuracy: 0.84375\n",
      "At: 2203 [==========>] Loss 0.09105353601619069  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.13759659823512824  - accuracy: 0.8125\n",
      "At: 2205 [==========>] Loss 0.09206105289615404  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.09622053194561928  - accuracy: 0.84375\n",
      "At: 2207 [==========>] Loss 0.08708373558253585  - accuracy: 0.90625\n",
      "At: 2208 [==========>] Loss 0.1641727364211484  - accuracy: 0.71875\n",
      "At: 2209 [==========>] Loss 0.1944536151199025  - accuracy: 0.65625\n",
      "At: 2210 [==========>] Loss 0.12751294904934068  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.14809740858249398  - accuracy: 0.8125\n",
      "At: 2212 [==========>] Loss 0.10040232802963726  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.10375429743429726  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.14074313693471463  - accuracy: 0.78125\n",
      "At: 2215 [==========>] Loss 0.13326713710582885  - accuracy: 0.84375\n",
      "At: 2216 [==========>] Loss 0.11159690231499156  - accuracy: 0.875\n",
      "At: 2217 [==========>] Loss 0.09334178788452338  - accuracy: 0.875\n",
      "At: 2218 [==========>] Loss 0.15803667933244187  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.06871203310563506  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.10315357921494102  - accuracy: 0.84375\n",
      "At: 2221 [==========>] Loss 0.17726997201725986  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.11479881697665376  - accuracy: 0.84375\n",
      "At: 2223 [==========>] Loss 0.14705719694715735  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.13250457836015717  - accuracy: 0.8125\n",
      "At: 2225 [==========>] Loss 0.1458920163770635  - accuracy: 0.8125\n",
      "At: 2226 [==========>] Loss 0.13281351489146428  - accuracy: 0.84375\n",
      "At: 2227 [==========>] Loss 0.1679227384107565  - accuracy: 0.75\n",
      "At: 2228 [==========>] Loss 0.06944832647956453  - accuracy: 0.875\n",
      "At: 2229 [==========>] Loss 0.1550573738922908  - accuracy: 0.75\n",
      "At: 2230 [==========>] Loss 0.1214939301806853  - accuracy: 0.8125\n",
      "At: 2231 [==========>] Loss 0.1453858874270134  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.15579880663573173  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.14648425022709685  - accuracy: 0.84375\n",
      "At: 2234 [==========>] Loss 0.14232324099187837  - accuracy: 0.75\n",
      "At: 2235 [==========>] Loss 0.0921761183429206  - accuracy: 0.84375\n",
      "At: 2236 [==========>] Loss 0.07113052176957271  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.11365745659430092  - accuracy: 0.84375\n",
      "At: 2238 [==========>] Loss 0.13255371297174426  - accuracy: 0.84375\n",
      "At: 2239 [==========>] Loss 0.17752155169375358  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.17016007795788368  - accuracy: 0.75\n",
      "At: 2241 [==========>] Loss 0.12114684544754942  - accuracy: 0.8125\n",
      "At: 2242 [==========>] Loss 0.1703625651192633  - accuracy: 0.8125\n",
      "At: 2243 [==========>] Loss 0.07510398783143032  - accuracy: 0.90625\n",
      "At: 2244 [==========>] Loss 0.08390827236586332  - accuracy: 0.90625\n",
      "At: 2245 [==========>] Loss 0.06550514991723702  - accuracy: 0.90625\n",
      "At: 2246 [==========>] Loss 0.15471815221910307  - accuracy: 0.8125\n",
      "At: 2247 [==========>] Loss 0.12233627675124899  - accuracy: 0.75\n",
      "At: 2248 [==========>] Loss 0.1614776258019339  - accuracy: 0.8125\n",
      "At: 2249 [==========>] Loss 0.12024960872905369  - accuracy: 0.8125\n",
      "At: 2250 [==========>] Loss 0.07919661806505601  - accuracy: 0.875\n",
      "At: 2251 [==========>] Loss 0.09320372806605105  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.10840216261058352  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.10686640087534195  - accuracy: 0.8125\n",
      "At: 2254 [==========>] Loss 0.1584944350699663  - accuracy: 0.71875\n",
      "At: 2255 [==========>] Loss 0.14727482116418933  - accuracy: 0.78125\n",
      "At: 2256 [==========>] Loss 0.1495300078252838  - accuracy: 0.8125\n",
      "At: 2257 [==========>] Loss 0.11625135331216699  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.1397744903437072  - accuracy: 0.75\n",
      "At: 2259 [==========>] Loss 0.1279564629374192  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.17411070909689197  - accuracy: 0.75\n",
      "At: 2261 [==========>] Loss 0.0891760229336861  - accuracy: 0.875\n",
      "At: 2262 [==========>] Loss 0.15115780071638235  - accuracy: 0.875\n",
      "At: 2263 [==========>] Loss 0.15033254486642156  - accuracy: 0.8125\n",
      "At: 2264 [==========>] Loss 0.10535779117882038  - accuracy: 0.875\n",
      "At: 2265 [==========>] Loss 0.0886743958932501  - accuracy: 0.875\n",
      "At: 2266 [==========>] Loss 0.12202205690500509  - accuracy: 0.84375\n",
      "At: 2267 [==========>] Loss 0.07137148275607266  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.08578930580356664  - accuracy: 0.9375\n",
      "At: 2269 [==========>] Loss 0.053307994407841404  - accuracy: 0.9375\n",
      "At: 2270 [==========>] Loss 0.09888662418856889  - accuracy: 0.90625\n",
      "At: 2271 [==========>] Loss 0.1522249264833702  - accuracy: 0.78125\n",
      "At: 2272 [==========>] Loss 0.09470881273950818  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.12182795510727362  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.08928361086189872  - accuracy: 0.84375\n",
      "At: 2275 [==========>] Loss 0.08832484247318773  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.0993489079647604  - accuracy: 0.90625\n",
      "At: 2277 [==========>] Loss 0.11750551599840084  - accuracy: 0.875\n",
      "At: 2278 [==========>] Loss 0.10680081591179019  - accuracy: 0.875\n",
      "At: 2279 [==========>] Loss 0.13670767989037796  - accuracy: 0.84375\n",
      "At: 2280 [==========>] Loss 0.1296934220090043  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.11816261047883458  - accuracy: 0.84375\n",
      "At: 2282 [==========>] Loss 0.08035847161985013  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.15393792590204314  - accuracy: 0.78125\n",
      "At: 2284 [==========>] Loss 0.13889631827781398  - accuracy: 0.6875\n",
      "At: 2285 [==========>] Loss 0.13553200251009234  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.14499526517217254  - accuracy: 0.84375\n",
      "At: 2287 [==========>] Loss 0.140063087515981  - accuracy: 0.78125\n",
      "At: 2288 [==========>] Loss 0.08171326999760205  - accuracy: 0.90625\n",
      "At: 2289 [==========>] Loss 0.13136352897794212  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.0597344315489872  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.13144994244444616  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.08799654196383899  - accuracy: 0.875\n",
      "At: 2293 [==========>] Loss 0.06418777353947656  - accuracy: 0.90625\n",
      "At: 2294 [==========>] Loss 0.0692778055901698  - accuracy: 0.90625\n",
      "At: 2295 [==========>] Loss 0.14802566846058463  - accuracy: 0.78125\n",
      "At: 2296 [==========>] Loss 0.1616873519981309  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.06903154787883772  - accuracy: 0.9375\n",
      "At: 2298 [==========>] Loss 0.11713577592844193  - accuracy: 0.875\n",
      "At: 2299 [==========>] Loss 0.11055000653606498  - accuracy: 0.84375\n",
      "At: 2300 [==========>] Loss 0.13415356496875985  - accuracy: 0.78125\n",
      "At: 2301 [==========>] Loss 0.2035949967781729  - accuracy: 0.71875\n",
      "At: 2302 [==========>] Loss 0.17491245770417116  - accuracy: 0.8125\n",
      "At: 2303 [==========>] Loss 0.06649033270561691  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.08988052381563907  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10597709094549407  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.13226263965271912  - accuracy: 0.8125\n",
      "At: 2307 [==========>] Loss 0.1639468749183507  - accuracy: 0.78125\n",
      "At: 2308 [==========>] Loss 0.19099074564100338  - accuracy: 0.75\n",
      "At: 2309 [==========>] Loss 0.13948285459743376  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.13838483114421837  - accuracy: 0.84375\n",
      "At: 2311 [==========>] Loss 0.14205134971865485  - accuracy: 0.8125\n",
      "At: 2312 [==========>] Loss 0.12326093696495041  - accuracy: 0.84375\n",
      "At: 2313 [==========>] Loss 0.08157471423719526  - accuracy: 0.90625\n",
      "At: 2314 [==========>] Loss 0.118588820127972  - accuracy: 0.78125\n",
      "At: 2315 [==========>] Loss 0.13247983141069236  - accuracy: 0.78125\n",
      "At: 2316 [==========>] Loss 0.1201385203619072  - accuracy: 0.78125\n",
      "At: 2317 [==========>] Loss 0.1488894176041612  - accuracy: 0.78125\n",
      "At: 2318 [==========>] Loss 0.17432838012431356  - accuracy: 0.71875\n",
      "At: 2319 [==========>] Loss 0.12044052912600514  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.09777210104040487  - accuracy: 0.84375\n",
      "At: 2321 [==========>] Loss 0.12569421214232243  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.174535302966078  - accuracy: 0.78125\n",
      "At: 2323 [==========>] Loss 0.148617203015422  - accuracy: 0.78125\n",
      "At: 2324 [==========>] Loss 0.1584134664683657  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.13641041075438226  - accuracy: 0.78125\n",
      "At: 2326 [==========>] Loss 0.06699646010048325  - accuracy: 0.9375\n",
      "At: 2327 [==========>] Loss 0.0597225663918185  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.14305684248638753  - accuracy: 0.8125\n",
      "At: 2329 [==========>] Loss 0.08996086883458261  - accuracy: 0.90625\n",
      "At: 2330 [==========>] Loss 0.15372397240577618  - accuracy: 0.8125\n",
      "At: 2331 [==========>] Loss 0.10655369721796777  - accuracy: 0.875\n",
      "At: 2332 [==========>] Loss 0.09846220956388853  - accuracy: 0.875\n",
      "At: 2333 [==========>] Loss 0.09688762941358035  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.16100085412910303  - accuracy: 0.78125\n",
      "At: 2335 [==========>] Loss 0.09944946298301152  - accuracy: 0.84375\n",
      "At: 2336 [==========>] Loss 0.1086991294888673  - accuracy: 0.84375\n",
      "At: 2337 [==========>] Loss 0.11576320840556582  - accuracy: 0.84375\n",
      "At: 2338 [==========>] Loss 0.11111393573742909  - accuracy: 0.84375\n",
      "At: 2339 [==========>] Loss 0.08963732623478385  - accuracy: 0.90625\n",
      "At: 2340 [==========>] Loss 0.1418252293191277  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.14491965328576034  - accuracy: 0.78125\n",
      "At: 2342 [==========>] Loss 0.16804932828952307  - accuracy: 0.75\n",
      "At: 2343 [==========>] Loss 0.08793375631385955  - accuracy: 0.875\n",
      "At: 2344 [==========>] Loss 0.20193940095479665  - accuracy: 0.71875\n",
      "At: 2345 [==========>] Loss 0.14898512310941694  - accuracy: 0.8125\n",
      "At: 2346 [==========>] Loss 0.09361199383901482  - accuracy: 0.875\n",
      "At: 2347 [==========>] Loss 0.1286671997968219  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.04610492925739543  - accuracy: 0.96875\n",
      "At: 2349 [==========>] Loss 0.09315442276990867  - accuracy: 0.875\n",
      "At: 2350 [==========>] Loss 0.1089779248294741  - accuracy: 0.8125\n",
      "At: 2351 [==========>] Loss 0.0998107014863932  - accuracy: 0.875\n",
      "At: 2352 [==========>] Loss 0.10541332520646807  - accuracy: 0.8125\n",
      "At: 2353 [==========>] Loss 0.09021846836165734  - accuracy: 0.8125\n",
      "At: 2354 [==========>] Loss 0.1526578031704574  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.0602341590980336  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.12535869784535994  - accuracy: 0.78125\n",
      "At: 2357 [==========>] Loss 0.1429160669024792  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.16712539938050439  - accuracy: 0.78125\n",
      "At: 2359 [==========>] Loss 0.17807466451712234  - accuracy: 0.75\n",
      "At: 2360 [==========>] Loss 0.1303125694851022  - accuracy: 0.78125\n",
      "At: 2361 [==========>] Loss 0.16548105390083917  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.06615769664258406  - accuracy: 0.96875\n",
      "At: 2363 [==========>] Loss 0.16735753304122467  - accuracy: 0.78125\n",
      "At: 2364 [==========>] Loss 0.08298244738244213  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.06944732192524376  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.1609250504973553  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.08821526381315652  - accuracy: 0.90625\n",
      "At: 2368 [==========>] Loss 0.10245472940672445  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.12308629580681953  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.08615250201600186  - accuracy: 0.90625\n",
      "At: 2371 [==========>] Loss 0.07496397253542333  - accuracy: 0.90625\n",
      "At: 2372 [==========>] Loss 0.1401349741881966  - accuracy: 0.84375\n",
      "At: 2373 [==========>] Loss 0.10053595836400775  - accuracy: 0.875\n",
      "At: 2374 [==========>] Loss 0.09846900637636179  - accuracy: 0.84375\n",
      "At: 2375 [==========>] Loss 0.0667227771846357  - accuracy: 0.875\n",
      "At: 2376 [==========>] Loss 0.08632539938442155  - accuracy: 0.875\n",
      "At: 2377 [==========>] Loss 0.08528007025363635  - accuracy: 0.90625\n",
      "At: 2378 [==========>] Loss 0.10875651011943925  - accuracy: 0.84375\n",
      "At: 2379 [==========>] Loss 0.168585298138768  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.06520776273076728  - accuracy: 0.9375\n",
      "At: 2381 [==========>] Loss 0.07170412271347343  - accuracy: 0.90625\n",
      "At: 2382 [==========>] Loss 0.1155105228147992  - accuracy: 0.8125\n",
      "At: 2383 [==========>] Loss 0.11879896850236205  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.1077048863356668  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.12071638485201806  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.09441673145990884  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.08033562359803798  - accuracy: 0.90625\n",
      "At: 2388 [==========>] Loss 0.1113166766773908  - accuracy: 0.84375\n",
      "At: 2389 [==========>] Loss 0.04261321943677251  - accuracy: 0.96875\n",
      "At: 2390 [==========>] Loss 0.06611293440536771  - accuracy: 0.96875\n",
      "At: 2391 [==========>] Loss 0.1593101639074246  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.11826608471206676  - accuracy: 0.84375\n",
      "At: 2393 [==========>] Loss 0.11158973173786628  - accuracy: 0.8125\n",
      "At: 2394 [==========>] Loss 0.06946162997893097  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.07882020418448049  - accuracy: 0.90625\n",
      "At: 2396 [==========>] Loss 0.07309531296465424  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.07924939904836321  - accuracy: 0.9375\n",
      "At: 2398 [==========>] Loss 0.1339401223570694  - accuracy: 0.78125\n",
      "At: 2399 [==========>] Loss 0.1621418674118819  - accuracy: 0.75\n",
      "At: 2400 [==========>] Loss 0.09011755343529027  - accuracy: 0.9375\n",
      "At: 2401 [==========>] Loss 0.08051127853246523  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.09612750780471627  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.17272511239857852  - accuracy: 0.75\n",
      "At: 2404 [==========>] Loss 0.1578586637998305  - accuracy: 0.78125\n",
      "At: 2405 [==========>] Loss 0.06663914770679841  - accuracy: 0.9375\n",
      "At: 2406 [==========>] Loss 0.11687868257961126  - accuracy: 0.84375\n",
      "At: 2407 [==========>] Loss 0.12432442064023183  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.11524253229003686  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.13821800379610732  - accuracy: 0.8125\n",
      "At: 2410 [==========>] Loss 0.12110362003694004  - accuracy: 0.84375\n",
      "At: 2411 [==========>] Loss 0.08036665629497546  - accuracy: 0.875\n",
      "At: 2412 [==========>] Loss 0.08262690633245523  - accuracy: 0.90625\n",
      "At: 2413 [==========>] Loss 0.0964559679705112  - accuracy: 0.84375\n",
      "At: 2414 [==========>] Loss 0.05670182231869753  - accuracy: 0.96875\n",
      "At: 2415 [==========>] Loss 0.09817647335677376  - accuracy: 0.8125\n",
      "At: 2416 [==========>] Loss 0.11660549900164074  - accuracy: 0.84375\n",
      "At: 2417 [==========>] Loss 0.1722629266850813  - accuracy: 0.78125\n",
      "At: 2418 [==========>] Loss 0.11994112309583932  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.14120581220075148  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.16178928912816398  - accuracy: 0.75\n",
      "At: 2421 [==========>] Loss 0.08817473780099186  - accuracy: 0.9375\n",
      "At: 2422 [==========>] Loss 0.1378578523265588  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.13567564755692202  - accuracy: 0.8125\n",
      "At: 2424 [==========>] Loss 0.10628495442209498  - accuracy: 0.84375\n",
      "At: 2425 [==========>] Loss 0.08838212794000838  - accuracy: 0.9375\n",
      "At: 2426 [==========>] Loss 0.2093092337374165  - accuracy: 0.71875\n",
      "At: 2427 [==========>] Loss 0.126354713309435  - accuracy: 0.8125\n",
      "At: 2428 [==========>] Loss 0.07382248762028719  - accuracy: 0.90625\n",
      "At: 2429 [==========>] Loss 0.11019772974389284  - accuracy: 0.84375\n",
      "At: 2430 [==========>] Loss 0.16218198270862438  - accuracy: 0.78125\n",
      "At: 2431 [==========>] Loss 0.15952548321362703  - accuracy: 0.78125\n",
      "At: 2432 [==========>] Loss 0.07733431535468478  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.05375212355015322  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.042300937726531575  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.14757671517140913  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.08203806316178976  - accuracy: 0.84375\n",
      "At: 2437 [==========>] Loss 0.18662492771565065  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.12315891609970117  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.11847962421833526  - accuracy: 0.8125\n",
      "At: 2440 [==========>] Loss 0.11458074702852136  - accuracy: 0.8125\n",
      "At: 2441 [==========>] Loss 0.11912806729477045  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.12997036520488353  - accuracy: 0.8125\n",
      "At: 2443 [==========>] Loss 0.12965581752363559  - accuracy: 0.78125\n",
      "At: 2444 [==========>] Loss 0.0889240525573407  - accuracy: 0.875\n",
      "At: 2445 [==========>] Loss 0.0641009897328563  - accuracy: 0.90625\n",
      "At: 2446 [==========>] Loss 0.1697011959979411  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.17086487241020965  - accuracy: 0.71875\n",
      "At: 2448 [==========>] Loss 0.10417331277548302  - accuracy: 0.84375\n",
      "At: 2449 [==========>] Loss 0.09185850322016628  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.0824673265094444  - accuracy: 0.90625\n",
      "At: 2451 [==========>] Loss 0.07073246329221788  - accuracy: 0.90625\n",
      "At: 2452 [==========>] Loss 0.13271583316722593  - accuracy: 0.84375\n",
      "At: 2453 [==========>] Loss 0.1522181172825617  - accuracy: 0.78125\n",
      "At: 2454 [==========>] Loss 0.14142316998900514  - accuracy: 0.84375\n",
      "At: 2455 [==========>] Loss 0.13369962152199677  - accuracy: 0.8125\n",
      "At: 2456 [==========>] Loss 0.11320371907758843  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.17369105113166372  - accuracy: 0.78125\n",
      "At: 2458 [==========>] Loss 0.07867643894047255  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.13919490455795772  - accuracy: 0.8125\n",
      "At: 2460 [==========>] Loss 0.05693094674747946  - accuracy: 0.96875\n",
      "At: 2461 [==========>] Loss 0.07590574968780099  - accuracy: 0.84375\n",
      "At: 2462 [==========>] Loss 0.17562332765405075  - accuracy: 0.75\n",
      "At: 2463 [==========>] Loss 0.08479758474944724  - accuracy: 0.90625\n",
      "At: 2464 [==========>] Loss 0.15750150992683087  - accuracy: 0.78125\n",
      "At: 2465 [==========>] Loss 0.10361036566696147  - accuracy: 0.84375\n",
      "At: 2466 [==========>] Loss 0.0817542760288262  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.09214344672466226  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.085654704483587  - accuracy: 0.84375\n",
      "At: 2469 [==========>] Loss 0.1329714924890177  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.10907420921428246  - accuracy: 0.84375\n",
      "At: 2471 [==========>] Loss 0.10851400129828193  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.08204240997379741  - accuracy: 0.875\n",
      "At: 2473 [==========>] Loss 0.1209562805550797  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.0702291306790255  - accuracy: 0.90625\n",
      "At: 2475 [==========>] Loss 0.12307762204361601  - accuracy: 0.8125\n",
      "At: 2476 [==========>] Loss 0.061691719486512184  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.14781387971275559  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.12569349493918525  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.06914601926263911  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.1194541583791077  - accuracy: 0.84375\n",
      "At: 2481 [==========>] Loss 0.0708313228212561  - accuracy: 0.90625\n",
      "At: 2482 [==========>] Loss 0.15718046543212255  - accuracy: 0.8125\n",
      "At: 2483 [==========>] Loss 0.10421252607266183  - accuracy: 0.8125\n",
      "At: 2484 [==========>] Loss 0.08326473335772297  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.12599553220692847  - accuracy: 0.8125\n",
      "At: 2486 [==========>] Loss 0.11868617378192341  - accuracy: 0.875\n",
      "At: 2487 [==========>] Loss 0.1356662124099019  - accuracy: 0.8125\n",
      "At: 2488 [==========>] Loss 0.15024357491617996  - accuracy: 0.8125\n",
      "At: 2489 [==========>] Loss 0.18558259019347928  - accuracy: 0.71875\n",
      "At: 2490 [==========>] Loss 0.111145935010348  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.11336645546867662  - accuracy: 0.8125\n",
      "At: 2492 [==========>] Loss 0.11827027086095206  - accuracy: 0.84375\n",
      "At: 2493 [==========>] Loss 0.10534654981721657  - accuracy: 0.875\n",
      "At: 2494 [==========>] Loss 0.09315291055035169  - accuracy: 0.90625\n",
      "At: 2495 [==========>] Loss 0.09739481970333813  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.06475581317811872  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.2062602015849986  - accuracy: 0.71875\n",
      "At: 2498 [==========>] Loss 0.1351738342458465  - accuracy: 0.8125\n",
      "At: 2499 [==========>] Loss 0.07256999903030566  - accuracy: 0.90625\n",
      "At: 2500 [==========>] Loss 0.1510983390006725  - accuracy: 0.75\n",
      "At: 2501 [==========>] Loss 0.17325097121216226  - accuracy: 0.71875\n",
      "At: 2502 [==========>] Loss 0.1148070962134118  - accuracy: 0.84375\n",
      "At: 2503 [==========>] Loss 0.1417666822430384  - accuracy: 0.84375\n",
      "At: 2504 [==========>] Loss 0.1420178461905054  - accuracy: 0.78125\n",
      "At: 2505 [==========>] Loss 0.12347735773629924  - accuracy: 0.8125\n",
      "At: 2506 [==========>] Loss 0.09852772183715258  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.13186263183635186  - accuracy: 0.84375\n",
      "At: 2508 [==========>] Loss 0.0999730619029522  - accuracy: 0.875\n",
      "At: 2509 [==========>] Loss 0.1608689591522356  - accuracy: 0.78125\n",
      "At: 2510 [==========>] Loss 0.11821005336010343  - accuracy: 0.84375\n",
      "At: 2511 [==========>] Loss 0.16339443893109545  - accuracy: 0.75\n",
      "At: 2512 [==========>] Loss 0.09419257804293023  - accuracy: 0.8125\n",
      "At: 2513 [==========>] Loss 0.14506268511749482  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.1386082985283093  - accuracy: 0.8125\n",
      "At: 2515 [==========>] Loss 0.18218119730980276  - accuracy: 0.75\n",
      "At: 2516 [==========>] Loss 0.1832013489681179  - accuracy: 0.75\n",
      "At: 2517 [==========>] Loss 0.11018630571801519  - accuracy: 0.78125\n",
      "At: 2518 [==========>] Loss 0.1401565307515359  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.13135782571724933  - accuracy: 0.75\n",
      "At: 2520 [==========>] Loss 0.15740706669194354  - accuracy: 0.8125\n",
      "At: 2521 [==========>] Loss 0.10348329121610571  - accuracy: 0.875\n",
      "At: 2522 [==========>] Loss 0.21789085395374053  - accuracy: 0.65625\n",
      "At: 2523 [==========>] Loss 0.12144076631696127  - accuracy: 0.875\n",
      "At: 2524 [==========>] Loss 0.15462976730857125  - accuracy: 0.8125\n",
      "At: 2525 [==========>] Loss 0.08423350252051656  - accuracy: 0.90625\n",
      "At: 2526 [==========>] Loss 0.11593907167895426  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.12235251482569404  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.07079970077331528  - accuracy: 0.90625\n",
      "At: 2529 [==========>] Loss 0.13694914538366249  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.1306438881279963  - accuracy: 0.84375\n",
      "At: 2531 [==========>] Loss 0.042942099465047914  - accuracy: 0.96875\n",
      "At: 2532 [==========>] Loss 0.11635465976682363  - accuracy: 0.84375\n",
      "At: 2533 [==========>] Loss 0.1091175002992587  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.06271674931603977  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.07385058786234297  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.15962286675713777  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.09191802133275792  - accuracy: 0.90625\n",
      "At: 2538 [==========>] Loss 0.13702721862856043  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.09464950290674054  - accuracy: 0.90625\n",
      "At: 2540 [==========>] Loss 0.12193386111882756  - accuracy: 0.84375\n",
      "At: 2541 [==========>] Loss 0.07576201134546265  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.06276732807596128  - accuracy: 0.9375\n",
      "At: 2543 [==========>] Loss 0.16548015479109612  - accuracy: 0.78125\n",
      "At: 2544 [==========>] Loss 0.1143391229288136  - accuracy: 0.875\n",
      "At: 2545 [==========>] Loss 0.0715066079003443  - accuracy: 0.90625\n",
      "At: 2546 [==========>] Loss 0.13178115140487237  - accuracy: 0.84375\n",
      "At: 2547 [==========>] Loss 0.09578984824415071  - accuracy: 0.875\n",
      "At: 2548 [==========>] Loss 0.09440189065450275  - accuracy: 0.84375\n",
      "At: 2549 [==========>] Loss 0.0841220853636968  - accuracy: 0.90625\n",
      "At: 2550 [==========>] Loss 0.12500834670179578  - accuracy: 0.84375\n",
      "At: 2551 [==========>] Loss 0.12295838822174807  - accuracy: 0.8125\n",
      "At: 2552 [==========>] Loss 0.09732695619173348  - accuracy: 0.875\n",
      "At: 2553 [==========>] Loss 0.06938067085528543  - accuracy: 0.90625\n",
      "At: 2554 [==========>] Loss 0.14371683877963715  - accuracy: 0.78125\n",
      "At: 2555 [==========>] Loss 0.17534775234246278  - accuracy: 0.75\n",
      "At: 2556 [==========>] Loss 0.08247635832505164  - accuracy: 0.875\n",
      "At: 2557 [==========>] Loss 0.13932794245439578  - accuracy: 0.78125\n",
      "At: 2558 [==========>] Loss 0.07480752664901703  - accuracy: 0.90625\n",
      "At: 2559 [==========>] Loss 0.09051007587558602  - accuracy: 0.875\n",
      "At: 2560 [==========>] Loss 0.20031554316493277  - accuracy: 0.6875\n",
      "At: 2561 [==========>] Loss 0.10176827301855357  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.09976256669062411  - accuracy: 0.875\n",
      "At: 2563 [==========>] Loss 0.12237640172255176  - accuracy: 0.78125\n",
      "At: 2564 [==========>] Loss 0.09326919491005692  - accuracy: 0.875\n",
      "At: 2565 [==========>] Loss 0.1506180871780238  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1768851351343133  - accuracy: 0.78125\n",
      "At: 2567 [==========>] Loss 0.11696501523168398  - accuracy: 0.84375\n",
      "At: 2568 [==========>] Loss 0.11383586565742156  - accuracy: 0.8125\n",
      "At: 2569 [==========>] Loss 0.061543635954824025  - accuracy: 0.96875\n",
      "At: 2570 [==========>] Loss 0.1909520705035574  - accuracy: 0.6875\n",
      "At: 2571 [==========>] Loss 0.1050028887923762  - accuracy: 0.875\n",
      "At: 2572 [==========>] Loss 0.16533147087443334  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.1803513757466983  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.12932010163010102  - accuracy: 0.8125\n",
      "At: 2575 [==========>] Loss 0.05787739868110041  - accuracy: 0.96875\n",
      "At: 2576 [==========>] Loss 0.10933441059381492  - accuracy: 0.875\n",
      "At: 2577 [==========>] Loss 0.2119765965369147  - accuracy: 0.75\n",
      "At: 2578 [==========>] Loss 0.17556223231662627  - accuracy: 0.78125\n",
      "At: 2579 [==========>] Loss 0.1595896345079711  - accuracy: 0.75\n",
      "At: 2580 [==========>] Loss 0.1391784651980015  - accuracy: 0.78125\n",
      "At: 2581 [==========>] Loss 0.06226232667847087  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.22317231527750528  - accuracy: 0.6875\n",
      "At: 2583 [==========>] Loss 0.0660979716243047  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.20979751265209584  - accuracy: 0.6875\n",
      "At: 2585 [==========>] Loss 0.09222883046746534  - accuracy: 0.90625\n",
      "At: 2586 [==========>] Loss 0.09093550300100242  - accuracy: 0.78125\n",
      "At: 2587 [==========>] Loss 0.17744815214862722  - accuracy: 0.6875\n",
      "At: 2588 [==========>] Loss 0.12040692368093478  - accuracy: 0.875\n",
      "At: 2589 [==========>] Loss 0.09449335813942827  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.19958148896770034  - accuracy: 0.65625\n",
      "At: 2591 [==========>] Loss 0.114771780695836  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.0897140131084313  - accuracy: 0.875\n",
      "At: 2593 [==========>] Loss 0.09293484627610406  - accuracy: 0.875\n",
      "At: 2594 [==========>] Loss 0.07627597141136414  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.09230774448585452  - accuracy: 0.875\n",
      "At: 2596 [==========>] Loss 0.10312485798759646  - accuracy: 0.875\n",
      "At: 2597 [==========>] Loss 0.0740103245310618  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.10752641172502764  - accuracy: 0.8125\n",
      "At: 2599 [==========>] Loss 0.12999191008193528  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.13827674198220347  - accuracy: 0.8125\n",
      "At: 2601 [==========>] Loss 0.13745389808447117  - accuracy: 0.8125\n",
      "At: 2602 [==========>] Loss 0.08111061875967826  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.15004941588125148  - accuracy: 0.75\n",
      "At: 2604 [==========>] Loss 0.09344426818172771  - accuracy: 0.875\n",
      "At: 2605 [==========>] Loss 0.14418161159370751  - accuracy: 0.78125\n",
      "At: 2606 [==========>] Loss 0.16152317970369384  - accuracy: 0.8125\n",
      "At: 2607 [==========>] Loss 0.14055345549919418  - accuracy: 0.84375\n",
      "At: 2608 [==========>] Loss 0.06408882854003577  - accuracy: 0.90625\n",
      "At: 2609 [==========>] Loss 0.08809949568351601  - accuracy: 0.875\n",
      "At: 2610 [==========>] Loss 0.15477870225317708  - accuracy: 0.71875\n",
      "At: 2611 [==========>] Loss 0.13940794483539698  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.1001886078908874  - accuracy: 0.875\n",
      "At: 2613 [==========>] Loss 0.09341114258860196  - accuracy: 0.90625\n",
      "At: 2614 [==========>] Loss 0.12237711754743903  - accuracy: 0.8125\n",
      "At: 2615 [==========>] Loss 0.08639970902138279  - accuracy: 0.875\n",
      "At: 2616 [==========>] Loss 0.0657649847587761  - accuracy: 0.875\n",
      "At: 2617 [==========>] Loss 0.0737174690544872  - accuracy: 0.90625\n",
      "At: 2618 [==========>] Loss 0.08516543116309873  - accuracy: 0.90625\n",
      "At: 2619 [==========>] Loss 0.11511810161028081  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.1258287342270495  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.10338816818112662  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.10080225986638977  - accuracy: 0.875\n",
      "At: 2623 [==========>] Loss 0.0843268940582817  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.11737908805509804  - accuracy: 0.875\n",
      "At: 2625 [==========>] Loss 0.07443110038253271  - accuracy: 0.90625\n",
      "At: 2626 [==========>] Loss 0.056779796165852096  - accuracy: 0.9375\n",
      "At: 2627 [==========>] Loss 0.13620157653813722  - accuracy: 0.8125\n",
      "At: 2628 [==========>] Loss 0.06049975251211043  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.0976014767076088  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.09279535094665473  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.11446753389392467  - accuracy: 0.875\n",
      "At: 2632 [==========>] Loss 0.1275118472950467  - accuracy: 0.8125\n",
      "At: 2633 [==========>] Loss 0.09845468354506773  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.06080364445306794  - accuracy: 0.96875\n",
      "At: 2635 [==========>] Loss 0.20771939685551388  - accuracy: 0.65625\n",
      "At: 2636 [==========>] Loss 0.1311586382390877  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.1297100013660724  - accuracy: 0.8125\n",
      "At: 2638 [==========>] Loss 0.07266362335096985  - accuracy: 0.9375\n",
      "At: 2639 [==========>] Loss 0.08478487331406305  - accuracy: 0.875\n",
      "At: 2640 [==========>] Loss 0.1377177237415409  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.1551268573653339  - accuracy: 0.78125\n",
      "At: 2642 [==========>] Loss 0.16177688503130228  - accuracy: 0.75\n",
      "At: 2643 [==========>] Loss 0.10438780949619224  - accuracy: 0.84375\n",
      "At: 2644 [==========>] Loss 0.14711093650407903  - accuracy: 0.8125\n",
      "At: 2645 [==========>] Loss 0.07262628564267842  - accuracy: 0.9375\n",
      "At: 2646 [==========>] Loss 0.1329520946187118  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.09815015950907488  - accuracy: 0.84375\n",
      "At: 2648 [==========>] Loss 0.15045645657490286  - accuracy: 0.75\n",
      "At: 2649 [==========>] Loss 0.11886623293971925  - accuracy: 0.8125\n",
      "At: 2650 [==========>] Loss 0.11498181918790609  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.18001357547200286  - accuracy: 0.71875\n",
      "At: 2652 [==========>] Loss 0.11940963196750114  - accuracy: 0.8125\n",
      "At: 2653 [==========>] Loss 0.11442096886121424  - accuracy: 0.875\n",
      "At: 2654 [==========>] Loss 0.11046896481817367  - accuracy: 0.875\n",
      "At: 2655 [==========>] Loss 0.22358077584542763  - accuracy: 0.6875\n",
      "At: 2656 [==========>] Loss 0.018245014306197613  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.11447686418625136  - accuracy: 0.90625\n",
      "At: 2658 [==========>] Loss 0.07920330681531765  - accuracy: 0.84375\n",
      "At: 2659 [==========>] Loss 0.0603886555214342  - accuracy: 0.9375\n",
      "At: 2660 [==========>] Loss 0.09419510700302935  - accuracy: 0.875\n",
      "At: 2661 [==========>] Loss 0.06617122576335208  - accuracy: 0.90625\n",
      "At: 2662 [==========>] Loss 0.08547769458422613  - accuracy: 0.875\n",
      "At: 2663 [==========>] Loss 0.08439423528052828  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.08704028714318895  - accuracy: 0.96875\n",
      "At: 2665 [==========>] Loss 0.09119173599186228  - accuracy: 0.90625\n",
      "At: 2666 [==========>] Loss 0.14605316378172678  - accuracy: 0.75\n",
      "At: 2667 [==========>] Loss 0.13160575666971205  - accuracy: 0.84375\n",
      "At: 2668 [==========>] Loss 0.12606966724660354  - accuracy: 0.8125\n",
      "At: 2669 [==========>] Loss 0.13349262312031596  - accuracy: 0.78125\n",
      "At: 2670 [==========>] Loss 0.10257290793344581  - accuracy: 0.78125\n",
      "At: 2671 [==========>] Loss 0.06551075836632572  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.09741904655570179  - accuracy: 0.875\n",
      "At: 2673 [==========>] Loss 0.13215737061299127  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.11072348456693021  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.10215548866644283  - accuracy: 0.90625\n",
      "At: 2676 [==========>] Loss 0.1540146333134097  - accuracy: 0.75\n",
      "At: 2677 [==========>] Loss 0.11020061387950535  - accuracy: 0.90625\n",
      "At: 2678 [==========>] Loss 0.0642783719538358  - accuracy: 0.90625\n",
      "At: 2679 [==========>] Loss 0.07185446569274248  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.0800243421125928  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.09677492858713155  - accuracy: 0.9375\n",
      "At: 2682 [==========>] Loss 0.1582988218808843  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.19150905649114883  - accuracy: 0.75\n",
      "At: 2684 [==========>] Loss 0.09150704305489542  - accuracy: 0.875\n",
      "At: 2685 [==========>] Loss 0.0920944267672256  - accuracy: 0.84375\n",
      "At: 2686 [==========>] Loss 0.0566509819137101  - accuracy: 0.96875\n",
      "At: 2687 [==========>] Loss 0.13358818886750431  - accuracy: 0.84375\n",
      "At: 2688 [==========>] Loss 0.1638540164456167  - accuracy: 0.75\n",
      "At: 2689 [==========>] Loss 0.12570664177734364  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.12026583832312202  - accuracy: 0.78125\n",
      "Epochs  10 / 10\n",
      "At: 1 [==========>] Loss 0.1884036313722764  - accuracy: 0.78125\n",
      "At: 2 [==========>] Loss 0.23399834621865687  - accuracy: 0.71875\n",
      "At: 3 [==========>] Loss 0.1681466328147725  - accuracy: 0.75\n",
      "At: 4 [==========>] Loss 0.2408466202046468  - accuracy: 0.6875\n",
      "At: 5 [==========>] Loss 0.10741580410223653  - accuracy: 0.90625\n",
      "At: 6 [==========>] Loss 0.10691592487136148  - accuracy: 0.84375\n",
      "At: 7 [==========>] Loss 0.16357631476495973  - accuracy: 0.78125\n",
      "At: 8 [==========>] Loss 0.25552051849536805  - accuracy: 0.71875\n",
      "At: 9 [==========>] Loss 0.24947299582927374  - accuracy: 0.625\n",
      "At: 10 [==========>] Loss 0.23508015574222077  - accuracy: 0.65625\n",
      "At: 11 [==========>] Loss 0.2342628752841346  - accuracy: 0.65625\n",
      "At: 12 [==========>] Loss 0.2036214132306737  - accuracy: 0.71875\n",
      "At: 13 [==========>] Loss 0.21536076317065905  - accuracy: 0.75\n",
      "At: 14 [==========>] Loss 0.09759977736867084  - accuracy: 0.90625\n",
      "At: 15 [==========>] Loss 0.17962846412153624  - accuracy: 0.78125\n",
      "At: 16 [==========>] Loss 0.19526721325876029  - accuracy: 0.75\n",
      "At: 17 [==========>] Loss 0.16687080269523052  - accuracy: 0.78125\n",
      "At: 18 [==========>] Loss 0.25285893042738866  - accuracy: 0.6875\n",
      "At: 19 [==========>] Loss 0.15081760619784337  - accuracy: 0.8125\n",
      "At: 20 [==========>] Loss 0.13860672484573863  - accuracy: 0.8125\n",
      "At: 21 [==========>] Loss 0.23667470766566215  - accuracy: 0.65625\n",
      "At: 22 [==========>] Loss 0.1998987581788573  - accuracy: 0.71875\n",
      "At: 23 [==========>] Loss 0.11191301758475  - accuracy: 0.84375\n",
      "At: 24 [==========>] Loss 0.18897208488102207  - accuracy: 0.75\n",
      "At: 25 [==========>] Loss 0.23897774520163712  - accuracy: 0.75\n",
      "At: 26 [==========>] Loss 0.24550756358660536  - accuracy: 0.625\n",
      "At: 27 [==========>] Loss 0.20745230236454873  - accuracy: 0.65625\n",
      "At: 28 [==========>] Loss 0.18631897712360124  - accuracy: 0.78125\n",
      "At: 29 [==========>] Loss 0.21554427573686635  - accuracy: 0.8125\n",
      "At: 30 [==========>] Loss 0.19993550568943055  - accuracy: 0.71875\n",
      "At: 31 [==========>] Loss 0.28784968433520053  - accuracy: 0.65625\n",
      "At: 32 [==========>] Loss 0.21255808583033717  - accuracy: 0.71875\n",
      "At: 33 [==========>] Loss 0.13313083068465198  - accuracy: 0.84375\n",
      "At: 34 [==========>] Loss 0.15170752310883978  - accuracy: 0.78125\n",
      "At: 35 [==========>] Loss 0.13702326946292498  - accuracy: 0.875\n",
      "At: 36 [==========>] Loss 0.1720881315040923  - accuracy: 0.8125\n",
      "At: 37 [==========>] Loss 0.21068688650688883  - accuracy: 0.75\n",
      "At: 38 [==========>] Loss 0.28381812554354197  - accuracy: 0.65625\n",
      "At: 39 [==========>] Loss 0.1688766535973014  - accuracy: 0.84375\n",
      "At: 40 [==========>] Loss 0.23671490738330264  - accuracy: 0.6875\n",
      "At: 41 [==========>] Loss 0.0729736014344562  - accuracy: 0.9375\n",
      "At: 42 [==========>] Loss 0.1751609893021024  - accuracy: 0.6875\n",
      "At: 43 [==========>] Loss 0.20248279483548676  - accuracy: 0.75\n",
      "At: 44 [==========>] Loss 0.17285804236733013  - accuracy: 0.78125\n",
      "At: 45 [==========>] Loss 0.09730118840832933  - accuracy: 0.84375\n",
      "At: 46 [==========>] Loss 0.1994929665446173  - accuracy: 0.78125\n",
      "At: 47 [==========>] Loss 0.1972566865267018  - accuracy: 0.78125\n",
      "At: 48 [==========>] Loss 0.1607694661118659  - accuracy: 0.8125\n",
      "At: 49 [==========>] Loss 0.11540565075740841  - accuracy: 0.84375\n",
      "At: 50 [==========>] Loss 0.2098471738808062  - accuracy: 0.75\n",
      "At: 51 [==========>] Loss 0.20500490919307157  - accuracy: 0.75\n",
      "At: 52 [==========>] Loss 0.2270938219981073  - accuracy: 0.65625\n",
      "At: 53 [==========>] Loss 0.1595556170796124  - accuracy: 0.84375\n",
      "At: 54 [==========>] Loss 0.12324924666460665  - accuracy: 0.84375\n",
      "At: 55 [==========>] Loss 0.21154898000638023  - accuracy: 0.6875\n",
      "At: 56 [==========>] Loss 0.19515337400096017  - accuracy: 0.75\n",
      "At: 57 [==========>] Loss 0.18942128759582066  - accuracy: 0.78125\n",
      "At: 58 [==========>] Loss 0.2363630445182832  - accuracy: 0.71875\n",
      "At: 59 [==========>] Loss 0.17832262813834088  - accuracy: 0.78125\n",
      "At: 60 [==========>] Loss 0.21817563502096715  - accuracy: 0.625\n",
      "At: 61 [==========>] Loss 0.23733880886051387  - accuracy: 0.65625\n",
      "At: 62 [==========>] Loss 0.21230674316347026  - accuracy: 0.6875\n",
      "At: 63 [==========>] Loss 0.19561929812882592  - accuracy: 0.8125\n",
      "At: 64 [==========>] Loss 0.19593688807388543  - accuracy: 0.71875\n",
      "At: 65 [==========>] Loss 0.2759389248530459  - accuracy: 0.65625\n",
      "At: 66 [==========>] Loss 0.238456849456271  - accuracy: 0.75\n",
      "At: 67 [==========>] Loss 0.2117967568743892  - accuracy: 0.75\n",
      "At: 68 [==========>] Loss 0.10999750548313508  - accuracy: 0.875\n",
      "At: 69 [==========>] Loss 0.16678819852607232  - accuracy: 0.8125\n",
      "At: 70 [==========>] Loss 0.1723823220677857  - accuracy: 0.8125\n",
      "At: 71 [==========>] Loss 0.137794618195407  - accuracy: 0.8125\n",
      "At: 72 [==========>] Loss 0.14846256401498487  - accuracy: 0.8125\n",
      "At: 73 [==========>] Loss 0.14773917002004783  - accuracy: 0.84375\n",
      "At: 74 [==========>] Loss 0.21696443533225074  - accuracy: 0.71875\n",
      "At: 75 [==========>] Loss 0.2029897917972253  - accuracy: 0.75\n",
      "At: 76 [==========>] Loss 0.2753791376917286  - accuracy: 0.6875\n",
      "At: 77 [==========>] Loss 0.2100101843868336  - accuracy: 0.75\n",
      "At: 78 [==========>] Loss 0.1392494877322283  - accuracy: 0.8125\n",
      "At: 79 [==========>] Loss 0.16907145126319995  - accuracy: 0.84375\n",
      "At: 80 [==========>] Loss 0.19041979742626605  - accuracy: 0.75\n",
      "At: 81 [==========>] Loss 0.19499606292945815  - accuracy: 0.71875\n",
      "At: 82 [==========>] Loss 0.23350563527486512  - accuracy: 0.6875\n",
      "At: 83 [==========>] Loss 0.15209193112196764  - accuracy: 0.84375\n",
      "At: 84 [==========>] Loss 0.2042287175790446  - accuracy: 0.75\n",
      "At: 85 [==========>] Loss 0.20167776549853572  - accuracy: 0.75\n",
      "At: 86 [==========>] Loss 0.17762851548544434  - accuracy: 0.75\n",
      "At: 87 [==========>] Loss 0.16800336194993545  - accuracy: 0.8125\n",
      "At: 88 [==========>] Loss 0.350986153629261  - accuracy: 0.59375\n",
      "At: 89 [==========>] Loss 0.18896540676914486  - accuracy: 0.8125\n",
      "At: 90 [==========>] Loss 0.2613593490811845  - accuracy: 0.71875\n",
      "At: 91 [==========>] Loss 0.18969183802326967  - accuracy: 0.78125\n",
      "At: 92 [==========>] Loss 0.11376848394352489  - accuracy: 0.875\n",
      "At: 93 [==========>] Loss 0.1443088306193025  - accuracy: 0.84375\n",
      "At: 94 [==========>] Loss 0.13553633293548698  - accuracy: 0.875\n",
      "At: 95 [==========>] Loss 0.17407951756426365  - accuracy: 0.78125\n",
      "At: 96 [==========>] Loss 0.11991108326763977  - accuracy: 0.8125\n",
      "At: 97 [==========>] Loss 0.11503276327052102  - accuracy: 0.84375\n",
      "At: 98 [==========>] Loss 0.272888670612235  - accuracy: 0.65625\n",
      "At: 99 [==========>] Loss 0.1530402993984421  - accuracy: 0.8125\n",
      "At: 100 [==========>] Loss 0.15599063677629274  - accuracy: 0.8125\n",
      "At: 101 [==========>] Loss 0.15553647825188502  - accuracy: 0.8125\n",
      "At: 102 [==========>] Loss 0.1536880117643823  - accuracy: 0.8125\n",
      "At: 103 [==========>] Loss 0.13986491669900056  - accuracy: 0.78125\n",
      "At: 104 [==========>] Loss 0.14442425770258804  - accuracy: 0.8125\n",
      "At: 105 [==========>] Loss 0.18474909297663492  - accuracy: 0.78125\n",
      "At: 106 [==========>] Loss 0.22611015449493652  - accuracy: 0.75\n",
      "At: 107 [==========>] Loss 0.19647715359450615  - accuracy: 0.71875\n",
      "At: 108 [==========>] Loss 0.22423601462257936  - accuracy: 0.6875\n",
      "At: 109 [==========>] Loss 0.1389414629419514  - accuracy: 0.84375\n",
      "At: 110 [==========>] Loss 0.22534644822581512  - accuracy: 0.71875\n",
      "At: 111 [==========>] Loss 0.11120774522955677  - accuracy: 0.875\n",
      "At: 112 [==========>] Loss 0.17295668232415154  - accuracy: 0.78125\n",
      "At: 113 [==========>] Loss 0.185898959291743  - accuracy: 0.78125\n",
      "At: 114 [==========>] Loss 0.13699528192311725  - accuracy: 0.8125\n",
      "At: 115 [==========>] Loss 0.18203358047522702  - accuracy: 0.6875\n",
      "At: 116 [==========>] Loss 0.19330429173551528  - accuracy: 0.75\n",
      "At: 117 [==========>] Loss 0.15986265686558773  - accuracy: 0.78125\n",
      "At: 118 [==========>] Loss 0.17824896125159323  - accuracy: 0.71875\n",
      "At: 119 [==========>] Loss 0.11805044397540844  - accuracy: 0.8125\n",
      "At: 120 [==========>] Loss 0.17630561788040983  - accuracy: 0.71875\n",
      "At: 121 [==========>] Loss 0.15948287211953904  - accuracy: 0.8125\n",
      "At: 122 [==========>] Loss 0.19362736603385758  - accuracy: 0.6875\n",
      "At: 123 [==========>] Loss 0.1690686517276751  - accuracy: 0.8125\n",
      "At: 124 [==========>] Loss 0.23730407460334277  - accuracy: 0.71875\n",
      "At: 125 [==========>] Loss 0.18719186180230435  - accuracy: 0.78125\n",
      "At: 126 [==========>] Loss 0.24598033486958143  - accuracy: 0.71875\n",
      "At: 127 [==========>] Loss 0.17806651340779237  - accuracy: 0.78125\n",
      "At: 128 [==========>] Loss 0.2507012506680144  - accuracy: 0.65625\n",
      "At: 129 [==========>] Loss 0.12979510385924925  - accuracy: 0.8125\n",
      "At: 130 [==========>] Loss 0.2262267872268127  - accuracy: 0.71875\n",
      "At: 131 [==========>] Loss 0.20581186473781568  - accuracy: 0.75\n",
      "At: 132 [==========>] Loss 0.21862546962261797  - accuracy: 0.65625\n",
      "At: 133 [==========>] Loss 0.2080562313141237  - accuracy: 0.71875\n",
      "At: 134 [==========>] Loss 0.14180057534950563  - accuracy: 0.8125\n",
      "At: 135 [==========>] Loss 0.16558238125673402  - accuracy: 0.8125\n",
      "At: 136 [==========>] Loss 0.14395539897437376  - accuracy: 0.78125\n",
      "At: 137 [==========>] Loss 0.13051520472401418  - accuracy: 0.84375\n",
      "At: 138 [==========>] Loss 0.2114995264358775  - accuracy: 0.71875\n",
      "At: 139 [==========>] Loss 0.13980556732390592  - accuracy: 0.84375\n",
      "At: 140 [==========>] Loss 0.16150737254584535  - accuracy: 0.8125\n",
      "At: 141 [==========>] Loss 0.3407690817160286  - accuracy: 0.5625\n",
      "At: 142 [==========>] Loss 0.15330677835990314  - accuracy: 0.78125\n",
      "At: 143 [==========>] Loss 0.189473935655843  - accuracy: 0.78125\n",
      "At: 144 [==========>] Loss 0.1375066917504985  - accuracy: 0.8125\n",
      "At: 145 [==========>] Loss 0.1106915702880065  - accuracy: 0.875\n",
      "At: 146 [==========>] Loss 0.1574067498690673  - accuracy: 0.84375\n",
      "At: 147 [==========>] Loss 0.16598063117346137  - accuracy: 0.78125\n",
      "At: 148 [==========>] Loss 0.17458730671798472  - accuracy: 0.8125\n",
      "At: 149 [==========>] Loss 0.18398455116779142  - accuracy: 0.8125\n",
      "At: 150 [==========>] Loss 0.1299475420462376  - accuracy: 0.8125\n",
      "At: 151 [==========>] Loss 0.20353327863619763  - accuracy: 0.78125\n",
      "At: 152 [==========>] Loss 0.14236603210935755  - accuracy: 0.78125\n",
      "At: 153 [==========>] Loss 0.21640430049368153  - accuracy: 0.75\n",
      "At: 154 [==========>] Loss 0.14739823741678512  - accuracy: 0.78125\n",
      "At: 155 [==========>] Loss 0.25234278325985704  - accuracy: 0.6875\n",
      "At: 156 [==========>] Loss 0.13785188212712018  - accuracy: 0.84375\n",
      "At: 157 [==========>] Loss 0.26569339325749597  - accuracy: 0.6875\n",
      "At: 158 [==========>] Loss 0.17571977653986776  - accuracy: 0.78125\n",
      "At: 159 [==========>] Loss 0.16920492992778546  - accuracy: 0.84375\n",
      "At: 160 [==========>] Loss 0.17443220469530082  - accuracy: 0.78125\n",
      "At: 161 [==========>] Loss 0.1384601608140845  - accuracy: 0.84375\n",
      "At: 162 [==========>] Loss 0.16335722042132161  - accuracy: 0.8125\n",
      "At: 163 [==========>] Loss 0.19031223230300587  - accuracy: 0.75\n",
      "At: 164 [==========>] Loss 0.20800485610271807  - accuracy: 0.75\n",
      "At: 165 [==========>] Loss 0.20534244150030467  - accuracy: 0.71875\n",
      "At: 166 [==========>] Loss 0.19225323406619937  - accuracy: 0.75\n",
      "At: 167 [==========>] Loss 0.10150414222630327  - accuracy: 0.875\n",
      "At: 168 [==========>] Loss 0.26693969405350315  - accuracy: 0.6875\n",
      "At: 169 [==========>] Loss 0.15814804962230566  - accuracy: 0.8125\n",
      "At: 170 [==========>] Loss 0.16349417062699456  - accuracy: 0.8125\n",
      "At: 171 [==========>] Loss 0.22645026641789442  - accuracy: 0.71875\n",
      "At: 172 [==========>] Loss 0.24314011740919872  - accuracy: 0.75\n",
      "At: 173 [==========>] Loss 0.29283176739320865  - accuracy: 0.625\n",
      "At: 174 [==========>] Loss 0.1739269529995502  - accuracy: 0.78125\n",
      "At: 175 [==========>] Loss 0.17799864323834896  - accuracy: 0.75\n",
      "At: 176 [==========>] Loss 0.16618941773256354  - accuracy: 0.78125\n",
      "At: 177 [==========>] Loss 0.09346846579103844  - accuracy: 0.875\n",
      "At: 178 [==========>] Loss 0.15367310911032733  - accuracy: 0.78125\n",
      "At: 179 [==========>] Loss 0.168323779511332  - accuracy: 0.78125\n",
      "At: 180 [==========>] Loss 0.1695293835111675  - accuracy: 0.8125\n",
      "At: 181 [==========>] Loss 0.08493006260035016  - accuracy: 0.9375\n",
      "At: 182 [==========>] Loss 0.24468090986733687  - accuracy: 0.71875\n",
      "At: 183 [==========>] Loss 0.1693666162166237  - accuracy: 0.8125\n",
      "At: 184 [==========>] Loss 0.18174652780210862  - accuracy: 0.84375\n",
      "At: 185 [==========>] Loss 0.11883425278516674  - accuracy: 0.84375\n",
      "At: 186 [==========>] Loss 0.1796038524246055  - accuracy: 0.78125\n",
      "At: 187 [==========>] Loss 0.1796526329799774  - accuracy: 0.78125\n",
      "At: 188 [==========>] Loss 0.18659965513798807  - accuracy: 0.8125\n",
      "At: 189 [==========>] Loss 0.20416068897208384  - accuracy: 0.78125\n",
      "At: 190 [==========>] Loss 0.11050072343600795  - accuracy: 0.90625\n",
      "At: 191 [==========>] Loss 0.2357425057159984  - accuracy: 0.6875\n",
      "At: 192 [==========>] Loss 0.19450157234588653  - accuracy: 0.78125\n",
      "At: 193 [==========>] Loss 0.24697391576677763  - accuracy: 0.65625\n",
      "At: 194 [==========>] Loss 0.20792285534593605  - accuracy: 0.6875\n",
      "At: 195 [==========>] Loss 0.16715743908082892  - accuracy: 0.8125\n",
      "At: 196 [==========>] Loss 0.1310973600689829  - accuracy: 0.8125\n",
      "At: 197 [==========>] Loss 0.14554788949861286  - accuracy: 0.84375\n",
      "At: 198 [==========>] Loss 0.1323119921767794  - accuracy: 0.84375\n",
      "At: 199 [==========>] Loss 0.08565887744928855  - accuracy: 0.875\n",
      "At: 200 [==========>] Loss 0.1735425490728987  - accuracy: 0.75\n",
      "At: 201 [==========>] Loss 0.14958857304888923  - accuracy: 0.8125\n",
      "At: 202 [==========>] Loss 0.11983819644874254  - accuracy: 0.78125\n",
      "At: 203 [==========>] Loss 0.1403627991047067  - accuracy: 0.90625\n",
      "At: 204 [==========>] Loss 0.1671502698610559  - accuracy: 0.75\n",
      "At: 205 [==========>] Loss 0.1695021516825451  - accuracy: 0.78125\n",
      "At: 206 [==========>] Loss 0.0864881167314063  - accuracy: 0.90625\n",
      "At: 207 [==========>] Loss 0.1174675329319233  - accuracy: 0.8125\n",
      "At: 208 [==========>] Loss 0.22216352865827876  - accuracy: 0.71875\n",
      "At: 209 [==========>] Loss 0.23318303154782932  - accuracy: 0.75\n",
      "At: 210 [==========>] Loss 0.10761590912544286  - accuracy: 0.875\n",
      "At: 211 [==========>] Loss 0.17009528266428298  - accuracy: 0.75\n",
      "At: 212 [==========>] Loss 0.19939415664236973  - accuracy: 0.71875\n",
      "At: 213 [==========>] Loss 0.18336446720649116  - accuracy: 0.78125\n",
      "At: 214 [==========>] Loss 0.24649461872618322  - accuracy: 0.6875\n",
      "At: 215 [==========>] Loss 0.13900028403083825  - accuracy: 0.8125\n",
      "At: 216 [==========>] Loss 0.17735342085689018  - accuracy: 0.78125\n",
      "At: 217 [==========>] Loss 0.1912380300210998  - accuracy: 0.71875\n",
      "At: 218 [==========>] Loss 0.1594187613436765  - accuracy: 0.78125\n",
      "At: 219 [==========>] Loss 0.25116054351702555  - accuracy: 0.71875\n",
      "At: 220 [==========>] Loss 0.17198391507997296  - accuracy: 0.75\n",
      "At: 221 [==========>] Loss 0.16589781602498427  - accuracy: 0.75\n",
      "At: 222 [==========>] Loss 0.0731238129726588  - accuracy: 0.90625\n",
      "At: 223 [==========>] Loss 0.25749392177443975  - accuracy: 0.6875\n",
      "At: 224 [==========>] Loss 0.18417048240144748  - accuracy: 0.8125\n",
      "At: 225 [==========>] Loss 0.15469878436929108  - accuracy: 0.84375\n",
      "At: 226 [==========>] Loss 0.17013815712221203  - accuracy: 0.78125\n",
      "At: 227 [==========>] Loss 0.170363744036029  - accuracy: 0.75\n",
      "At: 228 [==========>] Loss 0.19855726958588166  - accuracy: 0.78125\n",
      "At: 229 [==========>] Loss 0.18318253080200453  - accuracy: 0.78125\n",
      "At: 230 [==========>] Loss 0.18907362541128364  - accuracy: 0.78125\n",
      "At: 231 [==========>] Loss 0.24397273694560723  - accuracy: 0.6875\n",
      "At: 232 [==========>] Loss 0.1941275429805155  - accuracy: 0.71875\n",
      "At: 233 [==========>] Loss 0.20759041968698275  - accuracy: 0.75\n",
      "At: 234 [==========>] Loss 0.1588144688376457  - accuracy: 0.875\n",
      "At: 235 [==========>] Loss 0.1934504163607586  - accuracy: 0.6875\n",
      "At: 236 [==========>] Loss 0.17423902528860075  - accuracy: 0.75\n",
      "At: 237 [==========>] Loss 0.11618056705490262  - accuracy: 0.84375\n",
      "At: 238 [==========>] Loss 0.14858499969476138  - accuracy: 0.8125\n",
      "At: 239 [==========>] Loss 0.15500613874401117  - accuracy: 0.78125\n",
      "At: 240 [==========>] Loss 0.20355152647523897  - accuracy: 0.75\n",
      "At: 241 [==========>] Loss 0.1331806438975288  - accuracy: 0.84375\n",
      "At: 242 [==========>] Loss 0.14716929798089487  - accuracy: 0.8125\n",
      "At: 243 [==========>] Loss 0.12572968781249574  - accuracy: 0.8125\n",
      "At: 244 [==========>] Loss 0.13202534292088192  - accuracy: 0.8125\n",
      "At: 245 [==========>] Loss 0.1218768223382742  - accuracy: 0.84375\n",
      "At: 246 [==========>] Loss 0.16442243251600952  - accuracy: 0.84375\n",
      "At: 247 [==========>] Loss 0.1745055409927636  - accuracy: 0.78125\n",
      "At: 248 [==========>] Loss 0.13115391332482113  - accuracy: 0.84375\n",
      "At: 249 [==========>] Loss 0.08373259986153152  - accuracy: 0.90625\n",
      "At: 250 [==========>] Loss 0.22490532092934187  - accuracy: 0.6875\n",
      "At: 251 [==========>] Loss 0.21632502056902048  - accuracy: 0.71875\n",
      "At: 252 [==========>] Loss 0.12179266180805189  - accuracy: 0.90625\n",
      "At: 253 [==========>] Loss 0.1890613112710426  - accuracy: 0.78125\n",
      "At: 254 [==========>] Loss 0.06567151488271822  - accuracy: 0.9375\n",
      "At: 255 [==========>] Loss 0.18206872378538141  - accuracy: 0.78125\n",
      "At: 256 [==========>] Loss 0.19516697352684811  - accuracy: 0.71875\n",
      "At: 257 [==========>] Loss 0.08309285844190153  - accuracy: 0.90625\n",
      "At: 258 [==========>] Loss 0.17698303138469604  - accuracy: 0.78125\n",
      "At: 259 [==========>] Loss 0.13611948057615442  - accuracy: 0.84375\n",
      "At: 260 [==========>] Loss 0.11449108547264068  - accuracy: 0.875\n",
      "At: 261 [==========>] Loss 0.07784775818578485  - accuracy: 0.90625\n",
      "At: 262 [==========>] Loss 0.13947757247991344  - accuracy: 0.78125\n",
      "At: 263 [==========>] Loss 0.10552247149299529  - accuracy: 0.875\n",
      "At: 264 [==========>] Loss 0.10412482460138286  - accuracy: 0.84375\n",
      "At: 265 [==========>] Loss 0.24043520089068254  - accuracy: 0.6875\n",
      "At: 266 [==========>] Loss 0.2391321744873806  - accuracy: 0.65625\n",
      "At: 267 [==========>] Loss 0.1538146901642191  - accuracy: 0.8125\n",
      "At: 268 [==========>] Loss 0.2530871802855921  - accuracy: 0.6875\n",
      "At: 269 [==========>] Loss 0.15680476171223956  - accuracy: 0.78125\n",
      "At: 270 [==========>] Loss 0.28353606382252317  - accuracy: 0.625\n",
      "At: 271 [==========>] Loss 0.23435243841653752  - accuracy: 0.6875\n",
      "At: 272 [==========>] Loss 0.11063072755583875  - accuracy: 0.875\n",
      "At: 273 [==========>] Loss 0.17892224905391663  - accuracy: 0.78125\n",
      "At: 274 [==========>] Loss 0.18079449051183707  - accuracy: 0.78125\n",
      "At: 275 [==========>] Loss 0.04398485603187144  - accuracy: 0.96875\n",
      "At: 276 [==========>] Loss 0.19325269202151374  - accuracy: 0.75\n",
      "At: 277 [==========>] Loss 0.11596851433118241  - accuracy: 0.875\n",
      "At: 278 [==========>] Loss 0.14594666503907594  - accuracy: 0.8125\n",
      "At: 279 [==========>] Loss 0.10899691697681752  - accuracy: 0.90625\n",
      "At: 280 [==========>] Loss 0.13573039032251671  - accuracy: 0.8125\n",
      "At: 281 [==========>] Loss 0.1630972037624894  - accuracy: 0.8125\n",
      "At: 282 [==========>] Loss 0.246408310062146  - accuracy: 0.65625\n",
      "At: 283 [==========>] Loss 0.13924631884188826  - accuracy: 0.84375\n",
      "At: 284 [==========>] Loss 0.0899706245806011  - accuracy: 0.90625\n",
      "At: 285 [==========>] Loss 0.1609611130401436  - accuracy: 0.8125\n",
      "At: 286 [==========>] Loss 0.06279843858260295  - accuracy: 0.90625\n",
      "At: 287 [==========>] Loss 0.20209138555721812  - accuracy: 0.75\n",
      "At: 288 [==========>] Loss 0.0976269845236438  - accuracy: 0.875\n",
      "At: 289 [==========>] Loss 0.08229438872102278  - accuracy: 0.90625\n",
      "At: 290 [==========>] Loss 0.0983860425267144  - accuracy: 0.875\n",
      "At: 291 [==========>] Loss 0.12675381844000716  - accuracy: 0.78125\n",
      "At: 292 [==========>] Loss 0.15066301501882534  - accuracy: 0.78125\n",
      "At: 293 [==========>] Loss 0.2587737337344951  - accuracy: 0.6875\n",
      "At: 294 [==========>] Loss 0.1995615559001246  - accuracy: 0.75\n",
      "At: 295 [==========>] Loss 0.15661099281143095  - accuracy: 0.8125\n",
      "At: 296 [==========>] Loss 0.09598716972547916  - accuracy: 0.875\n",
      "At: 297 [==========>] Loss 0.10518578332424552  - accuracy: 0.84375\n",
      "At: 298 [==========>] Loss 0.15142535992144696  - accuracy: 0.8125\n",
      "At: 299 [==========>] Loss 0.24158030758976434  - accuracy: 0.71875\n",
      "At: 300 [==========>] Loss 0.20176897623630455  - accuracy: 0.78125\n",
      "At: 301 [==========>] Loss 0.15272935786894132  - accuracy: 0.8125\n",
      "At: 302 [==========>] Loss 0.13924193471288596  - accuracy: 0.84375\n",
      "At: 303 [==========>] Loss 0.08256205328132188  - accuracy: 0.875\n",
      "At: 304 [==========>] Loss 0.18614901018794172  - accuracy: 0.75\n",
      "At: 305 [==========>] Loss 0.21307132313823368  - accuracy: 0.6875\n",
      "At: 306 [==========>] Loss 0.1378779754732785  - accuracy: 0.84375\n",
      "At: 307 [==========>] Loss 0.22698063458109896  - accuracy: 0.71875\n",
      "At: 308 [==========>] Loss 0.16061649954774307  - accuracy: 0.75\n",
      "At: 309 [==========>] Loss 0.10579249669038757  - accuracy: 0.90625\n",
      "At: 310 [==========>] Loss 0.1828922910204876  - accuracy: 0.71875\n",
      "At: 311 [==========>] Loss 0.06985306587670916  - accuracy: 0.90625\n",
      "At: 312 [==========>] Loss 0.14664836656523175  - accuracy: 0.84375\n",
      "At: 313 [==========>] Loss 0.11624316716176807  - accuracy: 0.84375\n",
      "At: 314 [==========>] Loss 0.20994924910622062  - accuracy: 0.71875\n",
      "At: 315 [==========>] Loss 0.1391653445397965  - accuracy: 0.84375\n",
      "At: 316 [==========>] Loss 0.2078229179238378  - accuracy: 0.6875\n",
      "At: 317 [==========>] Loss 0.3088633184010918  - accuracy: 0.625\n",
      "At: 318 [==========>] Loss 0.14505086738574408  - accuracy: 0.8125\n",
      "At: 319 [==========>] Loss 0.09548888214223981  - accuracy: 0.875\n",
      "At: 320 [==========>] Loss 0.23024542959291097  - accuracy: 0.6875\n",
      "At: 321 [==========>] Loss 0.2461440060479244  - accuracy: 0.6875\n",
      "At: 322 [==========>] Loss 0.06638208374887354  - accuracy: 0.875\n",
      "At: 323 [==========>] Loss 0.10540255963721035  - accuracy: 0.875\n",
      "At: 324 [==========>] Loss 0.16151319702458444  - accuracy: 0.78125\n",
      "At: 325 [==========>] Loss 0.10960030166268592  - accuracy: 0.875\n",
      "At: 326 [==========>] Loss 0.21020669432397074  - accuracy: 0.71875\n",
      "At: 327 [==========>] Loss 0.11457819648340473  - accuracy: 0.84375\n",
      "At: 328 [==========>] Loss 0.10466933676596613  - accuracy: 0.84375\n",
      "At: 329 [==========>] Loss 0.1410687173719426  - accuracy: 0.875\n",
      "At: 330 [==========>] Loss 0.1508945605268951  - accuracy: 0.8125\n",
      "At: 331 [==========>] Loss 0.1769800743736082  - accuracy: 0.71875\n",
      "At: 332 [==========>] Loss 0.2892737645287194  - accuracy: 0.5625\n",
      "At: 333 [==========>] Loss 0.13801383630920377  - accuracy: 0.8125\n",
      "At: 334 [==========>] Loss 0.09441057832467518  - accuracy: 0.90625\n",
      "At: 335 [==========>] Loss 0.15472693064215393  - accuracy: 0.78125\n",
      "At: 336 [==========>] Loss 0.14521735880030454  - accuracy: 0.84375\n",
      "At: 337 [==========>] Loss 0.18589669190169794  - accuracy: 0.8125\n",
      "At: 338 [==========>] Loss 0.16753134915594675  - accuracy: 0.8125\n",
      "At: 339 [==========>] Loss 0.21014390042684333  - accuracy: 0.71875\n",
      "At: 340 [==========>] Loss 0.12730752369188775  - accuracy: 0.84375\n",
      "At: 341 [==========>] Loss 0.13484967031955492  - accuracy: 0.84375\n",
      "At: 342 [==========>] Loss 0.1375703831549971  - accuracy: 0.8125\n",
      "At: 343 [==========>] Loss 0.26819125034377606  - accuracy: 0.65625\n",
      "At: 344 [==========>] Loss 0.233427700764072  - accuracy: 0.65625\n",
      "At: 345 [==========>] Loss 0.20433996317231112  - accuracy: 0.6875\n",
      "At: 346 [==========>] Loss 0.11452170447827952  - accuracy: 0.8125\n",
      "At: 347 [==========>] Loss 0.12926130773555364  - accuracy: 0.84375\n",
      "At: 348 [==========>] Loss 0.11840354212108302  - accuracy: 0.8125\n",
      "At: 349 [==========>] Loss 0.200721467870782  - accuracy: 0.71875\n",
      "At: 350 [==========>] Loss 0.1077357807695152  - accuracy: 0.90625\n",
      "At: 351 [==========>] Loss 0.17654262967340284  - accuracy: 0.78125\n",
      "At: 352 [==========>] Loss 0.12037022412941034  - accuracy: 0.84375\n",
      "At: 353 [==========>] Loss 0.1408863221678661  - accuracy: 0.8125\n",
      "At: 354 [==========>] Loss 0.24733723250640308  - accuracy: 0.71875\n",
      "At: 355 [==========>] Loss 0.08375246988627871  - accuracy: 0.90625\n",
      "At: 356 [==========>] Loss 0.21705271582225705  - accuracy: 0.75\n",
      "At: 357 [==========>] Loss 0.13384194709700303  - accuracy: 0.84375\n",
      "At: 358 [==========>] Loss 0.1124128949558959  - accuracy: 0.84375\n",
      "At: 359 [==========>] Loss 0.07272941389126747  - accuracy: 0.9375\n",
      "At: 360 [==========>] Loss 0.1371058970443711  - accuracy: 0.84375\n",
      "At: 361 [==========>] Loss 0.09640450124066842  - accuracy: 0.875\n",
      "At: 362 [==========>] Loss 0.1965415128132686  - accuracy: 0.75\n",
      "At: 363 [==========>] Loss 0.10288864829830113  - accuracy: 0.84375\n",
      "At: 364 [==========>] Loss 0.2305263499864192  - accuracy: 0.6875\n",
      "At: 365 [==========>] Loss 0.10718201701933261  - accuracy: 0.84375\n",
      "At: 366 [==========>] Loss 0.1847683335818045  - accuracy: 0.75\n",
      "At: 367 [==========>] Loss 0.17254477515500388  - accuracy: 0.71875\n",
      "At: 368 [==========>] Loss 0.17175748326086604  - accuracy: 0.78125\n",
      "At: 369 [==========>] Loss 0.1420198342712713  - accuracy: 0.84375\n",
      "At: 370 [==========>] Loss 0.20428765174170596  - accuracy: 0.6875\n",
      "At: 371 [==========>] Loss 0.1111663916555584  - accuracy: 0.875\n",
      "At: 372 [==========>] Loss 0.12934468916041525  - accuracy: 0.875\n",
      "At: 373 [==========>] Loss 0.13146275679425506  - accuracy: 0.84375\n",
      "At: 374 [==========>] Loss 0.09421765893736982  - accuracy: 0.9375\n",
      "At: 375 [==========>] Loss 0.1787360579674017  - accuracy: 0.75\n",
      "At: 376 [==========>] Loss 0.10101951916984477  - accuracy: 0.90625\n",
      "At: 377 [==========>] Loss 0.20533230781942619  - accuracy: 0.6875\n",
      "At: 378 [==========>] Loss 0.18452666179764082  - accuracy: 0.75\n",
      "At: 379 [==========>] Loss 0.18139623414534153  - accuracy: 0.6875\n",
      "At: 380 [==========>] Loss 0.1489905523193235  - accuracy: 0.78125\n",
      "At: 381 [==========>] Loss 0.18052365618807897  - accuracy: 0.75\n",
      "At: 382 [==========>] Loss 0.08686669501814953  - accuracy: 0.9375\n",
      "At: 383 [==========>] Loss 0.1818998355119199  - accuracy: 0.78125\n",
      "At: 384 [==========>] Loss 0.16792859566045557  - accuracy: 0.6875\n",
      "At: 385 [==========>] Loss 0.1467509627922643  - accuracy: 0.78125\n",
      "At: 386 [==========>] Loss 0.19390533643345154  - accuracy: 0.75\n",
      "At: 387 [==========>] Loss 0.05799430819081291  - accuracy: 0.9375\n",
      "At: 388 [==========>] Loss 0.2184661490354728  - accuracy: 0.75\n",
      "At: 389 [==========>] Loss 0.17020309057767782  - accuracy: 0.75\n",
      "At: 390 [==========>] Loss 0.12232450873842245  - accuracy: 0.8125\n",
      "At: 391 [==========>] Loss 0.13841552060308462  - accuracy: 0.8125\n",
      "At: 392 [==========>] Loss 0.17519788382258877  - accuracy: 0.78125\n",
      "At: 393 [==========>] Loss 0.2086060537644857  - accuracy: 0.71875\n",
      "At: 394 [==========>] Loss 0.08606078593081501  - accuracy: 0.90625\n",
      "At: 395 [==========>] Loss 0.18823412051113692  - accuracy: 0.75\n",
      "At: 396 [==========>] Loss 0.1445014839235391  - accuracy: 0.8125\n",
      "At: 397 [==========>] Loss 0.13671706256838953  - accuracy: 0.8125\n",
      "At: 398 [==========>] Loss 0.22024922040737097  - accuracy: 0.65625\n",
      "At: 399 [==========>] Loss 0.19363380521965146  - accuracy: 0.75\n",
      "At: 400 [==========>] Loss 0.20087869514594522  - accuracy: 0.6875\n",
      "At: 401 [==========>] Loss 0.126134299425729  - accuracy: 0.84375\n",
      "At: 402 [==========>] Loss 0.11004163181644261  - accuracy: 0.875\n",
      "At: 403 [==========>] Loss 0.03540185478736692  - accuracy: 0.96875\n",
      "At: 404 [==========>] Loss 0.101755170484572  - accuracy: 0.875\n",
      "At: 405 [==========>] Loss 0.21611920698496262  - accuracy: 0.75\n",
      "At: 406 [==========>] Loss 0.17031411377959688  - accuracy: 0.78125\n",
      "At: 407 [==========>] Loss 0.16679960782073802  - accuracy: 0.75\n",
      "At: 408 [==========>] Loss 0.16929915596914247  - accuracy: 0.78125\n",
      "At: 409 [==========>] Loss 0.22781843278252895  - accuracy: 0.71875\n",
      "At: 410 [==========>] Loss 0.1066711555771688  - accuracy: 0.875\n",
      "At: 411 [==========>] Loss 0.11165787803360526  - accuracy: 0.84375\n",
      "At: 412 [==========>] Loss 0.17172685129330315  - accuracy: 0.78125\n",
      "At: 413 [==========>] Loss 0.12518690285285855  - accuracy: 0.75\n",
      "At: 414 [==========>] Loss 0.18983162237403337  - accuracy: 0.6875\n",
      "At: 415 [==========>] Loss 0.1579857835304005  - accuracy: 0.8125\n",
      "At: 416 [==========>] Loss 0.20666395628520748  - accuracy: 0.6875\n",
      "At: 417 [==========>] Loss 0.17493321316180005  - accuracy: 0.75\n",
      "At: 418 [==========>] Loss 0.11928961408197405  - accuracy: 0.78125\n",
      "At: 419 [==========>] Loss 0.13154871498537524  - accuracy: 0.8125\n",
      "At: 420 [==========>] Loss 0.15129045809075428  - accuracy: 0.75\n",
      "At: 421 [==========>] Loss 0.13432446526086947  - accuracy: 0.8125\n",
      "At: 422 [==========>] Loss 0.12079752479061777  - accuracy: 0.84375\n",
      "At: 423 [==========>] Loss 0.11008886261658943  - accuracy: 0.875\n",
      "At: 424 [==========>] Loss 0.22385593617987146  - accuracy: 0.65625\n",
      "At: 425 [==========>] Loss 0.1674618093354653  - accuracy: 0.75\n",
      "At: 426 [==========>] Loss 0.13795186754540437  - accuracy: 0.84375\n",
      "At: 427 [==========>] Loss 0.2004838668210998  - accuracy: 0.71875\n",
      "At: 428 [==========>] Loss 0.2844269794366157  - accuracy: 0.65625\n",
      "At: 429 [==========>] Loss 0.15843794026717303  - accuracy: 0.78125\n",
      "At: 430 [==========>] Loss 0.13811892472136653  - accuracy: 0.84375\n",
      "At: 431 [==========>] Loss 0.11839388512024732  - accuracy: 0.84375\n",
      "At: 432 [==========>] Loss 0.12022225155239025  - accuracy: 0.875\n",
      "At: 433 [==========>] Loss 0.10176697112510424  - accuracy: 0.875\n",
      "At: 434 [==========>] Loss 0.1088160749333175  - accuracy: 0.875\n",
      "At: 435 [==========>] Loss 0.1614340449063133  - accuracy: 0.78125\n",
      "At: 436 [==========>] Loss 0.18034372876554025  - accuracy: 0.78125\n",
      "At: 437 [==========>] Loss 0.18402456014311594  - accuracy: 0.75\n",
      "At: 438 [==========>] Loss 0.1523952625688662  - accuracy: 0.8125\n",
      "At: 439 [==========>] Loss 0.13210874771027656  - accuracy: 0.84375\n",
      "At: 440 [==========>] Loss 0.08682509013178216  - accuracy: 0.90625\n",
      "At: 441 [==========>] Loss 0.14888734254262942  - accuracy: 0.78125\n",
      "At: 442 [==========>] Loss 0.19627014435073747  - accuracy: 0.71875\n",
      "At: 443 [==========>] Loss 0.13489885198136925  - accuracy: 0.8125\n",
      "At: 444 [==========>] Loss 0.210418368808229  - accuracy: 0.75\n",
      "At: 445 [==========>] Loss 0.1562826218431307  - accuracy: 0.8125\n",
      "At: 446 [==========>] Loss 0.2514014304025699  - accuracy: 0.65625\n",
      "At: 447 [==========>] Loss 0.14391585483360003  - accuracy: 0.8125\n",
      "At: 448 [==========>] Loss 0.17341686494564662  - accuracy: 0.75\n",
      "At: 449 [==========>] Loss 0.14011159804592357  - accuracy: 0.78125\n",
      "At: 450 [==========>] Loss 0.1928785622868046  - accuracy: 0.71875\n",
      "At: 451 [==========>] Loss 0.12610581415879918  - accuracy: 0.8125\n",
      "At: 452 [==========>] Loss 0.14495538969992516  - accuracy: 0.84375\n",
      "At: 453 [==========>] Loss 0.11816286639649268  - accuracy: 0.75\n",
      "At: 454 [==========>] Loss 0.21241820963544555  - accuracy: 0.75\n",
      "At: 455 [==========>] Loss 0.1557231348889456  - accuracy: 0.75\n",
      "At: 456 [==========>] Loss 0.14390803082322928  - accuracy: 0.8125\n",
      "At: 457 [==========>] Loss 0.1906759811036694  - accuracy: 0.75\n",
      "At: 458 [==========>] Loss 0.10970506681079603  - accuracy: 0.875\n",
      "At: 459 [==========>] Loss 0.21401345239242384  - accuracy: 0.6875\n",
      "At: 460 [==========>] Loss 0.1341991965538872  - accuracy: 0.8125\n",
      "At: 461 [==========>] Loss 0.20996991628594092  - accuracy: 0.75\n",
      "At: 462 [==========>] Loss 0.15075330875511014  - accuracy: 0.78125\n",
      "At: 463 [==========>] Loss 0.14212875584955798  - accuracy: 0.8125\n",
      "At: 464 [==========>] Loss 0.15882394035254604  - accuracy: 0.75\n",
      "At: 465 [==========>] Loss 0.22915404757663704  - accuracy: 0.65625\n",
      "At: 466 [==========>] Loss 0.11123123470304523  - accuracy: 0.84375\n",
      "At: 467 [==========>] Loss 0.1964060743672157  - accuracy: 0.78125\n",
      "At: 468 [==========>] Loss 0.11904628137656537  - accuracy: 0.8125\n",
      "At: 469 [==========>] Loss 0.13140245863880648  - accuracy: 0.84375\n",
      "At: 470 [==========>] Loss 0.10606498741241613  - accuracy: 0.875\n",
      "At: 471 [==========>] Loss 0.1862575874575675  - accuracy: 0.78125\n",
      "At: 472 [==========>] Loss 0.127572363661988  - accuracy: 0.8125\n",
      "At: 473 [==========>] Loss 0.13788367414657282  - accuracy: 0.8125\n",
      "At: 474 [==========>] Loss 0.1886431304372328  - accuracy: 0.78125\n",
      "At: 475 [==========>] Loss 0.1570846194995773  - accuracy: 0.78125\n",
      "At: 476 [==========>] Loss 0.15380644882135436  - accuracy: 0.8125\n",
      "At: 477 [==========>] Loss 0.11863773662651715  - accuracy: 0.84375\n",
      "At: 478 [==========>] Loss 0.1154418997551426  - accuracy: 0.875\n",
      "At: 479 [==========>] Loss 0.14900142565569996  - accuracy: 0.75\n",
      "At: 480 [==========>] Loss 0.1740281028510425  - accuracy: 0.78125\n",
      "At: 481 [==========>] Loss 0.1343713247698388  - accuracy: 0.84375\n",
      "At: 482 [==========>] Loss 0.10960796935700848  - accuracy: 0.875\n",
      "At: 483 [==========>] Loss 0.14246651732023896  - accuracy: 0.78125\n",
      "At: 484 [==========>] Loss 0.08869581307912819  - accuracy: 0.875\n",
      "At: 485 [==========>] Loss 0.14738918193210632  - accuracy: 0.8125\n",
      "At: 486 [==========>] Loss 0.20836355926889094  - accuracy: 0.71875\n",
      "At: 487 [==========>] Loss 0.1507654975091289  - accuracy: 0.8125\n",
      "At: 488 [==========>] Loss 0.12869402069034452  - accuracy: 0.84375\n",
      "At: 489 [==========>] Loss 0.1316937161680543  - accuracy: 0.78125\n",
      "At: 490 [==========>] Loss 0.1230604783478629  - accuracy: 0.78125\n",
      "At: 491 [==========>] Loss 0.13689758553992434  - accuracy: 0.8125\n",
      "At: 492 [==========>] Loss 0.23010813615539888  - accuracy: 0.6875\n",
      "At: 493 [==========>] Loss 0.09793209616957776  - accuracy: 0.875\n",
      "At: 494 [==========>] Loss 0.16523852726954413  - accuracy: 0.75\n",
      "At: 495 [==========>] Loss 0.12612022658772992  - accuracy: 0.8125\n",
      "At: 496 [==========>] Loss 0.15563285666164434  - accuracy: 0.78125\n",
      "At: 497 [==========>] Loss 0.16990542963042482  - accuracy: 0.78125\n",
      "At: 498 [==========>] Loss 0.06117945665838095  - accuracy: 0.9375\n",
      "At: 499 [==========>] Loss 0.15641400991244794  - accuracy: 0.78125\n",
      "At: 500 [==========>] Loss 0.10372695945093321  - accuracy: 0.84375\n",
      "At: 501 [==========>] Loss 0.18371931847785547  - accuracy: 0.78125\n",
      "At: 502 [==========>] Loss 0.15769709176345645  - accuracy: 0.75\n",
      "At: 503 [==========>] Loss 0.08667658370200955  - accuracy: 0.90625\n",
      "At: 504 [==========>] Loss 0.12128529647015554  - accuracy: 0.8125\n",
      "At: 505 [==========>] Loss 0.15399803489575414  - accuracy: 0.8125\n",
      "At: 506 [==========>] Loss 0.23945227195632857  - accuracy: 0.65625\n",
      "At: 507 [==========>] Loss 0.09889078958769582  - accuracy: 0.84375\n",
      "At: 508 [==========>] Loss 0.10922854046508215  - accuracy: 0.8125\n",
      "At: 509 [==========>] Loss 0.20444014631339308  - accuracy: 0.65625\n",
      "At: 510 [==========>] Loss 0.2536859802610538  - accuracy: 0.53125\n",
      "At: 511 [==========>] Loss 0.14510289963484643  - accuracy: 0.8125\n",
      "At: 512 [==========>] Loss 0.1595655674394725  - accuracy: 0.78125\n",
      "At: 513 [==========>] Loss 0.19577834997136812  - accuracy: 0.71875\n",
      "At: 514 [==========>] Loss 0.21291341101306904  - accuracy: 0.6875\n",
      "At: 515 [==========>] Loss 0.09553053181938809  - accuracy: 0.9375\n",
      "At: 516 [==========>] Loss 0.2077837023968552  - accuracy: 0.75\n",
      "At: 517 [==========>] Loss 0.16279872203398985  - accuracy: 0.78125\n",
      "At: 518 [==========>] Loss 0.13091367591562875  - accuracy: 0.8125\n",
      "At: 519 [==========>] Loss 0.12966044648752512  - accuracy: 0.84375\n",
      "At: 520 [==========>] Loss 0.13606077618852105  - accuracy: 0.8125\n",
      "At: 521 [==========>] Loss 0.1276831073619622  - accuracy: 0.84375\n",
      "At: 522 [==========>] Loss 0.1344374277008177  - accuracy: 0.875\n",
      "At: 523 [==========>] Loss 0.1440803805784764  - accuracy: 0.78125\n",
      "At: 524 [==========>] Loss 0.11463415034237395  - accuracy: 0.875\n",
      "At: 525 [==========>] Loss 0.15799004468817124  - accuracy: 0.78125\n",
      "At: 526 [==========>] Loss 0.20543595769681677  - accuracy: 0.6875\n",
      "At: 527 [==========>] Loss 0.26676976332541735  - accuracy: 0.59375\n",
      "At: 528 [==========>] Loss 0.17246798915436695  - accuracy: 0.75\n",
      "At: 529 [==========>] Loss 0.12239761443301422  - accuracy: 0.8125\n",
      "At: 530 [==========>] Loss 0.19171244935536053  - accuracy: 0.71875\n",
      "At: 531 [==========>] Loss 0.16158427558597743  - accuracy: 0.75\n",
      "At: 532 [==========>] Loss 0.11694442109180898  - accuracy: 0.875\n",
      "At: 533 [==========>] Loss 0.1004474681324066  - accuracy: 0.8125\n",
      "At: 534 [==========>] Loss 0.17429789265708914  - accuracy: 0.78125\n",
      "At: 535 [==========>] Loss 0.139841575798125  - accuracy: 0.8125\n",
      "At: 536 [==========>] Loss 0.1730056410761487  - accuracy: 0.75\n",
      "At: 537 [==========>] Loss 0.10896554888064805  - accuracy: 0.84375\n",
      "At: 538 [==========>] Loss 0.1338907239142521  - accuracy: 0.84375\n",
      "At: 539 [==========>] Loss 0.11943255307895832  - accuracy: 0.8125\n",
      "At: 540 [==========>] Loss 0.20487356407256185  - accuracy: 0.6875\n",
      "At: 541 [==========>] Loss 0.164980037220056  - accuracy: 0.75\n",
      "At: 542 [==========>] Loss 0.12030577984090933  - accuracy: 0.8125\n",
      "At: 543 [==========>] Loss 0.16469851502476338  - accuracy: 0.71875\n",
      "At: 544 [==========>] Loss 0.2121220244252394  - accuracy: 0.65625\n",
      "At: 545 [==========>] Loss 0.0956998122035051  - accuracy: 0.875\n",
      "At: 546 [==========>] Loss 0.17710573808077645  - accuracy: 0.78125\n",
      "At: 547 [==========>] Loss 0.14464783879175316  - accuracy: 0.78125\n",
      "At: 548 [==========>] Loss 0.10943600247296392  - accuracy: 0.875\n",
      "At: 549 [==========>] Loss 0.1389393157521776  - accuracy: 0.75\n",
      "At: 550 [==========>] Loss 0.09952567279568955  - accuracy: 0.875\n",
      "At: 551 [==========>] Loss 0.11807206320350846  - accuracy: 0.8125\n",
      "At: 552 [==========>] Loss 0.15984000079353816  - accuracy: 0.75\n",
      "At: 553 [==========>] Loss 0.1157457532188307  - accuracy: 0.875\n",
      "At: 554 [==========>] Loss 0.09948750650686347  - accuracy: 0.84375\n",
      "At: 555 [==========>] Loss 0.1531500335261861  - accuracy: 0.75\n",
      "At: 556 [==========>] Loss 0.144369424491022  - accuracy: 0.78125\n",
      "At: 557 [==========>] Loss 0.14402482648035098  - accuracy: 0.8125\n",
      "At: 558 [==========>] Loss 0.1316791150383636  - accuracy: 0.875\n",
      "At: 559 [==========>] Loss 0.17826908769612043  - accuracy: 0.78125\n",
      "At: 560 [==========>] Loss 0.12547736829713793  - accuracy: 0.8125\n",
      "At: 561 [==========>] Loss 0.12860092430343156  - accuracy: 0.84375\n",
      "At: 562 [==========>] Loss 0.07022165430090506  - accuracy: 0.9375\n",
      "At: 563 [==========>] Loss 0.13508263731089992  - accuracy: 0.8125\n",
      "At: 564 [==========>] Loss 0.17218180331286864  - accuracy: 0.75\n",
      "At: 565 [==========>] Loss 0.12527484223711377  - accuracy: 0.8125\n",
      "At: 566 [==========>] Loss 0.1826665067777254  - accuracy: 0.71875\n",
      "At: 567 [==========>] Loss 0.1838917441848751  - accuracy: 0.75\n",
      "At: 568 [==========>] Loss 0.19995298802214279  - accuracy: 0.65625\n",
      "At: 569 [==========>] Loss 0.17986983224355668  - accuracy: 0.78125\n",
      "At: 570 [==========>] Loss 0.14978807523422963  - accuracy: 0.78125\n",
      "At: 571 [==========>] Loss 0.11538452888642198  - accuracy: 0.8125\n",
      "At: 572 [==========>] Loss 0.12165964859006079  - accuracy: 0.8125\n",
      "At: 573 [==========>] Loss 0.11205247704125827  - accuracy: 0.875\n",
      "At: 574 [==========>] Loss 0.1416193699196815  - accuracy: 0.8125\n",
      "At: 575 [==========>] Loss 0.1372392859103239  - accuracy: 0.84375\n",
      "At: 576 [==========>] Loss 0.09088013003160593  - accuracy: 0.90625\n",
      "At: 577 [==========>] Loss 0.18643705449627868  - accuracy: 0.75\n",
      "At: 578 [==========>] Loss 0.16538577649948322  - accuracy: 0.75\n",
      "At: 579 [==========>] Loss 0.11742231115666082  - accuracy: 0.8125\n",
      "At: 580 [==========>] Loss 0.1495260161485808  - accuracy: 0.78125\n",
      "At: 581 [==========>] Loss 0.13102937638960452  - accuracy: 0.78125\n",
      "At: 582 [==========>] Loss 0.15358113026420328  - accuracy: 0.78125\n",
      "At: 583 [==========>] Loss 0.18228872411787925  - accuracy: 0.71875\n",
      "At: 584 [==========>] Loss 0.10894022213563229  - accuracy: 0.84375\n",
      "At: 585 [==========>] Loss 0.17247765244379815  - accuracy: 0.625\n",
      "At: 586 [==========>] Loss 0.09007667784387853  - accuracy: 0.90625\n",
      "At: 587 [==========>] Loss 0.1727178462672113  - accuracy: 0.75\n",
      "At: 588 [==========>] Loss 0.15979039197613965  - accuracy: 0.78125\n",
      "At: 589 [==========>] Loss 0.1263977994718411  - accuracy: 0.84375\n",
      "At: 590 [==========>] Loss 0.09677926393690685  - accuracy: 0.90625\n",
      "At: 591 [==========>] Loss 0.12095126625816655  - accuracy: 0.84375\n",
      "At: 592 [==========>] Loss 0.09607500668537865  - accuracy: 0.90625\n",
      "At: 593 [==========>] Loss 0.16279696433990323  - accuracy: 0.71875\n",
      "At: 594 [==========>] Loss 0.1679765511248109  - accuracy: 0.8125\n",
      "At: 595 [==========>] Loss 0.15230661044669774  - accuracy: 0.84375\n",
      "At: 596 [==========>] Loss 0.15570190852976082  - accuracy: 0.78125\n",
      "At: 597 [==========>] Loss 0.2433556841739311  - accuracy: 0.71875\n",
      "At: 598 [==========>] Loss 0.1451410172124827  - accuracy: 0.8125\n",
      "At: 599 [==========>] Loss 0.14798222072742917  - accuracy: 0.78125\n",
      "At: 600 [==========>] Loss 0.08888773756881066  - accuracy: 0.90625\n",
      "At: 601 [==========>] Loss 0.10618191412808128  - accuracy: 0.84375\n",
      "At: 602 [==========>] Loss 0.11145497585096595  - accuracy: 0.8125\n",
      "At: 603 [==========>] Loss 0.1867218856025265  - accuracy: 0.78125\n",
      "At: 604 [==========>] Loss 0.1722221507513695  - accuracy: 0.71875\n",
      "At: 605 [==========>] Loss 0.09672697149624002  - accuracy: 0.875\n",
      "At: 606 [==========>] Loss 0.1324808289048729  - accuracy: 0.75\n",
      "At: 607 [==========>] Loss 0.09021753739803863  - accuracy: 0.84375\n",
      "At: 608 [==========>] Loss 0.1357713825778723  - accuracy: 0.78125\n",
      "At: 609 [==========>] Loss 0.08688670906410921  - accuracy: 0.84375\n",
      "At: 610 [==========>] Loss 0.1751665488324718  - accuracy: 0.75\n",
      "At: 611 [==========>] Loss 0.11416366203397149  - accuracy: 0.875\n",
      "At: 612 [==========>] Loss 0.12486134572775551  - accuracy: 0.84375\n",
      "At: 613 [==========>] Loss 0.11954828813436394  - accuracy: 0.8125\n",
      "At: 614 [==========>] Loss 0.14957286750482698  - accuracy: 0.8125\n",
      "At: 615 [==========>] Loss 0.17456699569191547  - accuracy: 0.75\n",
      "At: 616 [==========>] Loss 0.13843042504855368  - accuracy: 0.78125\n",
      "At: 617 [==========>] Loss 0.13805703773781344  - accuracy: 0.8125\n",
      "At: 618 [==========>] Loss 0.1869992340641182  - accuracy: 0.65625\n",
      "At: 619 [==========>] Loss 0.09653329876482668  - accuracy: 0.90625\n",
      "At: 620 [==========>] Loss 0.19330635843658017  - accuracy: 0.78125\n",
      "At: 621 [==========>] Loss 0.08073148256389022  - accuracy: 0.90625\n",
      "At: 622 [==========>] Loss 0.1816526632690333  - accuracy: 0.75\n",
      "At: 623 [==========>] Loss 0.11357969235242728  - accuracy: 0.875\n",
      "At: 624 [==========>] Loss 0.09392310825025968  - accuracy: 0.90625\n",
      "At: 625 [==========>] Loss 0.1214636823189056  - accuracy: 0.8125\n",
      "At: 626 [==========>] Loss 0.13647341952838898  - accuracy: 0.84375\n",
      "At: 627 [==========>] Loss 0.13389827388678738  - accuracy: 0.84375\n",
      "At: 628 [==========>] Loss 0.15922089330775382  - accuracy: 0.84375\n",
      "At: 629 [==========>] Loss 0.21583278112147006  - accuracy: 0.75\n",
      "At: 630 [==========>] Loss 0.23006027579591165  - accuracy: 0.65625\n",
      "At: 631 [==========>] Loss 0.16941551277452704  - accuracy: 0.75\n",
      "At: 632 [==========>] Loss 0.14791585314635075  - accuracy: 0.78125\n",
      "At: 633 [==========>] Loss 0.17692075402882165  - accuracy: 0.78125\n",
      "At: 634 [==========>] Loss 0.15844385790662865  - accuracy: 0.78125\n",
      "At: 635 [==========>] Loss 0.13452152444902754  - accuracy: 0.78125\n",
      "At: 636 [==========>] Loss 0.15989341418882846  - accuracy: 0.8125\n",
      "At: 637 [==========>] Loss 0.13420631940940658  - accuracy: 0.8125\n",
      "At: 638 [==========>] Loss 0.10926064137685915  - accuracy: 0.90625\n",
      "At: 639 [==========>] Loss 0.1556728751679533  - accuracy: 0.75\n",
      "At: 640 [==========>] Loss 0.19109412308011298  - accuracy: 0.71875\n",
      "At: 641 [==========>] Loss 0.11165967796757775  - accuracy: 0.84375\n",
      "At: 642 [==========>] Loss 0.1292924527902682  - accuracy: 0.84375\n",
      "At: 643 [==========>] Loss 0.13189466010810175  - accuracy: 0.78125\n",
      "At: 644 [==========>] Loss 0.11148723924250598  - accuracy: 0.8125\n",
      "At: 645 [==========>] Loss 0.1296291612460111  - accuracy: 0.8125\n",
      "At: 646 [==========>] Loss 0.14322327147945021  - accuracy: 0.78125\n",
      "At: 647 [==========>] Loss 0.1688602186918326  - accuracy: 0.75\n",
      "At: 648 [==========>] Loss 0.11208630915063086  - accuracy: 0.84375\n",
      "At: 649 [==========>] Loss 0.17265080667339838  - accuracy: 0.78125\n",
      "At: 650 [==========>] Loss 0.11298416050619618  - accuracy: 0.875\n",
      "At: 651 [==========>] Loss 0.18238065322471275  - accuracy: 0.71875\n",
      "At: 652 [==========>] Loss 0.10023695458775438  - accuracy: 0.90625\n",
      "At: 653 [==========>] Loss 0.11630426182727531  - accuracy: 0.84375\n",
      "At: 654 [==========>] Loss 0.10888539250268317  - accuracy: 0.90625\n",
      "At: 655 [==========>] Loss 0.17585515615631644  - accuracy: 0.71875\n",
      "At: 656 [==========>] Loss 0.11128232016116735  - accuracy: 0.84375\n",
      "At: 657 [==========>] Loss 0.18095193294037448  - accuracy: 0.71875\n",
      "At: 658 [==========>] Loss 0.1151011107334139  - accuracy: 0.84375\n",
      "At: 659 [==========>] Loss 0.13975333456601813  - accuracy: 0.8125\n",
      "At: 660 [==========>] Loss 0.12335355849958676  - accuracy: 0.875\n",
      "At: 661 [==========>] Loss 0.12874836361114758  - accuracy: 0.8125\n",
      "At: 662 [==========>] Loss 0.11667175020112244  - accuracy: 0.8125\n",
      "At: 663 [==========>] Loss 0.10991911045747163  - accuracy: 0.875\n",
      "At: 664 [==========>] Loss 0.1193809005625794  - accuracy: 0.84375\n",
      "At: 665 [==========>] Loss 0.17501641255005387  - accuracy: 0.6875\n",
      "At: 666 [==========>] Loss 0.14609161529093762  - accuracy: 0.78125\n",
      "At: 667 [==========>] Loss 0.14591179108095587  - accuracy: 0.78125\n",
      "At: 668 [==========>] Loss 0.14566466910509718  - accuracy: 0.84375\n",
      "At: 669 [==========>] Loss 0.1298676498548341  - accuracy: 0.8125\n",
      "At: 670 [==========>] Loss 0.1801784303615232  - accuracy: 0.75\n",
      "At: 671 [==========>] Loss 0.10698229042609851  - accuracy: 0.875\n",
      "At: 672 [==========>] Loss 0.14126447491341027  - accuracy: 0.8125\n",
      "At: 673 [==========>] Loss 0.06254978502266781  - accuracy: 0.96875\n",
      "At: 674 [==========>] Loss 0.1303072691729229  - accuracy: 0.8125\n",
      "At: 675 [==========>] Loss 0.09537534431384431  - accuracy: 0.84375\n",
      "At: 676 [==========>] Loss 0.14234685194540284  - accuracy: 0.8125\n",
      "At: 677 [==========>] Loss 0.150743261756869  - accuracy: 0.75\n",
      "At: 678 [==========>] Loss 0.11876187176078666  - accuracy: 0.8125\n",
      "At: 679 [==========>] Loss 0.1152327702054945  - accuracy: 0.875\n",
      "At: 680 [==========>] Loss 0.13618998246108271  - accuracy: 0.8125\n",
      "At: 681 [==========>] Loss 0.12043800008375173  - accuracy: 0.84375\n",
      "At: 682 [==========>] Loss 0.1382165060358603  - accuracy: 0.84375\n",
      "At: 683 [==========>] Loss 0.16797871825055333  - accuracy: 0.78125\n",
      "At: 684 [==========>] Loss 0.11355201697361683  - accuracy: 0.84375\n",
      "At: 685 [==========>] Loss 0.1185621641462466  - accuracy: 0.78125\n",
      "At: 686 [==========>] Loss 0.10531739993775474  - accuracy: 0.875\n",
      "At: 687 [==========>] Loss 0.05010458263606868  - accuracy: 0.96875\n",
      "At: 688 [==========>] Loss 0.08024510393096941  - accuracy: 0.9375\n",
      "At: 689 [==========>] Loss 0.15215276946483952  - accuracy: 0.8125\n",
      "At: 690 [==========>] Loss 0.14450317269450275  - accuracy: 0.84375\n",
      "At: 691 [==========>] Loss 0.08944089586961865  - accuracy: 0.90625\n",
      "At: 692 [==========>] Loss 0.14391953743372524  - accuracy: 0.8125\n",
      "At: 693 [==========>] Loss 0.14007265152231757  - accuracy: 0.8125\n",
      "At: 694 [==========>] Loss 0.19878644038705828  - accuracy: 0.71875\n",
      "At: 695 [==========>] Loss 0.16185679250996765  - accuracy: 0.8125\n",
      "At: 696 [==========>] Loss 0.16399795147226487  - accuracy: 0.75\n",
      "At: 697 [==========>] Loss 0.19443768637951725  - accuracy: 0.6875\n",
      "At: 698 [==========>] Loss 0.11358344118984984  - accuracy: 0.84375\n",
      "At: 699 [==========>] Loss 0.1166658712684069  - accuracy: 0.875\n",
      "At: 700 [==========>] Loss 0.10195514810961764  - accuracy: 0.84375\n",
      "At: 701 [==========>] Loss 0.14780181404138698  - accuracy: 0.75\n",
      "At: 702 [==========>] Loss 0.09648128450751328  - accuracy: 0.875\n",
      "At: 703 [==========>] Loss 0.13616134481888723  - accuracy: 0.8125\n",
      "At: 704 [==========>] Loss 0.17889213613889962  - accuracy: 0.6875\n",
      "At: 705 [==========>] Loss 0.2008825554822618  - accuracy: 0.75\n",
      "At: 706 [==========>] Loss 0.15048438885161197  - accuracy: 0.75\n",
      "At: 707 [==========>] Loss 0.12495773662211311  - accuracy: 0.8125\n",
      "At: 708 [==========>] Loss 0.13843909819132516  - accuracy: 0.75\n",
      "At: 709 [==========>] Loss 0.20565089815671322  - accuracy: 0.71875\n",
      "At: 710 [==========>] Loss 0.1510059398017879  - accuracy: 0.78125\n",
      "At: 711 [==========>] Loss 0.1910583506261797  - accuracy: 0.71875\n",
      "At: 712 [==========>] Loss 0.14703548837431493  - accuracy: 0.8125\n",
      "At: 713 [==========>] Loss 0.14702953179900896  - accuracy: 0.75\n",
      "At: 714 [==========>] Loss 0.19299883579207308  - accuracy: 0.71875\n",
      "At: 715 [==========>] Loss 0.1259798353899371  - accuracy: 0.8125\n",
      "At: 716 [==========>] Loss 0.11380869410007435  - accuracy: 0.84375\n",
      "At: 717 [==========>] Loss 0.061075661314771976  - accuracy: 0.96875\n",
      "At: 718 [==========>] Loss 0.18760980543142566  - accuracy: 0.71875\n",
      "At: 719 [==========>] Loss 0.0988843801352589  - accuracy: 0.84375\n",
      "At: 720 [==========>] Loss 0.14019225638200808  - accuracy: 0.84375\n",
      "At: 721 [==========>] Loss 0.08646590901246638  - accuracy: 0.90625\n",
      "At: 722 [==========>] Loss 0.15109134436068672  - accuracy: 0.8125\n",
      "At: 723 [==========>] Loss 0.12314082199538048  - accuracy: 0.8125\n",
      "At: 724 [==========>] Loss 0.09083526452393662  - accuracy: 0.875\n",
      "At: 725 [==========>] Loss 0.14157484028044148  - accuracy: 0.8125\n",
      "At: 726 [==========>] Loss 0.1594295223107683  - accuracy: 0.78125\n",
      "At: 727 [==========>] Loss 0.12731697133450595  - accuracy: 0.84375\n",
      "At: 728 [==========>] Loss 0.17389662857733595  - accuracy: 0.75\n",
      "At: 729 [==========>] Loss 0.20772441452101986  - accuracy: 0.65625\n",
      "At: 730 [==========>] Loss 0.18075749130387322  - accuracy: 0.6875\n",
      "At: 731 [==========>] Loss 0.19040981656620523  - accuracy: 0.6875\n",
      "At: 732 [==========>] Loss 0.16348781999804143  - accuracy: 0.75\n",
      "At: 733 [==========>] Loss 0.11060374346900692  - accuracy: 0.875\n",
      "At: 734 [==========>] Loss 0.1608286834529239  - accuracy: 0.75\n",
      "At: 735 [==========>] Loss 0.10416065460832823  - accuracy: 0.8125\n",
      "At: 736 [==========>] Loss 0.11048645712153259  - accuracy: 0.8125\n",
      "At: 737 [==========>] Loss 0.14131403043979912  - accuracy: 0.75\n",
      "At: 738 [==========>] Loss 0.11319282028253291  - accuracy: 0.84375\n",
      "At: 739 [==========>] Loss 0.0790137392324104  - accuracy: 0.9375\n",
      "At: 740 [==========>] Loss 0.1391976360768017  - accuracy: 0.8125\n",
      "At: 741 [==========>] Loss 0.1369013064179964  - accuracy: 0.78125\n",
      "At: 742 [==========>] Loss 0.13261670326526787  - accuracy: 0.8125\n",
      "At: 743 [==========>] Loss 0.12760590594949187  - accuracy: 0.8125\n",
      "At: 744 [==========>] Loss 0.14947508048710947  - accuracy: 0.8125\n",
      "At: 745 [==========>] Loss 0.1767392094241499  - accuracy: 0.71875\n",
      "At: 746 [==========>] Loss 0.1500645253713364  - accuracy: 0.8125\n",
      "At: 747 [==========>] Loss 0.14431951393386466  - accuracy: 0.84375\n",
      "At: 748 [==========>] Loss 0.15179149332931444  - accuracy: 0.78125\n",
      "At: 749 [==========>] Loss 0.13907858747363508  - accuracy: 0.78125\n",
      "At: 750 [==========>] Loss 0.10056095983406133  - accuracy: 0.90625\n",
      "At: 751 [==========>] Loss 0.13307799091907885  - accuracy: 0.84375\n",
      "At: 752 [==========>] Loss 0.07880467506844993  - accuracy: 0.90625\n",
      "At: 753 [==========>] Loss 0.17947876290960982  - accuracy: 0.71875\n",
      "At: 754 [==========>] Loss 0.16695298590004354  - accuracy: 0.71875\n",
      "At: 755 [==========>] Loss 0.10075276405736552  - accuracy: 0.84375\n",
      "At: 756 [==========>] Loss 0.20024348924371738  - accuracy: 0.65625\n",
      "At: 757 [==========>] Loss 0.07583995604958266  - accuracy: 0.9375\n",
      "At: 758 [==========>] Loss 0.10430316868983373  - accuracy: 0.84375\n",
      "At: 759 [==========>] Loss 0.07067946690636742  - accuracy: 0.90625\n",
      "At: 760 [==========>] Loss 0.18950575044609946  - accuracy: 0.71875\n",
      "At: 761 [==========>] Loss 0.14358064618938424  - accuracy: 0.75\n",
      "At: 762 [==========>] Loss 0.14870503821490802  - accuracy: 0.78125\n",
      "At: 763 [==========>] Loss 0.12790939459899336  - accuracy: 0.84375\n",
      "At: 764 [==========>] Loss 0.1167873091718197  - accuracy: 0.84375\n",
      "At: 765 [==========>] Loss 0.12261719535383063  - accuracy: 0.84375\n",
      "At: 766 [==========>] Loss 0.10954149398661785  - accuracy: 0.84375\n",
      "At: 767 [==========>] Loss 0.12872140887435313  - accuracy: 0.8125\n",
      "At: 768 [==========>] Loss 0.15536983079601518  - accuracy: 0.78125\n",
      "At: 769 [==========>] Loss 0.11543055339326275  - accuracy: 0.78125\n",
      "At: 770 [==========>] Loss 0.10951042980159922  - accuracy: 0.875\n",
      "At: 771 [==========>] Loss 0.20648540662150855  - accuracy: 0.75\n",
      "At: 772 [==========>] Loss 0.12059336630955868  - accuracy: 0.875\n",
      "At: 773 [==========>] Loss 0.08293993425585405  - accuracy: 0.90625\n",
      "At: 774 [==========>] Loss 0.12661477259840137  - accuracy: 0.71875\n",
      "At: 775 [==========>] Loss 0.22527076967630258  - accuracy: 0.65625\n",
      "At: 776 [==========>] Loss 0.15707434729524491  - accuracy: 0.78125\n",
      "At: 777 [==========>] Loss 0.0797730313946875  - accuracy: 0.90625\n",
      "At: 778 [==========>] Loss 0.1716329612413265  - accuracy: 0.71875\n",
      "At: 779 [==========>] Loss 0.13325446452021586  - accuracy: 0.8125\n",
      "At: 780 [==========>] Loss 0.08797695938677462  - accuracy: 0.875\n",
      "At: 781 [==========>] Loss 0.1619563124716844  - accuracy: 0.78125\n",
      "At: 782 [==========>] Loss 0.1543973968322258  - accuracy: 0.78125\n",
      "At: 783 [==========>] Loss 0.1839294168454684  - accuracy: 0.78125\n",
      "At: 784 [==========>] Loss 0.1869129663424131  - accuracy: 0.625\n",
      "At: 785 [==========>] Loss 0.1943103229646589  - accuracy: 0.71875\n",
      "At: 786 [==========>] Loss 0.0880453301811602  - accuracy: 0.875\n",
      "At: 787 [==========>] Loss 0.1355059259337544  - accuracy: 0.84375\n",
      "At: 788 [==========>] Loss 0.07325284453789657  - accuracy: 0.9375\n",
      "At: 789 [==========>] Loss 0.14330954152557562  - accuracy: 0.78125\n",
      "At: 790 [==========>] Loss 0.1308258178092033  - accuracy: 0.8125\n",
      "At: 791 [==========>] Loss 0.16538084806461012  - accuracy: 0.8125\n",
      "At: 792 [==========>] Loss 0.17608408905772188  - accuracy: 0.6875\n",
      "At: 793 [==========>] Loss 0.11764654305773167  - accuracy: 0.875\n",
      "At: 794 [==========>] Loss 0.1514322560242461  - accuracy: 0.78125\n",
      "At: 795 [==========>] Loss 0.10310715551041577  - accuracy: 0.90625\n",
      "At: 796 [==========>] Loss 0.17591172959366722  - accuracy: 0.75\n",
      "At: 797 [==========>] Loss 0.15992095054329866  - accuracy: 0.71875\n",
      "At: 798 [==========>] Loss 0.14671064841827702  - accuracy: 0.8125\n",
      "At: 799 [==========>] Loss 0.07530745972288906  - accuracy: 0.90625\n",
      "At: 800 [==========>] Loss 0.17805410303557295  - accuracy: 0.71875\n",
      "At: 801 [==========>] Loss 0.11386466846398771  - accuracy: 0.84375\n",
      "At: 802 [==========>] Loss 0.16050958699091505  - accuracy: 0.71875\n",
      "At: 803 [==========>] Loss 0.1766682367970591  - accuracy: 0.75\n",
      "At: 804 [==========>] Loss 0.12482326068940243  - accuracy: 0.84375\n",
      "At: 805 [==========>] Loss 0.13171755604298974  - accuracy: 0.8125\n",
      "At: 806 [==========>] Loss 0.09259315850068747  - accuracy: 0.875\n",
      "At: 807 [==========>] Loss 0.10357512589141565  - accuracy: 0.84375\n",
      "At: 808 [==========>] Loss 0.16776230275706955  - accuracy: 0.75\n",
      "At: 809 [==========>] Loss 0.11477359688897443  - accuracy: 0.875\n",
      "At: 810 [==========>] Loss 0.15601947902116275  - accuracy: 0.78125\n",
      "At: 811 [==========>] Loss 0.11986765353735637  - accuracy: 0.875\n",
      "At: 812 [==========>] Loss 0.11114223272268607  - accuracy: 0.84375\n",
      "At: 813 [==========>] Loss 0.2116529369361284  - accuracy: 0.625\n",
      "At: 814 [==========>] Loss 0.1669525997349352  - accuracy: 0.78125\n",
      "At: 815 [==========>] Loss 0.1597163461000396  - accuracy: 0.78125\n",
      "At: 816 [==========>] Loss 0.159635768866988  - accuracy: 0.78125\n",
      "At: 817 [==========>] Loss 0.07961309031424653  - accuracy: 0.90625\n",
      "At: 818 [==========>] Loss 0.12858141699979017  - accuracy: 0.84375\n",
      "At: 819 [==========>] Loss 0.1210628315379716  - accuracy: 0.8125\n",
      "At: 820 [==========>] Loss 0.12365284501497512  - accuracy: 0.84375\n",
      "At: 821 [==========>] Loss 0.1405793113738533  - accuracy: 0.84375\n",
      "At: 822 [==========>] Loss 0.1356769303739771  - accuracy: 0.78125\n",
      "At: 823 [==========>] Loss 0.164157298586832  - accuracy: 0.75\n",
      "At: 824 [==========>] Loss 0.11225565149930447  - accuracy: 0.875\n",
      "At: 825 [==========>] Loss 0.19240710565058933  - accuracy: 0.71875\n",
      "At: 826 [==========>] Loss 0.10524617059608174  - accuracy: 0.8125\n",
      "At: 827 [==========>] Loss 0.08973262699303501  - accuracy: 0.90625\n",
      "At: 828 [==========>] Loss 0.06858511851052218  - accuracy: 0.90625\n",
      "At: 829 [==========>] Loss 0.09652665525661887  - accuracy: 0.875\n",
      "At: 830 [==========>] Loss 0.10940045003533769  - accuracy: 0.84375\n",
      "At: 831 [==========>] Loss 0.078504100411333  - accuracy: 0.90625\n",
      "At: 832 [==========>] Loss 0.11216415132576663  - accuracy: 0.875\n",
      "At: 833 [==========>] Loss 0.17267645284335956  - accuracy: 0.75\n",
      "At: 834 [==========>] Loss 0.1289703115577566  - accuracy: 0.8125\n",
      "At: 835 [==========>] Loss 0.08565579675259696  - accuracy: 0.90625\n",
      "At: 836 [==========>] Loss 0.13442158890745937  - accuracy: 0.84375\n",
      "At: 837 [==========>] Loss 0.10819231080339708  - accuracy: 0.84375\n",
      "At: 838 [==========>] Loss 0.09303968733040094  - accuracy: 0.875\n",
      "At: 839 [==========>] Loss 0.10243281188742072  - accuracy: 0.84375\n",
      "At: 840 [==========>] Loss 0.1270470749751878  - accuracy: 0.84375\n",
      "At: 841 [==========>] Loss 0.06333460821535587  - accuracy: 0.9375\n",
      "At: 842 [==========>] Loss 0.07364032819712246  - accuracy: 0.90625\n",
      "At: 843 [==========>] Loss 0.09921829489594383  - accuracy: 0.84375\n",
      "At: 844 [==========>] Loss 0.144867626444306  - accuracy: 0.78125\n",
      "At: 845 [==========>] Loss 0.1334415226007339  - accuracy: 0.84375\n",
      "At: 846 [==========>] Loss 0.1329811074907622  - accuracy: 0.84375\n",
      "At: 847 [==========>] Loss 0.07494648702278627  - accuracy: 0.875\n",
      "At: 848 [==========>] Loss 0.14189642554477871  - accuracy: 0.78125\n",
      "At: 849 [==========>] Loss 0.14651882061790594  - accuracy: 0.78125\n",
      "At: 850 [==========>] Loss 0.09527992968314902  - accuracy: 0.875\n",
      "At: 851 [==========>] Loss 0.10683136088396789  - accuracy: 0.84375\n",
      "At: 852 [==========>] Loss 0.11469966254821695  - accuracy: 0.84375\n",
      "At: 853 [==========>] Loss 0.14434721943425077  - accuracy: 0.78125\n",
      "At: 854 [==========>] Loss 0.1940690616896235  - accuracy: 0.71875\n",
      "At: 855 [==========>] Loss 0.09025098856625179  - accuracy: 0.90625\n",
      "At: 856 [==========>] Loss 0.08694217079377495  - accuracy: 0.9375\n",
      "At: 857 [==========>] Loss 0.08874119406731582  - accuracy: 0.90625\n",
      "At: 858 [==========>] Loss 0.23247173925556291  - accuracy: 0.59375\n",
      "At: 859 [==========>] Loss 0.11618307355753116  - accuracy: 0.8125\n",
      "At: 860 [==========>] Loss 0.0830423573649474  - accuracy: 0.9375\n",
      "At: 861 [==========>] Loss 0.08528159800581751  - accuracy: 0.875\n",
      "At: 862 [==========>] Loss 0.08310041587871175  - accuracy: 0.90625\n",
      "At: 863 [==========>] Loss 0.13941316333897752  - accuracy: 0.8125\n",
      "At: 864 [==========>] Loss 0.19873915100033412  - accuracy: 0.6875\n",
      "At: 865 [==========>] Loss 0.17792716087024862  - accuracy: 0.71875\n",
      "At: 866 [==========>] Loss 0.17278262903679617  - accuracy: 0.71875\n",
      "At: 867 [==========>] Loss 0.07949510022894511  - accuracy: 0.875\n",
      "At: 868 [==========>] Loss 0.16007829173011184  - accuracy: 0.75\n",
      "At: 869 [==========>] Loss 0.1559070806540327  - accuracy: 0.78125\n",
      "At: 870 [==========>] Loss 0.13838535234541702  - accuracy: 0.8125\n",
      "At: 871 [==========>] Loss 0.081717056879192  - accuracy: 0.90625\n",
      "At: 872 [==========>] Loss 0.10521621923259496  - accuracy: 0.875\n",
      "At: 873 [==========>] Loss 0.1586643045875348  - accuracy: 0.75\n",
      "At: 874 [==========>] Loss 0.16008867632275497  - accuracy: 0.78125\n",
      "At: 875 [==========>] Loss 0.10739018388005045  - accuracy: 0.875\n",
      "At: 876 [==========>] Loss 0.11771180814829187  - accuracy: 0.84375\n",
      "At: 877 [==========>] Loss 0.17957329846497458  - accuracy: 0.71875\n",
      "At: 878 [==========>] Loss 0.04464677237565366  - accuracy: 0.96875\n",
      "At: 879 [==========>] Loss 0.15081378107139143  - accuracy: 0.75\n",
      "At: 880 [==========>] Loss 0.12860983914086188  - accuracy: 0.84375\n",
      "At: 881 [==========>] Loss 0.18015592427688182  - accuracy: 0.78125\n",
      "At: 882 [==========>] Loss 0.11173621573511902  - accuracy: 0.8125\n",
      "At: 883 [==========>] Loss 0.13027206574302402  - accuracy: 0.78125\n",
      "At: 884 [==========>] Loss 0.13174795318783938  - accuracy: 0.78125\n",
      "At: 885 [==========>] Loss 0.10383121309342996  - accuracy: 0.84375\n",
      "At: 886 [==========>] Loss 0.10389928478731678  - accuracy: 0.84375\n",
      "At: 887 [==========>] Loss 0.15959803564631275  - accuracy: 0.71875\n",
      "At: 888 [==========>] Loss 0.14976132152334826  - accuracy: 0.8125\n",
      "At: 889 [==========>] Loss 0.10576941067464657  - accuracy: 0.84375\n",
      "At: 890 [==========>] Loss 0.13396547672535183  - accuracy: 0.8125\n",
      "At: 891 [==========>] Loss 0.08649096714111007  - accuracy: 0.90625\n",
      "At: 892 [==========>] Loss 0.12401857587831777  - accuracy: 0.78125\n",
      "At: 893 [==========>] Loss 0.15563176918027666  - accuracy: 0.78125\n",
      "At: 894 [==========>] Loss 0.1420876756152552  - accuracy: 0.78125\n",
      "At: 895 [==========>] Loss 0.10243496078884201  - accuracy: 0.875\n",
      "At: 896 [==========>] Loss 0.1175132831807241  - accuracy: 0.8125\n",
      "At: 897 [==========>] Loss 0.160963155082949  - accuracy: 0.78125\n",
      "At: 898 [==========>] Loss 0.10541638368651043  - accuracy: 0.90625\n",
      "At: 899 [==========>] Loss 0.09927612983032108  - accuracy: 0.84375\n",
      "At: 900 [==========>] Loss 0.14453401356395496  - accuracy: 0.8125\n",
      "At: 901 [==========>] Loss 0.14920500420795235  - accuracy: 0.8125\n",
      "At: 902 [==========>] Loss 0.12050234631790083  - accuracy: 0.8125\n",
      "At: 903 [==========>] Loss 0.13728956876356516  - accuracy: 0.8125\n",
      "At: 904 [==========>] Loss 0.09935160418810611  - accuracy: 0.875\n",
      "At: 905 [==========>] Loss 0.08274382743921868  - accuracy: 0.875\n",
      "At: 906 [==========>] Loss 0.09979930668429292  - accuracy: 0.90625\n",
      "At: 907 [==========>] Loss 0.15074618827087766  - accuracy: 0.8125\n",
      "At: 908 [==========>] Loss 0.10886475176146643  - accuracy: 0.875\n",
      "At: 909 [==========>] Loss 0.09851070926571917  - accuracy: 0.90625\n",
      "At: 910 [==========>] Loss 0.11802928739839388  - accuracy: 0.8125\n",
      "At: 911 [==========>] Loss 0.13449445597099324  - accuracy: 0.78125\n",
      "At: 912 [==========>] Loss 0.16502990706437243  - accuracy: 0.6875\n",
      "At: 913 [==========>] Loss 0.12684172820736928  - accuracy: 0.8125\n",
      "At: 914 [==========>] Loss 0.16701874616343354  - accuracy: 0.8125\n",
      "At: 915 [==========>] Loss 0.13789799226726135  - accuracy: 0.8125\n",
      "At: 916 [==========>] Loss 0.18739380733413008  - accuracy: 0.65625\n",
      "At: 917 [==========>] Loss 0.18058642568846728  - accuracy: 0.75\n",
      "At: 918 [==========>] Loss 0.2012723309064775  - accuracy: 0.6875\n",
      "At: 919 [==========>] Loss 0.1218772314242614  - accuracy: 0.8125\n",
      "At: 920 [==========>] Loss 0.11292445570542925  - accuracy: 0.84375\n",
      "At: 921 [==========>] Loss 0.15334454501127134  - accuracy: 0.78125\n",
      "At: 922 [==========>] Loss 0.1649232106790699  - accuracy: 0.8125\n",
      "At: 923 [==========>] Loss 0.10412011679062774  - accuracy: 0.90625\n",
      "At: 924 [==========>] Loss 0.1778082654400403  - accuracy: 0.71875\n",
      "At: 925 [==========>] Loss 0.1550438481145015  - accuracy: 0.8125\n",
      "At: 926 [==========>] Loss 0.11461643260543189  - accuracy: 0.84375\n",
      "At: 927 [==========>] Loss 0.1026127095323977  - accuracy: 0.8125\n",
      "At: 928 [==========>] Loss 0.12441719029596864  - accuracy: 0.84375\n",
      "At: 929 [==========>] Loss 0.1489223001086578  - accuracy: 0.78125\n",
      "At: 930 [==========>] Loss 0.11190833690451048  - accuracy: 0.8125\n",
      "At: 931 [==========>] Loss 0.1419096988584639  - accuracy: 0.78125\n",
      "At: 932 [==========>] Loss 0.08906031061525108  - accuracy: 0.90625\n",
      "At: 933 [==========>] Loss 0.08149638783215246  - accuracy: 0.875\n",
      "At: 934 [==========>] Loss 0.12390124272899489  - accuracy: 0.875\n",
      "At: 935 [==========>] Loss 0.06804942686860414  - accuracy: 0.9375\n",
      "At: 936 [==========>] Loss 0.12986830129964289  - accuracy: 0.78125\n",
      "At: 937 [==========>] Loss 0.16562066583271012  - accuracy: 0.78125\n",
      "At: 938 [==========>] Loss 0.09871867433752989  - accuracy: 0.9375\n",
      "At: 939 [==========>] Loss 0.09932169098745101  - accuracy: 0.875\n",
      "At: 940 [==========>] Loss 0.20288840107195538  - accuracy: 0.65625\n",
      "At: 941 [==========>] Loss 0.09702410012520533  - accuracy: 0.84375\n",
      "At: 942 [==========>] Loss 0.1600998960612141  - accuracy: 0.78125\n",
      "At: 943 [==========>] Loss 0.12031401458982574  - accuracy: 0.8125\n",
      "At: 944 [==========>] Loss 0.12463057806258866  - accuracy: 0.78125\n",
      "At: 945 [==========>] Loss 0.08234513766686855  - accuracy: 0.90625\n",
      "At: 946 [==========>] Loss 0.12042462200150025  - accuracy: 0.84375\n",
      "At: 947 [==========>] Loss 0.13446358695139754  - accuracy: 0.8125\n",
      "At: 948 [==========>] Loss 0.19261458846905552  - accuracy: 0.71875\n",
      "At: 949 [==========>] Loss 0.06679099172752812  - accuracy: 0.90625\n",
      "At: 950 [==========>] Loss 0.1218943080794114  - accuracy: 0.8125\n",
      "At: 951 [==========>] Loss 0.08602030264265706  - accuracy: 0.9375\n",
      "At: 952 [==========>] Loss 0.06248617050114409  - accuracy: 0.96875\n",
      "At: 953 [==========>] Loss 0.07632930867046299  - accuracy: 0.875\n",
      "At: 954 [==========>] Loss 0.09965813165013193  - accuracy: 0.84375\n",
      "At: 955 [==========>] Loss 0.13177771899350427  - accuracy: 0.8125\n",
      "At: 956 [==========>] Loss 0.09395714220986429  - accuracy: 0.875\n",
      "At: 957 [==========>] Loss 0.1531234187240968  - accuracy: 0.78125\n",
      "At: 958 [==========>] Loss 0.07506631774104126  - accuracy: 0.90625\n",
      "At: 959 [==========>] Loss 0.140558922921027  - accuracy: 0.8125\n",
      "At: 960 [==========>] Loss 0.10941742968318431  - accuracy: 0.875\n",
      "At: 961 [==========>] Loss 0.10991632457356267  - accuracy: 0.84375\n",
      "At: 962 [==========>] Loss 0.10786018796260277  - accuracy: 0.84375\n",
      "At: 963 [==========>] Loss 0.09349030301188549  - accuracy: 0.875\n",
      "At: 964 [==========>] Loss 0.15668655721350389  - accuracy: 0.75\n",
      "At: 965 [==========>] Loss 0.13411584222581396  - accuracy: 0.8125\n",
      "At: 966 [==========>] Loss 0.17985095441013838  - accuracy: 0.75\n",
      "At: 967 [==========>] Loss 0.09340127744648272  - accuracy: 0.875\n",
      "At: 968 [==========>] Loss 0.13661203100790592  - accuracy: 0.8125\n",
      "At: 969 [==========>] Loss 0.12656574953105124  - accuracy: 0.84375\n",
      "At: 970 [==========>] Loss 0.09618459834942736  - accuracy: 0.8125\n",
      "At: 971 [==========>] Loss 0.10977425320006431  - accuracy: 0.8125\n",
      "At: 972 [==========>] Loss 0.06621934232404003  - accuracy: 0.90625\n",
      "At: 973 [==========>] Loss 0.12370389564313036  - accuracy: 0.8125\n",
      "At: 974 [==========>] Loss 0.0847494251304215  - accuracy: 0.875\n",
      "At: 975 [==========>] Loss 0.11263093890931314  - accuracy: 0.8125\n",
      "At: 976 [==========>] Loss 0.09124151346564002  - accuracy: 0.875\n",
      "At: 977 [==========>] Loss 0.10798364610488115  - accuracy: 0.84375\n",
      "At: 978 [==========>] Loss 0.1711944012238261  - accuracy: 0.75\n",
      "At: 979 [==========>] Loss 0.09468616914636901  - accuracy: 0.875\n",
      "At: 980 [==========>] Loss 0.15216609949851473  - accuracy: 0.78125\n",
      "At: 981 [==========>] Loss 0.160656352908307  - accuracy: 0.75\n",
      "At: 982 [==========>] Loss 0.0544473922744121  - accuracy: 0.9375\n",
      "At: 983 [==========>] Loss 0.12160928624546333  - accuracy: 0.8125\n",
      "At: 984 [==========>] Loss 0.10539985987842304  - accuracy: 0.84375\n",
      "At: 985 [==========>] Loss 0.18573199335995672  - accuracy: 0.6875\n",
      "At: 986 [==========>] Loss 0.09180489114331661  - accuracy: 0.90625\n",
      "At: 987 [==========>] Loss 0.1548950739524791  - accuracy: 0.78125\n",
      "At: 988 [==========>] Loss 0.0920263437619369  - accuracy: 0.875\n",
      "At: 989 [==========>] Loss 0.12214240523430511  - accuracy: 0.84375\n",
      "At: 990 [==========>] Loss 0.1532693288232367  - accuracy: 0.71875\n",
      "At: 991 [==========>] Loss 0.13076882011445198  - accuracy: 0.84375\n",
      "At: 992 [==========>] Loss 0.20858465947356725  - accuracy: 0.71875\n",
      "At: 993 [==========>] Loss 0.14938671261099148  - accuracy: 0.78125\n",
      "At: 994 [==========>] Loss 0.1391543995333327  - accuracy: 0.84375\n",
      "At: 995 [==========>] Loss 0.14854469321132135  - accuracy: 0.78125\n",
      "At: 996 [==========>] Loss 0.08063269687355049  - accuracy: 0.875\n",
      "At: 997 [==========>] Loss 0.09701131713120928  - accuracy: 0.8125\n",
      "At: 998 [==========>] Loss 0.1031118136710833  - accuracy: 0.875\n",
      "At: 999 [==========>] Loss 0.11812877489600179  - accuracy: 0.8125\n",
      "At: 1000 [==========>] Loss 0.18303518256718626  - accuracy: 0.71875\n",
      "At: 1001 [==========>] Loss 0.16756824170661222  - accuracy: 0.75\n",
      "At: 1002 [==========>] Loss 0.21376223204702474  - accuracy: 0.6875\n",
      "At: 1003 [==========>] Loss 0.14716675696552037  - accuracy: 0.75\n",
      "At: 1004 [==========>] Loss 0.11294363795064224  - accuracy: 0.84375\n",
      "At: 1005 [==========>] Loss 0.10139578003124902  - accuracy: 0.875\n",
      "At: 1006 [==========>] Loss 0.07546301968360841  - accuracy: 0.90625\n",
      "At: 1007 [==========>] Loss 0.13364740623387672  - accuracy: 0.875\n",
      "At: 1008 [==========>] Loss 0.1533436835606741  - accuracy: 0.78125\n",
      "At: 1009 [==========>] Loss 0.12204551984758613  - accuracy: 0.8125\n",
      "At: 1010 [==========>] Loss 0.14070995174937456  - accuracy: 0.75\n",
      "At: 1011 [==========>] Loss 0.1455720996338825  - accuracy: 0.8125\n",
      "At: 1012 [==========>] Loss 0.0876394234644847  - accuracy: 0.84375\n",
      "At: 1013 [==========>] Loss 0.07976025778930224  - accuracy: 0.90625\n",
      "At: 1014 [==========>] Loss 0.08121021454679553  - accuracy: 0.90625\n",
      "At: 1015 [==========>] Loss 0.19445202935900124  - accuracy: 0.6875\n",
      "At: 1016 [==========>] Loss 0.12818419301890271  - accuracy: 0.84375\n",
      "At: 1017 [==========>] Loss 0.14169895223858925  - accuracy: 0.8125\n",
      "At: 1018 [==========>] Loss 0.16043182785581392  - accuracy: 0.78125\n",
      "At: 1019 [==========>] Loss 0.19907626142476176  - accuracy: 0.65625\n",
      "At: 1020 [==========>] Loss 0.1140819673890697  - accuracy: 0.78125\n",
      "At: 1021 [==========>] Loss 0.11875958411299038  - accuracy: 0.8125\n",
      "At: 1022 [==========>] Loss 0.09870598601632899  - accuracy: 0.90625\n",
      "At: 1023 [==========>] Loss 0.18090510870822912  - accuracy: 0.71875\n",
      "At: 1024 [==========>] Loss 0.20062769656645807  - accuracy: 0.71875\n",
      "At: 1025 [==========>] Loss 0.17370151850992138  - accuracy: 0.75\n",
      "At: 1026 [==========>] Loss 0.12792335217553324  - accuracy: 0.78125\n",
      "At: 1027 [==========>] Loss 0.13989056505162706  - accuracy: 0.8125\n",
      "At: 1028 [==========>] Loss 0.20810956164470748  - accuracy: 0.71875\n",
      "At: 1029 [==========>] Loss 0.1163926586592173  - accuracy: 0.8125\n",
      "At: 1030 [==========>] Loss 0.08291902333385635  - accuracy: 0.90625\n",
      "At: 1031 [==========>] Loss 0.15461508151822456  - accuracy: 0.75\n",
      "At: 1032 [==========>] Loss 0.12642374860695704  - accuracy: 0.84375\n",
      "At: 1033 [==========>] Loss 0.1263162428970102  - accuracy: 0.84375\n",
      "At: 1034 [==========>] Loss 0.07506686601425225  - accuracy: 0.90625\n",
      "At: 1035 [==========>] Loss 0.10158920981218567  - accuracy: 0.875\n",
      "At: 1036 [==========>] Loss 0.12769783034974752  - accuracy: 0.8125\n",
      "At: 1037 [==========>] Loss 0.13934990212636755  - accuracy: 0.8125\n",
      "At: 1038 [==========>] Loss 0.09596572175468969  - accuracy: 0.875\n",
      "At: 1039 [==========>] Loss 0.09558896174604532  - accuracy: 0.90625\n",
      "At: 1040 [==========>] Loss 0.09063486448016075  - accuracy: 0.875\n",
      "At: 1041 [==========>] Loss 0.16192727466439158  - accuracy: 0.78125\n",
      "At: 1042 [==========>] Loss 0.10648878094977934  - accuracy: 0.8125\n",
      "At: 1043 [==========>] Loss 0.15580302614684996  - accuracy: 0.75\n",
      "At: 1044 [==========>] Loss 0.13819329779690662  - accuracy: 0.8125\n",
      "At: 1045 [==========>] Loss 0.14708731804007863  - accuracy: 0.8125\n",
      "At: 1046 [==========>] Loss 0.14480963497305765  - accuracy: 0.75\n",
      "At: 1047 [==========>] Loss 0.13086468006351626  - accuracy: 0.84375\n",
      "At: 1048 [==========>] Loss 0.18200954314120524  - accuracy: 0.8125\n",
      "At: 1049 [==========>] Loss 0.15659363648070646  - accuracy: 0.84375\n",
      "At: 1050 [==========>] Loss 0.12669462985675217  - accuracy: 0.84375\n",
      "At: 1051 [==========>] Loss 0.1087383703194585  - accuracy: 0.875\n",
      "At: 1052 [==========>] Loss 0.1221932754923721  - accuracy: 0.8125\n",
      "At: 1053 [==========>] Loss 0.12405130635275238  - accuracy: 0.75\n",
      "At: 1054 [==========>] Loss 0.12456614678642816  - accuracy: 0.84375\n",
      "At: 1055 [==========>] Loss 0.1970354490528971  - accuracy: 0.6875\n",
      "At: 1056 [==========>] Loss 0.1445256881465579  - accuracy: 0.78125\n",
      "At: 1057 [==========>] Loss 0.14345614988800487  - accuracy: 0.8125\n",
      "At: 1058 [==========>] Loss 0.07361199415336142  - accuracy: 0.90625\n",
      "At: 1059 [==========>] Loss 0.08324520126500476  - accuracy: 0.875\n",
      "At: 1060 [==========>] Loss 0.07676310640991163  - accuracy: 0.90625\n",
      "At: 1061 [==========>] Loss 0.08768367916519368  - accuracy: 0.875\n",
      "At: 1062 [==========>] Loss 0.18956075249606197  - accuracy: 0.75\n",
      "At: 1063 [==========>] Loss 0.11139824823631908  - accuracy: 0.84375\n",
      "At: 1064 [==========>] Loss 0.12928206111631596  - accuracy: 0.8125\n",
      "At: 1065 [==========>] Loss 0.09520135487030415  - accuracy: 0.875\n",
      "At: 1066 [==========>] Loss 0.08419040001311207  - accuracy: 0.90625\n",
      "At: 1067 [==========>] Loss 0.11805480337032374  - accuracy: 0.875\n",
      "At: 1068 [==========>] Loss 0.13297316227573705  - accuracy: 0.8125\n",
      "At: 1069 [==========>] Loss 0.1294623914522869  - accuracy: 0.8125\n",
      "At: 1070 [==========>] Loss 0.134382282190052  - accuracy: 0.8125\n",
      "At: 1071 [==========>] Loss 0.12709605473932517  - accuracy: 0.8125\n",
      "At: 1072 [==========>] Loss 0.10200217580622065  - accuracy: 0.90625\n",
      "At: 1073 [==========>] Loss 0.15715112932384515  - accuracy: 0.75\n",
      "At: 1074 [==========>] Loss 0.17071907731728522  - accuracy: 0.6875\n",
      "At: 1075 [==========>] Loss 0.115758186487057  - accuracy: 0.84375\n",
      "At: 1076 [==========>] Loss 0.10620943242849523  - accuracy: 0.875\n",
      "At: 1077 [==========>] Loss 0.09547338595042906  - accuracy: 0.84375\n",
      "At: 1078 [==========>] Loss 0.09681446665299395  - accuracy: 0.875\n",
      "At: 1079 [==========>] Loss 0.10643865542112622  - accuracy: 0.875\n",
      "At: 1080 [==========>] Loss 0.1536197842899584  - accuracy: 0.71875\n",
      "At: 1081 [==========>] Loss 0.10121282228088031  - accuracy: 0.9375\n",
      "At: 1082 [==========>] Loss 0.08563937425007964  - accuracy: 0.90625\n",
      "At: 1083 [==========>] Loss 0.124052791067727  - accuracy: 0.8125\n",
      "At: 1084 [==========>] Loss 0.08773985697394027  - accuracy: 0.9375\n",
      "At: 1085 [==========>] Loss 0.12864978845227829  - accuracy: 0.8125\n",
      "At: 1086 [==========>] Loss 0.1096028934609243  - accuracy: 0.8125\n",
      "At: 1087 [==========>] Loss 0.08807115210635841  - accuracy: 0.875\n",
      "At: 1088 [==========>] Loss 0.17316567882447043  - accuracy: 0.75\n",
      "At: 1089 [==========>] Loss 0.06784064802855319  - accuracy: 0.9375\n",
      "At: 1090 [==========>] Loss 0.0879565264401459  - accuracy: 0.90625\n",
      "At: 1091 [==========>] Loss 0.1857552737648356  - accuracy: 0.65625\n",
      "At: 1092 [==========>] Loss 0.13143850662225753  - accuracy: 0.84375\n",
      "At: 1093 [==========>] Loss 0.14288179091928427  - accuracy: 0.84375\n",
      "At: 1094 [==========>] Loss 0.13151853345669473  - accuracy: 0.8125\n",
      "At: 1095 [==========>] Loss 0.11124420324659437  - accuracy: 0.84375\n",
      "At: 1096 [==========>] Loss 0.08195112942593137  - accuracy: 0.90625\n",
      "At: 1097 [==========>] Loss 0.05879599197060226  - accuracy: 0.9375\n",
      "At: 1098 [==========>] Loss 0.1757669001763366  - accuracy: 0.75\n",
      "At: 1099 [==========>] Loss 0.14565800718594105  - accuracy: 0.8125\n",
      "At: 1100 [==========>] Loss 0.09241781441225172  - accuracy: 0.875\n",
      "At: 1101 [==========>] Loss 0.10660125564688204  - accuracy: 0.875\n",
      "At: 1102 [==========>] Loss 0.10770232764032267  - accuracy: 0.84375\n",
      "At: 1103 [==========>] Loss 0.05899761778840783  - accuracy: 0.9375\n",
      "At: 1104 [==========>] Loss 0.07754426744207565  - accuracy: 0.90625\n",
      "At: 1105 [==========>] Loss 0.06774016954796247  - accuracy: 0.875\n",
      "At: 1106 [==========>] Loss 0.10109041562452728  - accuracy: 0.875\n",
      "At: 1107 [==========>] Loss 0.1916737449688813  - accuracy: 0.6875\n",
      "At: 1108 [==========>] Loss 0.09892007592753849  - accuracy: 0.875\n",
      "At: 1109 [==========>] Loss 0.07424017462233308  - accuracy: 0.9375\n",
      "At: 1110 [==========>] Loss 0.10152420378179552  - accuracy: 0.875\n",
      "At: 1111 [==========>] Loss 0.1853297522066257  - accuracy: 0.6875\n",
      "At: 1112 [==========>] Loss 0.12541096488877812  - accuracy: 0.78125\n",
      "At: 1113 [==========>] Loss 0.14281014348925186  - accuracy: 0.84375\n",
      "At: 1114 [==========>] Loss 0.05829635293483894  - accuracy: 0.9375\n",
      "At: 1115 [==========>] Loss 0.10663139370328995  - accuracy: 0.84375\n",
      "At: 1116 [==========>] Loss 0.11012519415804126  - accuracy: 0.875\n",
      "At: 1117 [==========>] Loss 0.06056575925323067  - accuracy: 0.9375\n",
      "At: 1118 [==========>] Loss 0.11130362867660584  - accuracy: 0.84375\n",
      "At: 1119 [==========>] Loss 0.16097758799724096  - accuracy: 0.75\n",
      "At: 1120 [==========>] Loss 0.08693665726766409  - accuracy: 0.875\n",
      "At: 1121 [==========>] Loss 0.11011221262189269  - accuracy: 0.8125\n",
      "At: 1122 [==========>] Loss 0.08155753163702527  - accuracy: 0.90625\n",
      "At: 1123 [==========>] Loss 0.1262197120871918  - accuracy: 0.84375\n",
      "At: 1124 [==========>] Loss 0.10919184119106903  - accuracy: 0.875\n",
      "At: 1125 [==========>] Loss 0.15173359719216464  - accuracy: 0.78125\n",
      "At: 1126 [==========>] Loss 0.0889148765859683  - accuracy: 0.84375\n",
      "At: 1127 [==========>] Loss 0.13361317107456647  - accuracy: 0.78125\n",
      "At: 1128 [==========>] Loss 0.07645796976738752  - accuracy: 0.90625\n",
      "At: 1129 [==========>] Loss 0.14813564092310322  - accuracy: 0.78125\n",
      "At: 1130 [==========>] Loss 0.10512811133106832  - accuracy: 0.90625\n",
      "At: 1131 [==========>] Loss 0.11295162231261907  - accuracy: 0.8125\n",
      "At: 1132 [==========>] Loss 0.10711767794024987  - accuracy: 0.84375\n",
      "At: 1133 [==========>] Loss 0.11338540325100985  - accuracy: 0.8125\n",
      "At: 1134 [==========>] Loss 0.08206887094636275  - accuracy: 0.875\n",
      "At: 1135 [==========>] Loss 0.08267634860282028  - accuracy: 0.875\n",
      "At: 1136 [==========>] Loss 0.15325068736966918  - accuracy: 0.84375\n",
      "At: 1137 [==========>] Loss 0.11729335494024459  - accuracy: 0.84375\n",
      "At: 1138 [==========>] Loss 0.09995796440745239  - accuracy: 0.84375\n",
      "At: 1139 [==========>] Loss 0.0894775487884688  - accuracy: 0.90625\n",
      "At: 1140 [==========>] Loss 0.16935793197050314  - accuracy: 0.8125\n",
      "At: 1141 [==========>] Loss 0.11754792161877348  - accuracy: 0.8125\n",
      "At: 1142 [==========>] Loss 0.13089552558323633  - accuracy: 0.8125\n",
      "At: 1143 [==========>] Loss 0.07361435015775422  - accuracy: 0.90625\n",
      "At: 1144 [==========>] Loss 0.12021846848583286  - accuracy: 0.8125\n",
      "At: 1145 [==========>] Loss 0.13401321188107787  - accuracy: 0.78125\n",
      "At: 1146 [==========>] Loss 0.14461012060081666  - accuracy: 0.78125\n",
      "At: 1147 [==========>] Loss 0.20892367028299072  - accuracy: 0.75\n",
      "At: 1148 [==========>] Loss 0.06855234654636072  - accuracy: 0.9375\n",
      "At: 1149 [==========>] Loss 0.1162067366137812  - accuracy: 0.84375\n",
      "At: 1150 [==========>] Loss 0.13058204534443912  - accuracy: 0.8125\n",
      "At: 1151 [==========>] Loss 0.17451287655056072  - accuracy: 0.71875\n",
      "At: 1152 [==========>] Loss 0.12538834622261968  - accuracy: 0.8125\n",
      "At: 1153 [==========>] Loss 0.17381622394875323  - accuracy: 0.78125\n",
      "At: 1154 [==========>] Loss 0.09780182689990265  - accuracy: 0.84375\n",
      "At: 1155 [==========>] Loss 0.10308987568332194  - accuracy: 0.84375\n",
      "At: 1156 [==========>] Loss 0.13541682336427102  - accuracy: 0.8125\n",
      "At: 1157 [==========>] Loss 0.11046599267370041  - accuracy: 0.84375\n",
      "At: 1158 [==========>] Loss 0.148345494385742  - accuracy: 0.78125\n",
      "At: 1159 [==========>] Loss 0.09747724482586015  - accuracy: 0.875\n",
      "At: 1160 [==========>] Loss 0.08258341359890517  - accuracy: 0.90625\n",
      "At: 1161 [==========>] Loss 0.08963640772100503  - accuracy: 0.90625\n",
      "At: 1162 [==========>] Loss 0.1260511554255921  - accuracy: 0.8125\n",
      "At: 1163 [==========>] Loss 0.15358171065637416  - accuracy: 0.75\n",
      "At: 1164 [==========>] Loss 0.09517551840913052  - accuracy: 0.84375\n",
      "At: 1165 [==========>] Loss 0.14573021663781766  - accuracy: 0.8125\n",
      "At: 1166 [==========>] Loss 0.07467792037108129  - accuracy: 0.90625\n",
      "At: 1167 [==========>] Loss 0.11728541995312544  - accuracy: 0.78125\n",
      "At: 1168 [==========>] Loss 0.1254091301154999  - accuracy: 0.8125\n",
      "At: 1169 [==========>] Loss 0.10455090304802109  - accuracy: 0.84375\n",
      "At: 1170 [==========>] Loss 0.1800500179545681  - accuracy: 0.71875\n",
      "At: 1171 [==========>] Loss 0.09793641022868886  - accuracy: 0.875\n",
      "At: 1172 [==========>] Loss 0.10051162857809343  - accuracy: 0.8125\n",
      "At: 1173 [==========>] Loss 0.11763917446061006  - accuracy: 0.8125\n",
      "At: 1174 [==========>] Loss 0.18734088685929517  - accuracy: 0.78125\n",
      "At: 1175 [==========>] Loss 0.10425514053710896  - accuracy: 0.875\n",
      "At: 1176 [==========>] Loss 0.11874207402345203  - accuracy: 0.875\n",
      "At: 1177 [==========>] Loss 0.07651449624043351  - accuracy: 0.875\n",
      "At: 1178 [==========>] Loss 0.1317416645813547  - accuracy: 0.8125\n",
      "At: 1179 [==========>] Loss 0.12680085763021826  - accuracy: 0.84375\n",
      "At: 1180 [==========>] Loss 0.1672498561362196  - accuracy: 0.75\n",
      "At: 1181 [==========>] Loss 0.07880617523474912  - accuracy: 0.875\n",
      "At: 1182 [==========>] Loss 0.10720760448424907  - accuracy: 0.875\n",
      "At: 1183 [==========>] Loss 0.1374525037446896  - accuracy: 0.84375\n",
      "At: 1184 [==========>] Loss 0.14202511943849297  - accuracy: 0.8125\n",
      "At: 1185 [==========>] Loss 0.07722669228613128  - accuracy: 0.875\n",
      "At: 1186 [==========>] Loss 0.15406020862652275  - accuracy: 0.75\n",
      "At: 1187 [==========>] Loss 0.10722179491192624  - accuracy: 0.8125\n",
      "At: 1188 [==========>] Loss 0.06485695270532793  - accuracy: 0.96875\n",
      "At: 1189 [==========>] Loss 0.13664593027263314  - accuracy: 0.8125\n",
      "At: 1190 [==========>] Loss 0.09008588548851515  - accuracy: 0.90625\n",
      "At: 1191 [==========>] Loss 0.17432480465885375  - accuracy: 0.71875\n",
      "At: 1192 [==========>] Loss 0.06603713856892895  - accuracy: 0.96875\n",
      "At: 1193 [==========>] Loss 0.10647107383986544  - accuracy: 0.84375\n",
      "At: 1194 [==========>] Loss 0.12855263390538788  - accuracy: 0.875\n",
      "At: 1195 [==========>] Loss 0.11837261269281696  - accuracy: 0.8125\n",
      "At: 1196 [==========>] Loss 0.11183858240841488  - accuracy: 0.8125\n",
      "At: 1197 [==========>] Loss 0.08948360526199031  - accuracy: 0.875\n",
      "At: 1198 [==========>] Loss 0.07711261827441432  - accuracy: 0.875\n",
      "At: 1199 [==========>] Loss 0.20637165434856952  - accuracy: 0.6875\n",
      "At: 1200 [==========>] Loss 0.06715044418343014  - accuracy: 0.90625\n",
      "At: 1201 [==========>] Loss 0.13480770089099114  - accuracy: 0.8125\n",
      "At: 1202 [==========>] Loss 0.16646361887871744  - accuracy: 0.75\n",
      "At: 1203 [==========>] Loss 0.14731844956660284  - accuracy: 0.75\n",
      "At: 1204 [==========>] Loss 0.08862514832859666  - accuracy: 0.90625\n",
      "At: 1205 [==========>] Loss 0.04925430291975215  - accuracy: 0.9375\n",
      "At: 1206 [==========>] Loss 0.12275405971236732  - accuracy: 0.8125\n",
      "At: 1207 [==========>] Loss 0.15244164115072462  - accuracy: 0.78125\n",
      "At: 1208 [==========>] Loss 0.10182509171627005  - accuracy: 0.875\n",
      "At: 1209 [==========>] Loss 0.11151466598473958  - accuracy: 0.8125\n",
      "At: 1210 [==========>] Loss 0.1171586932205361  - accuracy: 0.84375\n",
      "At: 1211 [==========>] Loss 0.1556168739043014  - accuracy: 0.8125\n",
      "At: 1212 [==========>] Loss 0.10376085549126023  - accuracy: 0.84375\n",
      "At: 1213 [==========>] Loss 0.16284842045318626  - accuracy: 0.75\n",
      "At: 1214 [==========>] Loss 0.1531014970462078  - accuracy: 0.78125\n",
      "At: 1215 [==========>] Loss 0.16200437214600433  - accuracy: 0.75\n",
      "At: 1216 [==========>] Loss 0.13866618074914183  - accuracy: 0.84375\n",
      "At: 1217 [==========>] Loss 0.06904553411953529  - accuracy: 0.90625\n",
      "At: 1218 [==========>] Loss 0.09276072348372918  - accuracy: 0.9375\n",
      "At: 1219 [==========>] Loss 0.11786291227309302  - accuracy: 0.84375\n",
      "At: 1220 [==========>] Loss 0.13271978259091965  - accuracy: 0.8125\n",
      "At: 1221 [==========>] Loss 0.11234225085703312  - accuracy: 0.875\n",
      "At: 1222 [==========>] Loss 0.1902574671078178  - accuracy: 0.6875\n",
      "At: 1223 [==========>] Loss 0.10830324269795413  - accuracy: 0.875\n",
      "At: 1224 [==========>] Loss 0.0766281095518524  - accuracy: 0.90625\n",
      "At: 1225 [==========>] Loss 0.058197550998518824  - accuracy: 0.96875\n",
      "At: 1226 [==========>] Loss 0.09442714572095436  - accuracy: 0.84375\n",
      "At: 1227 [==========>] Loss 0.14842268284529342  - accuracy: 0.78125\n",
      "At: 1228 [==========>] Loss 0.1354915801565407  - accuracy: 0.8125\n",
      "At: 1229 [==========>] Loss 0.1433076592947885  - accuracy: 0.78125\n",
      "At: 1230 [==========>] Loss 0.15203239486629275  - accuracy: 0.8125\n",
      "At: 1231 [==========>] Loss 0.12242395334503986  - accuracy: 0.8125\n",
      "At: 1232 [==========>] Loss 0.09584133976965627  - accuracy: 0.875\n",
      "At: 1233 [==========>] Loss 0.11525586389736706  - accuracy: 0.84375\n",
      "At: 1234 [==========>] Loss 0.14153531952906578  - accuracy: 0.71875\n",
      "At: 1235 [==========>] Loss 0.05952167536880464  - accuracy: 0.9375\n",
      "At: 1236 [==========>] Loss 0.17063075844975567  - accuracy: 0.78125\n",
      "At: 1237 [==========>] Loss 0.08780713428636262  - accuracy: 0.9375\n",
      "At: 1238 [==========>] Loss 0.10207773506335321  - accuracy: 0.84375\n",
      "At: 1239 [==========>] Loss 0.16695939728898737  - accuracy: 0.8125\n",
      "At: 1240 [==========>] Loss 0.1006373208693798  - accuracy: 0.875\n",
      "At: 1241 [==========>] Loss 0.1020976915014904  - accuracy: 0.84375\n",
      "At: 1242 [==========>] Loss 0.1336513752903375  - accuracy: 0.84375\n",
      "At: 1243 [==========>] Loss 0.12675682364527727  - accuracy: 0.875\n",
      "At: 1244 [==========>] Loss 0.12931670766457307  - accuracy: 0.78125\n",
      "At: 1245 [==========>] Loss 0.07746183655823172  - accuracy: 0.875\n",
      "At: 1246 [==========>] Loss 0.06148810645435279  - accuracy: 0.9375\n",
      "At: 1247 [==========>] Loss 0.161981762038043  - accuracy: 0.75\n",
      "At: 1248 [==========>] Loss 0.11983886103510591  - accuracy: 0.8125\n",
      "At: 1249 [==========>] Loss 0.15808455146282008  - accuracy: 0.78125\n",
      "At: 1250 [==========>] Loss 0.13240644913034244  - accuracy: 0.8125\n",
      "At: 1251 [==========>] Loss 0.12204756076557027  - accuracy: 0.84375\n",
      "At: 1252 [==========>] Loss 0.07648385266468978  - accuracy: 0.90625\n",
      "At: 1253 [==========>] Loss 0.0997328789459859  - accuracy: 0.875\n",
      "At: 1254 [==========>] Loss 0.1342239798597893  - accuracy: 0.75\n",
      "At: 1255 [==========>] Loss 0.08549200470794432  - accuracy: 0.90625\n",
      "At: 1256 [==========>] Loss 0.12927224094533324  - accuracy: 0.84375\n",
      "At: 1257 [==========>] Loss 0.1285309069005908  - accuracy: 0.8125\n",
      "At: 1258 [==========>] Loss 0.07647714744439796  - accuracy: 0.90625\n",
      "At: 1259 [==========>] Loss 0.14431442575576509  - accuracy: 0.78125\n",
      "At: 1260 [==========>] Loss 0.11239820199137683  - accuracy: 0.84375\n",
      "At: 1261 [==========>] Loss 0.1386505261734255  - accuracy: 0.78125\n",
      "At: 1262 [==========>] Loss 0.12303488760592063  - accuracy: 0.84375\n",
      "At: 1263 [==========>] Loss 0.10513388065772056  - accuracy: 0.90625\n",
      "At: 1264 [==========>] Loss 0.0921508103698902  - accuracy: 0.9375\n",
      "At: 1265 [==========>] Loss 0.12811223634324326  - accuracy: 0.84375\n",
      "At: 1266 [==========>] Loss 0.11519261002593133  - accuracy: 0.8125\n",
      "At: 1267 [==========>] Loss 0.12115554430691583  - accuracy: 0.8125\n",
      "At: 1268 [==========>] Loss 0.14755765970676812  - accuracy: 0.78125\n",
      "At: 1269 [==========>] Loss 0.12881021948996424  - accuracy: 0.84375\n",
      "At: 1270 [==========>] Loss 0.10040110995337252  - accuracy: 0.875\n",
      "At: 1271 [==========>] Loss 0.1666003222334386  - accuracy: 0.71875\n",
      "At: 1272 [==========>] Loss 0.0734503451490465  - accuracy: 0.90625\n",
      "At: 1273 [==========>] Loss 0.17184061231505107  - accuracy: 0.71875\n",
      "At: 1274 [==========>] Loss 0.13174845138818803  - accuracy: 0.78125\n",
      "At: 1275 [==========>] Loss 0.08470840495118243  - accuracy: 0.84375\n",
      "At: 1276 [==========>] Loss 0.08107491861446631  - accuracy: 0.9375\n",
      "At: 1277 [==========>] Loss 0.1023965413219325  - accuracy: 0.84375\n",
      "At: 1278 [==========>] Loss 0.13047370873734093  - accuracy: 0.8125\n",
      "At: 1279 [==========>] Loss 0.07982103361534555  - accuracy: 0.90625\n",
      "At: 1280 [==========>] Loss 0.11120278239258766  - accuracy: 0.84375\n",
      "At: 1281 [==========>] Loss 0.1442492512342497  - accuracy: 0.71875\n",
      "At: 1282 [==========>] Loss 0.11940464879837409  - accuracy: 0.8125\n",
      "At: 1283 [==========>] Loss 0.14594688114994822  - accuracy: 0.75\n",
      "At: 1284 [==========>] Loss 0.14061594838207814  - accuracy: 0.8125\n",
      "At: 1285 [==========>] Loss 0.07429208023974306  - accuracy: 0.875\n",
      "At: 1286 [==========>] Loss 0.11103897030798336  - accuracy: 0.78125\n",
      "At: 1287 [==========>] Loss 0.10400637952994014  - accuracy: 0.875\n",
      "At: 1288 [==========>] Loss 0.18423020393136713  - accuracy: 0.71875\n",
      "At: 1289 [==========>] Loss 0.12481711835892544  - accuracy: 0.78125\n",
      "At: 1290 [==========>] Loss 0.13963744608699508  - accuracy: 0.8125\n",
      "At: 1291 [==========>] Loss 0.141986429186951  - accuracy: 0.8125\n",
      "At: 1292 [==========>] Loss 0.1143500498023125  - accuracy: 0.8125\n",
      "At: 1293 [==========>] Loss 0.14312171989708006  - accuracy: 0.78125\n",
      "At: 1294 [==========>] Loss 0.14300418169617304  - accuracy: 0.78125\n",
      "At: 1295 [==========>] Loss 0.1565118724048111  - accuracy: 0.75\n",
      "At: 1296 [==========>] Loss 0.1845830153510891  - accuracy: 0.6875\n",
      "At: 1297 [==========>] Loss 0.1374540388414682  - accuracy: 0.78125\n",
      "At: 1298 [==========>] Loss 0.08820391046129172  - accuracy: 0.90625\n",
      "At: 1299 [==========>] Loss 0.1830886097343374  - accuracy: 0.71875\n",
      "At: 1300 [==========>] Loss 0.13700948801306337  - accuracy: 0.8125\n",
      "At: 1301 [==========>] Loss 0.12181725186281694  - accuracy: 0.84375\n",
      "At: 1302 [==========>] Loss 0.06941697315492801  - accuracy: 0.9375\n",
      "At: 1303 [==========>] Loss 0.08389123776729727  - accuracy: 0.875\n",
      "At: 1304 [==========>] Loss 0.1229756472819715  - accuracy: 0.8125\n",
      "At: 1305 [==========>] Loss 0.13865948926088623  - accuracy: 0.78125\n",
      "At: 1306 [==========>] Loss 0.0819828678000277  - accuracy: 0.90625\n",
      "At: 1307 [==========>] Loss 0.16455971846878342  - accuracy: 0.75\n",
      "At: 1308 [==========>] Loss 0.07928297414670034  - accuracy: 0.90625\n",
      "At: 1309 [==========>] Loss 0.1574316742654978  - accuracy: 0.78125\n",
      "At: 1310 [==========>] Loss 0.15156415888948394  - accuracy: 0.78125\n",
      "At: 1311 [==========>] Loss 0.16347257036007404  - accuracy: 0.71875\n",
      "At: 1312 [==========>] Loss 0.07497100723534117  - accuracy: 0.90625\n",
      "At: 1313 [==========>] Loss 0.1780830803647789  - accuracy: 0.71875\n",
      "At: 1314 [==========>] Loss 0.04727889266026997  - accuracy: 0.9375\n",
      "At: 1315 [==========>] Loss 0.15020240506405183  - accuracy: 0.8125\n",
      "At: 1316 [==========>] Loss 0.1142020409159213  - accuracy: 0.84375\n",
      "At: 1317 [==========>] Loss 0.08358996840453219  - accuracy: 0.90625\n",
      "At: 1318 [==========>] Loss 0.11303971304466762  - accuracy: 0.8125\n",
      "At: 1319 [==========>] Loss 0.11633531016492529  - accuracy: 0.84375\n",
      "At: 1320 [==========>] Loss 0.11287276286921827  - accuracy: 0.84375\n",
      "At: 1321 [==========>] Loss 0.06772848517143654  - accuracy: 0.96875\n",
      "At: 1322 [==========>] Loss 0.13728151423683738  - accuracy: 0.8125\n",
      "At: 1323 [==========>] Loss 0.0827963993984103  - accuracy: 0.90625\n",
      "At: 1324 [==========>] Loss 0.12817417888984012  - accuracy: 0.8125\n",
      "At: 1325 [==========>] Loss 0.08592719811783708  - accuracy: 0.90625\n",
      "At: 1326 [==========>] Loss 0.0845079024472074  - accuracy: 0.84375\n",
      "At: 1327 [==========>] Loss 0.1337907592780473  - accuracy: 0.78125\n",
      "At: 1328 [==========>] Loss 0.09875166630823262  - accuracy: 0.84375\n",
      "At: 1329 [==========>] Loss 0.06936290029432715  - accuracy: 0.875\n",
      "At: 1330 [==========>] Loss 0.12323658922291966  - accuracy: 0.84375\n",
      "At: 1331 [==========>] Loss 0.18191745505611354  - accuracy: 0.71875\n",
      "At: 1332 [==========>] Loss 0.13359202638320006  - accuracy: 0.78125\n",
      "At: 1333 [==========>] Loss 0.12663321057759036  - accuracy: 0.84375\n",
      "At: 1334 [==========>] Loss 0.11038914090745475  - accuracy: 0.875\n",
      "At: 1335 [==========>] Loss 0.12963653050529614  - accuracy: 0.78125\n",
      "At: 1336 [==========>] Loss 0.09183456626241784  - accuracy: 0.84375\n",
      "At: 1337 [==========>] Loss 0.1476744928151568  - accuracy: 0.84375\n",
      "At: 1338 [==========>] Loss 0.12757301632932325  - accuracy: 0.84375\n",
      "At: 1339 [==========>] Loss 0.10813456431701036  - accuracy: 0.84375\n",
      "At: 1340 [==========>] Loss 0.124190151002527  - accuracy: 0.84375\n",
      "At: 1341 [==========>] Loss 0.08852994621580237  - accuracy: 0.84375\n",
      "At: 1342 [==========>] Loss 0.12049290909107375  - accuracy: 0.84375\n",
      "At: 1343 [==========>] Loss 0.15661316160930738  - accuracy: 0.78125\n",
      "At: 1344 [==========>] Loss 0.1859189714973855  - accuracy: 0.71875\n",
      "At: 1345 [==========>] Loss 0.09626075618311156  - accuracy: 0.84375\n",
      "At: 1346 [==========>] Loss 0.11880473175596165  - accuracy: 0.8125\n",
      "At: 1347 [==========>] Loss 0.10570561937181411  - accuracy: 0.875\n",
      "At: 1348 [==========>] Loss 0.09786407383186396  - accuracy: 0.84375\n",
      "At: 1349 [==========>] Loss 0.14633626651745874  - accuracy: 0.78125\n",
      "At: 1350 [==========>] Loss 0.1194737490530823  - accuracy: 0.84375\n",
      "At: 1351 [==========>] Loss 0.08286912812098911  - accuracy: 0.84375\n",
      "At: 1352 [==========>] Loss 0.08803150789457168  - accuracy: 0.9375\n",
      "At: 1353 [==========>] Loss 0.16429663962552743  - accuracy: 0.71875\n",
      "At: 1354 [==========>] Loss 0.18181915250414699  - accuracy: 0.75\n",
      "At: 1355 [==========>] Loss 0.07352955797482068  - accuracy: 0.90625\n",
      "At: 1356 [==========>] Loss 0.120081071336603  - accuracy: 0.84375\n",
      "At: 1357 [==========>] Loss 0.10898255716565464  - accuracy: 0.84375\n",
      "At: 1358 [==========>] Loss 0.14356017094455334  - accuracy: 0.8125\n",
      "At: 1359 [==========>] Loss 0.07802687324251792  - accuracy: 0.8125\n",
      "At: 1360 [==========>] Loss 0.15277251737301628  - accuracy: 0.8125\n",
      "At: 1361 [==========>] Loss 0.11144057281578856  - accuracy: 0.8125\n",
      "At: 1362 [==========>] Loss 0.12988419572996082  - accuracy: 0.84375\n",
      "At: 1363 [==========>] Loss 0.0926267689566423  - accuracy: 0.875\n",
      "At: 1364 [==========>] Loss 0.15508521131954223  - accuracy: 0.75\n",
      "At: 1365 [==========>] Loss 0.11291224686183635  - accuracy: 0.875\n",
      "At: 1366 [==========>] Loss 0.1342486775825001  - accuracy: 0.8125\n",
      "At: 1367 [==========>] Loss 0.10108569789172905  - accuracy: 0.875\n",
      "At: 1368 [==========>] Loss 0.15919175969061147  - accuracy: 0.75\n",
      "At: 1369 [==========>] Loss 0.08951748757804495  - accuracy: 0.9375\n",
      "At: 1370 [==========>] Loss 0.11578917214085763  - accuracy: 0.84375\n",
      "At: 1371 [==========>] Loss 0.1670432240954558  - accuracy: 0.8125\n",
      "At: 1372 [==========>] Loss 0.11336375630751182  - accuracy: 0.8125\n",
      "At: 1373 [==========>] Loss 0.11872185023679216  - accuracy: 0.875\n",
      "At: 1374 [==========>] Loss 0.12682254241878813  - accuracy: 0.8125\n",
      "At: 1375 [==========>] Loss 0.13649048611729553  - accuracy: 0.75\n",
      "At: 1376 [==========>] Loss 0.111050690139367  - accuracy: 0.8125\n",
      "At: 1377 [==========>] Loss 0.17382170808948222  - accuracy: 0.71875\n",
      "At: 1378 [==========>] Loss 0.12804679982309203  - accuracy: 0.84375\n",
      "At: 1379 [==========>] Loss 0.13285555980520378  - accuracy: 0.84375\n",
      "At: 1380 [==========>] Loss 0.11832681371264642  - accuracy: 0.8125\n",
      "At: 1381 [==========>] Loss 0.09349759978789263  - accuracy: 0.875\n",
      "At: 1382 [==========>] Loss 0.12185072480435083  - accuracy: 0.875\n",
      "At: 1383 [==========>] Loss 0.110478703793949  - accuracy: 0.84375\n",
      "At: 1384 [==========>] Loss 0.12074881424567133  - accuracy: 0.78125\n",
      "At: 1385 [==========>] Loss 0.17420347895244243  - accuracy: 0.71875\n",
      "At: 1386 [==========>] Loss 0.17024506510682855  - accuracy: 0.71875\n",
      "At: 1387 [==========>] Loss 0.06591983795799573  - accuracy: 0.90625\n",
      "At: 1388 [==========>] Loss 0.14636412133880083  - accuracy: 0.84375\n",
      "At: 1389 [==========>] Loss 0.09218063867865349  - accuracy: 0.875\n",
      "At: 1390 [==========>] Loss 0.12756442626348624  - accuracy: 0.8125\n",
      "At: 1391 [==========>] Loss 0.11574959396272591  - accuracy: 0.8125\n",
      "At: 1392 [==========>] Loss 0.08936194107774798  - accuracy: 0.90625\n",
      "At: 1393 [==========>] Loss 0.12003365672582174  - accuracy: 0.8125\n",
      "At: 1394 [==========>] Loss 0.10759536369716174  - accuracy: 0.8125\n",
      "At: 1395 [==========>] Loss 0.21658788343111768  - accuracy: 0.59375\n",
      "At: 1396 [==========>] Loss 0.06544992462040178  - accuracy: 0.90625\n",
      "At: 1397 [==========>] Loss 0.1267639215721112  - accuracy: 0.84375\n",
      "At: 1398 [==========>] Loss 0.11928807280303332  - accuracy: 0.84375\n",
      "At: 1399 [==========>] Loss 0.0831858156750608  - accuracy: 0.90625\n",
      "At: 1400 [==========>] Loss 0.10880417074487211  - accuracy: 0.84375\n",
      "At: 1401 [==========>] Loss 0.07436325121101509  - accuracy: 0.90625\n",
      "At: 1402 [==========>] Loss 0.1395271240218411  - accuracy: 0.84375\n",
      "At: 1403 [==========>] Loss 0.13684693091505062  - accuracy: 0.75\n",
      "At: 1404 [==========>] Loss 0.08300998278508316  - accuracy: 0.875\n",
      "At: 1405 [==========>] Loss 0.08243461053092863  - accuracy: 0.90625\n",
      "At: 1406 [==========>] Loss 0.16517493585316376  - accuracy: 0.71875\n",
      "At: 1407 [==========>] Loss 0.10870276761432235  - accuracy: 0.84375\n",
      "At: 1408 [==========>] Loss 0.12531753285426106  - accuracy: 0.84375\n",
      "At: 1409 [==========>] Loss 0.05242896518533793  - accuracy: 0.9375\n",
      "At: 1410 [==========>] Loss 0.12465399930584889  - accuracy: 0.75\n",
      "At: 1411 [==========>] Loss 0.15059006506519423  - accuracy: 0.8125\n",
      "At: 1412 [==========>] Loss 0.1199566613213908  - accuracy: 0.84375\n",
      "At: 1413 [==========>] Loss 0.09135248125646739  - accuracy: 0.84375\n",
      "At: 1414 [==========>] Loss 0.15728819626177193  - accuracy: 0.78125\n",
      "At: 1415 [==========>] Loss 0.0902896725356904  - accuracy: 0.90625\n",
      "At: 1416 [==========>] Loss 0.14564089215642403  - accuracy: 0.8125\n",
      "At: 1417 [==========>] Loss 0.09385414530730513  - accuracy: 0.875\n",
      "At: 1418 [==========>] Loss 0.1381307742575669  - accuracy: 0.8125\n",
      "At: 1419 [==========>] Loss 0.13203522633785092  - accuracy: 0.78125\n",
      "At: 1420 [==========>] Loss 0.07948606642963404  - accuracy: 0.90625\n",
      "At: 1421 [==========>] Loss 0.09019257116425804  - accuracy: 0.90625\n",
      "At: 1422 [==========>] Loss 0.1311192823510533  - accuracy: 0.875\n",
      "At: 1423 [==========>] Loss 0.13154323362922699  - accuracy: 0.8125\n",
      "At: 1424 [==========>] Loss 0.14012686908495042  - accuracy: 0.84375\n",
      "At: 1425 [==========>] Loss 0.08757992011364492  - accuracy: 0.90625\n",
      "At: 1426 [==========>] Loss 0.12516922302689712  - accuracy: 0.84375\n",
      "At: 1427 [==========>] Loss 0.10276938877380393  - accuracy: 0.90625\n",
      "At: 1428 [==========>] Loss 0.11400654545846899  - accuracy: 0.875\n",
      "At: 1429 [==========>] Loss 0.15201138190934835  - accuracy: 0.75\n",
      "At: 1430 [==========>] Loss 0.08310897152214453  - accuracy: 0.875\n",
      "At: 1431 [==========>] Loss 0.11285354197294378  - accuracy: 0.8125\n",
      "At: 1432 [==========>] Loss 0.11816616102454221  - accuracy: 0.84375\n",
      "At: 1433 [==========>] Loss 0.15106054851878697  - accuracy: 0.78125\n",
      "At: 1434 [==========>] Loss 0.12878017360993282  - accuracy: 0.8125\n",
      "At: 1435 [==========>] Loss 0.1115355112350977  - accuracy: 0.875\n",
      "At: 1436 [==========>] Loss 0.07906638831255307  - accuracy: 0.90625\n",
      "At: 1437 [==========>] Loss 0.11029783215408163  - accuracy: 0.8125\n",
      "At: 1438 [==========>] Loss 0.151188896294243  - accuracy: 0.8125\n",
      "At: 1439 [==========>] Loss 0.1352770965670674  - accuracy: 0.8125\n",
      "At: 1440 [==========>] Loss 0.10523395819920839  - accuracy: 0.84375\n",
      "At: 1441 [==========>] Loss 0.08001249338437273  - accuracy: 0.90625\n",
      "At: 1442 [==========>] Loss 0.10257564015432713  - accuracy: 0.8125\n",
      "At: 1443 [==========>] Loss 0.13545156559647553  - accuracy: 0.78125\n",
      "At: 1444 [==========>] Loss 0.09767330324480283  - accuracy: 0.875\n",
      "At: 1445 [==========>] Loss 0.2114839538348255  - accuracy: 0.65625\n",
      "At: 1446 [==========>] Loss 0.18071721028660395  - accuracy: 0.6875\n",
      "At: 1447 [==========>] Loss 0.14567421765704724  - accuracy: 0.84375\n",
      "At: 1448 [==========>] Loss 0.08108282786088389  - accuracy: 0.84375\n",
      "At: 1449 [==========>] Loss 0.12148196702688882  - accuracy: 0.8125\n",
      "At: 1450 [==========>] Loss 0.13866842954723957  - accuracy: 0.78125\n",
      "At: 1451 [==========>] Loss 0.13093191951337957  - accuracy: 0.84375\n",
      "At: 1452 [==========>] Loss 0.11016796206932941  - accuracy: 0.875\n",
      "At: 1453 [==========>] Loss 0.05953036863837336  - accuracy: 0.9375\n",
      "At: 1454 [==========>] Loss 0.1475104262709429  - accuracy: 0.8125\n",
      "At: 1455 [==========>] Loss 0.09880465806775095  - accuracy: 0.9375\n",
      "At: 1456 [==========>] Loss 0.09641243531766204  - accuracy: 0.875\n",
      "At: 1457 [==========>] Loss 0.0817907702257876  - accuracy: 0.84375\n",
      "At: 1458 [==========>] Loss 0.1568376773650301  - accuracy: 0.8125\n",
      "At: 1459 [==========>] Loss 0.1125312585449961  - accuracy: 0.84375\n",
      "At: 1460 [==========>] Loss 0.16275685468903206  - accuracy: 0.78125\n",
      "At: 1461 [==========>] Loss 0.11141503641877513  - accuracy: 0.84375\n",
      "At: 1462 [==========>] Loss 0.15440972092546668  - accuracy: 0.75\n",
      "At: 1463 [==========>] Loss 0.09686636116934597  - accuracy: 0.875\n",
      "At: 1464 [==========>] Loss 0.16696317116764042  - accuracy: 0.71875\n",
      "At: 1465 [==========>] Loss 0.10701781558831719  - accuracy: 0.84375\n",
      "At: 1466 [==========>] Loss 0.09759988287902202  - accuracy: 0.875\n",
      "At: 1467 [==========>] Loss 0.17987746749347094  - accuracy: 0.71875\n",
      "At: 1468 [==========>] Loss 0.15350988445610822  - accuracy: 0.75\n",
      "At: 1469 [==========>] Loss 0.17058811885010627  - accuracy: 0.8125\n",
      "At: 1470 [==========>] Loss 0.1293292169204751  - accuracy: 0.78125\n",
      "At: 1471 [==========>] Loss 0.1333672528751815  - accuracy: 0.78125\n",
      "At: 1472 [==========>] Loss 0.06880855824302162  - accuracy: 0.9375\n",
      "At: 1473 [==========>] Loss 0.09296793525934516  - accuracy: 0.875\n",
      "At: 1474 [==========>] Loss 0.21607579237200586  - accuracy: 0.6875\n",
      "At: 1475 [==========>] Loss 0.14945984689367708  - accuracy: 0.78125\n",
      "At: 1476 [==========>] Loss 0.08491724612842076  - accuracy: 0.90625\n",
      "At: 1477 [==========>] Loss 0.07689155968654278  - accuracy: 0.9375\n",
      "At: 1478 [==========>] Loss 0.08282653384140669  - accuracy: 0.90625\n",
      "At: 1479 [==========>] Loss 0.12843025227041088  - accuracy: 0.84375\n",
      "At: 1480 [==========>] Loss 0.0910682407424333  - accuracy: 0.90625\n",
      "At: 1481 [==========>] Loss 0.13724110449996216  - accuracy: 0.78125\n",
      "At: 1482 [==========>] Loss 0.09632927209173547  - accuracy: 0.84375\n",
      "At: 1483 [==========>] Loss 0.19909979304284736  - accuracy: 0.75\n",
      "At: 1484 [==========>] Loss 0.12654713856665784  - accuracy: 0.78125\n",
      "At: 1485 [==========>] Loss 0.1340637493790683  - accuracy: 0.8125\n",
      "At: 1486 [==========>] Loss 0.08997307744311371  - accuracy: 0.875\n",
      "At: 1487 [==========>] Loss 0.06258798618782455  - accuracy: 0.90625\n",
      "At: 1488 [==========>] Loss 0.13191279986417073  - accuracy: 0.8125\n",
      "At: 1489 [==========>] Loss 0.1580965429903416  - accuracy: 0.75\n",
      "At: 1490 [==========>] Loss 0.10472911327492365  - accuracy: 0.84375\n",
      "At: 1491 [==========>] Loss 0.18686938925679444  - accuracy: 0.75\n",
      "At: 1492 [==========>] Loss 0.12844371889493295  - accuracy: 0.8125\n",
      "At: 1493 [==========>] Loss 0.18688035007033818  - accuracy: 0.6875\n",
      "At: 1494 [==========>] Loss 0.1368079772797576  - accuracy: 0.75\n",
      "At: 1495 [==========>] Loss 0.12553152367442344  - accuracy: 0.78125\n",
      "At: 1496 [==========>] Loss 0.11146237229631942  - accuracy: 0.84375\n",
      "At: 1497 [==========>] Loss 0.16487041616078316  - accuracy: 0.75\n",
      "At: 1498 [==========>] Loss 0.1340378415747417  - accuracy: 0.8125\n",
      "At: 1499 [==========>] Loss 0.11825414121372689  - accuracy: 0.875\n",
      "At: 1500 [==========>] Loss 0.1262212149185431  - accuracy: 0.8125\n",
      "At: 1501 [==========>] Loss 0.09904828894333545  - accuracy: 0.90625\n",
      "At: 1502 [==========>] Loss 0.1425664634155451  - accuracy: 0.78125\n",
      "At: 1503 [==========>] Loss 0.14578883850593216  - accuracy: 0.8125\n",
      "At: 1504 [==========>] Loss 0.11148403681139185  - accuracy: 0.90625\n",
      "At: 1505 [==========>] Loss 0.12188600381446806  - accuracy: 0.875\n",
      "At: 1506 [==========>] Loss 0.18099299787089737  - accuracy: 0.71875\n",
      "At: 1507 [==========>] Loss 0.12294719214327668  - accuracy: 0.84375\n",
      "At: 1508 [==========>] Loss 0.2120871866746492  - accuracy: 0.6875\n",
      "At: 1509 [==========>] Loss 0.10852621608606877  - accuracy: 0.84375\n",
      "At: 1510 [==========>] Loss 0.10169254194277091  - accuracy: 0.84375\n",
      "At: 1511 [==========>] Loss 0.10896093319963501  - accuracy: 0.875\n",
      "At: 1512 [==========>] Loss 0.06752476196444197  - accuracy: 0.90625\n",
      "At: 1513 [==========>] Loss 0.1574863226581295  - accuracy: 0.78125\n",
      "At: 1514 [==========>] Loss 0.18137532744871634  - accuracy: 0.71875\n",
      "At: 1515 [==========>] Loss 0.13569056511655223  - accuracy: 0.875\n",
      "At: 1516 [==========>] Loss 0.1238528759530281  - accuracy: 0.8125\n",
      "At: 1517 [==========>] Loss 0.12417723267978335  - accuracy: 0.8125\n",
      "At: 1518 [==========>] Loss 0.10653553454138548  - accuracy: 0.8125\n",
      "At: 1519 [==========>] Loss 0.1575527857015691  - accuracy: 0.75\n",
      "At: 1520 [==========>] Loss 0.10100704393529968  - accuracy: 0.84375\n",
      "At: 1521 [==========>] Loss 0.10272799942048015  - accuracy: 0.875\n",
      "At: 1522 [==========>] Loss 0.15952205942117686  - accuracy: 0.75\n",
      "At: 1523 [==========>] Loss 0.11377099454048445  - accuracy: 0.84375\n",
      "At: 1524 [==========>] Loss 0.1414797902174788  - accuracy: 0.75\n",
      "At: 1525 [==========>] Loss 0.13813128038733746  - accuracy: 0.8125\n",
      "At: 1526 [==========>] Loss 0.10692854414640483  - accuracy: 0.84375\n",
      "At: 1527 [==========>] Loss 0.15399094439444277  - accuracy: 0.8125\n",
      "At: 1528 [==========>] Loss 0.17111145185288668  - accuracy: 0.75\n",
      "At: 1529 [==========>] Loss 0.07698040221644131  - accuracy: 0.90625\n",
      "At: 1530 [==========>] Loss 0.06726378599568589  - accuracy: 0.96875\n",
      "At: 1531 [==========>] Loss 0.12029643744658922  - accuracy: 0.8125\n",
      "At: 1532 [==========>] Loss 0.17051684423840477  - accuracy: 0.75\n",
      "At: 1533 [==========>] Loss 0.15312317794183103  - accuracy: 0.78125\n",
      "At: 1534 [==========>] Loss 0.11491923860110784  - accuracy: 0.84375\n",
      "At: 1535 [==========>] Loss 0.1270917591053686  - accuracy: 0.84375\n",
      "At: 1536 [==========>] Loss 0.13037273825959567  - accuracy: 0.8125\n",
      "At: 1537 [==========>] Loss 0.11656990251035934  - accuracy: 0.8125\n",
      "At: 1538 [==========>] Loss 0.13285187695162898  - accuracy: 0.8125\n",
      "At: 1539 [==========>] Loss 0.07788562273856627  - accuracy: 0.9375\n",
      "At: 1540 [==========>] Loss 0.1685567507302388  - accuracy: 0.71875\n",
      "At: 1541 [==========>] Loss 0.10064575852221114  - accuracy: 0.90625\n",
      "At: 1542 [==========>] Loss 0.08350266996420305  - accuracy: 0.875\n",
      "At: 1543 [==========>] Loss 0.13373187897998295  - accuracy: 0.8125\n",
      "At: 1544 [==========>] Loss 0.1315621284465986  - accuracy: 0.78125\n",
      "At: 1545 [==========>] Loss 0.17153707897470225  - accuracy: 0.71875\n",
      "At: 1546 [==========>] Loss 0.12741898779032643  - accuracy: 0.75\n",
      "At: 1547 [==========>] Loss 0.1535868425451439  - accuracy: 0.78125\n",
      "At: 1548 [==========>] Loss 0.11705399274051256  - accuracy: 0.90625\n",
      "At: 1549 [==========>] Loss 0.1464756877243122  - accuracy: 0.8125\n",
      "At: 1550 [==========>] Loss 0.05976266135443778  - accuracy: 0.9375\n",
      "At: 1551 [==========>] Loss 0.147147565419695  - accuracy: 0.78125\n",
      "At: 1552 [==========>] Loss 0.11620878980833335  - accuracy: 0.875\n",
      "At: 1553 [==========>] Loss 0.07786681127771719  - accuracy: 0.90625\n",
      "At: 1554 [==========>] Loss 0.12054173468985677  - accuracy: 0.84375\n",
      "At: 1555 [==========>] Loss 0.11060976765181783  - accuracy: 0.84375\n",
      "At: 1556 [==========>] Loss 0.169338937106624  - accuracy: 0.78125\n",
      "At: 1557 [==========>] Loss 0.0838289397099206  - accuracy: 0.875\n",
      "At: 1558 [==========>] Loss 0.13263159334889685  - accuracy: 0.84375\n",
      "At: 1559 [==========>] Loss 0.10091178618247755  - accuracy: 0.84375\n",
      "At: 1560 [==========>] Loss 0.11582559164305435  - accuracy: 0.8125\n",
      "At: 1561 [==========>] Loss 0.15084579907478393  - accuracy: 0.84375\n",
      "At: 1562 [==========>] Loss 0.07984700505179426  - accuracy: 0.875\n",
      "At: 1563 [==========>] Loss 0.10335963430196105  - accuracy: 0.9375\n",
      "At: 1564 [==========>] Loss 0.10498854663631599  - accuracy: 0.875\n",
      "At: 1565 [==========>] Loss 0.1394100255769818  - accuracy: 0.71875\n",
      "At: 1566 [==========>] Loss 0.15140363199252718  - accuracy: 0.8125\n",
      "At: 1567 [==========>] Loss 0.1496139181119013  - accuracy: 0.75\n",
      "At: 1568 [==========>] Loss 0.07237257562914859  - accuracy: 0.9375\n",
      "At: 1569 [==========>] Loss 0.09895977250579123  - accuracy: 0.875\n",
      "At: 1570 [==========>] Loss 0.1024725415043123  - accuracy: 0.875\n",
      "At: 1571 [==========>] Loss 0.1283487095045087  - accuracy: 0.8125\n",
      "At: 1572 [==========>] Loss 0.14071462816182453  - accuracy: 0.84375\n",
      "At: 1573 [==========>] Loss 0.06401636650842764  - accuracy: 0.90625\n",
      "At: 1574 [==========>] Loss 0.11738460681763892  - accuracy: 0.8125\n",
      "At: 1575 [==========>] Loss 0.09675711024945935  - accuracy: 0.875\n",
      "At: 1576 [==========>] Loss 0.13999764980376106  - accuracy: 0.84375\n",
      "At: 1577 [==========>] Loss 0.0865688067034133  - accuracy: 0.90625\n",
      "At: 1578 [==========>] Loss 0.09418661012132151  - accuracy: 0.875\n",
      "At: 1579 [==========>] Loss 0.08526759009042556  - accuracy: 0.90625\n",
      "At: 1580 [==========>] Loss 0.09692983177577091  - accuracy: 0.875\n",
      "At: 1581 [==========>] Loss 0.10937002493493235  - accuracy: 0.84375\n",
      "At: 1582 [==========>] Loss 0.16161567166037766  - accuracy: 0.75\n",
      "At: 1583 [==========>] Loss 0.10241781324932774  - accuracy: 0.8125\n",
      "At: 1584 [==========>] Loss 0.09944276189882212  - accuracy: 0.90625\n",
      "At: 1585 [==========>] Loss 0.08922088862223915  - accuracy: 0.84375\n",
      "At: 1586 [==========>] Loss 0.11498547558057731  - accuracy: 0.90625\n",
      "At: 1587 [==========>] Loss 0.08851965153207861  - accuracy: 0.84375\n",
      "At: 1588 [==========>] Loss 0.13371202001644  - accuracy: 0.8125\n",
      "At: 1589 [==========>] Loss 0.1605172568092711  - accuracy: 0.71875\n",
      "At: 1590 [==========>] Loss 0.12074880344045266  - accuracy: 0.84375\n",
      "At: 1591 [==========>] Loss 0.10893711964093122  - accuracy: 0.84375\n",
      "At: 1592 [==========>] Loss 0.06811547166810267  - accuracy: 0.96875\n",
      "At: 1593 [==========>] Loss 0.17351971927781243  - accuracy: 0.78125\n",
      "At: 1594 [==========>] Loss 0.08541939833774524  - accuracy: 0.875\n",
      "At: 1595 [==========>] Loss 0.13957714820744604  - accuracy: 0.78125\n",
      "At: 1596 [==========>] Loss 0.1765567826153968  - accuracy: 0.75\n",
      "At: 1597 [==========>] Loss 0.13811534610402332  - accuracy: 0.8125\n",
      "At: 1598 [==========>] Loss 0.16251535706563938  - accuracy: 0.75\n",
      "At: 1599 [==========>] Loss 0.20340334136314434  - accuracy: 0.71875\n",
      "At: 1600 [==========>] Loss 0.14867907738907954  - accuracy: 0.8125\n",
      "At: 1601 [==========>] Loss 0.08560078925490935  - accuracy: 0.90625\n",
      "At: 1602 [==========>] Loss 0.1076172157917767  - accuracy: 0.875\n",
      "At: 1603 [==========>] Loss 0.18015123257518675  - accuracy: 0.71875\n",
      "At: 1604 [==========>] Loss 0.22469576259135063  - accuracy: 0.65625\n",
      "At: 1605 [==========>] Loss 0.0845621865881679  - accuracy: 0.875\n",
      "At: 1606 [==========>] Loss 0.10338973831476238  - accuracy: 0.875\n",
      "At: 1607 [==========>] Loss 0.1790889493244575  - accuracy: 0.65625\n",
      "At: 1608 [==========>] Loss 0.16857708001906216  - accuracy: 0.6875\n",
      "At: 1609 [==========>] Loss 0.13617976545874308  - accuracy: 0.78125\n",
      "At: 1610 [==========>] Loss 0.15414187896129952  - accuracy: 0.78125\n",
      "At: 1611 [==========>] Loss 0.10337811665837365  - accuracy: 0.84375\n",
      "At: 1612 [==========>] Loss 0.08919393543051732  - accuracy: 0.84375\n",
      "At: 1613 [==========>] Loss 0.11341891413116886  - accuracy: 0.8125\n",
      "At: 1614 [==========>] Loss 0.13780918631200867  - accuracy: 0.78125\n",
      "At: 1615 [==========>] Loss 0.09277647187864038  - accuracy: 0.875\n",
      "At: 1616 [==========>] Loss 0.13054028359027214  - accuracy: 0.78125\n",
      "At: 1617 [==========>] Loss 0.08682593452473716  - accuracy: 0.875\n",
      "At: 1618 [==========>] Loss 0.11922269241701293  - accuracy: 0.84375\n",
      "At: 1619 [==========>] Loss 0.16958354475653434  - accuracy: 0.6875\n",
      "At: 1620 [==========>] Loss 0.11800462118157962  - accuracy: 0.8125\n",
      "At: 1621 [==========>] Loss 0.13835042019405575  - accuracy: 0.8125\n",
      "At: 1622 [==========>] Loss 0.14767855157432497  - accuracy: 0.78125\n",
      "At: 1623 [==========>] Loss 0.07843788798164952  - accuracy: 0.90625\n",
      "At: 1624 [==========>] Loss 0.15503623579079912  - accuracy: 0.8125\n",
      "At: 1625 [==========>] Loss 0.14824883956927337  - accuracy: 0.8125\n",
      "At: 1626 [==========>] Loss 0.1172056942985283  - accuracy: 0.84375\n",
      "At: 1627 [==========>] Loss 0.07636650245696118  - accuracy: 0.9375\n",
      "At: 1628 [==========>] Loss 0.14061933880061628  - accuracy: 0.78125\n",
      "At: 1629 [==========>] Loss 0.09823152672807597  - accuracy: 0.84375\n",
      "At: 1630 [==========>] Loss 0.08985708890852798  - accuracy: 0.875\n",
      "At: 1631 [==========>] Loss 0.11034286819894809  - accuracy: 0.875\n",
      "At: 1632 [==========>] Loss 0.10207719318570524  - accuracy: 0.90625\n",
      "At: 1633 [==========>] Loss 0.08536113289763596  - accuracy: 0.90625\n",
      "At: 1634 [==========>] Loss 0.10504941479099456  - accuracy: 0.84375\n",
      "At: 1635 [==========>] Loss 0.1870380301212568  - accuracy: 0.75\n",
      "At: 1636 [==========>] Loss 0.11214579642545147  - accuracy: 0.875\n",
      "At: 1637 [==========>] Loss 0.08056697746220012  - accuracy: 0.90625\n",
      "At: 1638 [==========>] Loss 0.15430791602567284  - accuracy: 0.78125\n",
      "At: 1639 [==========>] Loss 0.1809196770761772  - accuracy: 0.6875\n",
      "At: 1640 [==========>] Loss 0.12966822341228815  - accuracy: 0.8125\n",
      "At: 1641 [==========>] Loss 0.08812988920293202  - accuracy: 0.875\n",
      "At: 1642 [==========>] Loss 0.11375877992845015  - accuracy: 0.84375\n",
      "At: 1643 [==========>] Loss 0.09873287534089634  - accuracy: 0.875\n",
      "At: 1644 [==========>] Loss 0.11064484132769992  - accuracy: 0.84375\n",
      "At: 1645 [==========>] Loss 0.07211315659764692  - accuracy: 0.875\n",
      "At: 1646 [==========>] Loss 0.12536045713281116  - accuracy: 0.78125\n",
      "At: 1647 [==========>] Loss 0.17086558660264095  - accuracy: 0.78125\n",
      "At: 1648 [==========>] Loss 0.12208722912747076  - accuracy: 0.84375\n",
      "At: 1649 [==========>] Loss 0.07463854474781378  - accuracy: 0.9375\n",
      "At: 1650 [==========>] Loss 0.07321862999541943  - accuracy: 0.9375\n",
      "At: 1651 [==========>] Loss 0.12419918643542416  - accuracy: 0.875\n",
      "At: 1652 [==========>] Loss 0.0710418430158704  - accuracy: 0.90625\n",
      "At: 1653 [==========>] Loss 0.09432702675488365  - accuracy: 0.90625\n",
      "At: 1654 [==========>] Loss 0.07008333192786989  - accuracy: 0.875\n",
      "At: 1655 [==========>] Loss 0.14130183185702097  - accuracy: 0.84375\n",
      "At: 1656 [==========>] Loss 0.12470624800826402  - accuracy: 0.84375\n",
      "At: 1657 [==========>] Loss 0.14611575490110318  - accuracy: 0.78125\n",
      "At: 1658 [==========>] Loss 0.19060705385616422  - accuracy: 0.6875\n",
      "At: 1659 [==========>] Loss 0.08507856407171531  - accuracy: 0.875\n",
      "At: 1660 [==========>] Loss 0.045292615647213985  - accuracy: 1.0\n",
      "At: 1661 [==========>] Loss 0.07532800716707147  - accuracy: 0.90625\n",
      "At: 1662 [==========>] Loss 0.10391106223683413  - accuracy: 0.875\n",
      "At: 1663 [==========>] Loss 0.16491982580378814  - accuracy: 0.75\n",
      "At: 1664 [==========>] Loss 0.11070661437827167  - accuracy: 0.84375\n",
      "At: 1665 [==========>] Loss 0.10530325427438458  - accuracy: 0.84375\n",
      "At: 1666 [==========>] Loss 0.10344408027377716  - accuracy: 0.84375\n",
      "At: 1667 [==========>] Loss 0.1646053124909105  - accuracy: 0.75\n",
      "At: 1668 [==========>] Loss 0.15240097371287817  - accuracy: 0.71875\n",
      "At: 1669 [==========>] Loss 0.0996816039114792  - accuracy: 0.84375\n",
      "At: 1670 [==========>] Loss 0.05495697137608683  - accuracy: 0.90625\n",
      "At: 1671 [==========>] Loss 0.11043457469545695  - accuracy: 0.875\n",
      "At: 1672 [==========>] Loss 0.12536252330284806  - accuracy: 0.78125\n",
      "At: 1673 [==========>] Loss 0.07812265082099483  - accuracy: 0.90625\n",
      "At: 1674 [==========>] Loss 0.1759320289103225  - accuracy: 0.71875\n",
      "At: 1675 [==========>] Loss 0.18442319410849736  - accuracy: 0.71875\n",
      "At: 1676 [==========>] Loss 0.21181315530354183  - accuracy: 0.6875\n",
      "At: 1677 [==========>] Loss 0.0991701940363068  - accuracy: 0.84375\n",
      "At: 1678 [==========>] Loss 0.17443452027240197  - accuracy: 0.78125\n",
      "At: 1679 [==========>] Loss 0.12428874098326896  - accuracy: 0.84375\n",
      "At: 1680 [==========>] Loss 0.14855182475834203  - accuracy: 0.75\n",
      "At: 1681 [==========>] Loss 0.13189877257011884  - accuracy: 0.875\n",
      "At: 1682 [==========>] Loss 0.10583105536353057  - accuracy: 0.875\n",
      "At: 1683 [==========>] Loss 0.2051291463110108  - accuracy: 0.75\n",
      "At: 1684 [==========>] Loss 0.11668125707173099  - accuracy: 0.8125\n",
      "At: 1685 [==========>] Loss 0.09586610247686864  - accuracy: 0.90625\n",
      "At: 1686 [==========>] Loss 0.11431497245205804  - accuracy: 0.8125\n",
      "At: 1687 [==========>] Loss 0.165572319401652  - accuracy: 0.75\n",
      "At: 1688 [==========>] Loss 0.041464607874895594  - accuracy: 0.96875\n",
      "At: 1689 [==========>] Loss 0.12370732513461609  - accuracy: 0.84375\n",
      "At: 1690 [==========>] Loss 0.10145530608100134  - accuracy: 0.875\n",
      "At: 1691 [==========>] Loss 0.11345066468882734  - accuracy: 0.8125\n",
      "At: 1692 [==========>] Loss 0.18444313227863185  - accuracy: 0.78125\n",
      "At: 1693 [==========>] Loss 0.1066151608132192  - accuracy: 0.90625\n",
      "At: 1694 [==========>] Loss 0.08380576219418599  - accuracy: 0.875\n",
      "At: 1695 [==========>] Loss 0.10970974624985312  - accuracy: 0.84375\n",
      "At: 1696 [==========>] Loss 0.15620447857476633  - accuracy: 0.78125\n",
      "At: 1697 [==========>] Loss 0.10600529196278802  - accuracy: 0.875\n",
      "At: 1698 [==========>] Loss 0.06714572191757842  - accuracy: 0.90625\n",
      "At: 1699 [==========>] Loss 0.10223465254095648  - accuracy: 0.875\n",
      "At: 1700 [==========>] Loss 0.1191989062449355  - accuracy: 0.8125\n",
      "At: 1701 [==========>] Loss 0.10548709279041424  - accuracy: 0.84375\n",
      "At: 1702 [==========>] Loss 0.105179601110967  - accuracy: 0.84375\n",
      "At: 1703 [==========>] Loss 0.16169275953717627  - accuracy: 0.75\n",
      "At: 1704 [==========>] Loss 0.08131535414711064  - accuracy: 0.90625\n",
      "At: 1705 [==========>] Loss 0.11030551980347404  - accuracy: 0.875\n",
      "At: 1706 [==========>] Loss 0.16357836738672513  - accuracy: 0.78125\n",
      "At: 1707 [==========>] Loss 0.18151541675197624  - accuracy: 0.78125\n",
      "At: 1708 [==========>] Loss 0.09991144635777947  - accuracy: 0.90625\n",
      "At: 1709 [==========>] Loss 0.15524912703699822  - accuracy: 0.84375\n",
      "At: 1710 [==========>] Loss 0.16139008794191131  - accuracy: 0.8125\n",
      "At: 1711 [==========>] Loss 0.0846214323272127  - accuracy: 0.90625\n",
      "At: 1712 [==========>] Loss 0.09942701566184048  - accuracy: 0.84375\n",
      "At: 1713 [==========>] Loss 0.0854325899731686  - accuracy: 0.84375\n",
      "At: 1714 [==========>] Loss 0.15161741833199752  - accuracy: 0.8125\n",
      "At: 1715 [==========>] Loss 0.09297386769154453  - accuracy: 0.875\n",
      "At: 1716 [==========>] Loss 0.04257047347123698  - accuracy: 0.96875\n",
      "At: 1717 [==========>] Loss 0.07648648569083841  - accuracy: 0.875\n",
      "At: 1718 [==========>] Loss 0.12374895578161704  - accuracy: 0.875\n",
      "At: 1719 [==========>] Loss 0.10242104784321679  - accuracy: 0.875\n",
      "At: 1720 [==========>] Loss 0.07253368931996747  - accuracy: 0.9375\n",
      "At: 1721 [==========>] Loss 0.1410348600107283  - accuracy: 0.84375\n",
      "At: 1722 [==========>] Loss 0.08160056077673043  - accuracy: 0.90625\n",
      "At: 1723 [==========>] Loss 0.1736481042392425  - accuracy: 0.65625\n",
      "At: 1724 [==========>] Loss 0.08447739466682624  - accuracy: 0.875\n",
      "At: 1725 [==========>] Loss 0.1740748005246445  - accuracy: 0.75\n",
      "At: 1726 [==========>] Loss 0.10492897363374204  - accuracy: 0.875\n",
      "At: 1727 [==========>] Loss 0.12095278624740882  - accuracy: 0.84375\n",
      "At: 1728 [==========>] Loss 0.0994899483691072  - accuracy: 0.875\n",
      "At: 1729 [==========>] Loss 0.15913886935535404  - accuracy: 0.75\n",
      "At: 1730 [==========>] Loss 0.13679598270359294  - accuracy: 0.78125\n",
      "At: 1731 [==========>] Loss 0.10815190478224729  - accuracy: 0.84375\n",
      "At: 1732 [==========>] Loss 0.08082084682193658  - accuracy: 0.84375\n",
      "At: 1733 [==========>] Loss 0.17808830798959988  - accuracy: 0.75\n",
      "At: 1734 [==========>] Loss 0.10304741474774942  - accuracy: 0.84375\n",
      "At: 1735 [==========>] Loss 0.15528647985619287  - accuracy: 0.8125\n",
      "At: 1736 [==========>] Loss 0.09699367214546287  - accuracy: 0.875\n",
      "At: 1737 [==========>] Loss 0.13300150673184447  - accuracy: 0.78125\n",
      "At: 1738 [==========>] Loss 0.11829074378533029  - accuracy: 0.8125\n",
      "At: 1739 [==========>] Loss 0.11718302846619584  - accuracy: 0.84375\n",
      "At: 1740 [==========>] Loss 0.09935172787609253  - accuracy: 0.875\n",
      "At: 1741 [==========>] Loss 0.16530515143044613  - accuracy: 0.71875\n",
      "At: 1742 [==========>] Loss 0.056398018942888255  - accuracy: 0.90625\n",
      "At: 1743 [==========>] Loss 0.14118370580547626  - accuracy: 0.78125\n",
      "At: 1744 [==========>] Loss 0.10220923203763978  - accuracy: 0.84375\n",
      "At: 1745 [==========>] Loss 0.13122964051369898  - accuracy: 0.8125\n",
      "At: 1746 [==========>] Loss 0.15692846357160506  - accuracy: 0.8125\n",
      "At: 1747 [==========>] Loss 0.11400545646552544  - accuracy: 0.8125\n",
      "At: 1748 [==========>] Loss 0.11823825357382146  - accuracy: 0.875\n",
      "At: 1749 [==========>] Loss 0.10052812692162462  - accuracy: 0.875\n",
      "At: 1750 [==========>] Loss 0.08624640040379027  - accuracy: 0.875\n",
      "At: 1751 [==========>] Loss 0.17042792137529905  - accuracy: 0.78125\n",
      "At: 1752 [==========>] Loss 0.1058768331060436  - accuracy: 0.84375\n",
      "At: 1753 [==========>] Loss 0.07404500390432425  - accuracy: 0.9375\n",
      "At: 1754 [==========>] Loss 0.14022265367560222  - accuracy: 0.8125\n",
      "At: 1755 [==========>] Loss 0.0805861365417087  - accuracy: 0.90625\n",
      "At: 1756 [==========>] Loss 0.15590245240004183  - accuracy: 0.78125\n",
      "At: 1757 [==========>] Loss 0.15384616436401102  - accuracy: 0.78125\n",
      "At: 1758 [==========>] Loss 0.06861380463192128  - accuracy: 0.9375\n",
      "At: 1759 [==========>] Loss 0.08347031347722868  - accuracy: 0.90625\n",
      "At: 1760 [==========>] Loss 0.10899633003194192  - accuracy: 0.8125\n",
      "At: 1761 [==========>] Loss 0.1035823372576461  - accuracy: 0.8125\n",
      "At: 1762 [==========>] Loss 0.12797002839522328  - accuracy: 0.84375\n",
      "At: 1763 [==========>] Loss 0.10256171842617101  - accuracy: 0.84375\n",
      "At: 1764 [==========>] Loss 0.11594133411607042  - accuracy: 0.8125\n",
      "At: 1765 [==========>] Loss 0.13850359867259965  - accuracy: 0.8125\n",
      "At: 1766 [==========>] Loss 0.07889895041353592  - accuracy: 0.90625\n",
      "At: 1767 [==========>] Loss 0.08672934335188011  - accuracy: 0.9375\n",
      "At: 1768 [==========>] Loss 0.10416014601458673  - accuracy: 0.84375\n",
      "At: 1769 [==========>] Loss 0.06044904792837516  - accuracy: 0.875\n",
      "At: 1770 [==========>] Loss 0.08943976553331856  - accuracy: 0.9375\n",
      "At: 1771 [==========>] Loss 0.16191342024731628  - accuracy: 0.78125\n",
      "At: 1772 [==========>] Loss 0.13514278392647816  - accuracy: 0.84375\n",
      "At: 1773 [==========>] Loss 0.11075469271646418  - accuracy: 0.84375\n",
      "At: 1774 [==========>] Loss 0.13391026868245745  - accuracy: 0.875\n",
      "At: 1775 [==========>] Loss 0.11284307101390115  - accuracy: 0.84375\n",
      "At: 1776 [==========>] Loss 0.11590099655689975  - accuracy: 0.90625\n",
      "At: 1777 [==========>] Loss 0.11255489536871915  - accuracy: 0.875\n",
      "At: 1778 [==========>] Loss 0.10726226484099811  - accuracy: 0.8125\n",
      "At: 1779 [==========>] Loss 0.08930224942923823  - accuracy: 0.90625\n",
      "At: 1780 [==========>] Loss 0.09761659716545873  - accuracy: 0.875\n",
      "At: 1781 [==========>] Loss 0.1754921042730145  - accuracy: 0.75\n",
      "At: 1782 [==========>] Loss 0.09293583097695685  - accuracy: 0.84375\n",
      "At: 1783 [==========>] Loss 0.14863537782232872  - accuracy: 0.84375\n",
      "At: 1784 [==========>] Loss 0.08395839942582414  - accuracy: 0.90625\n",
      "At: 1785 [==========>] Loss 0.10636041172121077  - accuracy: 0.8125\n",
      "At: 1786 [==========>] Loss 0.14033062562758328  - accuracy: 0.75\n",
      "At: 1787 [==========>] Loss 0.13090510894540552  - accuracy: 0.8125\n",
      "At: 1788 [==========>] Loss 0.09332726153891327  - accuracy: 0.875\n",
      "At: 1789 [==========>] Loss 0.11521618810522452  - accuracy: 0.875\n",
      "At: 1790 [==========>] Loss 0.16000000448341553  - accuracy: 0.75\n",
      "At: 1791 [==========>] Loss 0.07140118555412789  - accuracy: 0.90625\n",
      "At: 1792 [==========>] Loss 0.1181713943423679  - accuracy: 0.84375\n",
      "At: 1793 [==========>] Loss 0.08767842848261265  - accuracy: 0.84375\n",
      "At: 1794 [==========>] Loss 0.14963742957975584  - accuracy: 0.75\n",
      "At: 1795 [==========>] Loss 0.09520371050766138  - accuracy: 0.875\n",
      "At: 1796 [==========>] Loss 0.12973343634633977  - accuracy: 0.8125\n",
      "At: 1797 [==========>] Loss 0.12228325343406882  - accuracy: 0.84375\n",
      "At: 1798 [==========>] Loss 0.13650057216950706  - accuracy: 0.8125\n",
      "At: 1799 [==========>] Loss 0.05704969882124149  - accuracy: 0.9375\n",
      "At: 1800 [==========>] Loss 0.11304223813363269  - accuracy: 0.8125\n",
      "At: 1801 [==========>] Loss 0.1698616856135513  - accuracy: 0.75\n",
      "At: 1802 [==========>] Loss 0.17117254722939432  - accuracy: 0.75\n",
      "At: 1803 [==========>] Loss 0.18370452321774117  - accuracy: 0.75\n",
      "At: 1804 [==========>] Loss 0.13420502667792328  - accuracy: 0.78125\n",
      "At: 1805 [==========>] Loss 0.04317117484191325  - accuracy: 0.96875\n",
      "At: 1806 [==========>] Loss 0.12613404913471732  - accuracy: 0.84375\n",
      "At: 1807 [==========>] Loss 0.17573873515182953  - accuracy: 0.78125\n",
      "At: 1808 [==========>] Loss 0.16353310616747335  - accuracy: 0.84375\n",
      "At: 1809 [==========>] Loss 0.06942434961247691  - accuracy: 0.9375\n",
      "At: 1810 [==========>] Loss 0.1387439787581319  - accuracy: 0.8125\n",
      "At: 1811 [==========>] Loss 0.15617906922843527  - accuracy: 0.71875\n",
      "At: 1812 [==========>] Loss 0.08497221981032407  - accuracy: 0.90625\n",
      "At: 1813 [==========>] Loss 0.12642741461245746  - accuracy: 0.84375\n",
      "At: 1814 [==========>] Loss 0.10019381180313983  - accuracy: 0.875\n",
      "At: 1815 [==========>] Loss 0.16884122870618612  - accuracy: 0.78125\n",
      "At: 1816 [==========>] Loss 0.046017730832848894  - accuracy: 0.96875\n",
      "At: 1817 [==========>] Loss 0.13045652253724518  - accuracy: 0.78125\n",
      "At: 1818 [==========>] Loss 0.15396394734836133  - accuracy: 0.8125\n",
      "At: 1819 [==========>] Loss 0.1742775757287388  - accuracy: 0.78125\n",
      "At: 1820 [==========>] Loss 0.10409052562440296  - accuracy: 0.84375\n",
      "At: 1821 [==========>] Loss 0.09922540749063469  - accuracy: 0.84375\n",
      "At: 1822 [==========>] Loss 0.16468350226195033  - accuracy: 0.75\n",
      "At: 1823 [==========>] Loss 0.1619327328412491  - accuracy: 0.75\n",
      "At: 1824 [==========>] Loss 0.16515227268988847  - accuracy: 0.78125\n",
      "At: 1825 [==========>] Loss 0.1104764776885972  - accuracy: 0.84375\n",
      "At: 1826 [==========>] Loss 0.06989997357653979  - accuracy: 0.9375\n",
      "At: 1827 [==========>] Loss 0.11847548937299712  - accuracy: 0.84375\n",
      "At: 1828 [==========>] Loss 0.14133504809391373  - accuracy: 0.84375\n",
      "At: 1829 [==========>] Loss 0.13871951320476206  - accuracy: 0.8125\n",
      "At: 1830 [==========>] Loss 0.15888389832539804  - accuracy: 0.78125\n",
      "At: 1831 [==========>] Loss 0.12247907981838638  - accuracy: 0.84375\n",
      "At: 1832 [==========>] Loss 0.13449311965259492  - accuracy: 0.84375\n",
      "At: 1833 [==========>] Loss 0.10089576373711198  - accuracy: 0.875\n",
      "At: 1834 [==========>] Loss 0.08397173478422826  - accuracy: 0.875\n",
      "At: 1835 [==========>] Loss 0.1461870740373324  - accuracy: 0.78125\n",
      "At: 1836 [==========>] Loss 0.12214744144495066  - accuracy: 0.8125\n",
      "At: 1837 [==========>] Loss 0.04206531310599839  - accuracy: 0.96875\n",
      "At: 1838 [==========>] Loss 0.10587889132837326  - accuracy: 0.8125\n",
      "At: 1839 [==========>] Loss 0.08650061371997396  - accuracy: 0.8125\n",
      "At: 1840 [==========>] Loss 0.11469577478392398  - accuracy: 0.84375\n",
      "At: 1841 [==========>] Loss 0.09816063029822515  - accuracy: 0.90625\n",
      "At: 1842 [==========>] Loss 0.14395893635231155  - accuracy: 0.78125\n",
      "At: 1843 [==========>] Loss 0.1245959474457993  - accuracy: 0.8125\n",
      "At: 1844 [==========>] Loss 0.12040190677474283  - accuracy: 0.8125\n",
      "At: 1845 [==========>] Loss 0.1516297767670729  - accuracy: 0.78125\n",
      "At: 1846 [==========>] Loss 0.11053277643845194  - accuracy: 0.875\n",
      "At: 1847 [==========>] Loss 0.0518905989716226  - accuracy: 0.9375\n",
      "At: 1848 [==========>] Loss 0.05797184389824209  - accuracy: 0.9375\n",
      "At: 1849 [==========>] Loss 0.16029348192509763  - accuracy: 0.75\n",
      "At: 1850 [==========>] Loss 0.043090261440622876  - accuracy: 0.96875\n",
      "At: 1851 [==========>] Loss 0.16565859528843982  - accuracy: 0.78125\n",
      "At: 1852 [==========>] Loss 0.08713952992391208  - accuracy: 0.875\n",
      "At: 1853 [==========>] Loss 0.1362192113574895  - accuracy: 0.78125\n",
      "At: 1854 [==========>] Loss 0.12547505712679768  - accuracy: 0.8125\n",
      "At: 1855 [==========>] Loss 0.16487015232165303  - accuracy: 0.78125\n",
      "At: 1856 [==========>] Loss 0.11232777875306872  - accuracy: 0.875\n",
      "At: 1857 [==========>] Loss 0.1449697292169254  - accuracy: 0.8125\n",
      "At: 1858 [==========>] Loss 0.11570762140166348  - accuracy: 0.84375\n",
      "At: 1859 [==========>] Loss 0.155743537014833  - accuracy: 0.8125\n",
      "At: 1860 [==========>] Loss 0.1421823046308441  - accuracy: 0.78125\n",
      "At: 1861 [==========>] Loss 0.08917156169916005  - accuracy: 0.875\n",
      "At: 1862 [==========>] Loss 0.19595339243913965  - accuracy: 0.6875\n",
      "At: 1863 [==========>] Loss 0.1415356734289328  - accuracy: 0.75\n",
      "At: 1864 [==========>] Loss 0.14594171501481035  - accuracy: 0.75\n",
      "At: 1865 [==========>] Loss 0.08910096678372342  - accuracy: 0.90625\n",
      "At: 1866 [==========>] Loss 0.224870926355489  - accuracy: 0.6875\n",
      "At: 1867 [==========>] Loss 0.10699845793620148  - accuracy: 0.8125\n",
      "At: 1868 [==========>] Loss 0.13535983138627133  - accuracy: 0.78125\n",
      "At: 1869 [==========>] Loss 0.19672369437214154  - accuracy: 0.6875\n",
      "At: 1870 [==========>] Loss 0.10939997195766439  - accuracy: 0.875\n",
      "At: 1871 [==========>] Loss 0.12093445220415819  - accuracy: 0.78125\n",
      "At: 1872 [==========>] Loss 0.1581632997180637  - accuracy: 0.75\n",
      "At: 1873 [==========>] Loss 0.0814847663186371  - accuracy: 0.90625\n",
      "At: 1874 [==========>] Loss 0.12330620088474328  - accuracy: 0.875\n",
      "At: 1875 [==========>] Loss 0.09718554631240617  - accuracy: 0.875\n",
      "At: 1876 [==========>] Loss 0.20808338080023508  - accuracy: 0.65625\n",
      "At: 1877 [==========>] Loss 0.11220666610040349  - accuracy: 0.875\n",
      "At: 1878 [==========>] Loss 0.11588060515813986  - accuracy: 0.84375\n",
      "At: 1879 [==========>] Loss 0.12584516874154572  - accuracy: 0.84375\n",
      "At: 1880 [==========>] Loss 0.12086505814557266  - accuracy: 0.75\n",
      "At: 1881 [==========>] Loss 0.099728156017895  - accuracy: 0.875\n",
      "At: 1882 [==========>] Loss 0.10519071762588504  - accuracy: 0.8125\n",
      "At: 1883 [==========>] Loss 0.11343878819060453  - accuracy: 0.8125\n",
      "At: 1884 [==========>] Loss 0.08750787703241704  - accuracy: 0.90625\n",
      "At: 1885 [==========>] Loss 0.12422272079252039  - accuracy: 0.84375\n",
      "At: 1886 [==========>] Loss 0.13123652375297  - accuracy: 0.8125\n",
      "At: 1887 [==========>] Loss 0.0856315721491999  - accuracy: 0.90625\n",
      "At: 1888 [==========>] Loss 0.15350996508336268  - accuracy: 0.78125\n",
      "At: 1889 [==========>] Loss 0.10845519432561018  - accuracy: 0.84375\n",
      "At: 1890 [==========>] Loss 0.17194410775420133  - accuracy: 0.8125\n",
      "At: 1891 [==========>] Loss 0.08122012616285113  - accuracy: 0.90625\n",
      "At: 1892 [==========>] Loss 0.07838138669461608  - accuracy: 0.90625\n",
      "At: 1893 [==========>] Loss 0.07562837571911488  - accuracy: 0.875\n",
      "At: 1894 [==========>] Loss 0.09760772958848282  - accuracy: 0.84375\n",
      "At: 1895 [==========>] Loss 0.07671359706497455  - accuracy: 0.875\n",
      "At: 1896 [==========>] Loss 0.1101148205417623  - accuracy: 0.8125\n",
      "At: 1897 [==========>] Loss 0.07084203412466891  - accuracy: 0.90625\n",
      "At: 1898 [==========>] Loss 0.12958160537549895  - accuracy: 0.78125\n",
      "At: 1899 [==========>] Loss 0.09731885814598377  - accuracy: 0.84375\n",
      "At: 1900 [==========>] Loss 0.12865150340356876  - accuracy: 0.84375\n",
      "At: 1901 [==========>] Loss 0.08481017033057664  - accuracy: 0.90625\n",
      "At: 1902 [==========>] Loss 0.13573679749846035  - accuracy: 0.78125\n",
      "At: 1903 [==========>] Loss 0.11385904064424382  - accuracy: 0.8125\n",
      "At: 1904 [==========>] Loss 0.05682113370431822  - accuracy: 0.9375\n",
      "At: 1905 [==========>] Loss 0.1355407404192064  - accuracy: 0.8125\n",
      "At: 1906 [==========>] Loss 0.09526731115807802  - accuracy: 0.8125\n",
      "At: 1907 [==========>] Loss 0.07111027445350879  - accuracy: 0.90625\n",
      "At: 1908 [==========>] Loss 0.09032114373916136  - accuracy: 0.875\n",
      "At: 1909 [==========>] Loss 0.08461109661231488  - accuracy: 0.90625\n",
      "At: 1910 [==========>] Loss 0.06314143080971657  - accuracy: 0.90625\n",
      "At: 1911 [==========>] Loss 0.11926544411525719  - accuracy: 0.84375\n",
      "At: 1912 [==========>] Loss 0.11835828940613631  - accuracy: 0.84375\n",
      "At: 1913 [==========>] Loss 0.1675473480951537  - accuracy: 0.71875\n",
      "At: 1914 [==========>] Loss 0.09420036920194409  - accuracy: 0.90625\n",
      "At: 1915 [==========>] Loss 0.10663065800753276  - accuracy: 0.875\n",
      "At: 1916 [==========>] Loss 0.1536525842205565  - accuracy: 0.8125\n",
      "At: 1917 [==========>] Loss 0.17660331463440054  - accuracy: 0.71875\n",
      "At: 1918 [==========>] Loss 0.1594996349066726  - accuracy: 0.75\n",
      "At: 1919 [==========>] Loss 0.08856541027765463  - accuracy: 0.84375\n",
      "At: 1920 [==========>] Loss 0.09762103300602859  - accuracy: 0.84375\n",
      "At: 1921 [==========>] Loss 0.13821601408436474  - accuracy: 0.78125\n",
      "At: 1922 [==========>] Loss 0.15076002981028133  - accuracy: 0.78125\n",
      "At: 1923 [==========>] Loss 0.19300397662101654  - accuracy: 0.625\n",
      "At: 1924 [==========>] Loss 0.12763158930501478  - accuracy: 0.78125\n",
      "At: 1925 [==========>] Loss 0.16401302881484225  - accuracy: 0.75\n",
      "At: 1926 [==========>] Loss 0.09539195300442793  - accuracy: 0.875\n",
      "At: 1927 [==========>] Loss 0.08690933909404154  - accuracy: 0.875\n",
      "At: 1928 [==========>] Loss 0.12053422106020303  - accuracy: 0.84375\n",
      "At: 1929 [==========>] Loss 0.1840188197747548  - accuracy: 0.78125\n",
      "At: 1930 [==========>] Loss 0.20323649382761272  - accuracy: 0.6875\n",
      "At: 1931 [==========>] Loss 0.09606518468985684  - accuracy: 0.90625\n",
      "At: 1932 [==========>] Loss 0.15782468275311884  - accuracy: 0.78125\n",
      "At: 1933 [==========>] Loss 0.08158843465657833  - accuracy: 0.90625\n",
      "At: 1934 [==========>] Loss 0.15898892857779706  - accuracy: 0.8125\n",
      "At: 1935 [==========>] Loss 0.111959595396045  - accuracy: 0.84375\n",
      "At: 1936 [==========>] Loss 0.09109395713578394  - accuracy: 0.875\n",
      "At: 1937 [==========>] Loss 0.12634717311523097  - accuracy: 0.78125\n",
      "At: 1938 [==========>] Loss 0.168483319744528  - accuracy: 0.75\n",
      "At: 1939 [==========>] Loss 0.10203663571143436  - accuracy: 0.875\n",
      "At: 1940 [==========>] Loss 0.10805200523616718  - accuracy: 0.875\n",
      "At: 1941 [==========>] Loss 0.1114829363262863  - accuracy: 0.84375\n",
      "At: 1942 [==========>] Loss 0.1340668765924528  - accuracy: 0.78125\n",
      "At: 1943 [==========>] Loss 0.1220538992892318  - accuracy: 0.875\n",
      "At: 1944 [==========>] Loss 0.09561904517309272  - accuracy: 0.90625\n",
      "At: 1945 [==========>] Loss 0.13731642412802902  - accuracy: 0.8125\n",
      "At: 1946 [==========>] Loss 0.1055279325204384  - accuracy: 0.84375\n",
      "At: 1947 [==========>] Loss 0.12323119259441452  - accuracy: 0.84375\n",
      "At: 1948 [==========>] Loss 0.11156284524963914  - accuracy: 0.8125\n",
      "At: 1949 [==========>] Loss 0.05790926907004247  - accuracy: 0.96875\n",
      "At: 1950 [==========>] Loss 0.11421145719837233  - accuracy: 0.84375\n",
      "At: 1951 [==========>] Loss 0.12641963015337676  - accuracy: 0.84375\n",
      "At: 1952 [==========>] Loss 0.07028472789253734  - accuracy: 0.90625\n",
      "At: 1953 [==========>] Loss 0.05921839256188217  - accuracy: 0.90625\n",
      "At: 1954 [==========>] Loss 0.18674863629121116  - accuracy: 0.71875\n",
      "At: 1955 [==========>] Loss 0.05540694806624161  - accuracy: 0.9375\n",
      "At: 1956 [==========>] Loss 0.11933188099221621  - accuracy: 0.8125\n",
      "At: 1957 [==========>] Loss 0.0661263517844843  - accuracy: 0.9375\n",
      "At: 1958 [==========>] Loss 0.0955690392770897  - accuracy: 0.9375\n",
      "At: 1959 [==========>] Loss 0.11085120262992404  - accuracy: 0.8125\n",
      "At: 1960 [==========>] Loss 0.054802490922189515  - accuracy: 0.90625\n",
      "At: 1961 [==========>] Loss 0.18509689421938003  - accuracy: 0.78125\n",
      "At: 1962 [==========>] Loss 0.1971434656414286  - accuracy: 0.71875\n",
      "At: 1963 [==========>] Loss 0.06559679682263202  - accuracy: 0.9375\n",
      "At: 1964 [==========>] Loss 0.18614436800758433  - accuracy: 0.6875\n",
      "At: 1965 [==========>] Loss 0.14697320859848317  - accuracy: 0.78125\n",
      "At: 1966 [==========>] Loss 0.1178396065834417  - accuracy: 0.84375\n",
      "At: 1967 [==========>] Loss 0.1277424804452118  - accuracy: 0.8125\n",
      "At: 1968 [==========>] Loss 0.18210148354163724  - accuracy: 0.71875\n",
      "At: 1969 [==========>] Loss 0.14591030643007258  - accuracy: 0.8125\n",
      "At: 1970 [==========>] Loss 0.1392851347417287  - accuracy: 0.8125\n",
      "At: 1971 [==========>] Loss 0.18319718555125378  - accuracy: 0.75\n",
      "At: 1972 [==========>] Loss 0.09161483572645267  - accuracy: 0.875\n",
      "At: 1973 [==========>] Loss 0.1195598990485265  - accuracy: 0.78125\n",
      "At: 1974 [==========>] Loss 0.1295446936061811  - accuracy: 0.75\n",
      "At: 1975 [==========>] Loss 0.1506956847674009  - accuracy: 0.8125\n",
      "At: 1976 [==========>] Loss 0.05702649591281801  - accuracy: 0.9375\n",
      "At: 1977 [==========>] Loss 0.10752653570266946  - accuracy: 0.84375\n",
      "At: 1978 [==========>] Loss 0.12155422598449919  - accuracy: 0.8125\n",
      "At: 1979 [==========>] Loss 0.12837429674753026  - accuracy: 0.84375\n",
      "At: 1980 [==========>] Loss 0.13972654448150648  - accuracy: 0.875\n",
      "At: 1981 [==========>] Loss 0.14205406477664792  - accuracy: 0.75\n",
      "At: 1982 [==========>] Loss 0.06971810048604754  - accuracy: 0.90625\n",
      "At: 1983 [==========>] Loss 0.14708669422442266  - accuracy: 0.75\n",
      "At: 1984 [==========>] Loss 0.0734543750425882  - accuracy: 0.875\n",
      "At: 1985 [==========>] Loss 0.15438012475441115  - accuracy: 0.78125\n",
      "At: 1986 [==========>] Loss 0.17188773319379969  - accuracy: 0.75\n",
      "At: 1987 [==========>] Loss 0.11774883848502238  - accuracy: 0.84375\n",
      "At: 1988 [==========>] Loss 0.09650870318213514  - accuracy: 0.84375\n",
      "At: 1989 [==========>] Loss 0.10529657576157994  - accuracy: 0.84375\n",
      "At: 1990 [==========>] Loss 0.08551982185851936  - accuracy: 0.90625\n",
      "At: 1991 [==========>] Loss 0.14700010760045512  - accuracy: 0.8125\n",
      "At: 1992 [==========>] Loss 0.10005647682146729  - accuracy: 0.875\n",
      "At: 1993 [==========>] Loss 0.15526269554845967  - accuracy: 0.71875\n",
      "At: 1994 [==========>] Loss 0.14412800760651484  - accuracy: 0.8125\n",
      "At: 1995 [==========>] Loss 0.18786527903554107  - accuracy: 0.71875\n",
      "At: 1996 [==========>] Loss 0.0933258000604695  - accuracy: 0.84375\n",
      "At: 1997 [==========>] Loss 0.19820011083315958  - accuracy: 0.6875\n",
      "At: 1998 [==========>] Loss 0.13680896627510053  - accuracy: 0.75\n",
      "At: 1999 [==========>] Loss 0.08733151354812427  - accuracy: 0.90625\n",
      "At: 2000 [==========>] Loss 0.14929831812324534  - accuracy: 0.75\n",
      "At: 2001 [==========>] Loss 0.08036627857607745  - accuracy: 0.90625\n",
      "At: 2002 [==========>] Loss 0.07632942265239823  - accuracy: 0.9375\n",
      "At: 2003 [==========>] Loss 0.133744576898837  - accuracy: 0.84375\n",
      "At: 2004 [==========>] Loss 0.15841200386332002  - accuracy: 0.78125\n",
      "At: 2005 [==========>] Loss 0.1057166992116868  - accuracy: 0.84375\n",
      "At: 2006 [==========>] Loss 0.13429950093818718  - accuracy: 0.875\n",
      "At: 2007 [==========>] Loss 0.09270731090940293  - accuracy: 0.875\n",
      "At: 2008 [==========>] Loss 0.14495644227804866  - accuracy: 0.84375\n",
      "At: 2009 [==========>] Loss 0.13472013612900635  - accuracy: 0.875\n",
      "At: 2010 [==========>] Loss 0.1269073947497106  - accuracy: 0.84375\n",
      "At: 2011 [==========>] Loss 0.13509528270168397  - accuracy: 0.8125\n",
      "At: 2012 [==========>] Loss 0.1265678249146961  - accuracy: 0.8125\n",
      "At: 2013 [==========>] Loss 0.10171462405845533  - accuracy: 0.90625\n",
      "At: 2014 [==========>] Loss 0.21346397751122786  - accuracy: 0.59375\n",
      "At: 2015 [==========>] Loss 0.0656886203945965  - accuracy: 0.9375\n",
      "At: 2016 [==========>] Loss 0.11840914272302511  - accuracy: 0.84375\n",
      "At: 2017 [==========>] Loss 0.06904503983488577  - accuracy: 0.9375\n",
      "At: 2018 [==========>] Loss 0.0890836282964827  - accuracy: 0.875\n",
      "At: 2019 [==========>] Loss 0.12399817162556853  - accuracy: 0.8125\n",
      "At: 2020 [==========>] Loss 0.0704661626870541  - accuracy: 0.90625\n",
      "At: 2021 [==========>] Loss 0.11893469543682852  - accuracy: 0.8125\n",
      "At: 2022 [==========>] Loss 0.15973903087736435  - accuracy: 0.71875\n",
      "At: 2023 [==========>] Loss 0.09887352523068628  - accuracy: 0.875\n",
      "At: 2024 [==========>] Loss 0.08121931362647798  - accuracy: 0.90625\n",
      "At: 2025 [==========>] Loss 0.18668960589272288  - accuracy: 0.78125\n",
      "At: 2026 [==========>] Loss 0.08416447687419543  - accuracy: 0.90625\n",
      "At: 2027 [==========>] Loss 0.13046569498019836  - accuracy: 0.78125\n",
      "At: 2028 [==========>] Loss 0.1086632553468004  - accuracy: 0.875\n",
      "At: 2029 [==========>] Loss 0.11268214662934059  - accuracy: 0.875\n",
      "At: 2030 [==========>] Loss 0.14486381694332504  - accuracy: 0.84375\n",
      "At: 2031 [==========>] Loss 0.1579400079715737  - accuracy: 0.75\n",
      "At: 2032 [==========>] Loss 0.1129215537891049  - accuracy: 0.875\n",
      "At: 2033 [==========>] Loss 0.17525897535533413  - accuracy: 0.71875\n",
      "At: 2034 [==========>] Loss 0.20700935992697655  - accuracy: 0.625\n",
      "At: 2035 [==========>] Loss 0.11783498139222312  - accuracy: 0.875\n",
      "At: 2036 [==========>] Loss 0.09298694719622654  - accuracy: 0.875\n",
      "At: 2037 [==========>] Loss 0.14096970575497686  - accuracy: 0.71875\n",
      "At: 2038 [==========>] Loss 0.11267289434284633  - accuracy: 0.8125\n",
      "At: 2039 [==========>] Loss 0.08085549387534413  - accuracy: 0.875\n",
      "At: 2040 [==========>] Loss 0.08512243092378718  - accuracy: 0.84375\n",
      "At: 2041 [==========>] Loss 0.06124395021278837  - accuracy: 0.90625\n",
      "At: 2042 [==========>] Loss 0.14011687780297044  - accuracy: 0.8125\n",
      "At: 2043 [==========>] Loss 0.09773188424508755  - accuracy: 0.875\n",
      "At: 2044 [==========>] Loss 0.08339907135244204  - accuracy: 0.875\n",
      "At: 2045 [==========>] Loss 0.22845298807007686  - accuracy: 0.6875\n",
      "At: 2046 [==========>] Loss 0.056258906262824745  - accuracy: 0.9375\n",
      "At: 2047 [==========>] Loss 0.07482739326559143  - accuracy: 0.9375\n",
      "At: 2048 [==========>] Loss 0.1030743027824639  - accuracy: 0.84375\n",
      "At: 2049 [==========>] Loss 0.12187128963068337  - accuracy: 0.875\n",
      "At: 2050 [==========>] Loss 0.15554576469409787  - accuracy: 0.75\n",
      "At: 2051 [==========>] Loss 0.17093794797772416  - accuracy: 0.78125\n",
      "At: 2052 [==========>] Loss 0.07771221576174192  - accuracy: 0.9375\n",
      "At: 2053 [==========>] Loss 0.10031089864748957  - accuracy: 0.84375\n",
      "At: 2054 [==========>] Loss 0.11574905652946471  - accuracy: 0.84375\n",
      "At: 2055 [==========>] Loss 0.06914411894217654  - accuracy: 0.90625\n",
      "At: 2056 [==========>] Loss 0.10244700635804742  - accuracy: 0.875\n",
      "At: 2057 [==========>] Loss 0.12852891234771716  - accuracy: 0.8125\n",
      "At: 2058 [==========>] Loss 0.11449522321199843  - accuracy: 0.84375\n",
      "At: 2059 [==========>] Loss 0.1663050621563623  - accuracy: 0.71875\n",
      "At: 2060 [==========>] Loss 0.12947416728036215  - accuracy: 0.84375\n",
      "At: 2061 [==========>] Loss 0.1353422278424827  - accuracy: 0.8125\n",
      "At: 2062 [==========>] Loss 0.14447788560047453  - accuracy: 0.8125\n",
      "At: 2063 [==========>] Loss 0.09214991580611186  - accuracy: 0.875\n",
      "At: 2064 [==========>] Loss 0.16661505056965914  - accuracy: 0.75\n",
      "At: 2065 [==========>] Loss 0.04092898131937939  - accuracy: 0.96875\n",
      "At: 2066 [==========>] Loss 0.15966949071828826  - accuracy: 0.71875\n",
      "At: 2067 [==========>] Loss 0.09051262562858112  - accuracy: 0.8125\n",
      "At: 2068 [==========>] Loss 0.11704443083768822  - accuracy: 0.78125\n",
      "At: 2069 [==========>] Loss 0.1001135615772199  - accuracy: 0.875\n",
      "At: 2070 [==========>] Loss 0.12189192251918708  - accuracy: 0.8125\n",
      "At: 2071 [==========>] Loss 0.12745347988907324  - accuracy: 0.84375\n",
      "At: 2072 [==========>] Loss 0.06385470464950849  - accuracy: 0.96875\n",
      "At: 2073 [==========>] Loss 0.1143709959240805  - accuracy: 0.875\n",
      "At: 2074 [==========>] Loss 0.09070133778158743  - accuracy: 0.90625\n",
      "At: 2075 [==========>] Loss 0.11328728334571359  - accuracy: 0.8125\n",
      "At: 2076 [==========>] Loss 0.14478888713621668  - accuracy: 0.78125\n",
      "At: 2077 [==========>] Loss 0.1463505424017348  - accuracy: 0.78125\n",
      "At: 2078 [==========>] Loss 0.11393197719684503  - accuracy: 0.875\n",
      "At: 2079 [==========>] Loss 0.06763993432257953  - accuracy: 0.875\n",
      "At: 2080 [==========>] Loss 0.11827500508078358  - accuracy: 0.8125\n",
      "At: 2081 [==========>] Loss 0.14201180572645716  - accuracy: 0.8125\n",
      "At: 2082 [==========>] Loss 0.10079884441447386  - accuracy: 0.84375\n",
      "At: 2083 [==========>] Loss 0.18950087695288795  - accuracy: 0.71875\n",
      "At: 2084 [==========>] Loss 0.09547770352336421  - accuracy: 0.875\n",
      "At: 2085 [==========>] Loss 0.09315530969020305  - accuracy: 0.90625\n",
      "At: 2086 [==========>] Loss 0.10529299668264064  - accuracy: 0.84375\n",
      "At: 2087 [==========>] Loss 0.15233983416442676  - accuracy: 0.78125\n",
      "At: 2088 [==========>] Loss 0.08414025267885042  - accuracy: 0.875\n",
      "At: 2089 [==========>] Loss 0.09290866701457623  - accuracy: 0.90625\n",
      "At: 2090 [==========>] Loss 0.09609751047235457  - accuracy: 0.875\n",
      "At: 2091 [==========>] Loss 0.11081665952686254  - accuracy: 0.875\n",
      "At: 2092 [==========>] Loss 0.09277510182280525  - accuracy: 0.90625\n",
      "At: 2093 [==========>] Loss 0.13613267362835108  - accuracy: 0.84375\n",
      "At: 2094 [==========>] Loss 0.1228178127927518  - accuracy: 0.8125\n",
      "At: 2095 [==========>] Loss 0.11688091802152033  - accuracy: 0.875\n",
      "At: 2096 [==========>] Loss 0.15752136935312078  - accuracy: 0.78125\n",
      "At: 2097 [==========>] Loss 0.11969255820411595  - accuracy: 0.8125\n",
      "At: 2098 [==========>] Loss 0.10719762762257257  - accuracy: 0.8125\n",
      "At: 2099 [==========>] Loss 0.10182779313256637  - accuracy: 0.84375\n",
      "At: 2100 [==========>] Loss 0.07136831543334451  - accuracy: 0.90625\n",
      "At: 2101 [==========>] Loss 0.14392866825397288  - accuracy: 0.8125\n",
      "At: 2102 [==========>] Loss 0.0853429530259678  - accuracy: 0.84375\n",
      "At: 2103 [==========>] Loss 0.1487111177543461  - accuracy: 0.8125\n",
      "At: 2104 [==========>] Loss 0.11728781815536601  - accuracy: 0.8125\n",
      "At: 2105 [==========>] Loss 0.15420324348912814  - accuracy: 0.78125\n",
      "At: 2106 [==========>] Loss 0.14792168378869086  - accuracy: 0.78125\n",
      "At: 2107 [==========>] Loss 0.10916111539326039  - accuracy: 0.84375\n",
      "At: 2108 [==========>] Loss 0.15084341832236686  - accuracy: 0.8125\n",
      "At: 2109 [==========>] Loss 0.12556284209156038  - accuracy: 0.8125\n",
      "At: 2110 [==========>] Loss 0.05188575917158467  - accuracy: 0.90625\n",
      "At: 2111 [==========>] Loss 0.11050976098478618  - accuracy: 0.875\n",
      "At: 2112 [==========>] Loss 0.0890468328830539  - accuracy: 0.84375\n",
      "At: 2113 [==========>] Loss 0.09921086849745661  - accuracy: 0.84375\n",
      "At: 2114 [==========>] Loss 0.1317503943569675  - accuracy: 0.8125\n",
      "At: 2115 [==========>] Loss 0.11040788127142678  - accuracy: 0.875\n",
      "At: 2116 [==========>] Loss 0.11186599306752165  - accuracy: 0.84375\n",
      "At: 2117 [==========>] Loss 0.10139072416605716  - accuracy: 0.875\n",
      "At: 2118 [==========>] Loss 0.1500522782305866  - accuracy: 0.75\n",
      "At: 2119 [==========>] Loss 0.08632450453139959  - accuracy: 0.875\n",
      "At: 2120 [==========>] Loss 0.14445995827323943  - accuracy: 0.84375\n",
      "At: 2121 [==========>] Loss 0.14915679881156993  - accuracy: 0.78125\n",
      "At: 2122 [==========>] Loss 0.13719476621627216  - accuracy: 0.78125\n",
      "At: 2123 [==========>] Loss 0.1667665294084584  - accuracy: 0.78125\n",
      "At: 2124 [==========>] Loss 0.1427434768374402  - accuracy: 0.8125\n",
      "At: 2125 [==========>] Loss 0.10386368646819379  - accuracy: 0.875\n",
      "At: 2126 [==========>] Loss 0.05433819906455817  - accuracy: 0.9375\n",
      "At: 2127 [==========>] Loss 0.12589938062642894  - accuracy: 0.78125\n",
      "At: 2128 [==========>] Loss 0.11415656514385737  - accuracy: 0.8125\n",
      "At: 2129 [==========>] Loss 0.13507265486238118  - accuracy: 0.84375\n",
      "At: 2130 [==========>] Loss 0.05975412378499877  - accuracy: 0.90625\n",
      "At: 2131 [==========>] Loss 0.10751596711673414  - accuracy: 0.875\n",
      "At: 2132 [==========>] Loss 0.203919860177236  - accuracy: 0.65625\n",
      "At: 2133 [==========>] Loss 0.1405369673129562  - accuracy: 0.84375\n",
      "At: 2134 [==========>] Loss 0.11779468630207778  - accuracy: 0.78125\n",
      "At: 2135 [==========>] Loss 0.07308884450589785  - accuracy: 0.9375\n",
      "At: 2136 [==========>] Loss 0.11460840554004023  - accuracy: 0.8125\n",
      "At: 2137 [==========>] Loss 0.12242421785762173  - accuracy: 0.8125\n",
      "At: 2138 [==========>] Loss 0.0968410422515891  - accuracy: 0.875\n",
      "At: 2139 [==========>] Loss 0.18389705760475666  - accuracy: 0.75\n",
      "At: 2140 [==========>] Loss 0.09558053050754388  - accuracy: 0.84375\n",
      "At: 2141 [==========>] Loss 0.1205372129373189  - accuracy: 0.8125\n",
      "At: 2142 [==========>] Loss 0.11700362848419227  - accuracy: 0.875\n",
      "At: 2143 [==========>] Loss 0.09469898583553685  - accuracy: 0.875\n",
      "At: 2144 [==========>] Loss 0.0782347963069836  - accuracy: 0.875\n",
      "At: 2145 [==========>] Loss 0.10387245591437097  - accuracy: 0.8125\n",
      "At: 2146 [==========>] Loss 0.1707143022220409  - accuracy: 0.71875\n",
      "At: 2147 [==========>] Loss 0.11640664105722541  - accuracy: 0.8125\n",
      "At: 2148 [==========>] Loss 0.17340744334221286  - accuracy: 0.75\n",
      "At: 2149 [==========>] Loss 0.1166521595205178  - accuracy: 0.78125\n",
      "At: 2150 [==========>] Loss 0.08308034184130934  - accuracy: 0.875\n",
      "At: 2151 [==========>] Loss 0.105692797426576  - accuracy: 0.90625\n",
      "At: 2152 [==========>] Loss 0.20239187095459532  - accuracy: 0.6875\n",
      "At: 2153 [==========>] Loss 0.19151234079866514  - accuracy: 0.6875\n",
      "At: 2154 [==========>] Loss 0.16481933629891446  - accuracy: 0.75\n",
      "At: 2155 [==========>] Loss 0.1544755739209973  - accuracy: 0.78125\n",
      "At: 2156 [==========>] Loss 0.09582898050264183  - accuracy: 0.84375\n",
      "At: 2157 [==========>] Loss 0.15387548235170603  - accuracy: 0.8125\n",
      "At: 2158 [==========>] Loss 0.1376413584095374  - accuracy: 0.75\n",
      "At: 2159 [==========>] Loss 0.05363707453055637  - accuracy: 0.9375\n",
      "At: 2160 [==========>] Loss 0.13266226996328584  - accuracy: 0.875\n",
      "At: 2161 [==========>] Loss 0.10854267822291683  - accuracy: 0.84375\n",
      "At: 2162 [==========>] Loss 0.09723411795357874  - accuracy: 0.875\n",
      "At: 2163 [==========>] Loss 0.12391282573910582  - accuracy: 0.84375\n",
      "At: 2164 [==========>] Loss 0.17462646434966567  - accuracy: 0.78125\n",
      "At: 2165 [==========>] Loss 0.08513937802357716  - accuracy: 0.90625\n",
      "At: 2166 [==========>] Loss 0.10653932596584456  - accuracy: 0.84375\n",
      "At: 2167 [==========>] Loss 0.08168736750059169  - accuracy: 0.9375\n",
      "At: 2168 [==========>] Loss 0.07276030828598543  - accuracy: 0.90625\n",
      "At: 2169 [==========>] Loss 0.10978318839048194  - accuracy: 0.84375\n",
      "At: 2170 [==========>] Loss 0.10288819713523045  - accuracy: 0.90625\n",
      "At: 2171 [==========>] Loss 0.15743069206524124  - accuracy: 0.71875\n",
      "At: 2172 [==========>] Loss 0.08527949311364785  - accuracy: 0.875\n",
      "At: 2173 [==========>] Loss 0.1319452582767035  - accuracy: 0.8125\n",
      "At: 2174 [==========>] Loss 0.1251403563641334  - accuracy: 0.84375\n",
      "At: 2175 [==========>] Loss 0.12762666640313364  - accuracy: 0.78125\n",
      "At: 2176 [==========>] Loss 0.12088298340601092  - accuracy: 0.875\n",
      "At: 2177 [==========>] Loss 0.13575220924961656  - accuracy: 0.75\n",
      "At: 2178 [==========>] Loss 0.09318757972406373  - accuracy: 0.90625\n",
      "At: 2179 [==========>] Loss 0.13659488703835077  - accuracy: 0.78125\n",
      "At: 2180 [==========>] Loss 0.09387685810249108  - accuracy: 0.90625\n",
      "At: 2181 [==========>] Loss 0.16850104435875704  - accuracy: 0.71875\n",
      "At: 2182 [==========>] Loss 0.12314213556006565  - accuracy: 0.8125\n",
      "At: 2183 [==========>] Loss 0.23140203364628542  - accuracy: 0.65625\n",
      "At: 2184 [==========>] Loss 0.06555875168605461  - accuracy: 0.90625\n",
      "At: 2185 [==========>] Loss 0.08902006000274514  - accuracy: 0.90625\n",
      "At: 2186 [==========>] Loss 0.14227954899630701  - accuracy: 0.8125\n",
      "At: 2187 [==========>] Loss 0.17127284252776753  - accuracy: 0.6875\n",
      "At: 2188 [==========>] Loss 0.08450291897838236  - accuracy: 0.90625\n",
      "At: 2189 [==========>] Loss 0.07631290980319018  - accuracy: 0.90625\n",
      "At: 2190 [==========>] Loss 0.11375096051611794  - accuracy: 0.8125\n",
      "At: 2191 [==========>] Loss 0.12463940937900643  - accuracy: 0.78125\n",
      "At: 2192 [==========>] Loss 0.09319690375358596  - accuracy: 0.875\n",
      "At: 2193 [==========>] Loss 0.1242602247474577  - accuracy: 0.78125\n",
      "At: 2194 [==========>] Loss 0.08912367349941824  - accuracy: 0.84375\n",
      "At: 2195 [==========>] Loss 0.18187365392348592  - accuracy: 0.625\n",
      "At: 2196 [==========>] Loss 0.16523209141840967  - accuracy: 0.78125\n",
      "At: 2197 [==========>] Loss 0.08482428351432617  - accuracy: 0.90625\n",
      "At: 2198 [==========>] Loss 0.09101432359591499  - accuracy: 0.9375\n",
      "At: 2199 [==========>] Loss 0.0588367137875363  - accuracy: 0.9375\n",
      "At: 2200 [==========>] Loss 0.054183486304633606  - accuracy: 0.90625\n",
      "At: 2201 [==========>] Loss 0.10282550379137023  - accuracy: 0.84375\n",
      "At: 2202 [==========>] Loss 0.08390665670133277  - accuracy: 0.90625\n",
      "At: 2203 [==========>] Loss 0.09183077000512443  - accuracy: 0.875\n",
      "At: 2204 [==========>] Loss 0.11115983175921482  - accuracy: 0.875\n",
      "At: 2205 [==========>] Loss 0.09349498851626328  - accuracy: 0.875\n",
      "At: 2206 [==========>] Loss 0.10831187408696785  - accuracy: 0.8125\n",
      "At: 2207 [==========>] Loss 0.09683775424483829  - accuracy: 0.84375\n",
      "At: 2208 [==========>] Loss 0.17683746367898495  - accuracy: 0.75\n",
      "At: 2209 [==========>] Loss 0.17795370847285086  - accuracy: 0.71875\n",
      "At: 2210 [==========>] Loss 0.11720830718703963  - accuracy: 0.84375\n",
      "At: 2211 [==========>] Loss 0.15646108376489276  - accuracy: 0.78125\n",
      "At: 2212 [==========>] Loss 0.10529846997157927  - accuracy: 0.875\n",
      "At: 2213 [==========>] Loss 0.12443936628597067  - accuracy: 0.84375\n",
      "At: 2214 [==========>] Loss 0.14182803222792797  - accuracy: 0.78125\n",
      "At: 2215 [==========>] Loss 0.12401940839440719  - accuracy: 0.875\n",
      "At: 2216 [==========>] Loss 0.1404442340280048  - accuracy: 0.8125\n",
      "At: 2217 [==========>] Loss 0.12817944996248448  - accuracy: 0.8125\n",
      "At: 2218 [==========>] Loss 0.16969805944137809  - accuracy: 0.8125\n",
      "At: 2219 [==========>] Loss 0.06750961535128082  - accuracy: 0.9375\n",
      "At: 2220 [==========>] Loss 0.11670037245554961  - accuracy: 0.8125\n",
      "At: 2221 [==========>] Loss 0.15765054072124912  - accuracy: 0.78125\n",
      "At: 2222 [==========>] Loss 0.08753745201068056  - accuracy: 0.875\n",
      "At: 2223 [==========>] Loss 0.14517786553735534  - accuracy: 0.75\n",
      "At: 2224 [==========>] Loss 0.12747515064842388  - accuracy: 0.84375\n",
      "At: 2225 [==========>] Loss 0.11805915379512473  - accuracy: 0.84375\n",
      "At: 2226 [==========>] Loss 0.12464807365022437  - accuracy: 0.8125\n",
      "At: 2227 [==========>] Loss 0.20256542850016945  - accuracy: 0.6875\n",
      "At: 2228 [==========>] Loss 0.09550711926440181  - accuracy: 0.90625\n",
      "At: 2229 [==========>] Loss 0.15308360907277743  - accuracy: 0.78125\n",
      "At: 2230 [==========>] Loss 0.12304181603274021  - accuracy: 0.8125\n",
      "At: 2231 [==========>] Loss 0.14144445884719314  - accuracy: 0.84375\n",
      "At: 2232 [==========>] Loss 0.15426266161020105  - accuracy: 0.78125\n",
      "At: 2233 [==========>] Loss 0.16283251223250533  - accuracy: 0.8125\n",
      "At: 2234 [==========>] Loss 0.15902099940612865  - accuracy: 0.71875\n",
      "At: 2235 [==========>] Loss 0.14005256540101713  - accuracy: 0.8125\n",
      "At: 2236 [==========>] Loss 0.07764727548466682  - accuracy: 0.90625\n",
      "At: 2237 [==========>] Loss 0.11770168603498442  - accuracy: 0.8125\n",
      "At: 2238 [==========>] Loss 0.15209982339870118  - accuracy: 0.78125\n",
      "At: 2239 [==========>] Loss 0.16653489097678156  - accuracy: 0.75\n",
      "At: 2240 [==========>] Loss 0.162398453196198  - accuracy: 0.75\n",
      "At: 2241 [==========>] Loss 0.14011879553696732  - accuracy: 0.75\n",
      "At: 2242 [==========>] Loss 0.16478267390386558  - accuracy: 0.75\n",
      "At: 2243 [==========>] Loss 0.07319850153436022  - accuracy: 0.9375\n",
      "At: 2244 [==========>] Loss 0.1084883571768924  - accuracy: 0.84375\n",
      "At: 2245 [==========>] Loss 0.0787688847787816  - accuracy: 0.875\n",
      "At: 2246 [==========>] Loss 0.1368499383974261  - accuracy: 0.78125\n",
      "At: 2247 [==========>] Loss 0.1126315126069989  - accuracy: 0.8125\n",
      "At: 2248 [==========>] Loss 0.16761490997094014  - accuracy: 0.75\n",
      "At: 2249 [==========>] Loss 0.10371565192754645  - accuracy: 0.84375\n",
      "At: 2250 [==========>] Loss 0.07200808165928119  - accuracy: 0.90625\n",
      "At: 2251 [==========>] Loss 0.07445788562411716  - accuracy: 0.875\n",
      "At: 2252 [==========>] Loss 0.1221396881361377  - accuracy: 0.8125\n",
      "At: 2253 [==========>] Loss 0.10714319949844968  - accuracy: 0.8125\n",
      "At: 2254 [==========>] Loss 0.1195974759035432  - accuracy: 0.8125\n",
      "At: 2255 [==========>] Loss 0.15325957802023632  - accuracy: 0.8125\n",
      "At: 2256 [==========>] Loss 0.14981349645000255  - accuracy: 0.75\n",
      "At: 2257 [==========>] Loss 0.0960960857134906  - accuracy: 0.84375\n",
      "At: 2258 [==========>] Loss 0.13190213136563883  - accuracy: 0.78125\n",
      "At: 2259 [==========>] Loss 0.1423288111127811  - accuracy: 0.8125\n",
      "At: 2260 [==========>] Loss 0.16063046058717045  - accuracy: 0.8125\n",
      "At: 2261 [==========>] Loss 0.11328546080229747  - accuracy: 0.90625\n",
      "At: 2262 [==========>] Loss 0.14288542168250556  - accuracy: 0.84375\n",
      "At: 2263 [==========>] Loss 0.1547663596117828  - accuracy: 0.78125\n",
      "At: 2264 [==========>] Loss 0.0834889347698892  - accuracy: 0.84375\n",
      "At: 2265 [==========>] Loss 0.11132724228644925  - accuracy: 0.84375\n",
      "At: 2266 [==========>] Loss 0.12194648185535148  - accuracy: 0.8125\n",
      "At: 2267 [==========>] Loss 0.059391733177783464  - accuracy: 0.90625\n",
      "At: 2268 [==========>] Loss 0.08875902160230512  - accuracy: 0.90625\n",
      "At: 2269 [==========>] Loss 0.05052196809941991  - accuracy: 0.9375\n",
      "At: 2270 [==========>] Loss 0.09927161317892391  - accuracy: 0.875\n",
      "At: 2271 [==========>] Loss 0.1475441128370282  - accuracy: 0.75\n",
      "At: 2272 [==========>] Loss 0.08608422725849352  - accuracy: 0.84375\n",
      "At: 2273 [==========>] Loss 0.11532141087706199  - accuracy: 0.8125\n",
      "At: 2274 [==========>] Loss 0.09860872008268823  - accuracy: 0.875\n",
      "At: 2275 [==========>] Loss 0.08291681449753231  - accuracy: 0.875\n",
      "At: 2276 [==========>] Loss 0.0947222454627929  - accuracy: 0.875\n",
      "At: 2277 [==========>] Loss 0.16637005178628955  - accuracy: 0.75\n",
      "At: 2278 [==========>] Loss 0.09411943147717847  - accuracy: 0.875\n",
      "At: 2279 [==========>] Loss 0.14559411040798875  - accuracy: 0.78125\n",
      "At: 2280 [==========>] Loss 0.13798716753490758  - accuracy: 0.78125\n",
      "At: 2281 [==========>] Loss 0.13774193024413184  - accuracy: 0.78125\n",
      "At: 2282 [==========>] Loss 0.07920029199817327  - accuracy: 0.9375\n",
      "At: 2283 [==========>] Loss 0.15626556755799753  - accuracy: 0.75\n",
      "At: 2284 [==========>] Loss 0.11684790451994494  - accuracy: 0.8125\n",
      "At: 2285 [==========>] Loss 0.1259537840322409  - accuracy: 0.75\n",
      "At: 2286 [==========>] Loss 0.13293235075823215  - accuracy: 0.8125\n",
      "At: 2287 [==========>] Loss 0.13566978232395477  - accuracy: 0.8125\n",
      "At: 2288 [==========>] Loss 0.07547460619771912  - accuracy: 0.9375\n",
      "At: 2289 [==========>] Loss 0.1455149047162926  - accuracy: 0.78125\n",
      "At: 2290 [==========>] Loss 0.06331038337863579  - accuracy: 0.9375\n",
      "At: 2291 [==========>] Loss 0.11915506501430886  - accuracy: 0.84375\n",
      "At: 2292 [==========>] Loss 0.08277590282673061  - accuracy: 0.90625\n",
      "At: 2293 [==========>] Loss 0.045466214040573535  - accuracy: 0.96875\n",
      "At: 2294 [==========>] Loss 0.053033867257653376  - accuracy: 0.9375\n",
      "At: 2295 [==========>] Loss 0.15194713981799252  - accuracy: 0.75\n",
      "At: 2296 [==========>] Loss 0.14739806656411036  - accuracy: 0.78125\n",
      "At: 2297 [==========>] Loss 0.10112121735404728  - accuracy: 0.875\n",
      "At: 2298 [==========>] Loss 0.10641336118899974  - accuracy: 0.84375\n",
      "At: 2299 [==========>] Loss 0.09606091043861553  - accuracy: 0.8125\n",
      "At: 2300 [==========>] Loss 0.14153663259486646  - accuracy: 0.78125\n",
      "At: 2301 [==========>] Loss 0.21598675925624744  - accuracy: 0.65625\n",
      "At: 2302 [==========>] Loss 0.18655429280211172  - accuracy: 0.71875\n",
      "At: 2303 [==========>] Loss 0.051201058873158525  - accuracy: 0.90625\n",
      "At: 2304 [==========>] Loss 0.11085463487328764  - accuracy: 0.875\n",
      "At: 2305 [==========>] Loss 0.10743081367591543  - accuracy: 0.875\n",
      "At: 2306 [==========>] Loss 0.10979785640580617  - accuracy: 0.84375\n",
      "At: 2307 [==========>] Loss 0.17447959272337948  - accuracy: 0.75\n",
      "At: 2308 [==========>] Loss 0.14568539445490464  - accuracy: 0.78125\n",
      "At: 2309 [==========>] Loss 0.14334261813581742  - accuracy: 0.8125\n",
      "At: 2310 [==========>] Loss 0.13068744399785603  - accuracy: 0.875\n",
      "At: 2311 [==========>] Loss 0.14159860855477496  - accuracy: 0.78125\n",
      "At: 2312 [==========>] Loss 0.10022220360882196  - accuracy: 0.875\n",
      "At: 2313 [==========>] Loss 0.06974520518967979  - accuracy: 0.9375\n",
      "At: 2314 [==========>] Loss 0.11290060585283393  - accuracy: 0.8125\n",
      "At: 2315 [==========>] Loss 0.09574212513135505  - accuracy: 0.8125\n",
      "At: 2316 [==========>] Loss 0.12812696436108645  - accuracy: 0.84375\n",
      "At: 2317 [==========>] Loss 0.1669855375782736  - accuracy: 0.75\n",
      "At: 2318 [==========>] Loss 0.16456138378333063  - accuracy: 0.75\n",
      "At: 2319 [==========>] Loss 0.10582307227369606  - accuracy: 0.84375\n",
      "At: 2320 [==========>] Loss 0.07883324870045806  - accuracy: 0.9375\n",
      "At: 2321 [==========>] Loss 0.1436122997097752  - accuracy: 0.78125\n",
      "At: 2322 [==========>] Loss 0.1842226066795091  - accuracy: 0.71875\n",
      "At: 2323 [==========>] Loss 0.13625869432624588  - accuracy: 0.8125\n",
      "At: 2324 [==========>] Loss 0.15552751521872768  - accuracy: 0.75\n",
      "At: 2325 [==========>] Loss 0.12169955312241096  - accuracy: 0.84375\n",
      "At: 2326 [==========>] Loss 0.09318469059972385  - accuracy: 0.875\n",
      "At: 2327 [==========>] Loss 0.0746688718192155  - accuracy: 0.9375\n",
      "At: 2328 [==========>] Loss 0.1105276998934659  - accuracy: 0.875\n",
      "At: 2329 [==========>] Loss 0.09823276416043294  - accuracy: 0.875\n",
      "At: 2330 [==========>] Loss 0.13075895774009055  - accuracy: 0.78125\n",
      "At: 2331 [==========>] Loss 0.09029240842133054  - accuracy: 0.90625\n",
      "At: 2332 [==========>] Loss 0.11879267030313974  - accuracy: 0.84375\n",
      "At: 2333 [==========>] Loss 0.11973152436955962  - accuracy: 0.875\n",
      "At: 2334 [==========>] Loss 0.15537664732992212  - accuracy: 0.78125\n",
      "At: 2335 [==========>] Loss 0.1010272801487078  - accuracy: 0.8125\n",
      "At: 2336 [==========>] Loss 0.0877116466512937  - accuracy: 0.875\n",
      "At: 2337 [==========>] Loss 0.1212883954497171  - accuracy: 0.875\n",
      "At: 2338 [==========>] Loss 0.11888956705964852  - accuracy: 0.8125\n",
      "At: 2339 [==========>] Loss 0.09047269478959463  - accuracy: 0.875\n",
      "At: 2340 [==========>] Loss 0.15060865780728386  - accuracy: 0.84375\n",
      "At: 2341 [==========>] Loss 0.11652289099955218  - accuracy: 0.84375\n",
      "At: 2342 [==========>] Loss 0.1467610640904155  - accuracy: 0.78125\n",
      "At: 2343 [==========>] Loss 0.115976744253137  - accuracy: 0.78125\n",
      "At: 2344 [==========>] Loss 0.18224258513983851  - accuracy: 0.65625\n",
      "At: 2345 [==========>] Loss 0.14368835529783236  - accuracy: 0.84375\n",
      "At: 2346 [==========>] Loss 0.08561258697284155  - accuracy: 0.9375\n",
      "At: 2347 [==========>] Loss 0.13847166556416693  - accuracy: 0.8125\n",
      "At: 2348 [==========>] Loss 0.06812910607232223  - accuracy: 0.9375\n",
      "At: 2349 [==========>] Loss 0.09183365777898841  - accuracy: 0.84375\n",
      "At: 2350 [==========>] Loss 0.12133165501502503  - accuracy: 0.84375\n",
      "At: 2351 [==========>] Loss 0.0866638358886273  - accuracy: 0.84375\n",
      "At: 2352 [==========>] Loss 0.09906423849738412  - accuracy: 0.875\n",
      "At: 2353 [==========>] Loss 0.08650280031128202  - accuracy: 0.84375\n",
      "At: 2354 [==========>] Loss 0.17730130688235834  - accuracy: 0.71875\n",
      "At: 2355 [==========>] Loss 0.06318233532875397  - accuracy: 0.96875\n",
      "At: 2356 [==========>] Loss 0.12802089512107673  - accuracy: 0.8125\n",
      "At: 2357 [==========>] Loss 0.15729767390529395  - accuracy: 0.78125\n",
      "At: 2358 [==========>] Loss 0.14828214849033725  - accuracy: 0.75\n",
      "At: 2359 [==========>] Loss 0.21630201588148873  - accuracy: 0.65625\n",
      "At: 2360 [==========>] Loss 0.1039319905297933  - accuracy: 0.84375\n",
      "At: 2361 [==========>] Loss 0.16584182266311331  - accuracy: 0.78125\n",
      "At: 2362 [==========>] Loss 0.087842185362281  - accuracy: 0.875\n",
      "At: 2363 [==========>] Loss 0.20778021583705497  - accuracy: 0.71875\n",
      "At: 2364 [==========>] Loss 0.05831514093130526  - accuracy: 0.90625\n",
      "At: 2365 [==========>] Loss 0.08707887771851978  - accuracy: 0.90625\n",
      "At: 2366 [==========>] Loss 0.16874576198589403  - accuracy: 0.71875\n",
      "At: 2367 [==========>] Loss 0.12539365462631824  - accuracy: 0.84375\n",
      "At: 2368 [==========>] Loss 0.08832414455537631  - accuracy: 0.875\n",
      "At: 2369 [==========>] Loss 0.09769842284254537  - accuracy: 0.84375\n",
      "At: 2370 [==========>] Loss 0.08963836593134428  - accuracy: 0.9375\n",
      "At: 2371 [==========>] Loss 0.0937916722978284  - accuracy: 0.875\n",
      "At: 2372 [==========>] Loss 0.15143644378565674  - accuracy: 0.78125\n",
      "At: 2373 [==========>] Loss 0.11828487507757496  - accuracy: 0.84375\n",
      "At: 2374 [==========>] Loss 0.09105448181760423  - accuracy: 0.90625\n",
      "At: 2375 [==========>] Loss 0.05185224554161963  - accuracy: 0.96875\n",
      "At: 2376 [==========>] Loss 0.10500318360977044  - accuracy: 0.84375\n",
      "At: 2377 [==========>] Loss 0.10175219409322273  - accuracy: 0.84375\n",
      "At: 2378 [==========>] Loss 0.11650103794559478  - accuracy: 0.875\n",
      "At: 2379 [==========>] Loss 0.16160584856500188  - accuracy: 0.75\n",
      "At: 2380 [==========>] Loss 0.06071666664113068  - accuracy: 0.90625\n",
      "At: 2381 [==========>] Loss 0.07914328048261751  - accuracy: 0.9375\n",
      "At: 2382 [==========>] Loss 0.09957923142187863  - accuracy: 0.90625\n",
      "At: 2383 [==========>] Loss 0.11806764828260782  - accuracy: 0.8125\n",
      "At: 2384 [==========>] Loss 0.11108585076683071  - accuracy: 0.84375\n",
      "At: 2385 [==========>] Loss 0.11187411870370928  - accuracy: 0.84375\n",
      "At: 2386 [==========>] Loss 0.09650607307927993  - accuracy: 0.84375\n",
      "At: 2387 [==========>] Loss 0.06397616490594135  - accuracy: 0.9375\n",
      "At: 2388 [==========>] Loss 0.12463198372475176  - accuracy: 0.8125\n",
      "At: 2389 [==========>] Loss 0.06617842044138522  - accuracy: 0.90625\n",
      "At: 2390 [==========>] Loss 0.09080616172297587  - accuracy: 0.90625\n",
      "At: 2391 [==========>] Loss 0.14826710935199902  - accuracy: 0.78125\n",
      "At: 2392 [==========>] Loss 0.11881188020909872  - accuracy: 0.8125\n",
      "At: 2393 [==========>] Loss 0.10522023388515576  - accuracy: 0.90625\n",
      "At: 2394 [==========>] Loss 0.06608315510152683  - accuracy: 0.90625\n",
      "At: 2395 [==========>] Loss 0.10158625670806394  - accuracy: 0.84375\n",
      "At: 2396 [==========>] Loss 0.07994152414330268  - accuracy: 0.875\n",
      "At: 2397 [==========>] Loss 0.0990057874211189  - accuracy: 0.875\n",
      "At: 2398 [==========>] Loss 0.0913885356506183  - accuracy: 0.875\n",
      "At: 2399 [==========>] Loss 0.16689673999717888  - accuracy: 0.71875\n",
      "At: 2400 [==========>] Loss 0.09302918817269215  - accuracy: 0.84375\n",
      "At: 2401 [==========>] Loss 0.08183424937764693  - accuracy: 0.90625\n",
      "At: 2402 [==========>] Loss 0.09097570759088487  - accuracy: 0.875\n",
      "At: 2403 [==========>] Loss 0.1953646586359003  - accuracy: 0.78125\n",
      "At: 2404 [==========>] Loss 0.13159697826197767  - accuracy: 0.84375\n",
      "At: 2405 [==========>] Loss 0.06104453612393443  - accuracy: 0.875\n",
      "At: 2406 [==========>] Loss 0.14137417371816904  - accuracy: 0.8125\n",
      "At: 2407 [==========>] Loss 0.12132367708287645  - accuracy: 0.875\n",
      "At: 2408 [==========>] Loss 0.11393925881314744  - accuracy: 0.875\n",
      "At: 2409 [==========>] Loss 0.15958570687797305  - accuracy: 0.78125\n",
      "At: 2410 [==========>] Loss 0.16148426348961709  - accuracy: 0.78125\n",
      "At: 2411 [==========>] Loss 0.09800464365289718  - accuracy: 0.90625\n",
      "At: 2412 [==========>] Loss 0.08220940305246476  - accuracy: 0.875\n",
      "At: 2413 [==========>] Loss 0.08405113987850131  - accuracy: 0.90625\n",
      "At: 2414 [==========>] Loss 0.054899819872964536  - accuracy: 0.96875\n",
      "At: 2415 [==========>] Loss 0.1076184869483959  - accuracy: 0.84375\n",
      "At: 2416 [==========>] Loss 0.09679274006093286  - accuracy: 0.875\n",
      "At: 2417 [==========>] Loss 0.12528662213003944  - accuracy: 0.875\n",
      "At: 2418 [==========>] Loss 0.09453417614620187  - accuracy: 0.875\n",
      "At: 2419 [==========>] Loss 0.1280262324146141  - accuracy: 0.84375\n",
      "At: 2420 [==========>] Loss 0.1450993506761181  - accuracy: 0.75\n",
      "At: 2421 [==========>] Loss 0.0970644147644372  - accuracy: 0.90625\n",
      "At: 2422 [==========>] Loss 0.1389004874635143  - accuracy: 0.78125\n",
      "At: 2423 [==========>] Loss 0.13198663455357645  - accuracy: 0.875\n",
      "At: 2424 [==========>] Loss 0.0920123622120148  - accuracy: 0.875\n",
      "At: 2425 [==========>] Loss 0.07096061493572467  - accuracy: 0.96875\n",
      "At: 2426 [==========>] Loss 0.21065102497215088  - accuracy: 0.65625\n",
      "At: 2427 [==========>] Loss 0.10328320545853367  - accuracy: 0.875\n",
      "At: 2428 [==========>] Loss 0.088721275922735  - accuracy: 0.875\n",
      "At: 2429 [==========>] Loss 0.12833491119410662  - accuracy: 0.8125\n",
      "At: 2430 [==========>] Loss 0.1309390483613569  - accuracy: 0.75\n",
      "At: 2431 [==========>] Loss 0.1310195044985925  - accuracy: 0.8125\n",
      "At: 2432 [==========>] Loss 0.07008914150813807  - accuracy: 0.9375\n",
      "At: 2433 [==========>] Loss 0.06129465185008258  - accuracy: 0.9375\n",
      "At: 2434 [==========>] Loss 0.04233492254188974  - accuracy: 0.96875\n",
      "At: 2435 [==========>] Loss 0.14151851229691711  - accuracy: 0.84375\n",
      "At: 2436 [==========>] Loss 0.10650440035207037  - accuracy: 0.875\n",
      "At: 2437 [==========>] Loss 0.17507973631768664  - accuracy: 0.71875\n",
      "At: 2438 [==========>] Loss 0.10459537160806502  - accuracy: 0.875\n",
      "At: 2439 [==========>] Loss 0.12755575645390815  - accuracy: 0.8125\n",
      "At: 2440 [==========>] Loss 0.12101878823122163  - accuracy: 0.84375\n",
      "At: 2441 [==========>] Loss 0.1037400232886224  - accuracy: 0.84375\n",
      "At: 2442 [==========>] Loss 0.12807274879291955  - accuracy: 0.84375\n",
      "At: 2443 [==========>] Loss 0.11283602393218509  - accuracy: 0.84375\n",
      "At: 2444 [==========>] Loss 0.08726992941842922  - accuracy: 0.90625\n",
      "At: 2445 [==========>] Loss 0.06106015480590192  - accuracy: 0.9375\n",
      "At: 2446 [==========>] Loss 0.15316179067975075  - accuracy: 0.78125\n",
      "At: 2447 [==========>] Loss 0.17003004694869067  - accuracy: 0.71875\n",
      "At: 2448 [==========>] Loss 0.11178582437646295  - accuracy: 0.78125\n",
      "At: 2449 [==========>] Loss 0.05722531818084319  - accuracy: 0.90625\n",
      "At: 2450 [==========>] Loss 0.07288720298654632  - accuracy: 0.9375\n",
      "At: 2451 [==========>] Loss 0.07292920995836805  - accuracy: 0.875\n",
      "At: 2452 [==========>] Loss 0.13680332019081293  - accuracy: 0.8125\n",
      "At: 2453 [==========>] Loss 0.1219161862381497  - accuracy: 0.875\n",
      "At: 2454 [==========>] Loss 0.13231968189752796  - accuracy: 0.8125\n",
      "At: 2455 [==========>] Loss 0.13551315123652358  - accuracy: 0.84375\n",
      "At: 2456 [==========>] Loss 0.1380002682945427  - accuracy: 0.84375\n",
      "At: 2457 [==========>] Loss 0.15642771580485537  - accuracy: 0.75\n",
      "At: 2458 [==========>] Loss 0.09619466421543743  - accuracy: 0.875\n",
      "At: 2459 [==========>] Loss 0.14610750102349462  - accuracy: 0.78125\n",
      "At: 2460 [==========>] Loss 0.06377511613926026  - accuracy: 0.875\n",
      "At: 2461 [==========>] Loss 0.06448313711115647  - accuracy: 0.9375\n",
      "At: 2462 [==========>] Loss 0.1581131771845245  - accuracy: 0.78125\n",
      "At: 2463 [==========>] Loss 0.11594053285142668  - accuracy: 0.78125\n",
      "At: 2464 [==========>] Loss 0.1388631431974679  - accuracy: 0.75\n",
      "At: 2465 [==========>] Loss 0.12381674227216222  - accuracy: 0.8125\n",
      "At: 2466 [==========>] Loss 0.08773674364634768  - accuracy: 0.90625\n",
      "At: 2467 [==========>] Loss 0.11201823276181426  - accuracy: 0.90625\n",
      "At: 2468 [==========>] Loss 0.06626586223346775  - accuracy: 0.90625\n",
      "At: 2469 [==========>] Loss 0.1338572617722264  - accuracy: 0.78125\n",
      "At: 2470 [==========>] Loss 0.10898207851819983  - accuracy: 0.875\n",
      "At: 2471 [==========>] Loss 0.10305114886837782  - accuracy: 0.875\n",
      "At: 2472 [==========>] Loss 0.10884637638677849  - accuracy: 0.84375\n",
      "At: 2473 [==========>] Loss 0.11150037765993724  - accuracy: 0.84375\n",
      "At: 2474 [==========>] Loss 0.07199926741061011  - accuracy: 0.875\n",
      "At: 2475 [==========>] Loss 0.13258376034027586  - accuracy: 0.75\n",
      "At: 2476 [==========>] Loss 0.06755704326141684  - accuracy: 0.9375\n",
      "At: 2477 [==========>] Loss 0.13313702427856494  - accuracy: 0.8125\n",
      "At: 2478 [==========>] Loss 0.10937878472350693  - accuracy: 0.84375\n",
      "At: 2479 [==========>] Loss 0.09983732740097367  - accuracy: 0.875\n",
      "At: 2480 [==========>] Loss 0.11861030827515534  - accuracy: 0.8125\n",
      "At: 2481 [==========>] Loss 0.05536603812126749  - accuracy: 0.9375\n",
      "At: 2482 [==========>] Loss 0.15495016086085725  - accuracy: 0.78125\n",
      "At: 2483 [==========>] Loss 0.10959480082667683  - accuracy: 0.8125\n",
      "At: 2484 [==========>] Loss 0.07913254336197523  - accuracy: 0.875\n",
      "At: 2485 [==========>] Loss 0.11428737446621548  - accuracy: 0.84375\n",
      "At: 2486 [==========>] Loss 0.12244540832346752  - accuracy: 0.90625\n",
      "At: 2487 [==========>] Loss 0.11896223211301704  - accuracy: 0.875\n",
      "At: 2488 [==========>] Loss 0.18708470301863683  - accuracy: 0.75\n",
      "At: 2489 [==========>] Loss 0.19547050380687214  - accuracy: 0.6875\n",
      "At: 2490 [==========>] Loss 0.09567128282161882  - accuracy: 0.875\n",
      "At: 2491 [==========>] Loss 0.14558351991086463  - accuracy: 0.78125\n",
      "At: 2492 [==========>] Loss 0.13166945059806257  - accuracy: 0.75\n",
      "At: 2493 [==========>] Loss 0.08071310850478426  - accuracy: 0.90625\n",
      "At: 2494 [==========>] Loss 0.08583071849967716  - accuracy: 0.875\n",
      "At: 2495 [==========>] Loss 0.10969433718086422  - accuracy: 0.875\n",
      "At: 2496 [==========>] Loss 0.05668813191657729  - accuracy: 0.9375\n",
      "At: 2497 [==========>] Loss 0.21099181155668345  - accuracy: 0.6875\n",
      "At: 2498 [==========>] Loss 0.1234824993227229  - accuracy: 0.875\n",
      "At: 2499 [==========>] Loss 0.05540224534543363  - accuracy: 0.96875\n",
      "At: 2500 [==========>] Loss 0.16884407361019232  - accuracy: 0.6875\n",
      "At: 2501 [==========>] Loss 0.1453996368323503  - accuracy: 0.75\n",
      "At: 2502 [==========>] Loss 0.09827504075295007  - accuracy: 0.90625\n",
      "At: 2503 [==========>] Loss 0.1221884137630164  - accuracy: 0.8125\n",
      "At: 2504 [==========>] Loss 0.15420065445377923  - accuracy: 0.78125\n",
      "At: 2505 [==========>] Loss 0.07777607148160118  - accuracy: 0.9375\n",
      "At: 2506 [==========>] Loss 0.0943954263215461  - accuracy: 0.90625\n",
      "At: 2507 [==========>] Loss 0.11893743140784202  - accuracy: 0.75\n",
      "At: 2508 [==========>] Loss 0.11813520852177858  - accuracy: 0.8125\n",
      "At: 2509 [==========>] Loss 0.14386168146648853  - accuracy: 0.8125\n",
      "At: 2510 [==========>] Loss 0.11822268000678594  - accuracy: 0.8125\n",
      "At: 2511 [==========>] Loss 0.14673903171810387  - accuracy: 0.84375\n",
      "At: 2512 [==========>] Loss 0.08792510517109045  - accuracy: 0.875\n",
      "At: 2513 [==========>] Loss 0.1550950733117009  - accuracy: 0.78125\n",
      "At: 2514 [==========>] Loss 0.14012990315318258  - accuracy: 0.78125\n",
      "At: 2515 [==========>] Loss 0.20607929698636407  - accuracy: 0.75\n",
      "At: 2516 [==========>] Loss 0.1807670939876626  - accuracy: 0.78125\n",
      "At: 2517 [==========>] Loss 0.12359091305912379  - accuracy: 0.8125\n",
      "At: 2518 [==========>] Loss 0.13257579804340272  - accuracy: 0.84375\n",
      "At: 2519 [==========>] Loss 0.09678271495143832  - accuracy: 0.8125\n",
      "At: 2520 [==========>] Loss 0.14411886067500415  - accuracy: 0.8125\n",
      "At: 2521 [==========>] Loss 0.09985794593005819  - accuracy: 0.84375\n",
      "At: 2522 [==========>] Loss 0.22185344248626998  - accuracy: 0.71875\n",
      "At: 2523 [==========>] Loss 0.12913291147754302  - accuracy: 0.8125\n",
      "At: 2524 [==========>] Loss 0.16241756748526773  - accuracy: 0.78125\n",
      "At: 2525 [==========>] Loss 0.07354453372920271  - accuracy: 0.9375\n",
      "At: 2526 [==========>] Loss 0.12824629252316905  - accuracy: 0.84375\n",
      "At: 2527 [==========>] Loss 0.12658823323739934  - accuracy: 0.8125\n",
      "At: 2528 [==========>] Loss 0.07432750747304515  - accuracy: 0.875\n",
      "At: 2529 [==========>] Loss 0.15038887686624627  - accuracy: 0.78125\n",
      "At: 2530 [==========>] Loss 0.14001639306030483  - accuracy: 0.78125\n",
      "At: 2531 [==========>] Loss 0.047304283851572655  - accuracy: 1.0\n",
      "At: 2532 [==========>] Loss 0.11683852276817422  - accuracy: 0.8125\n",
      "At: 2533 [==========>] Loss 0.09686453177703842  - accuracy: 0.90625\n",
      "At: 2534 [==========>] Loss 0.060016638743106795  - accuracy: 0.9375\n",
      "At: 2535 [==========>] Loss 0.06115785105489805  - accuracy: 0.9375\n",
      "At: 2536 [==========>] Loss 0.15520608015264603  - accuracy: 0.75\n",
      "At: 2537 [==========>] Loss 0.08696509250765493  - accuracy: 0.90625\n",
      "At: 2538 [==========>] Loss 0.15218735134947048  - accuracy: 0.78125\n",
      "At: 2539 [==========>] Loss 0.07744568918268707  - accuracy: 0.9375\n",
      "At: 2540 [==========>] Loss 0.142638917480476  - accuracy: 0.78125\n",
      "At: 2541 [==========>] Loss 0.08192217590123527  - accuracy: 0.875\n",
      "At: 2542 [==========>] Loss 0.06948443403369699  - accuracy: 0.90625\n",
      "At: 2543 [==========>] Loss 0.1480411666565608  - accuracy: 0.8125\n",
      "At: 2544 [==========>] Loss 0.13665169842076497  - accuracy: 0.84375\n",
      "At: 2545 [==========>] Loss 0.07123666125507762  - accuracy: 0.9375\n",
      "At: 2546 [==========>] Loss 0.14134146848867746  - accuracy: 0.8125\n",
      "At: 2547 [==========>] Loss 0.12894495279338686  - accuracy: 0.8125\n",
      "At: 2548 [==========>] Loss 0.0758073438805474  - accuracy: 0.90625\n",
      "At: 2549 [==========>] Loss 0.09311908602097377  - accuracy: 0.875\n",
      "At: 2550 [==========>] Loss 0.10845461016865368  - accuracy: 0.78125\n",
      "At: 2551 [==========>] Loss 0.12589586983080264  - accuracy: 0.875\n",
      "At: 2552 [==========>] Loss 0.1047760430175651  - accuracy: 0.84375\n",
      "At: 2553 [==========>] Loss 0.08738005812496338  - accuracy: 0.875\n",
      "At: 2554 [==========>] Loss 0.1523549208672786  - accuracy: 0.71875\n",
      "At: 2555 [==========>] Loss 0.1686681151355288  - accuracy: 0.6875\n",
      "At: 2556 [==========>] Loss 0.09580543145843663  - accuracy: 0.90625\n",
      "At: 2557 [==========>] Loss 0.1353102985736513  - accuracy: 0.75\n",
      "At: 2558 [==========>] Loss 0.0901259053767273  - accuracy: 0.84375\n",
      "At: 2559 [==========>] Loss 0.10900959824240011  - accuracy: 0.84375\n",
      "At: 2560 [==========>] Loss 0.20795812019380683  - accuracy: 0.6875\n",
      "At: 2561 [==========>] Loss 0.08196800185901443  - accuracy: 0.84375\n",
      "At: 2562 [==========>] Loss 0.10548062298895981  - accuracy: 0.84375\n",
      "At: 2563 [==========>] Loss 0.12718442993959136  - accuracy: 0.8125\n",
      "At: 2564 [==========>] Loss 0.10302991893143557  - accuracy: 0.84375\n",
      "At: 2565 [==========>] Loss 0.13339696438515602  - accuracy: 0.8125\n",
      "At: 2566 [==========>] Loss 0.1535745414360476  - accuracy: 0.8125\n",
      "At: 2567 [==========>] Loss 0.12653609299148005  - accuracy: 0.8125\n",
      "At: 2568 [==========>] Loss 0.12189556181889645  - accuracy: 0.84375\n",
      "At: 2569 [==========>] Loss 0.08003308758684811  - accuracy: 0.90625\n",
      "At: 2570 [==========>] Loss 0.1474060358617012  - accuracy: 0.84375\n",
      "At: 2571 [==========>] Loss 0.09080535493476294  - accuracy: 0.84375\n",
      "At: 2572 [==========>] Loss 0.16526339877710575  - accuracy: 0.78125\n",
      "At: 2573 [==========>] Loss 0.1839911065102775  - accuracy: 0.71875\n",
      "At: 2574 [==========>] Loss 0.11077438738346405  - accuracy: 0.84375\n",
      "At: 2575 [==========>] Loss 0.05695312708931845  - accuracy: 0.9375\n",
      "At: 2576 [==========>] Loss 0.1267975012137857  - accuracy: 0.78125\n",
      "At: 2577 [==========>] Loss 0.2169061018749836  - accuracy: 0.75\n",
      "At: 2578 [==========>] Loss 0.16596983900954315  - accuracy: 0.75\n",
      "At: 2579 [==========>] Loss 0.16910078180052185  - accuracy: 0.78125\n",
      "At: 2580 [==========>] Loss 0.14577820664734767  - accuracy: 0.71875\n",
      "At: 2581 [==========>] Loss 0.07155155329838395  - accuracy: 0.9375\n",
      "At: 2582 [==========>] Loss 0.1941668785962392  - accuracy: 0.71875\n",
      "At: 2583 [==========>] Loss 0.06449052283680626  - accuracy: 0.9375\n",
      "At: 2584 [==========>] Loss 0.2158566405715112  - accuracy: 0.65625\n",
      "At: 2585 [==========>] Loss 0.09124410069637441  - accuracy: 0.875\n",
      "At: 2586 [==========>] Loss 0.07593957196671372  - accuracy: 0.875\n",
      "At: 2587 [==========>] Loss 0.15890320670550834  - accuracy: 0.71875\n",
      "At: 2588 [==========>] Loss 0.11470666104987569  - accuracy: 0.8125\n",
      "At: 2589 [==========>] Loss 0.09497409880045327  - accuracy: 0.875\n",
      "At: 2590 [==========>] Loss 0.20154800211003743  - accuracy: 0.6875\n",
      "At: 2591 [==========>] Loss 0.12601671399911363  - accuracy: 0.84375\n",
      "At: 2592 [==========>] Loss 0.10477530851128017  - accuracy: 0.78125\n",
      "At: 2593 [==========>] Loss 0.07221249739989548  - accuracy: 0.90625\n",
      "At: 2594 [==========>] Loss 0.07698984870331141  - accuracy: 0.90625\n",
      "At: 2595 [==========>] Loss 0.0771815438292798  - accuracy: 0.90625\n",
      "At: 2596 [==========>] Loss 0.13499563364029996  - accuracy: 0.8125\n",
      "At: 2597 [==========>] Loss 0.09007284875954065  - accuracy: 0.90625\n",
      "At: 2598 [==========>] Loss 0.11376133809846169  - accuracy: 0.90625\n",
      "At: 2599 [==========>] Loss 0.14187147487034768  - accuracy: 0.78125\n",
      "At: 2600 [==========>] Loss 0.14207454814335638  - accuracy: 0.78125\n",
      "At: 2601 [==========>] Loss 0.11775756951740622  - accuracy: 0.84375\n",
      "At: 2602 [==========>] Loss 0.07362338411261228  - accuracy: 0.90625\n",
      "At: 2603 [==========>] Loss 0.13643406539512895  - accuracy: 0.8125\n",
      "At: 2604 [==========>] Loss 0.10949071268854985  - accuracy: 0.84375\n",
      "At: 2605 [==========>] Loss 0.1438500178991378  - accuracy: 0.8125\n",
      "At: 2606 [==========>] Loss 0.09936421204053333  - accuracy: 0.875\n",
      "At: 2607 [==========>] Loss 0.13373580503387605  - accuracy: 0.875\n",
      "At: 2608 [==========>] Loss 0.06774968991440139  - accuracy: 0.9375\n",
      "At: 2609 [==========>] Loss 0.08576695597724575  - accuracy: 0.90625\n",
      "At: 2610 [==========>] Loss 0.1470241148672662  - accuracy: 0.6875\n",
      "At: 2611 [==========>] Loss 0.14800917819783152  - accuracy: 0.78125\n",
      "At: 2612 [==========>] Loss 0.07756763965934281  - accuracy: 0.90625\n",
      "At: 2613 [==========>] Loss 0.10673060090621089  - accuracy: 0.875\n",
      "At: 2614 [==========>] Loss 0.09878854420613876  - accuracy: 0.875\n",
      "At: 2615 [==========>] Loss 0.06405796602966395  - accuracy: 0.90625\n",
      "At: 2616 [==========>] Loss 0.06158844588236963  - accuracy: 0.9375\n",
      "At: 2617 [==========>] Loss 0.1077214208859521  - accuracy: 0.8125\n",
      "At: 2618 [==========>] Loss 0.07464640354588158  - accuracy: 0.875\n",
      "At: 2619 [==========>] Loss 0.09513960327212995  - accuracy: 0.875\n",
      "At: 2620 [==========>] Loss 0.12476954955560346  - accuracy: 0.8125\n",
      "At: 2621 [==========>] Loss 0.10613003625428116  - accuracy: 0.84375\n",
      "At: 2622 [==========>] Loss 0.10095093720696632  - accuracy: 0.8125\n",
      "At: 2623 [==========>] Loss 0.08086402236696377  - accuracy: 0.90625\n",
      "At: 2624 [==========>] Loss 0.13861199410453598  - accuracy: 0.8125\n",
      "At: 2625 [==========>] Loss 0.0674282226339539  - accuracy: 0.9375\n",
      "At: 2626 [==========>] Loss 0.0627025446385868  - accuracy: 0.9375\n",
      "At: 2627 [==========>] Loss 0.13660341217147376  - accuracy: 0.78125\n",
      "At: 2628 [==========>] Loss 0.08666117599212297  - accuracy: 0.90625\n",
      "At: 2629 [==========>] Loss 0.09869773309370006  - accuracy: 0.875\n",
      "At: 2630 [==========>] Loss 0.10175963361992077  - accuracy: 0.875\n",
      "At: 2631 [==========>] Loss 0.1257150625216314  - accuracy: 0.8125\n",
      "At: 2632 [==========>] Loss 0.13503991858858838  - accuracy: 0.78125\n",
      "At: 2633 [==========>] Loss 0.09325227976018342  - accuracy: 0.90625\n",
      "At: 2634 [==========>] Loss 0.07109378304557377  - accuracy: 0.875\n",
      "At: 2635 [==========>] Loss 0.2168472552048934  - accuracy: 0.6875\n",
      "At: 2636 [==========>] Loss 0.12997540769473712  - accuracy: 0.78125\n",
      "At: 2637 [==========>] Loss 0.13422603646011133  - accuracy: 0.78125\n",
      "At: 2638 [==========>] Loss 0.09111255120175153  - accuracy: 0.90625\n",
      "At: 2639 [==========>] Loss 0.08983236538067337  - accuracy: 0.84375\n",
      "At: 2640 [==========>] Loss 0.15385518767530207  - accuracy: 0.78125\n",
      "At: 2641 [==========>] Loss 0.1443728040716177  - accuracy: 0.75\n",
      "At: 2642 [==========>] Loss 0.1639542296166358  - accuracy: 0.71875\n",
      "At: 2643 [==========>] Loss 0.06245167804999592  - accuracy: 0.875\n",
      "At: 2644 [==========>] Loss 0.14096323677181383  - accuracy: 0.78125\n",
      "At: 2645 [==========>] Loss 0.08281526259086966  - accuracy: 0.875\n",
      "At: 2646 [==========>] Loss 0.16074291686240133  - accuracy: 0.78125\n",
      "At: 2647 [==========>] Loss 0.12943191945442134  - accuracy: 0.8125\n",
      "At: 2648 [==========>] Loss 0.15280227919198808  - accuracy: 0.75\n",
      "At: 2649 [==========>] Loss 0.11650287739458298  - accuracy: 0.84375\n",
      "At: 2650 [==========>] Loss 0.10763548677873848  - accuracy: 0.875\n",
      "At: 2651 [==========>] Loss 0.1727381432708962  - accuracy: 0.75\n",
      "At: 2652 [==========>] Loss 0.11507900421465866  - accuracy: 0.875\n",
      "At: 2653 [==========>] Loss 0.12479686395466581  - accuracy: 0.84375\n",
      "At: 2654 [==========>] Loss 0.12656656394984675  - accuracy: 0.75\n",
      "At: 2655 [==========>] Loss 0.21457089276817015  - accuracy: 0.6875\n",
      "At: 2656 [==========>] Loss 0.021679602306394714  - accuracy: 1.0\n",
      "At: 2657 [==========>] Loss 0.1051768444248172  - accuracy: 0.84375\n",
      "At: 2658 [==========>] Loss 0.07407891845855832  - accuracy: 0.9375\n",
      "At: 2659 [==========>] Loss 0.07043395942174027  - accuracy: 0.875\n",
      "At: 2660 [==========>] Loss 0.07366297611194386  - accuracy: 0.90625\n",
      "At: 2661 [==========>] Loss 0.09257644872719688  - accuracy: 0.875\n",
      "At: 2662 [==========>] Loss 0.05112596004065685  - accuracy: 0.96875\n",
      "At: 2663 [==========>] Loss 0.0955824686660936  - accuracy: 0.875\n",
      "At: 2664 [==========>] Loss 0.0764719150821632  - accuracy: 0.9375\n",
      "At: 2665 [==========>] Loss 0.09168765090559805  - accuracy: 0.9375\n",
      "At: 2666 [==========>] Loss 0.1432887769877012  - accuracy: 0.8125\n",
      "At: 2667 [==========>] Loss 0.1409237180073123  - accuracy: 0.78125\n",
      "At: 2668 [==========>] Loss 0.15632940493738987  - accuracy: 0.71875\n",
      "At: 2669 [==========>] Loss 0.14222730010562615  - accuracy: 0.8125\n",
      "At: 2670 [==========>] Loss 0.077690019413228  - accuracy: 0.875\n",
      "At: 2671 [==========>] Loss 0.06321159691833952  - accuracy: 0.9375\n",
      "At: 2672 [==========>] Loss 0.10021065011028091  - accuracy: 0.84375\n",
      "At: 2673 [==========>] Loss 0.12482030806318264  - accuracy: 0.84375\n",
      "At: 2674 [==========>] Loss 0.10802757950871911  - accuracy: 0.84375\n",
      "At: 2675 [==========>] Loss 0.10427988565409807  - accuracy: 0.84375\n",
      "At: 2676 [==========>] Loss 0.1365060105961069  - accuracy: 0.78125\n",
      "At: 2677 [==========>] Loss 0.09942696290017794  - accuracy: 0.875\n",
      "At: 2678 [==========>] Loss 0.053437658260884105  - accuracy: 0.9375\n",
      "At: 2679 [==========>] Loss 0.07098876688112599  - accuracy: 0.90625\n",
      "At: 2680 [==========>] Loss 0.09532239091926444  - accuracy: 0.875\n",
      "At: 2681 [==========>] Loss 0.10288447662290136  - accuracy: 0.90625\n",
      "At: 2682 [==========>] Loss 0.1348327357044824  - accuracy: 0.78125\n",
      "At: 2683 [==========>] Loss 0.18587722839616355  - accuracy: 0.78125\n",
      "At: 2684 [==========>] Loss 0.07887733414438719  - accuracy: 0.90625\n",
      "At: 2685 [==========>] Loss 0.10766329702510079  - accuracy: 0.875\n",
      "At: 2686 [==========>] Loss 0.077854222544705  - accuracy: 0.90625\n",
      "At: 2687 [==========>] Loss 0.11640475767652492  - accuracy: 0.90625\n",
      "At: 2688 [==========>] Loss 0.15115437045203253  - accuracy: 0.8125\n",
      "At: 2689 [==========>] Loss 0.10917502468471244  - accuracy: 0.84375\n",
      "At: 2690 [==========>] Loss 0.15091343068234733  - accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import model.nn as nn\n",
    "import numpy as np\n",
    "from model.optimizer import optimizer as optimizer\n",
    "\n",
    "X = np.array(X_train).T\n",
    "Y = np.array([y_train])\n",
    "\n",
    "# optimize using  ADAM\n",
    "net = nn.nn([26, 32, 32, 1], ['relu', 'relu', 'sigmoid'], epochs=10)\n",
    "net.cost_function = 'mseloss'\n",
    "print('net architecture :')\n",
    "print(net)\n",
    "\n",
    "optim = optimizer.AdamOptimizer\n",
    "optim(X, Y, net, alpha=0.00009, lamb=0.05, print_at=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Adam:\n",
      " accuracy =  78.79308742915543\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(X_test).T\n",
    "prediction = net.forward(X_test)\n",
    "y_actual = np.array([y_test])\n",
    "\n",
    "prediction = 1 * (prediction >= 0.5)\n",
    "accuracy = np.sum(prediction == y_actual[0]) / prediction.shape[1]\n",
    "\n",
    "print('for Adam:\\n accuracy = ', accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21526"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = net.forward(X_test)\n",
    "result = pd.DataFrame({'id': [*range(1, prediction[0].size + 1)], 'prediction': prediction[0], 'actual': y_actual[0]})\n",
    "result.to_sql(con=database_connection, name='result', if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "76374d8ec0b744e304480311c81ddcb6252bf52deee285d3cfafd7434c325fc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
